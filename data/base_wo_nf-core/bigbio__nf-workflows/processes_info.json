{
    "createDecoyDb": {
        "name_process": "createDecoyDb",
        "string_process": "\nprocess createDecoyDb {\n\tcontainer 'biocontainers/searchgui:v2.8.6_cv2'\n\n\tinput:\n\tfile \"db.fasta\" from fasta_file\n\n\toutput:\n\tfile \"db_concatenated_target_decoy.fasta\" into fasta_decoy_db\n\n\tscript:\n\t\"\"\"\n\tjava -cp /home/biodocker/bin/SearchGUI-2.8.6/SearchGUI-2.8.6.jar eu.isas.searchgui.cmd.FastaCLI -decoy -in db.fasta\n\t\"\"\"\n}",
        "nb_lignes_process": 13,
        "string_script": "\t\"\"\"\n\tjava -cp /home/biodocker/bin/SearchGUI-2.8.6/SearchGUI-2.8.6.jar eu.isas.searchgui.cmd.FastaCLI -decoy -in db.fasta\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fasta_file"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fasta_decoy_db"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'biocontainers/searchgui:v2.8.6_cv2'"
        ],
        "when": "",
        "stub": ""
    },
    "createMsgfDbIndex": {
        "name_process": "createMsgfDbIndex",
        "string_process": "\nprocess createMsgfDbIndex {\n\tcontainer 'biocontainers/msgfp:v9949_cv3'\n\t                                                            \n\tvalidExitStatus 0,1\n\t\n\tinput:\n\tfile \"user.fasta\" from fasta_file\n\n\toutput:\n\tfile \"user.revCat*\" into msgf_fasta_index\n\n\tscript:\n\t\"\"\"\n\ttouch /tmp/test.mgf\n\tjava -jar /home/biodocker/bin/MSGFPlus_9949/MSGFPlus.jar -s /tmp/test.mgf -d user.fasta -tda 1\n\t\"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "\t\"\"\"\n\ttouch /tmp/test.mgf\n\tjava -jar /home/biodocker/bin/MSGFPlus_9949/MSGFPlus.jar -s /tmp/test.mgf -d user.fasta -tda 1\n\t\"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fasta_file"
        ],
        "nb_inputs": 1,
        "outputs": [
            "msgf_fasta_index"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'biocontainers/msgfp:v9949_cv3'",
            "validExitStatus 0,1"
        ],
        "when": "",
        "stub": ""
    },
    "searchMsgf": {
        "name_process": "searchMsgf",
        "string_process": "\nprocess searchMsgf {\n\tcontainer 'biocontainers/msgfp:v9949_cv3'\n\n\tinput:\n\tfile \"user.fasta\" from fasta_file\n\tfile msgf_fasta_index\n\tfile mgf_file_msgf from mgf_files_msgf\n\tfile msgf_mods\n\n\toutput:\n\tfile \"*.mzid\" into msgf_result\n\t\n\tscript:\n\t\"\"\"\n\tjava -jar /home/biodocker/bin/MSGFPlus_9949/MSGFPlus.jar \\\n\t-d user.fasta -s ${mgf_file_msgf} -t ${params.prec_tol}ppm -ti 0,1 -thread ${threads} \\\n\t-tda 1 -inst 3 -e 1 -ntt ${params.mc} -mod ${msgf_mods} -minCharge 2 -maxCharge 4 \\\n\t-addFeatures 1\n\t\"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "\t\"\"\"\n\tjava -jar /home/biodocker/bin/MSGFPlus_9949/MSGFPlus.jar \\\n\t-d user.fasta -s ${mgf_file_msgf} -t ${params.prec_tol}ppm -ti 0,1 -thread ${threads} \\\n\t-tda 1 -inst 3 -e 1 -ntt ${params.mc} -mod ${msgf_mods} -minCharge 2 -maxCharge 4 \\\n\t-addFeatures 1\n\t\"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fasta_file",
            "msgf_fasta_index",
            "mgf_files_msgf",
            "msgf_mods"
        ],
        "nb_inputs": 4,
        "outputs": [
            "msgf_result"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'biocontainers/msgfp:v9949_cv3'"
        ],
        "when": "",
        "stub": ""
    },
    "createTideIndex": {
        "name_process": "createTideIndex",
        "string_process": "\nprocess createTideIndex {\n\tmemory { 4.GB }\n\n\tinput:\n\tfile fasta_decoy_db\n\tfile tide_config_file\n\n\toutput:\n\tfile \"fasta-index/*\" into fasta_tide_index\n\n\tscript:\n\t\"\"\"\n\tcrux tide-index --parameter-file \"${tide_config_file}\" \"${fasta_decoy_db}\" fasta-index\n\t\"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "\t\"\"\"\n\tcrux tide-index --parameter-file \"${tide_config_file}\" \"${fasta_decoy_db}\" fasta-index\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "Crux"
        ],
        "tools_url": [
            "https://bio.tools/crux"
        ],
        "tools_dico": [
            {
                "name": "Crux",
                "uri": "https://bio.tools/crux",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3214",
                                    "term": "Spectral analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3214",
                                    "term": "Spectrum analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3214",
                                    "term": "Mass spectrum analysis"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2536",
                                "term": "Mass spectrometry data"
                            }
                        ],
                        "output": []
                    }
                ],
                "description": "Cross-platform suite of analysis tools for interpreting protein mass spectrometry data.",
                "homepage": "http://crux.ms/"
            }
        ],
        "inputs": [
            "fasta_decoy_db",
            "tide_config_file"
        ],
        "nb_inputs": 2,
        "outputs": [
            "fasta_tide_index"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "memory { 4.GB }"
        ],
        "when": "",
        "stub": ""
    },
    "searchTide": {
        "name_process": "searchTide",
        "string_process": "\nprocess searchTide {\n\t\n\tmemory { 4.GB }\n\tpublishDir \"result\"\n\n\tinput:\n\tfile fasta_tide_index\n\tfile mgf_file from mgf_files_2\n\n\toutput:\n\tfile \"*.txt\" into tide_result, all_tide_result\n\n\tscript:\n\t\"\"\"\n\tmkdir fasta-index\n\tmv ${fasta_tide_index} fasta-index/\n\tcrux tide-search --parameter-file \"${tide_config_file}\" \"${mgf_file}\" fasta-index\n\n\t# move and rename the result file\n\tmv crux-output/tide-search.txt ${mgf_file}.tide.txt\n\t\"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "\t\"\"\"\n\tmkdir fasta-index\n\tmv ${fasta_tide_index} fasta-index/\n\tcrux tide-search --parameter-file \"${tide_config_file}\" \"${mgf_file}\" fasta-index\n\n\t# move and rename the result file\n\tmv crux-output/tide-search.txt ${mgf_file}.tide.txt\n\t\"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [
            "Crux"
        ],
        "tools_url": [
            "https://bio.tools/crux"
        ],
        "tools_dico": [
            {
                "name": "Crux",
                "uri": "https://bio.tools/crux",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3214",
                                    "term": "Spectral analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3214",
                                    "term": "Spectrum analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3214",
                                    "term": "Mass spectrum analysis"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2536",
                                "term": "Mass spectrometry data"
                            }
                        ],
                        "output": []
                    }
                ],
                "description": "Cross-platform suite of analysis tools for interpreting protein mass spectrometry data.",
                "homepage": "http://crux.ms/"
            }
        ],
        "inputs": [
            "fasta_tide_index",
            "mgf_files_2"
        ],
        "nb_inputs": 2,
        "outputs": [
            "tide_result",
            "all_tide_result"
        ],
        "nb_outputs": 2,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "memory { 4.GB }",
            "publishDir \"result\""
        ],
        "when": "",
        "stub": ""
    },
    "mergeSearchResults": {
        "name_process": "mergeSearchResults",
        "string_process": "\nprocess mergeSearchResults {\n\n\tpublishDir \"result\"\n\tpublishDir \"result\", mode: 'copy', overwrite: true\n\n\tmemory { 16.GB * task.attempt }\n    errorStrategy 'retry'\n\n\tinput:\n\tset val(mgf_name), file(tide_txt), file(msgf_mzid) from combined_results\n\n\toutput:\n\tfile \"${mgf_name}.xml\" into pia_compilation\n\n\tscript:\n\t\"\"\"\n\tpia -Xmx${10 * task.attempt}g compiler -infile ${tide_txt} -infile ${msgf_mzid} -name \"${mgf_name}\" -outfile ${mgf_name}.xml\n\t\"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\t\"\"\"\n\tpia -Xmx${10 * task.attempt}g compiler -infile ${tide_txt} -infile ${msgf_mzid} -name \"${mgf_name}\" -outfile ${mgf_name}.xml\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "SPIA"
        ],
        "tools_url": [
            "https://bio.tools/spia"
        ],
        "tools_dico": [
            {
                "name": "SPIA",
                "uri": "https://bio.tools/spia",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarray experiment"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0602",
                            "term": "Molecular interactions, pathways and networks"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarrays"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene expression profiling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2497",
                                    "term": "Pathway or network analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differentially expressed gene identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential expression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene expression analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package implements the Signaling Pathway Impact Analysis which uses the information form a list of differentially expressed genes and their log fold changes together with signaling pathways topology, in order to identify the pathways most relevant to the condition under the study.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/SPIA.html"
            }
        ],
        "inputs": [
            "combined_results"
        ],
        "nb_inputs": 1,
        "outputs": [
            "pia_compilation"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "publishDir \"result\"",
            "publishDir \"result\", mode: 'copy', overwrite: true",
            "memory { 16.GB * task.attempt }",
            "errorStrategy 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "filterPiaResults": {
        "name_process": "filterPiaResults",
        "string_process": "\nprocess filterPiaResults {\n\tcontainer 'ypriverol/pia:1.3.10'\n\tpublishDir \"result\", mode: 'copy', overwrite: true\n\n\tmemory { 16.GB * task.attempt }\n    errorStrategy 'retry'\n\n\tinput:\n\tfile pia_xml from pia_compilation\n\tfile pia_config\n\n\toutput:\n\tfile \"*.mztab\" into final_result\n\n\tscript:\n\t\"\"\"\n\tpia -Xmx${10 * task.attempt}g inference -infile ${pia_xml} -paramFile ${pia_config} -psmExport ${pia_xml}.mztab mzTab\n\t\"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\t\"\"\"\n\tpia -Xmx${10 * task.attempt}g inference -infile ${pia_xml} -paramFile ${pia_config} -psmExport ${pia_xml}.mztab mzTab\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "SPIA"
        ],
        "tools_url": [
            "https://bio.tools/spia"
        ],
        "tools_dico": [
            {
                "name": "SPIA",
                "uri": "https://bio.tools/spia",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarray experiment"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0602",
                            "term": "Molecular interactions, pathways and networks"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarrays"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene expression profiling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2497",
                                    "term": "Pathway or network analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differentially expressed gene identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential expression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene expression analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package implements the Signaling Pathway Impact Analysis which uses the information form a list of differentially expressed genes and their log fold changes together with signaling pathways topology, in order to identify the pathways most relevant to the condition under the study.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/SPIA.html"
            }
        ],
        "inputs": [
            "pia_compilation",
            "pia_config"
        ],
        "nb_inputs": 2,
        "outputs": [
            "final_result"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'ypriverol/pia:1.3.10'",
            "publishDir \"result\", mode: 'copy', overwrite: true",
            "memory { 16.GB * task.attempt }",
            "errorStrategy 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "mergeCompleteSearchResults": {
        "name_process": "mergeCompleteSearchResults",
        "string_process": " process mergeCompleteSearchResults {\n \t\n \tpublishDir \"result\"\n\n \tmemory { 16.GB * task.attempt }\n    errorStrategy 'retry'\n\n \tinput:\n \tfile(input_files) from all_search_files.collect()\n\n \toutput:\n \tfile \"*.xml\" into merged_pia_compilation\n\n \tscript:\n \t\"\"\"\n \tpia -Xmx${10 * task.attempt}g compiler -infile ${(input_files as List).join(\" -infile \")} -name pia-fina-complete -outfile pia-final-merged.xml\n \t\"\"\"\n }",
        "nb_lignes_process": 16,
        "string_script": " \t\"\"\"\n \tpia -Xmx${10 * task.attempt}g compiler -infile ${(input_files as List).join(\" -infile \")} -name pia-fina-complete -outfile pia-final-merged.xml\n \t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "SPIA"
        ],
        "tools_url": [
            "https://bio.tools/spia"
        ],
        "tools_dico": [
            {
                "name": "SPIA",
                "uri": "https://bio.tools/spia",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarray experiment"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0602",
                            "term": "Molecular interactions, pathways and networks"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarrays"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene expression profiling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2497",
                                    "term": "Pathway or network analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differentially expressed gene identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential expression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene expression analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package implements the Signaling Pathway Impact Analysis which uses the information form a list of differentially expressed genes and their log fold changes together with signaling pathways topology, in order to identify the pathways most relevant to the condition under the study.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/SPIA.html"
            }
        ],
        "inputs": [
            "all_search_files"
        ],
        "nb_inputs": 1,
        "outputs": [
            "merged_pia_compilation"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "publishDir \"result\"",
            "memory { 16.GB * task.attempt }",
            "errorStrategy 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "filterPiaCompleteResults": {
        "name_process": "filterPiaCompleteResults",
        "string_process": " process filterPiaCompleteResults {\n \t\n \tpublishDir \"result\", mode: 'copy', overwrite: true\n\n \tmemory { 16.GB * task.attempt }\n     errorStrategy 'retry'\n\n \tinput:\n \tfile pia_xml from merged_pia_compilation\n \tfile pia_config\n\n \toutput:\n \tfile \"*.mztab\" into complete_final_result\n\n \tscript:\n \t\"\"\"\n \tpia -Xmx${10 * task.attempt}g inference -infile ${pia_xml} -paramFile ${pia_config} -psmExport ${pia_xml}.mztab mzTab\n \t\"\"\"\n }",
        "nb_lignes_process": 17,
        "string_script": " \t\"\"\"\n \tpia -Xmx${10 * task.attempt}g inference -infile ${pia_xml} -paramFile ${pia_config} -psmExport ${pia_xml}.mztab mzTab\n \t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "SPIA"
        ],
        "tools_url": [
            "https://bio.tools/spia"
        ],
        "tools_dico": [
            {
                "name": "SPIA",
                "uri": "https://bio.tools/spia",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarray experiment"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0602",
                            "term": "Molecular interactions, pathways and networks"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarrays"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene expression profiling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2497",
                                    "term": "Pathway or network analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differentially expressed gene identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential expression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene expression analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package implements the Signaling Pathway Impact Analysis which uses the information form a list of differentially expressed genes and their log fold changes together with signaling pathways topology, in order to identify the pathways most relevant to the condition under the study.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/SPIA.html"
            }
        ],
        "inputs": [
            "merged_pia_compilation",
            "pia_config"
        ],
        "nb_inputs": 2,
        "outputs": [
            "complete_final_result"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "publishDir \"result\", mode: 'copy', overwrite: true",
            "memory { 16.GB * task.attempt }",
            "errorStrategy 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "ensembl_fasta_download": {
        "name_process": "ensembl_fasta_download",
        "string_process": "\nprocess ensembl_fasta_download{\n    \n                                              \n    \n    input: \n    file ensembl_downloader_config\n\n    output:\n\tfile \"database_ensembl/*.gz\" into ensembl_fasta_gz_databases\n\n\tscript:\n\t\"\"\"\n\tpython ${container_path}pypgatk_cli.py ensembl-downloader --config_file ${ensembl_downloader_config} --taxonomy ${params.taxonomy} -sv -sc\n\t\"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "\t\"\"\"\n\tpython ${container_path}pypgatk_cli.py ensembl-downloader --config_file ${ensembl_downloader_config} --taxonomy ${params.taxonomy} -sv -sc\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ensembl_downloader_config"
        ],
        "nb_inputs": 1,
        "outputs": [
            "ensembl_fasta_gz_databases"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "gunzip_ensembl_files": {
        "name_process": "gunzip_ensembl_files",
        "string_process": "\nprocess gunzip_ensembl_files{\n    \n    publishDir \"result\", mode: 'copy', overwrite: true\n\n    input: \n    file(fasta_file) from ensembl_fasta_gz_databases\n\n    output: \n    file '*.pep.all.fa' into ensembl_protein_database\n    file '*cdna.all.fa' into ensembl_cdna_database, ensembl_cdna_database_sub\n    file '*ncrna.fa' into ensembl_ncrna_database, ensembl_ncrna_database_sub\n\tfile '*.gtf' into gtf\n\t\n    script: \n    \"\"\"\n    gunzip -d --force ${fasta_file}\n    \"\"\" \n}",
        "nb_lignes_process": 17,
        "string_script": "    \"\"\"\n    gunzip -d --force ${fasta_file}\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ensembl_fasta_gz_databases"
        ],
        "nb_inputs": 1,
        "outputs": [
            "ensembl_protein_database",
            "ensembl_cdna_database",
            "ensembl_cdna_database_sub",
            "ensembl_ncrna_database",
            "ensembl_ncrna_database_sub",
            "gtf"
        ],
        "nb_outputs": 6,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "publishDir \"result\", mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "merge_cdnas": {
        "name_process": "merge_cdnas",
        "string_process": "\nprocess merge_cdnas{\n  \n  input:\n  file a from ensembl_cdna_database_sub\n  file b from ensembl_ncrna_database_sub\n  \n  output: \n  file 'total_cdnas.fa' into total_cdnas\n\n  script: \n  \"\"\"\n  cat \"${a}\" \"${b}\" >> total_cdnas.fa\n  \"\"\" \n}",
        "nb_lignes_process": 13,
        "string_script": "  \"\"\"\n  cat \"${a}\" \"${b}\" >> total_cdnas.fa\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ensembl_cdna_database_sub",
            "ensembl_ncrna_database_sub"
        ],
        "nb_inputs": 2,
        "outputs": [
            "total_cdnas"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "add_ncrna": {
        "name_process": "add_ncrna",
        "string_process": "\nprocess add_ncrna{\n\n                                            \n  publishDir \"result\", mode: 'copy', overwrite: true\n  \n  when:\n  params.ncrna\n  \n  input:\n  file x from total_cdnas\n  file ensembl_config\n\n  output:\n  file 'ncRNAs_proteinDB.fa' into optional_ncrna\n\n  script:\n  \"\"\"\n  python ${container_path}pypgatk_cli.py dnaseq-to-proteindb --config_file \"${ensembl_config}\" --input_fasta ${x} --output_proteindb ncRNAs_proteinDB.fa --include_biotypes \"${biotypes['ncRNA']}\" --skip_including_all_cds --var_prefix ncRNA_\n  \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "  \"\"\"\n  python ${container_path}pypgatk_cli.py dnaseq-to-proteindb --config_file \"${ensembl_config}\" --input_fasta ${x} --output_proteindb ncRNAs_proteinDB.fa --include_biotypes \"${biotypes['ncRNA']}\" --skip_including_all_cds --var_prefix ncRNA_\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "total_cdnas",
            "ensembl_config"
        ],
        "nb_inputs": 2,
        "outputs": [
            "optional_ncrna"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "publishDir \"result\", mode: 'copy', overwrite: true"
        ],
        "when": "params.ncrna",
        "stub": ""
    },
    "add_pseudogenes": {
        "name_process": "add_pseudogenes",
        "string_process": "\nprocess add_pseudogenes {\n\n                                            \n  publishDir \"result\", mode: 'copy', overwrite: true\n  \n  when:\n  params.pseudogenes\n  \n  input:\n  file x from total_cdnas\n  file ensembl_config\n\n  output:\n  file 'pseudogenes_proteinDB.fa' into optional_pseudogenes\n\n  script:\n  \"\"\"\n  python ${container_path}pypgatk_cli.py dnaseq-to-proteindb --config_file \"${ensembl_config}\" --input_fasta \"${x}\" --output_proteindb pseudogenes_proteinDB.fa --include_biotypes \"${biotypes['pseudogene']}\" --skip_including_all_cds --var_prefix pseudo_\n  \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "  \"\"\"\n  python ${container_path}pypgatk_cli.py dnaseq-to-proteindb --config_file \"${ensembl_config}\" --input_fasta \"${x}\" --output_proteindb pseudogenes_proteinDB.fa --include_biotypes \"${biotypes['pseudogene']}\" --skip_including_all_cds --var_prefix pseudo_\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "total_cdnas",
            "ensembl_config"
        ],
        "nb_inputs": 2,
        "outputs": [
            "optional_pseudogenes"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "publishDir \"result\", mode: 'copy', overwrite: true"
        ],
        "when": "params.pseudogenes",
        "stub": ""
    },
    "add_altorfs": {
        "name_process": "add_altorfs",
        "string_process": "\nprocess add_altorfs {\n\n                                            \n  publishDir \"result\", mode: 'copy', overwrite: true\n  \n  when:\n  params.altorfs\n  \n  input:\n  file x from ensembl_cdna_database\n  file ensembl_config\n\n  output:\n  file('altorfs_proteinDB.fa') into optional_altorfs\n\n  script:\n  \"\"\"\n  python ${container_path}pypgatk_cli.py dnaseq-to-proteindb --config_file \"${ensembl_config}\" --input_fasta \"${x}\" --output_proteindb altorfs_proteinDB.fa --include_biotypes \"${biotypes['protein_coding']}'\" --skip_including_all_cds --var_prefix altorf_\n  \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "  \"\"\"\n  python ${container_path}pypgatk_cli.py dnaseq-to-proteindb --config_file \"${ensembl_config}\" --input_fasta \"${x}\" --output_proteindb altorfs_proteinDB.fa --include_biotypes \"${biotypes['protein_coding']}'\" --skip_including_all_cds --var_prefix altorf_\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ensembl_cdna_database",
            "ensembl_config"
        ],
        "nb_inputs": 2,
        "outputs": [
            "optional_altorfs"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "publishDir \"result\", mode: 'copy', overwrite: true"
        ],
        "when": "params.altorfs",
        "stub": ""
    },
    "cosmic_download": {
        "name_process": "cosmic_download",
        "string_process": "\nprocess cosmic_download {\n\t\n\t                                          \n\t\n\twhen:\n  \tparams.cosmic\n  \t\t\n\tinput:\n\tfile cosmic_config\n\t\n\toutput:\n\tfile \"database_cosmic/*.gz\" into cosmic_files\n\t\n\tscript:\n\t\"\"\"\n\tpython ${container_path}pypgatk_cli.py cosmic-downloader --config_file \"${cosmic_config}\" --username ${params.cosmic_user_name} --password ${params.cosmic_password}\n\t\"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "\t\"\"\"\n\tpython ${container_path}pypgatk_cli.py cosmic-downloader --config_file \"${cosmic_config}\" --username ${params.cosmic_user_name} --password ${params.cosmic_password}\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "cosmic_config"
        ],
        "nb_inputs": 1,
        "outputs": [
            "cosmic_files"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "params.cosmic",
        "stub": ""
    },
    "gunzip_cosmic_files": {
        "name_process": "gunzip_cosmic_files",
        "string_process": "\nprocess gunzip_cosmic_files{\n\n\twhen:\n  \tparams.cosmic\n  \t\n    input: \n    file(data_file) from cosmic_files\n\n\toutput: \n    file \"All_COSMIC_Genes.fasta\" into cosmic_genes \n\tfile \"CosmicMutantExport.tsv\" into cosmic_mutations\n\t\n\tscript:\n    \"\"\"\n    gunzip -d --force ${data_file}\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    gunzip -d --force ${data_file}\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "cosmic_files"
        ],
        "nb_inputs": 1,
        "outputs": [
            "cosmic_genes",
            "cosmic_mutations"
        ],
        "nb_outputs": 2,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "params.cosmic",
        "stub": ""
    },
    "cosmic_proteindb": {
        "name_process": "cosmic_proteindb",
        "string_process": "\nprocess cosmic_proteindb{\n\t\n\t                                          \n\tpublishDir \"result\", mode: 'copy', overwrite: true \n\t\n\twhen:\n  \tparams.cosmic\n  \t\t\n\tinput:\n\tfile g from cosmic_genes\n\tfile m from cosmic_mutations\n\t\n\toutput:\n\tfile 'cosmic_proteinDB*.fa' into cosmic_proteindbs\n\t\n\tscript:\n\t\"\"\"\n\tpython ${container_path}pypgatk_cli.py cosmic-to-proteindb --config_file \"${cosmic_config}\" --input_mutation ${m} --input_genes ${g} --tissue_type ${params.cosmic_tissue_type} ${split_by_tissue_type} --output_db cosmic_proteinDB.fa\n\t\"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "\t\"\"\"\n\tpython ${container_path}pypgatk_cli.py cosmic-to-proteindb --config_file \"${cosmic_config}\" --input_mutation ${m} --input_genes ${g} --tissue_type ${params.cosmic_tissue_type} ${split_by_tissue_type} --output_db cosmic_proteinDB.fa\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "cosmic_genes",
            "cosmic_mutations"
        ],
        "nb_inputs": 2,
        "outputs": [
            "cosmic_proteindbs"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "publishDir \"result\", mode: 'copy', overwrite: true"
        ],
        "when": "params.cosmic",
        "stub": ""
    },
    "ensembl_vcf_download": {
        "name_process": "ensembl_vcf_download",
        "string_process": "\nprocess ensembl_vcf_download{\n    \n                                              \n    \n    when:\n    params.ensembl\n    \n    input: \n    file ensembl_downloader_config\n\n    output:\n\tfile \"database_ensembl/*.vcf.gz\" into ensembl_vcf_gz_files\n\t\n\tscript:\n\t\"\"\"\n\tpython ${container_path}pypgatk_cli.py ensembl-downloader  --config_file ${ensembl_downloader_config} --taxonomy ${params.taxonomy} -sg -sp -sc -sd -sn\n\t\"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "\t\"\"\"\n\tpython ${container_path}pypgatk_cli.py ensembl-downloader  --config_file ${ensembl_downloader_config} --taxonomy ${params.taxonomy} -sg -sp -sc -sd -sn\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ensembl_downloader_config"
        ],
        "nb_inputs": 1,
        "outputs": [
            "ensembl_vcf_gz_files"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "params.ensembl",
        "stub": ""
    },
    "gunzip_vcf_ensembl_files": {
        "name_process": "gunzip_vcf_ensembl_files",
        "string_process": "\nprocess gunzip_vcf_ensembl_files{\n\n\twhen:\n    params.ensembl\n    \n    input: \n    file vcf_file from ensembl_vcf_gz_files.flatten().map{ file(it) }\n\n    output: \n    file \"*.vcf\" into ensembl_vcf_files\n    \n    script: \n    \"\"\"\n    gunzip -d --force $vcf_file\n  \t\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    gunzip -d --force $vcf_file\n  \t\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ensembl_vcf_gz_files"
        ],
        "nb_inputs": 1,
        "outputs": [
            "ensembl_vcf_files"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "params.ensembl",
        "stub": ""
    },
    "check_ensembl_vcf": {
        "name_process": "check_ensembl_vcf",
        "string_process": "\nprocess check_ensembl_vcf{\n\n\twhen:\n    params.ensembl\n    \n    input: \n    file vcf_file from ensembl_vcf_files\n\n    output: \n    file \"checked_*.vcf\" into ensembl_vcf_files_checked\n    \n    script: \n    \"\"\"\n    awk 'BEGIN{FS=OFS=\"\\t\"}{if(\\$1~\"#\" || (\\$5!=\"\" && \\$4!=\"\")) print}' $vcf_file > checked_$vcf_file\n    \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "    \"\"\"\n    awk 'BEGIN{FS=OFS=\"\\t\"}{if(\\$1~\"#\" || (\\$5!=\"\" && \\$4!=\"\")) print}' $vcf_file > checked_$vcf_file\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ensembl_vcf_files"
        ],
        "nb_inputs": 1,
        "outputs": [
            "ensembl_vcf_files_checked"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "params.ensembl",
        "stub": ""
    },
    "ensembl_vcf_proteinDB": {
        "name_process": "ensembl_vcf_proteinDB",
        "string_process": "\nprocess ensembl_vcf_proteinDB {\n\n\t                                          \n\t\n\twhen:\n\tparams.ensembl\n\n  \tinput:\n  \tfile v from ensembl_vcf_files_checked\n  \tfile f from total_cdnas\n  \tfile g from gtf\n  \tfile e from ensembl_config\n  \t\n  \toutput:\n  \tfile \"${v}_proteinDB.fa\" into proteinDB_vcf\n  \t\n    script:\n  \t\"\"\"\n  \tpython ${container_path}pypgatk_cli.py vcf-to-proteindb --config_file ${e} --af_field \"${af_field}\" --include_biotypes \"${biotypes['protein_coding']}\" --input_fasta ${f} --gene_annotations_gtf ${g} --vep_annotated_vcf ${v} --output_proteindb \"${v}_proteinDB.fa\"  --var_prefix ensvar\n  \t\"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "  \t\"\"\"\n  \tpython ${container_path}pypgatk_cli.py vcf-to-proteindb --config_file ${e} --af_field \"${af_field}\" --include_biotypes \"${biotypes['protein_coding']}\" --input_fasta ${f} --gene_annotations_gtf ${g} --vep_annotated_vcf ${v} --output_proteindb \"${v}_proteinDB.fa\"  --var_prefix ensvar\n  \t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ensembl_vcf_files_checked",
            "total_cdnas",
            "gtf",
            "ensembl_config"
        ],
        "nb_inputs": 4,
        "outputs": [
            "proteinDB_vcf"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "params.ensembl",
        "stub": ""
    },
    "gencode_download": {
        "name_process": "gencode_download",
        "string_process": "\nprocess gencode_download{\n\t\n\twhen:\n\tparams.gnomad\n\t\n\tinput:\n\tval g from params.gencode_url \n\t\n\toutput:\n\tfile(\"gencode.v19.pc_transcripts.fa\") into gencode_fasta\n\tfile(\"gencode.v19.annotation.gtf\") into gencode_gtf\n\t\n\tscript:\n\t\"\"\"\n\twget ${g}/gencode.v19.pc_transcripts.fa.gz \n\twget ${g}/gencode.v19.annotation.gtf.gz\n\tgunzip *.gz\n\t\"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\t\"\"\"\n\twget ${g}/gencode.v19.pc_transcripts.fa.gz \n\twget ${g}/gencode.v19.annotation.gtf.gz\n\tgunzip *.gz\n\t\"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "params"
        ],
        "nb_inputs": 1,
        "outputs": [
            "gencode_fasta",
            "gencode_gtf"
        ],
        "nb_outputs": 2,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "params.gnomad",
        "stub": ""
    },
    "gnomad_download": {
        "name_process": "gnomad_download",
        "string_process": "\nprocess gnomad_download{\n\t\n\twhen:\n\tparams.gnomad\n\t\n\tinput:\n\tval g from params.gnomad_file_url\n\t\n\toutput:\n\tfile \"*.vcf.bgz\" into gnomad_vcf_bgz\n\t\n\tscript:\n\t\"\"\"\n\tgsutil cp ${g} .\n\t\"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "\t\"\"\"\n\tgsutil cp ${g} .\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "NGSUtils"
        ],
        "tools_url": [
            "https://bio.tools/ngsutils"
        ],
        "tools_dico": [
            {
                "name": "NGSUtils",
                "uri": "https://bio.tools/ngsutils",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3227",
                                    "term": "Variant calling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3219",
                                    "term": "Read pre-processing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3187",
                                    "term": "Sequence contamination filtering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3227",
                                    "term": "Variant mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3219",
                                    "term": "Sequence read pre-processing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "NGSUtils is a suite of software tools for working with next-generation sequencing datasets",
                "homepage": "http://ngsutils.org"
            }
        ],
        "inputs": [
            "params"
        ],
        "nb_inputs": 1,
        "outputs": [
            "gnomad_vcf_bgz"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "params.gnomad",
        "stub": ""
    },
    "extract_gnomad_vcf": {
        "name_process": "extract_gnomad_vcf",
        "string_process": "\nprocess extract_gnomad_vcf{\n\t\n\twhen:\n\tparams.gnomad\n\t\n\tinput:\n\tfile g from gnomad_vcf_bgz.flatten().map{ file(it) }\n\t\n\toutput:\n\tfile \"*.vcf\" into gnomad_vcf_files\n\t\n\tscript:\n\t\"\"\"\n\t$ZCAT ${g} > ${g}.vcf\n\t\"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "\t\"\"\"\n\t$ZCAT ${g} > ${g}.vcf\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "gnomad_vcf_bgz"
        ],
        "nb_inputs": 1,
        "outputs": [
            "gnomad_vcf_files"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "params.gnomad",
        "stub": ""
    },
    "gnomad_proteindb": {
        "name_process": "gnomad_proteindb",
        "string_process": "\nprocess gnomad_proteindb{\n\t\n\t                                          \n\t\n\twhen:\n\tparams.gnomad\n\t\n\tinput:\n\tfile v from gnomad_vcf_files\n\tfile f from gencode_fasta\n\tfile g from gencode_gtf\n\tfile e from ensembl_config\n\t\n\toutput:\n\tfile \"${v}_proteinDB.fa\" into gnomad_vcf_proteindb\n\t\n\tscript:\n\t\"\"\"\n\tpython ${container_path}pypgatk_cli.py vcf-to-proteindb --config_file ${e} --vep_annotated_vcf ${v} --input_fasta ${f} --gene_annotations_gtf ${g} --output_proteindb \"${v}_proteinDB.fa\" --af_field controls_AF --transcript_index 6 --biotype_str transcript_type --annotation_field_name vep  --var_prefix gnomadvar\n\t\"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "\t\"\"\"\n\tpython ${container_path}pypgatk_cli.py vcf-to-proteindb --config_file ${e} --vep_annotated_vcf ${v} --input_fasta ${f} --gene_annotations_gtf ${g} --output_proteindb \"${v}_proteinDB.fa\" --af_field controls_AF --transcript_index 6 --biotype_str transcript_type --annotation_field_name vep  --var_prefix gnomadvar\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "gnomad_vcf_files",
            "gencode_fasta",
            "gencode_gtf",
            "ensembl_config"
        ],
        "nb_inputs": 4,
        "outputs": [
            "gnomad_vcf_proteindb"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "params.gnomad",
        "stub": ""
    },
    "cds_GRCh37_download": {
        "name_process": "cds_GRCh37_download",
        "string_process": "\nprocess cds_GRCh37_download{\n\t\n\twhen:\n\tparams.cbioportal\n\t\n\toutput:\n\tfile(\"Homo_sapiens.GRCh37.75.cds.all.fa\") into GRCh37_cds\n\t\n\tscript:\n\t\"\"\"\n\twget ftp://ftp.ensembl.org/pub/release-75/fasta/homo_sapiens/cds/Homo_sapiens.GRCh37.75.cds.all.fa.gz \n\tgunzip *.gz\n\t\"\"\"\n}",
        "nb_lignes_process": 13,
        "string_script": "\t\"\"\"\n\twget ftp://ftp.ensembl.org/pub/release-75/fasta/homo_sapiens/cds/Homo_sapiens.GRCh37.75.cds.all.fa.gz \n\tgunzip *.gz\n\t\"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [
            "GRCh37_cds"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "params.cbioportal",
        "stub": ""
    },
    "download_all_cbioportal": {
        "name_process": "download_all_cbioportal",
        "string_process": " process download_all_cbioportal {\n \t\n \twhen:\n\tparams.cbioportal\n\t\n \toutput:\n \tfile('cbioportal_allstudies_data_mutations_mskcc.txt') into cbio_mutations\n \tfile('cbioportal_allstudies_data_clinical_sample.txt') into cbio_samples\n \t\n \tscript:\n \t\"\"\"\n \tgit clone https://github.com/cBioPortal/datahub.git\n \tcd datahub\n \tgit lfs install --local --skip-smudge\n \tgit lfs pull -I public --include \"data*clinical*sample.txt\"\n \tgit lfs pull -I public --include \"data_mutations_mskcc.txt\"\n \tcd ..\n \tcat datahub/public/*/data_mutations_mskcc.txt > cbioportal_allstudies_data_mutations_mskcc.txt\n \tcat datahub/public/*/*data*clinical*sample.txt > cbioportal_allstudies_data_clinical_sample.txt\n \t\"\"\"\n }",
        "nb_lignes_process": 19,
        "string_script": " \t\"\"\"\n \tgit clone https://github.com/cBioPortal/datahub.git\n \tcd datahub\n \tgit lfs install --local --skip-smudge\n \tgit lfs pull -I public --include \"data*clinical*sample.txt\"\n \tgit lfs pull -I public --include \"data_mutations_mskcc.txt\"\n \tcd ..\n \tcat datahub/public/*/data_mutations_mskcc.txt > cbioportal_allstudies_data_mutations_mskcc.txt\n \tcat datahub/public/*/*data*clinical*sample.txt > cbioportal_allstudies_data_clinical_sample.txt\n \t\"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [
            "cbio_mutations",
            "cbio_samples"
        ],
        "nb_outputs": 2,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "params.cbioportal",
        "stub": ""
    },
    "cbioportal_proteindb": {
        "name_process": "cbioportal_proteindb",
        "string_process": " process cbioportal_proteindb{\n\t\n\tpublishDir \"result\", mode: 'copy', overwrite: true \n\t\n\twhen:\n\tparams.cbioportal\n\t\n\tinput:\n\tfile g from GRCh37_cds\n\tfile m from cbio_mutations\n\tfile s from cbio_samples\n\t\n\toutput:\n\tfile 'cbioPortal_proteinDB*.fa' into cBioportal_proteindb\n\t\n\tscript:\n\t\"\"\"\n\tpython ${container_path}pypgatk_cli.py cbioportal-to-proteindb --config_file \"${cbioportal_config}\" --input_mutation ${m} --input_cds ${g} --clinical_sample_file ${s} --tissue_type ${params.cbioportal_tissue_type} ${split_by_tissue_type} --output_db cbioPortal_proteinDB.fa\n\t\"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\t\"\"\"\n\tpython ${container_path}pypgatk_cli.py cbioportal-to-proteindb --config_file \"${cbioportal_config}\" --input_mutation ${m} --input_cds ${g} --clinical_sample_file ${s} --tissue_type ${params.cbioportal_tissue_type} ${split_by_tissue_type} --output_db cbioPortal_proteinDB.fa\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "GRCh37_cds",
            "cbio_mutations",
            "cbio_samples"
        ],
        "nb_inputs": 3,
        "outputs": [
            "cBioportal_proteindb"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "publishDir \"result\", mode: 'copy', overwrite: true"
        ],
        "when": "params.cbioportal",
        "stub": ""
    },
    "merge_proteindbs": {
        "name_process": "merge_proteindbs",
        "string_process": "\nprocess merge_proteindbs {\n\t\n\t                                          \n    publishDir \"result\", mode: 'copy', overwrite: true \n\n\tinput:\n\tfile(\"proteindb*\") from merged_databases.collect()\n\n\toutput:\n\tfile \"${params.final_database_protein}\" into protiendbs\n\t\n\tscript:\n\t\"\"\"\n\tcat proteindb* > ${params.final_database_protein}\n\t\"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "\t\"\"\"\n\tcat proteindb* > ${params.final_database_protein}\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "merged_databases"
        ],
        "nb_inputs": 1,
        "outputs": [
            "protiendbs"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "publishDir \"result\", mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "decoy": {
        "name_process": "decoy",
        "string_process": "\nprocess decoy {\n\t\n\t                                          \n    publishDir \"result\", mode: 'copy', overwrite: true \n\n\tinput:\n\tfile f from protiendbs\n\n\toutput:\n\tfile \"${params.decoy_prefix}${params.final_database_protein}\" into fasta_decoy_db\n\t\n\tscript:\n\t\"\"\"\n\tpython ${container_path}pypgatk_cli.py generate-decoy --config_file \"${protein_decoy_config}\" --input $f --decoy_prefix \"${params.decoy_prefix}\" --output \"${params.decoy_prefix}${params.final_database_protein}\" \n\t\"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "\t\"\"\"\n\tpython ${container_path}pypgatk_cli.py generate-decoy --config_file \"${protein_decoy_config}\" --input $f --decoy_prefix \"${params.decoy_prefix}\" --output \"${params.decoy_prefix}${params.final_database_protein}\" \n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "protiendbs"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fasta_decoy_db"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "publishDir \"result\", mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "runClustering": {
        "name_process": "runClustering",
        "string_process": "\nprocess runClustering {\n\tcontainer 'biocontainers/spectra-cluster-cli:vv1.1.2_cv2'\n\tpublishDir \"result\"\n\n\tinput:\n\tfile (\"*\") from annotated_mgf\n\n\toutput:\n\tfile \"*.clustering\" into clustering_result\n\n\tscript:\n\t\"\"\"\n\tif [ `ls -1 *.mgf | grep -c \".xt.mgf\"` -gt 0 ]; then\n\t\tENGINE=\"xtandem\"\n\telse\n\t\tENGINE=\"msgf\"\n\tfi\n\n\tspectra-cluster-cli -major_peak_jobs ${threads} \\\n\t-threshold_start 1 -threshold_end 0.99 -rounds 5 \\\n\t-precursor_tolerance ${params.prec_tol} -precursor_tolerance_unit ppm \\\n\t-fragment_tolerance ${params.frag_tol} -filter mz_150 -output_path \\${ENGINE}.clustering \\\n\t*.mgf\n\t\"\"\"\n}",
        "nb_lignes_process": 24,
        "string_script": "\t\"\"\"\n\tif [ `ls -1 *.mgf | grep -c \".xt.mgf\"` -gt 0 ]; then\n\t\tENGINE=\"xtandem\"\n\telse\n\t\tENGINE=\"msgf\"\n\tfi\n\n\tspectra-cluster-cli -major_peak_jobs ${threads} \\\n\t-threshold_start 1 -threshold_end 0.99 -rounds 5 \\\n\t-precursor_tolerance ${params.prec_tol} -precursor_tolerance_unit ppm \\\n\t-fragment_tolerance ${params.frag_tol} -filter mz_150 -output_path \\${ENGINE}.clustering \\\n\t*.mgf\n\t\"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "annotated_mgf"
        ],
        "nb_inputs": 1,
        "outputs": [
            "clustering_result"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'biocontainers/spectra-cluster-cli:vv1.1.2_cv2'",
            "publishDir \"result\""
        ],
        "when": "",
        "stub": ""
    },
    "runNewClustering": {
        "name_process": "runNewClustering",
        "string_process": "\nprocess runNewClustering {\n    container 'jgriss/spectra-cluster-2:develop'\n\n    input:\n    file  \"spectra*.mgf\" from mgf_files2\n\n    output:\n    file '*.clustering' into newClusteringResults, newClusteringResult2\n\n    script:\n    \"\"\"\n    spectra-cluster-2 -output.path ./result_95.clustering -rounds 5 -threshold.start 1 -threshold.end 0.95 *.mgf\n    spectra-cluster-2 -output.path ./result_97.clustering -rounds 5 -threshold.start 1 -threshold.end 0.97 *.mgf\n    spectra-cluster-2 -output.path ./result_99.clustering -rounds 5 -threshold.start 1 -threshold.end 0.99 *.mgf\n    \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "    \"\"\"\n    spectra-cluster-2 -output.path ./result_95.clustering -rounds 5 -threshold.start 1 -threshold.end 0.95 *.mgf\n    spectra-cluster-2 -output.path ./result_97.clustering -rounds 5 -threshold.start 1 -threshold.end 0.97 *.mgf\n    spectra-cluster-2 -output.path ./result_99.clustering -rounds 5 -threshold.start 1 -threshold.end 0.99 *.mgf\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "mgf_files2"
        ],
        "nb_inputs": 1,
        "outputs": [
            "newClusteringResults",
            "newClusteringResult2"
        ],
        "nb_outputs": 2,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'jgriss/spectra-cluster-2:develop'"
        ],
        "when": "",
        "stub": ""
    },
    "getStatistics": {
        "name_process": "getStatistics",
        "string_process": "\nprocess getStatistics {\n    container 'jgriss/spectra-cluster-py:latest'\n\n    input:\n    file \"spectra-cluster_*.clustering\" from clusteringResults1\n    file \"spectra-cluster-2_*.clustering\" from newClusteringResults1\n\n    output:\n    file '*.tsv' into clusteringStats, clusteringStatsForR\n\n    script:\n    \"\"\"\n    clustering_stats --output stats.tsv --min_size 3 *.clustering\n    \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "    \"\"\"\n    clustering_stats --output stats.tsv --min_size 3 *.clustering\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "clusteringResults1",
            "newClusteringResults1"
        ],
        "nb_inputs": 2,
        "outputs": [
            "clusteringStats",
            "clusteringStatsForR"
        ],
        "nb_outputs": 2,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'jgriss/spectra-cluster-py:latest'"
        ],
        "when": "",
        "stub": ""
    },
    "createPlot": {
        "name_process": "createPlot",
        "string_process": "\nprocess createPlot {\n    input:\n    file \"stats.tsv\" from clusteringStatsForR\n\n    output:\n    file \"*.png\" into statPlots\n\n    script:\n    '''\n    #!/usr/bin/env Rscript\n    library(ggplot2)\n    stats <- read.csv(\"stats.tsv\", sep = \"\\\\t\")\n    stats[, \"rel_clustered\"] <- stats[, \"clustered_spectra\"] / stats[, \"total_spectra\"]\n    stats[, \"rel_incorr\"] <- stats[, \"incorrect_spectra\"] / stats[, \"clustered_identified_spectra\"]\n\n    stats[, \"filename\"] <- gsub(\".clustering\", \"\", stats[, \"filename\"])\n\n    stats[, \"threshold\"] <- gsub(\".*_([0-9]*)\\$\", \"\\\\\\\\1\", stats[, \"filename\"])\n    stats[, \"algorithm\"] <- gsub(\"_[^_]*\\$\", \"\", stats[, \"filename\"])\n\n    plot.obj <- ggplot(stats, aes(x = rel_incorr, y = rel_clustered, group = algorithm, color = threshold)) +\n        geom_line(color = \"black\", alpha = \"0.5\") +\n        geom_point(aes(color = threshold)) +\n        labs(x = \"Rel. incorr. spectra\", y = \"Rel. clustered spectra\")\n\n    png(\"stats.png\", width = 2000, height = 2000, res = 300)\n    print(plot.obj)\n    dev.off()\n    '''\n}",
        "nb_lignes_process": 29,
        "string_script": "    '''\n    #!/usr/bin/env Rscript\n    library(ggplot2)\n    stats <- read.csv(\"stats.tsv\", sep = \"\\\\t\")\n    stats[, \"rel_clustered\"] <- stats[, \"clustered_spectra\"] / stats[, \"total_spectra\"]\n    stats[, \"rel_incorr\"] <- stats[, \"incorrect_spectra\"] / stats[, \"clustered_identified_spectra\"]\n\n    stats[, \"filename\"] <- gsub(\".clustering\", \"\", stats[, \"filename\"])\n\n    stats[, \"threshold\"] <- gsub(\".*_([0-9]*)\\$\", \"\\\\\\\\1\", stats[, \"filename\"])\n    stats[, \"algorithm\"] <- gsub(\"_[^_]*\\$\", \"\", stats[, \"filename\"])\n\n    plot.obj <- ggplot(stats, aes(x = rel_incorr, y = rel_clustered, group = algorithm, color = threshold)) +\n        geom_line(color = \"black\", alpha = \"0.5\") +\n        geom_point(aes(color = threshold)) +\n        labs(x = \"Rel. incorr. spectra\", y = \"Rel. clustered spectra\")\n\n    png(\"stats.png\", width = 2000, height = 2000, res = 300)\n    print(plot.obj)\n    dev.off()\n    '''",
        "nb_lignes_script": 20,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "clusteringStatsForR"
        ],
        "nb_inputs": 1,
        "outputs": [
            "statPlots"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "createFasta": {
        "name_process": "createFasta",
        "string_process": "\nprocess createFasta {\n    cpus 1\n    memory '10 GB'\n    container 'prvst/philosopher:3.2.9'\n\n    input:\n    file fasta_in_file\n\n    output:\n    file(\"*-decoys-contam-*.fasta\") into fasta_file\n\n    script:\n    \"\"\"\n    philosopher workspace --init\n\n    # add decoys + contaminants\n    philosopher database --custom ${fasta_in_file} --contam\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "    \"\"\"\n    philosopher workspace --init\n\n    # add decoys + contaminants\n    philosopher database --custom ${fasta_in_file} --contam\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fasta_in_file"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fasta_file"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "cpus 1",
            "memory '10 GB'",
            "container 'prvst/philosopher:3.2.9'"
        ],
        "when": "",
        "stub": ""
    },
    "runSearch": {
        "name_process": "runSearch",
        "string_process": "\nprocess runSearch {\n    cpus 5\n    memory '50 GB'\n    stageInMode 'link'\n    errorStrategy 'retry'\n    maxRetries 3\n\n    input:\n    file(\"*\") from raw_files.collect()\n    file(\"db.fasta\") from fasta_file\n    file msfragger_config_file\n\n    output:\n    file(\"*.pepXML\") into search_result_out\n\n    script:\n    \"\"\"\n    java -Xmx48G -jar /smb/tools/MSFragger-3.0/MSFragger-3.0/MSFragger-3.0.jar ${msfragger_config_file} ${params.file_pattern}\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "    \"\"\"\n    java -Xmx48G -jar /smb/tools/MSFragger-3.0/MSFragger-3.0/MSFragger-3.0.jar ${msfragger_config_file} ${params.file_pattern}\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "raw_files",
            "fasta_file",
            "msfragger_config_file"
        ],
        "nb_inputs": 3,
        "outputs": [
            "search_result_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "cpus 5",
            "memory '50 GB'",
            "stageInMode 'link'",
            "errorStrategy 'retry'",
            "maxRetries 3"
        ],
        "when": "",
        "stub": ""
    },
    "philosopher": {
        "name_process": "philosopher",
        "string_process": "\nprocess philosopher {\n    cpus 10\n    memory '10 GB'\n    container 'prvst/philosopher:3.2.9'\n    publishDir \"${params.mode}\", mode: 'copy'\n\n    input:\n    file(\"*\") from search_result\n    file fasta_file\n\n    output:\n    file(\"*.tsv\") into final_result\n    file(\"psm.tsv\") into psm_result\n    file(\"combined.prot.xml\") into combined_prot_result\n    file(\"*.pep.xml\") into pep_xml_results\n\n    script:\n    \"\"\"\n    # init the workspace\n    philosopher workspace --init\n\n    # annotate the database\n    philosopher database --annotate ${fasta_file}\n\n    # peptide prophet\n    for PEPXML in *.pepXML; do\n        philosopher peptideprophet \\\\\n            --database ${fasta_file} \\\\\n            --nonparam \\\\\n            --expectscore \\\\\n            --decoyprobs \\\\\n            --masswidth 1000.0 --clevel -2 \\${PEPXML}\n    done\n\n    # protein prophet\n    philosopher proteinprophet --maxppmdiff 2000000 --output combined *.pep.xml\n\n    # create the combined peptide result\n    philosopher filter --sequential --prot 0.01 --tag rev_ \\\n        --pepxml ./ \\\n        --protxml combined.prot.xml\n\n    # create the reports\n    philosopher report\n    \"\"\"\n}",
        "nb_lignes_process": 45,
        "string_script": "    \"\"\"\n    # init the workspace\n    philosopher workspace --init\n\n    # annotate the database\n    philosopher database --annotate ${fasta_file}\n\n    # peptide prophet\n    for PEPXML in *.pepXML; do\n        philosopher peptideprophet \\\\\n            --database ${fasta_file} \\\\\n            --nonparam \\\\\n            --expectscore \\\\\n            --decoyprobs \\\\\n            --masswidth 1000.0 --clevel -2 \\${PEPXML}\n    done\n\n    # protein prophet\n    philosopher proteinprophet --maxppmdiff 2000000 --output combined *.pep.xml\n\n    # create the combined peptide result\n    philosopher filter --sequential --prot 0.01 --tag rev_ \\\n        --pepxml ./ \\\n        --protxml combined.prot.xml\n\n    # create the reports\n    philosopher report\n    \"\"\"",
        "nb_lignes_script": 27,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "search_result",
            "fasta_file"
        ],
        "nb_inputs": 2,
        "outputs": [
            "final_result",
            "psm_result",
            "combined_prot_result",
            "pep_xml_results"
        ],
        "nb_outputs": 4,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "cpus 10",
            "memory '10 GB'",
            "container 'prvst/philosopher:3.2.9'",
            "publishDir \"${params.mode}\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "combinedPeptideResult": {
        "name_process": "combinedPeptideResult",
        "string_process": "\nprocess combinedPeptideResult {\n    cpus 10\n    memory '10 GB'\n    container 'prvst/philosopher:3.2.9'\n    publishDir \"${params.mode}\", mode: 'copy'\n\n    input:\n    file fasta_file\n    file prot_xml from combined_prot_result\n    file (\"*\") from search_result_II\n\n    output:\n    file(\"*.pep.xml\") into combined_pep_xml\n    file(\"protein.fas\") into protein_fasta\n\n    script:\n    \"\"\"\n    # init the workspace\n    philosopher workspace --init\n\n    # annotate the database\n    philosopher database --annotate ${fasta_file}\n\n    # run peptide prophet to combine all search results\n    philosopher peptideprophet --database ${fasta_file} \\\n        --combine \\\n        --decoyprobs \\\n        --accmass \\\n        --nonparam \\\n        --output combined \\\n        *.pepXML\n\n    # filter on the peptide and protein level\n    philosopher filter --razor --pepxml *.pep.xml --protxml ${prot_xml}\n    \"\"\"\n}",
        "nb_lignes_process": 35,
        "string_script": "    \"\"\"\n    # init the workspace\n    philosopher workspace --init\n\n    # annotate the database\n    philosopher database --annotate ${fasta_file}\n\n    # run peptide prophet to combine all search results\n    philosopher peptideprophet --database ${fasta_file} \\\n        --combine \\\n        --decoyprobs \\\n        --accmass \\\n        --nonparam \\\n        --output combined \\\n        *.pepXML\n\n    # filter on the peptide and protein level\n    philosopher filter --razor --pepxml *.pep.xml --protxml ${prot_xml}\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fasta_file",
            "combined_prot_result",
            "search_result_II"
        ],
        "nb_inputs": 3,
        "outputs": [
            "combined_pep_xml",
            "protein_fasta"
        ],
        "nb_outputs": 2,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "cpus 10",
            "memory '10 GB'",
            "container 'prvst/philosopher:3.2.9'",
            "publishDir \"${params.mode}\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "ptmShepherd": {
        "name_process": "ptmShepherd",
        "string_process": "\nprocess ptmShepherd {\n    cpus 10\n    memory '10 GB'\n    stageInMode 'link'\n    publishDir \"${params.mode}\", mode: 'copy'\n    when params.mode != \"narrow_search\"\n\n    input:\n    file(\"psm.tsv\") from psm_result\n    file(\"*\") from raw_files_II.collect()\n    file ptm_shepherd_jar\n    \n    output:\n    file(\"*.tsv\")\n\n    script:\n    \"\"\"\n    # use the default config\n    echo -e \"dataset = DATASET1 psm.tsv .\\\\n\" > ptm_shepherd.txt\n\n    java -Xmx10G -Dbatmass.io.libs.thermo.dir=${params.thermo_ext_dir} -jar ${ptm_shepherd_jar} ptm_shepherd.txt\n    \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "    \"\"\"\n    # use the default config\n    echo -e \"dataset = DATASET1 psm.tsv .\\\\n\" > ptm_shepherd.txt\n\n    java -Xmx10G -Dbatmass.io.libs.thermo.dir=${params.thermo_ext_dir} -jar ${ptm_shepherd_jar} ptm_shepherd.txt\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "psm_result",
            "raw_files_II",
            "ptm_shepherd_jar"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "cpus 10",
            "memory '10 GB'",
            "stageInMode 'link'",
            "publishDir \"${params.mode}\", mode: 'copy' when params.mode != \"narrow_search\""
        ],
        "when": "",
        "stub": ""
    },
    "convertSpeclibFiles": {
        "name_process": "convertSpeclibFiles",
        "string_process": "\nprocess convertSpeclibFiles {\n    cpus 2\n    container 'grosenberger/easypqp'\n    memory '5 GB'\n    stageInMode 'link'\n    when params.create_spec_lib == \"True\"\n\n    input:\n    tuple val(name), file(pep_xml_file), file(mzml_file) from combined_result\n\n    output:\n    file(\"*pkl\") into pkl_files\n\n    script:\n    \"\"\"\n    easypqp convert \\\n        --pepxml ${pep_xml_file} \\\n        --spectra ${mzml_file} \\\n        --exclude-range -1.5,3.5 \\\n        --psms ${name}.psmpkl \\\n        --peaks ${name}.peakpkl\n    \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "    \"\"\"\n    easypqp convert \\\n        --pepxml ${pep_xml_file} \\\n        --spectra ${mzml_file} \\\n        --exclude-range -1.5,3.5 \\\n        --psms ${name}.psmpkl \\\n        --peaks ${name}.peakpkl\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "combined_result"
        ],
        "nb_inputs": 1,
        "outputs": [
            "pkl_files"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "cpus 2",
            "container 'grosenberger/easypqp'",
            "memory '5 GB'",
            "stageInMode 'link' when params.create_spec_lib == \"True\""
        ],
        "when": "",
        "stub": ""
    },
    "DatabaseSearch": {
        "name_process": "DatabaseSearch",
        "string_process": "\nprocess DatabaseSearch {\n\n  input:\n  set val(setname), val(filename), file(x) from mzml_msgf\n\n  output:\n  set val(setname), val(filename), file(\"${sample}.mzid\") into mzids\n  \n  \"\"\"\n  msgf_plus -Xmx12G -thread 1 -d $protdb -s $x -o \"${filename}.mzid\" -mod $mods -tda 0 -t ${params.MS1tolerance} -ti -1,2 -m ${params.FragmentMethodID} -inst ${params.inst} -e ${params.EnzymeID} -protocol ${params.protocol} -ntt 2 -minLength 7 -maxLength 40 -minCharge 2 -maxCharge 4 -maxMissedCleavages 2 -n 1 -addFeatures 1\n  \"\"\"\n}",
        "nb_lignes_process": 11,
        "string_script": "\"\"\"\n  msgf_plus -Xmx12G -thread 1 -d $protdb -s $x -o \"${filename}.mzid\" -mod $mods -tda 0 -t ${params.MS1tolerance} -ti -1,2 -m ${params.FragmentMethodID} -inst ${params.inst} -e ${params.EnzymeID} -protocol ${params.protocol} -ntt 2 -minLength 7 -maxLength 40 -minCharge 2 -maxCharge 4 -maxMissedCleavages 2 -n 1 -addFeatures 1\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "mzml_msgf"
        ],
        "nb_inputs": 1,
        "outputs": [
            "mzids"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "ConvertmzidTotsv": {
        "name_process": "ConvertmzidTotsv",
        "string_process": "\nprocess ConvertmzidTotsv {\n\n  input:\n  set val(setname), file(x) from mzids\n  \n  output:\n  set val(setname), file('out.mzid.tsv') into mzidtsvs \n\n  \"\"\"\n  msgf_plus -Xmx3500M edu.ucsd.msjava.ui.MzIDToTsv -i $x -o out.mzid.tsv\n  \"\"\"\n}",
        "nb_lignes_process": 11,
        "string_script": "\"\"\"\n  msgf_plus -Xmx3500M edu.ucsd.msjava.ui.MzIDToTsv -i $x -o out.mzid.tsv\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "mzids"
        ],
        "nb_inputs": 1,
        "outputs": [
            "mzidtsvs"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "MergeTSVbyset": {
        "name_process": "MergeTSVbyset",
        "string_process": "\nprocess MergeTSVbyset {\n\n  publishDir \"${params.outdir}\", mode: 'copy', overwrite: true\n\n  input:\n  set val(setname), file('tsv?') from mzidtsvs_byset\n\n  output:\n  set val(setname), file(\"${setname}_allpsms.sorted.txt\") into merged_tsv\n\n  script:\n  \"\"\"\n  head -1 tsv1 |cut -f 1-14 > psmheader\n  sort -g -s -t \\$'\\\\t' -k 13,13 <(tail -q -n +2 tsv*) |cut -f 1-14 >psmmerge.sorted\n  cat psmheader psmmerge.sorted > ${setname}_allpsms.sorted.txt\n  \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "  \"\"\"\n  head -1 tsv1 |cut -f 1-14 > psmheader\n  sort -g -s -t \\$'\\\\t' -k 13,13 <(tail -q -n +2 tsv*) |cut -f 1-14 >psmmerge.sorted\n  cat psmheader psmmerge.sorted > ${setname}_allpsms.sorted.txt\n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "mzidtsvs_byset"
        ],
        "nb_inputs": 1,
        "outputs": [
            "merged_tsv"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "publishDir \"${params.outdir}\", mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "GlobalFDR": {
        "name_process": "GlobalFDR",
        "string_process": "\nprocess GlobalFDR {\n publishDir \"${params.outdir}\", mode: 'copy', overwrite: true\n\n input:\n set val(setname), file('sorted.psm') from globalFDRtsv\n \n output:\n set val(setname), file(\"${setname}_psms.globalFDR0.01.txt\") into all_psm\n \n script:\n \"\"\"\n GlobalFDR.py --input sorted.psm --output ${setname}_psms.globalFDR0.01.txt --decoy_prefix ${params.decoy_prefix} --psm_qval ${params.qval}\n \"\"\"\n\n}",
        "nb_lignes_process": 14,
        "string_script": " \"\"\"\n GlobalFDR.py --input sorted.psm --output ${setname}_psms.globalFDR0.01.txt --decoy_prefix ${params.decoy_prefix} --psm_qval ${params.qval}\n \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "globalFDRtsv"
        ],
        "nb_inputs": 1,
        "outputs": [
            "all_psm"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "publishDir \"${params.outdir}\", mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "subGroupFDR": {
        "name_process": "subGroupFDR",
        "string_process": "\nprocess subGroupFDR {\n publishDir \"${params.outdir}\", mode: 'copy', overwrite: true,saveAs: { it == \"fitcurve.png\" ? \"${setname}_fitcurve.png\" : it }\n\n input:\n set val(setname), file('sorted.psm') from subgroupFDRtsv\n\n output:\n set val(setname), file(\"${setname}_psms.txt\") into novel_psm\n file 'fitcurve.png' into plotout\n\n \"\"\"\n subgroupFDR.py --input sorted.psm  --decoy_prefix ${params.decoy_prefix} --group_target ${params.group_target} --group_decoy ${params.group_decoy} --output ${setname}_${params.group_target}.txt --psm_qval ${params.qval}\n \"\"\"\n}",
        "nb_lignes_process": 13,
        "string_script": "\"\"\"\n subgroupFDR.py --input sorted.psm  --decoy_prefix ${params.decoy_prefix} --group_target ${params.group_target} --group_decoy ${params.group_decoy} --output ${setname}_${params.group_target}.txt --psm_qval ${params.qval}\n \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "subgroupFDRtsv"
        ],
        "nb_inputs": 1,
        "outputs": [
            "novel_psm",
            "plotout"
        ],
        "nb_outputs": 2,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "publishDir \"${params.outdir}\", mode: 'copy', overwrite: true,saveAs: { it == \"fitcurve.png\" ? \"${setname}_fitcurve.png\" : it }"
        ],
        "when": "",
        "stub": ""
    },
    "peptideIdentification": {
        "name_process": "peptideIdentification",
        "string_process": "\nprocess peptideIdentification {\n\n   container 'mwalzer/openms-batteries-included:V2.4.0_proteomic_lfq_2'\n   publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true\n\n   memory { 10.GB * task.attempt }\n\n   input:\n   file mz_ml from mz_files\n   file \"database.fasta\" from fasta_file\n   file id_config\n\n   output:\n   file \"*.idXML\" into id_xmls\n\n   script:\n   \"\"\"\n   MSGFPlusAdapter -ini \"${id_config}\" -database database.fasta -in ${mz_ml} -out ${mz_ml}.idXML\n   \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "   \"\"\"\n   MSGFPlusAdapter -ini \"${id_config}\" -database database.fasta -in ${mz_ml} -out ${mz_ml}.idXML\n   \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "mz_files",
            "fasta_file",
            "id_config"
        ],
        "nb_inputs": 3,
        "outputs": [
            "id_xmls"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'mwalzer/openms-batteries-included:V2.4.0_proteomic_lfq_2'",
            "publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true",
            "memory { 10.GB * task.attempt }"
        ],
        "when": "",
        "stub": ""
    },
    "peptideIndexer": {
        "name_process": "peptideIndexer",
        "string_process": "\nprocess peptideIndexer {\n\n   container 'mwalzer/openms-batteries-included:V2.4.0_proteomic_lfq_2'\n   publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true\n\n   input:\n   file id_xml from id_xmls\n   file \"database.fasta\" from fasta_file\n   file index_config\n\n   output:\n   file \"*.idXML\" into index_xmls\n\n   script:\n   \"\"\"\n   PeptideIndexer -ini \"${index_config}\" -fasta database.fasta -in ${id_xml} -out ${id_xml.baseName}-index.idXML\n   \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "   \"\"\"\n   PeptideIndexer -ini \"${index_config}\" -fasta database.fasta -in ${id_xml} -out ${id_xml.baseName}-index.idXML\n   \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "id_xmls",
            "fasta_file",
            "index_config"
        ],
        "nb_inputs": 3,
        "outputs": [
            "index_xmls"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'mwalzer/openms-batteries-included:V2.4.0_proteomic_lfq_2'",
            "publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "peptideFDRCompute": {
        "name_process": "peptideFDRCompute",
        "string_process": "\nprocess peptideFDRCompute {\n\n   container 'mwalzer/openms-batteries-included:V2.4.0_proteomic_lfq'\n   publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true\n\n   input:\n   file index_xml from index_xmls\n   file fdr_config\n\n   output:\n   file \"*.idXML\" into fdr_xmls\n\n   script:\n   \"\"\"\n   FalseDiscoveryRate -ini \"${fdr_config}\" -in ${index_xml} -out ${index_xml.baseName}-fdr.idXML\n   \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "   \"\"\"\n   FalseDiscoveryRate -ini \"${fdr_config}\" -in ${index_xml} -out ${index_xml.baseName}-fdr.idXML\n   \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "index_xmls",
            "fdr_config"
        ],
        "nb_inputs": 2,
        "outputs": [
            "fdr_xmls"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'mwalzer/openms-batteries-included:V2.4.0_proteomic_lfq'",
            "publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "peptideFDRFilter": {
        "name_process": "peptideFDRFilter",
        "string_process": "\nprocess peptideFDRFilter {\n\n   container 'mwalzer/openms-batteries-included:V2.4.0_proteomic_lfq_2'\n   publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true\n\n   input:\n   file fdr_xml from fdr_xmls\n   file idfilter_config\n\n   output:\n   file \"*.idXML\" into peptide_xmls, peptide_convert_xmls\n\n   script:\n   \"\"\"\n   IDFilter -ini \"${idfilter_config}\" -in ${fdr_xml} -out ${fdr_xml.baseName}-filter.idXML\n   \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "   \"\"\"\n   IDFilter -ini \"${idfilter_config}\" -in ${fdr_xml} -out ${fdr_xml.baseName}-filter.idXML\n   \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fdr_xmls",
            "idfilter_config"
        ],
        "nb_inputs": 2,
        "outputs": [
            "peptide_xmls",
            "peptide_convert_xmls"
        ],
        "nb_outputs": 2,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'mwalzer/openms-batteries-included:V2.4.0_proteomic_lfq_2'",
            "publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "convertMZIdML": {
        "name_process": "convertMZIdML",
        "string_process": "\nprocess convertMZIdML{\n\n   container 'mwalzer/openms-batteries-included:V2.4.0_proteomic_lfq_2'\n   publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true\n\n   input:\n   file filter_file from peptide_convert_xmls\n\n\n   output:\n   file \"*.mzid\" into peptide_mzids\n\n   script:\n   \"\"\"\n   IDFileConverter -in ${filter_file} -out ${filter_file.baseName}.mzid\n   \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "   \"\"\"\n   IDFileConverter -in ${filter_file} -out ${filter_file.baseName}.mzid\n   \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "peptide_convert_xmls"
        ],
        "nb_inputs": 1,
        "outputs": [
            "peptide_mzids"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'mwalzer/openms-batteries-included:V2.4.0_proteomic_lfq_2'",
            "publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "idQualityControl": {
        "name_process": "idQualityControl",
        "string_process": "\nprocess idQualityControl{\n   container 'mwalzer/openms-batteries-included:V2.3.0_pepxmlpatch'\n   publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true\n\n   input:\n   set val(mzML), file(mzML_file), file(idx_file) from combined_results\n\n   output:\n   file \"*.qcML\" into qc_MLs\n\n   script:\n   \"\"\"\n   QCCalculator -in ${mzML_file} -id ${idx_file} -out ${mzML}.qcML\n   \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "   \"\"\"\n   QCCalculator -in ${mzML_file} -id ${idx_file} -out ${mzML}.qcML\n   \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "combined_results"
        ],
        "nb_inputs": 1,
        "outputs": [
            "qc_MLs"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'mwalzer/openms-batteries-included:V2.3.0_pepxmlpatch'",
            "publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "plotQualityControl": {
        "name_process": "plotQualityControl",
        "string_process": "\nprocess plotQualityControl{\n\n  container 'mwalzer/qc-plotter:latest'\n  publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true\n\n  input:\n  file qcML from qcMLs\n  each param from qc_tool_parameters\n\n\n  output:\n  file \"*.png\" into plots\n\n  script:\n  \"\"\"\n  qc_plot.sh -$param ${qcML}\n  \"\"\"\n\n}",
        "nb_lignes_process": 18,
        "string_script": "  \"\"\"\n  qc_plot.sh -$param ${qcML}\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "qcMLs",
            "qc_tool_parameters"
        ],
        "nb_inputs": 2,
        "outputs": [
            "plots"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'mwalzer/qc-plotter:latest'",
            "publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "indexPeptides": {
        "name_process": "indexPeptides",
        "string_process": "\nprocess indexPeptides {\n    container 'ypriverol/crux:3.2'\n    publishDir \"data/\"\n    \n    input:\n    file 'small-yeast.fasta' from peptides\n    file 'demo.ms2' from spectra\n\n    output:\n    file 'crux-output/tide-search.target.txt' into searchResults\n    file 'crux-output/tide-search.decoy.txt' into decoyResults\n\n    script:\n    \"\"\"\n    crux tide-index small-yeast.fasta yeast-index\n    crux tide-search --compute-sp T --mzid-output T demo.ms2 yeast-index\n    \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "    \"\"\"\n    crux tide-index small-yeast.fasta yeast-index\n    crux tide-search --compute-sp T --mzid-output T demo.ms2 yeast-index\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "Crux"
        ],
        "tools_url": [
            "https://bio.tools/crux"
        ],
        "tools_dico": [
            {
                "name": "Crux",
                "uri": "https://bio.tools/crux",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3214",
                                    "term": "Spectral analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3214",
                                    "term": "Spectrum analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3214",
                                    "term": "Mass spectrum analysis"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2536",
                                "term": "Mass spectrometry data"
                            }
                        ],
                        "output": []
                    }
                ],
                "description": "Cross-platform suite of analysis tools for interpreting protein mass spectrometry data.",
                "homepage": "http://crux.ms/"
            }
        ],
        "inputs": [
            "peptides",
            "spectra"
        ],
        "nb_inputs": 2,
        "outputs": [
            "searchResults",
            "decoyResults"
        ],
        "nb_outputs": 2,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'ypriverol/crux:3.2'",
            "publishDir \"data/\""
        ],
        "when": "",
        "stub": ""
    },
    "postProcess": {
        "name_process": "postProcess",
        "string_process": "\nprocess postProcess {\n    container 'containers.biocontainers.pro/biocontainers/crux:v2.1_cv2.588'\n\n    input:\n    file 'search.target.txt' from searchResults        \n    file 'search.decoy.txt' from decoyResults\n\n    output:\n    file 'crux-output/percolator.target.psms.txt' into percResults\n\n    script:\n    \"\"\"\n    crux percolator search.target.txt\n    \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "    \"\"\"\n    crux percolator search.target.txt\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "Crux"
        ],
        "tools_url": [
            "https://bio.tools/crux"
        ],
        "tools_dico": [
            {
                "name": "Crux",
                "uri": "https://bio.tools/crux",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3214",
                                    "term": "Spectral analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3214",
                                    "term": "Spectrum analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3214",
                                    "term": "Mass spectrum analysis"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2536",
                                "term": "Mass spectrometry data"
                            }
                        ],
                        "output": []
                    }
                ],
                "description": "Cross-platform suite of analysis tools for interpreting protein mass spectrometry data.",
                "homepage": "http://crux.ms/"
            }
        ],
        "inputs": [
            "searchResults",
            "decoyResults"
        ],
        "nb_inputs": 2,
        "outputs": [
            "percResults"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'containers.biocontainers.pro/biocontainers/crux:v2.1_cv2.588'"
        ],
        "when": "",
        "stub": ""
    },
    "downloadFiles": {
        "name_process": "downloadFiles",
        "string_process": "\nprocess downloadFiles {\n    container 'quay.io/biocontainers/gnu-wget:1.18--3'\n    \n    output:\n    file '*.raw' into rawFiles\n   \n    script: \n    \"\"\"\n    wget -v -r -nd -A \"*.raw\" --no-host-directories --cut-dirs=1 ${params.project_ftp}\n    \"\"\" \n}",
        "nb_lignes_process": 10,
        "string_script": "    \"\"\"\n    wget -v -r -nd -A \"*.raw\" --no-host-directories --cut-dirs=1 ${params.project_ftp}\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [
            "rawFiles"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'quay.io/biocontainers/gnu-wget:1.18--3'"
        ],
        "when": "",
        "stub": ""
    },
    "generateMetadata": {
        "name_process": "generateMetadata",
        "string_process": "\nprocess generateMetadata {\n\n    label 'big_mem'\n    container 'ypriverol/thermorawfileparser:0.2'\n    memory { 10.GB * task.attempt }\n    errorStrategy 'retry'\n    \n    publishDir 'data/', mode:'copy'\n \n    input:\n    file rawFile from rawFiles.flatten()\n    \n    output: \n    file '*.json' into metaResults\n    file '*.mzML' into spectraFiles\n\n    script:\n    \"\"\"\n    ThermoRawFileParser -i=${rawFile} -m=0 -f=1 -o=./ -v\n    \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "    \"\"\"\n    ThermoRawFileParser -i=${rawFile} -m=0 -f=1 -o=./ -v\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "ThermoRawFileParser"
        ],
        "tools_url": [
            "https://bio.tools/ThermoRawFileParser"
        ],
        "tools_dico": [
            {
                "name": "ThermoRawFileParser",
                "uri": "https://bio.tools/ThermoRawFileParser",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3172",
                            "term": "Metabolomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3520",
                            "term": "Proteomics experiment"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3215",
                                    "term": "Peak detection"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3695",
                                    "term": "Filtering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3434",
                                    "term": "Conversion"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3215",
                                    "term": "Peak assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3215",
                                    "term": "Peak finding"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0943",
                                "term": "Mass spectrum"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0943",
                                "term": "Mass spectrum"
                            }
                        ]
                    }
                ],
                "description": "Open-source, crossplatform tool that converts Thermo RAW files into open file formats such as MGF and to the HUPO-PSI standard file format mzML",
                "homepage": "https://github.com/compomics/ThermoRawFileParser"
            }
        ],
        "inputs": [
            "rawFiles"
        ],
        "nb_inputs": 1,
        "outputs": [
            "metaResults",
            "spectraFiles"
        ],
        "nb_outputs": 2,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "label 'big_mem'",
            "container 'ypriverol/thermorawfileparser:0.2'",
            "memory { 10.GB * task.attempt }",
            "errorStrategy 'retry'",
            "publishDir 'data/', mode:'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "IDfreeQualityControl": {
        "name_process": "IDfreeQualityControl",
        "string_process": "\nprocess IDfreeQualityControl{\n   container 'mwalzer/openms-batteries-included:V2.3.0_pepxmlpatch'\n   publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true\n\n   input:\n   file mzML_file from spectraFiles.flatten()\n\n   output:\n   file \"*.qcML\" into qcMLs\n\n   script:\n   \"\"\"\n   QCCalculator -in ${mzML_file} -out ${mzML}.qcML\n   \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "   \"\"\"\n   QCCalculator -in ${mzML_file} -out ${mzML}.qcML\n   \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "spectraFiles"
        ],
        "nb_inputs": 1,
        "outputs": [
            "qcMLs"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'mwalzer/openms-batteries-included:V2.3.0_pepxmlpatch'",
            "publishDir \"${params.result_folder}\", mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "createTandemConfig": {
        "name_process": "createTandemConfig",
        "string_process": "\nprocess createTandemConfig {\n\tinput:\n\tfile \"settings.xml\" from xtandem_template\n\n\toutput:\n\tfile \"adapted_settings.xml\" into xtandem_settings\n\n\tscript:\n\t\"\"\"\n\tsed -e 's|FRAG_TOL|${params.frag_tol}|' \\\n\t    -e 's|PREC_TOL|${params.prec_tol}|' \\\n\t    -e 's|MISSED_CLEAV|${params.mc}|' \\\n\t    -e 's|THREADS|$threads|' \\\n\t    ${xtandem_template} > adapted_settings.xml\n\t\"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "\t\"\"\"\n\tsed -e 's|FRAG_TOL|${params.frag_tol}|' \\\n\t    -e 's|PREC_TOL|${params.prec_tol}|' \\\n\t    -e 's|MISSED_CLEAV|${params.mc}|' \\\n\t    -e 's|THREADS|$threads|' \\\n\t    ${xtandem_template} > adapted_settings.xml\n\t\"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "xtandem_template"
        ],
        "nb_inputs": 1,
        "outputs": [
            "xtandem_settings"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "searchTandem": {
        "name_process": "searchTandem",
        "string_process": "\nprocess searchTandem {\n\tcontainer 'jgriss/tandem:v17-02-01-4'\n\n\tinput:\n\tfile xtandem_settings\n\tfile xtandem_taxonomy\n\tfile fasta_decoy_db\n\tfile mgf_file from mgf_files\n\n\toutput:\n        file \"${mgf_file}.xml.mzid\" into xtandem_result\n\n\tscript:\n\t\"\"\"\n\tsed -e 's|ORG_NAME|${mgf_file}|' ${xtandem_settings} > ${mgf_file}.settings.xml && \\\n\ttandem ${mgf_file}.settings.xml && \\\n\tsed -i 's|value=\"^XXX\"|value=\"_REVERSED\"|' ${mgf_file}.xml.mzid\n\t\"\"\"\t\n}",
        "nb_lignes_process": 18,
        "string_script": "\t\"\"\"\n\tsed -e 's|ORG_NAME|${mgf_file}|' ${xtandem_settings} > ${mgf_file}.settings.xml && \\\n\ttandem ${mgf_file}.settings.xml && \\\n\tsed -i 's|value=\"^XXX\"|value=\"_REVERSED\"|' ${mgf_file}.xml.mzid\n\t\"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "rTANDEM"
        ],
        "tools_url": [
            "https://bio.tools/rtandem"
        ],
        "tools_dico": [
            {
                "name": "rTANDEM",
                "uri": "https://bio.tools/rtandem",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2814",
                            "term": "Protein structure analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_2814",
                            "term": "Protein structure"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3092",
                                    "term": "Protein feature detection"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3092",
                                    "term": "Protein feature prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3092",
                                    "term": "Protein feature recognition"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package interfaces the tandem protein identification algorithm in R. Identification can be launched in the X!Tandem style, by using as sole parameter the path to a parameter file. But it also provides extended syntax and functions to streamline launching analyses, as well as function to convert results, parameters and taxonomy to/from R. A related package, shinyTANDEM, provides visualization interface for result objects.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/rTANDEM.html"
            }
        ],
        "inputs": [
            "xtandem_settings",
            "xtandem_taxonomy",
            "fasta_decoy_db",
            "mgf_files"
        ],
        "nb_inputs": 4,
        "outputs": [
            "xtandem_result"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'jgriss/tandem:v17-02-01-4'"
        ],
        "when": "",
        "stub": ""
    },
    "annotateMsgf": {
        "name_process": "annotateMsgf",
        "string_process": "\nprocess annotateMsgf {\n\tcontainer 'jgriss/spectra-cluster-py:latest'\n\n\tinput:\n\tset val(mgf_name), file(msgf_file), file(mgf_file) from msgf_combined\n\n\toutput:\n\tfile \"${mgf_name}.msgf.mgf\" into msgf_annotated\n\n\tscript:\n\t\"\"\"\n\tmgf_search_result_annotator --input ${mgf_file} --search ${msgf_file} --output ${mgf_name}.msgf.mgf --format \"MSGF_ident\" --fdr 0.01\n\t\"\"\"\n}",
        "nb_lignes_process": 13,
        "string_script": "\t\"\"\"\n\tmgf_search_result_annotator --input ${mgf_file} --search ${msgf_file} --output ${mgf_name}.msgf.mgf --format \"MSGF_ident\" --fdr 0.01\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "msgf_combined"
        ],
        "nb_inputs": 1,
        "outputs": [
            "msgf_annotated"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'jgriss/spectra-cluster-py:latest'"
        ],
        "when": "",
        "stub": ""
    },
    "annotateTandem": {
        "name_process": "annotateTandem",
        "string_process": "\nprocess annotateTandem {\n\tcontainer 'jgriss/spectra-cluster-py:latest'\n\n\tinput:\n\tset val(mgf_name), file(xtandem_file), file(mgf_file) from xtandem_combined\n\n\toutput:\n\tfile \"${mgf_name}.xt.mgf\" into xtandem_annotated\n\n\tscript:\n\t\"\"\"\n\tmgf_search_result_annotator --input ${mgf_file} --search ${xtandem_file} --output \"${mgf_name}.xt.mgf\" --format xtandem --fdr 0.01\n\t\"\"\"\n}",
        "nb_lignes_process": 13,
        "string_script": "\t\"\"\"\n\tmgf_search_result_annotator --input ${mgf_file} --search ${xtandem_file} --output \"${mgf_name}.xt.mgf\" --format xtandem --fdr 0.01\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "xtandem_combined"
        ],
        "nb_inputs": 1,
        "outputs": [
            "xtandem_annotated"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'jgriss/spectra-cluster-py:latest'"
        ],
        "when": "",
        "stub": ""
    },
    "transferIds": {
        "name_process": "transferIds",
        "string_process": "\nprocess transferIds {\n\tcontainer 'jgriss/spectra-cluster-py:latest'\n\tpublishDir \"result\"\n\n\tinput:\n\tfile clustering_file from clustering_result\n\tfile fasta_file\n\n\toutput:\n\tfile '*.spec_counts.tsv' into spectral_counts\n\n\tscript:\n\t\"\"\"\n\tid_transferer_cli --input \"${clustering_file}\" --output psm_quant.tsv \\\n\t\t--min_identified ${params.min_ident} --min_ratio ${params.min_ratio} \\\n\t\t--return_all_identified --only_unidentified\n\tprotein_annotator --input psm_quant.tsv --output ${clustering_file}.spec_counts.tsv \\\n\t\t--fasta \"${fasta_file}\" --ignore_il\n\t\"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "\t\"\"\"\n\tid_transferer_cli --input \"${clustering_file}\" --output psm_quant.tsv \\\n\t\t--min_identified ${params.min_ident} --min_ratio ${params.min_ratio} \\\n\t\t--return_all_identified --only_unidentified\n\tprotein_annotator --input psm_quant.tsv --output ${clustering_file}.spec_counts.tsv \\\n\t\t--fasta \"${fasta_file}\" --ignore_il\n\t\"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "clustering_result",
            "fasta_file"
        ],
        "nb_inputs": 2,
        "outputs": [
            "spectral_counts"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'jgriss/spectra-cluster-py:latest'",
            "publishDir \"result\""
        ],
        "when": "",
        "stub": ""
    },
    "updateMetadata": {
        "name_process": "updateMetadata",
        "string_process": "\nprocess updateMetadata {\n\n\n     container 'quay.io/pride/pride-py:0.0.8'\n\n     input:\n     file metadataFile from metaResults\n\n     when:\n     params.mode != 'test'\n\n     script:\n     \"\"\"\n     python3 /pridepy.py update-metadata -f $metadataFile -u $params.pride_username -p $params.pride_password\n     \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "     \"\"\"\n     python3 /pridepy.py update-metadata -f $metadataFile -u $params.pride_username -p $params.pride_password\n     \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "metaResults"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "container 'quay.io/pride/pride-py:0.0.8'"
        ],
        "when": "params.mode != 'test'",
        "stub": ""
    },
    "qcIdFreeMzMLs": {
        "name_process": "qcIdFreeMzMLs",
        "string_process": "\nprocess qcIdFreeMzMLs{\n\n    label 'big_mem'\n\n    container 'quay.io/biocontainers/bumbershoot:3_0_11579--0'\n    publishDir 'data/', mode:'copy'\n\n    input:\n    file fileMzML from spectraFiles.flatten()\n\n    output:\n    file '*.tsv' into qcMzMLFiles\n\n    script:\n    \"\"\"\n    quameter ${fileMzML} -MetricsType idfree -OutputFilepath ${fileMzML}-qc.tsv -cpus 5\n    \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "    \"\"\"\n    quameter ${fileMzML} -MetricsType idfree -OutputFilepath ${fileMzML}-qc.tsv -cpus 5\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "spectraFiles"
        ],
        "nb_inputs": 1,
        "outputs": [
            "qcMzMLFiles"
        ],
        "nb_outputs": 1,
        "name_workflow": "bigbio__nf-workflows",
        "directive": [
            "label 'big_mem'",
            "container 'quay.io/biocontainers/bumbershoot:3_0_11579--0'",
            "publishDir 'data/', mode:'copy'"
        ],
        "when": "",
        "stub": ""
    }
}