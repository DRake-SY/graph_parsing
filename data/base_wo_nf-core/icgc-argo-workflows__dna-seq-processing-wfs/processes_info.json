{
    "songGetAnalysis": {
        "name_process": "songGetAnalysis",
        "string_process": "\nprocess songGetAnalysis {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n    publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n    tag \"${analysis_id}\"\n\n    input:\n        val study_id\n        val analysis_id\n\n    output:\n        path \"*.analysis.json\", emit: json\n\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing search -a ${analysis_id} > ${analysis_id}.analysis.json\n        \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing search -a ${analysis_id} > ${analysis_id}.analysis.json\n        \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [
            "Nursing"
        ],
        "tools_url": [
            "https://bio.tools/Nursing"
        ],
        "tools_dico": [
            {
                "name": "Nursing",
                "uri": "https://bio.tools/Nursing",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Medical informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Critical care medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3376",
                            "term": "Medicines research and development"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Biomedical informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Healthcare informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Health informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Health and disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Clinical informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Acute medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Emergency medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Intensive care medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3376",
                            "term": "Drug discovery and development"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The use and abuse of credentials.\n\nA Guide to Nursing Credentials and Degrees.\n\nComplete List of Common Nursing Certifications.\n\nFrom LPN to MSN, the jumble of letters following a nurse's name can be confusing. Learn what these nursing credentials mean and how they should be listed.\n\nView 183 commonly recognized nursing certifications along with links to their certifying organizations.\n\nThe confusing nature of nursing credentials has led to widespread use of the term \u201calphabet soup.\u201d The letters that follow a nurse\u2019s name can be perplexing to professionals in the medical field, and especially to patients and families. To solve this problem, nursing credentials need to be displayed properly.\n\nNever fear. Nurse.org has compiled an alphabetical list of 183 different nursing certifications along with the appropriate acronyms and links to their certifying organizations",
                "homepage": "https://online.alvernia.edu/program-resources/nursing-credentials/"
            }
        ],
        "inputs": [
            "study_id",
            "analysis_id"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "maxRetries params.max_retries",
            "errorStrategy { sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long); return params.max_retries ? 'retry' : 'finish' }",
            "pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]",
            "cpus params.cpus",
            "memory \"${params.mem} GB\"",
            "container \"overture/song-client:${params.container_version}\"",
            "publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir",
            "tag \"${analysis_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "payloadGenDnaAlignment": {
        "name_process": "payloadGenDnaAlignment",
        "string_process": "\nprocess payloadGenDnaAlignment {\n  container \"${params.container ?: container[params.container_registry ?: default_container_registry]}:${params.container_version ?: version}\"\n  publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n  cpus params.cpus\n  memory \"${params.mem} GB\"\n\n  input:\n    path files_to_upload\n    path seq_experiment_analysis\n    path read_group_ubam_analysis\n    val wf_name\n    val wf_version\n\n  output:\n    path \"*.dna_alignment.payload.json\", emit: payload\n    path \"out/*\", emit: alignment_files\n\n  script:\n    args_read_group_ubam_analysis = read_group_ubam_analysis.size() > 0 ? \"-u ${read_group_ubam_analysis}\" : \"\"\n    \"\"\"\n    main.py \\\n      -f ${files_to_upload} \\\n      -a ${seq_experiment_analysis} \\\n      -w \"${wf_name}\" \\\n      -r ${workflow.runName} \\\n      -s ${workflow.sessionId} \\\n      -v ${wf_version} ${args_read_group_ubam_analysis}\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "    args_read_group_ubam_analysis = read_group_ubam_analysis.size() > 0 ? \"-u ${read_group_ubam_analysis}\" : \"\"\n    \"\"\"\n    main.py \\\n      -f ${files_to_upload} \\\n      -a ${seq_experiment_analysis} \\\n      -w \"${wf_name}\" \\\n      -r ${workflow.runName} \\\n      -s ${workflow.sessionId} \\\n      -v ${wf_version} ${args_read_group_ubam_analysis}\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "files_to_upload",
            "seq_experiment_analysis",
            "read_group_ubam_analysis",
            "wf_name",
            "wf_version"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "container \"${params.container ?: container[params.container_registry ?: default_container_registry]}:${params.container_version ?: version}\"",
            "publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir",
            "cpus params.cpus",
            "memory \"${params.mem} GB\""
        ],
        "when": "",
        "stub": ""
    },
    "songSubmit": {
        "name_process": "songSubmit",
        "string_process": "\nprocess songSubmit {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n    \n    tag \"${study_id}\"\n    label \"songSubmit\"\n    \n    input:\n        val study_id\n        path payload\n    \n    output:\n        stdout()\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        set -euxo pipefail\n        sing submit -f ${payload} | jq -er .analysisId | tr -d '\\\\n'\n        \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        set -euxo pipefail\n        sing submit -f ${payload} | jq -er .analysisId | tr -d '\\\\n'\n        \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "Nursing"
        ],
        "tools_url": [
            "https://bio.tools/Nursing"
        ],
        "tools_dico": [
            {
                "name": "Nursing",
                "uri": "https://bio.tools/Nursing",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Medical informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Critical care medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3376",
                            "term": "Medicines research and development"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Biomedical informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Healthcare informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Health informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Health and disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Clinical informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Acute medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Emergency medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Intensive care medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3376",
                            "term": "Drug discovery and development"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The use and abuse of credentials.\n\nA Guide to Nursing Credentials and Degrees.\n\nComplete List of Common Nursing Certifications.\n\nFrom LPN to MSN, the jumble of letters following a nurse's name can be confusing. Learn what these nursing credentials mean and how they should be listed.\n\nView 183 commonly recognized nursing certifications along with links to their certifying organizations.\n\nThe confusing nature of nursing credentials has led to widespread use of the term \u201calphabet soup.\u201d The letters that follow a nurse\u2019s name can be perplexing to professionals in the medical field, and especially to patients and families. To solve this problem, nursing credentials need to be displayed properly.\n\nNever fear. Nurse.org has compiled an alphabetical list of 183 different nursing certifications along with the appropriate acronyms and links to their certifying organizations",
                "homepage": "https://online.alvernia.edu/program-resources/nursing-credentials/"
            }
        ],
        "inputs": [
            "study_id",
            "payload"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "maxRetries params.max_retries",
            "errorStrategy { sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long); return params.max_retries ? 'retry' : 'finish' }",
            "pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]",
            "cpus params.cpus",
            "memory \"${params.mem} GB\"",
            "container \"overture/song-client:${params.container_version}\"",
            "tag \"${study_id}\"",
            "label \"songSubmit\""
        ],
        "when": "",
        "stub": ""
    },
    "metadataParser": {
        "name_process": "metadataParser",
        "string_process": "\nprocess metadataParser {\n  container \"quay.io/icgc-argo/metadata-parser:metadata-parser.${params.container_version ?: version}\"\n  cpus params.cpus\n  memory \"${params.mem} GB\"\n\n  input:\n    path metadata_analysis\n\n  output:\n    env STUDY_ID, emit: study_id\n    env DONOR_ID, emit: donor_id\n    env EXP, emit: experimental_strategy\n    env PAIRED, emit: paired\n\n  script:\n    \"\"\"\n    set -euxo pipefail\n    STUDY_ID=`cat ${metadata_analysis} | jq -er '.studyId' | tr -d '\\\\n'`\n    DONOR_ID=`cat ${metadata_analysis} | jq -er '.samples[0].donor.donorId' | tr -d '\\\\n'`\n    EXP=`cat ${metadata_analysis} | jq -er '.experiment | if (.experimental_strategy | length)>0 then .experimental_strategy else .library_strategy end' | tr -d '\\\\n'`\n    PAIRED=`cat ${metadata_analysis} | jq -er '[.read_groups[] | .is_paired_end] | all | tostring' | tr -d '\\\\n'`\n    \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "    \"\"\"\n    set -euxo pipefail\n    STUDY_ID=`cat ${metadata_analysis} | jq -er '.studyId' | tr -d '\\\\n'`\n    DONOR_ID=`cat ${metadata_analysis} | jq -er '.samples[0].donor.donorId' | tr -d '\\\\n'`\n    EXP=`cat ${metadata_analysis} | jq -er '.experiment | if (.experimental_strategy | length)>0 then .experimental_strategy else .library_strategy end' | tr -d '\\\\n'`\n    PAIRED=`cat ${metadata_analysis} | jq -er '[.read_groups[] | .is_paired_end] | all | tostring' | tr -d '\\\\n'`\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "GALLO"
        ],
        "tools_url": [
            "https://bio.tools/gallo"
        ],
        "tools_dico": [
            {
                "name": "GALLO",
                "uri": "https://bio.tools/gallo",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3473",
                            "term": "Data mining"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3500",
                            "term": "Zoology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2885",
                            "term": "DNA polymorphism"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype and phenotype"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3500",
                            "term": "Metazoa"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3500",
                            "term": "Animal biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3500",
                            "term": "Animal"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3500",
                            "term": "Animals"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype and phenotype resources"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype-phenotype analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype-phenotype"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Genetic mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3501",
                                    "term": "Enrichment analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3196",
                                    "term": "Genotyping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Genetic map construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Linkage mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Functional mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Genetic cartography"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Genetic map generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3501",
                                    "term": "Enrichment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3501",
                                    "term": "Over-representation analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An R package for genomic annotation and integration of multiple data sources in livestock for positional candidate loci.\n\nGALLO: Genomic Annotation in Livestock for positional candidate LOci",
                "homepage": "https://github.com/pablobio/GALLO"
            }
        ],
        "inputs": [
            "metadata_analysis"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "container \"quay.io/icgc-argo/metadata-parser:metadata-parser.${params.container_version ?: version}\"",
            "cpus params.cpus",
            "memory \"${params.mem} GB\""
        ],
        "when": "",
        "stub": ""
    },
    "bamMergeSortMarkdup": {
        "name_process": "bamMergeSortMarkdup",
        "string_process": "\nprocess bamMergeSortMarkdup {\n  container \"${params.container ?: container[params.container_registry ?: default_container_registry]}:${params.container_version ?: version}\"\n  publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n  cpus params.cpus\n  memory \"${params.mem} GB\"\n\n  input:\n    path aligned_lane_bams\n    path ref_genome_gz\n    path ref_genome_gz_secondary_file\n    val tempdir\n\n  output:\n    path \"${params.aligned_basename}.{bam,cram}\", emit: merged_seq\n    path \"${params.aligned_basename}.{bam.bai,cram.crai}\", emit: merged_seq_idx\n    path \"${params.aligned_basename}.duplicates_metrics.tgz\", optional: true, emit: duplicates_metrics\n\n  script:\n    arg_markdup = params.markdup ? \"-d\" : \"\"\n    arg_lossy = params.lossy ? \"-l\" : \"\"\n    arg_tempdir = tempdir != 'NO_DIR' ? \"-t ${tempdir}\" : \"\"\n    \"\"\"\n    main.py \\\n      -i ${aligned_lane_bams} \\\n      -r ${ref_genome_gz} \\\n      -n ${params.cpus} \\\n      -b ${params.aligned_basename} ${arg_markdup} \\\n      -o ${params.output_format} ${arg_lossy} ${arg_tempdir}\n    \"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "    arg_markdup = params.markdup ? \"-d\" : \"\"\n    arg_lossy = params.lossy ? \"-l\" : \"\"\n    arg_tempdir = tempdir != 'NO_DIR' ? \"-t ${tempdir}\" : \"\"\n    \"\"\"\n    main.py \\\n      -i ${aligned_lane_bams} \\\n      -r ${ref_genome_gz} \\\n      -n ${params.cpus} \\\n      -b ${params.aligned_basename} ${arg_markdup} \\\n      -o ${params.output_format} ${arg_lossy} ${arg_tempdir}\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "aligned_lane_bams",
            "ref_genome_gz",
            "ref_genome_gz_secondary_file",
            "tempdir"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "container \"${params.container ?: container[params.container_registry ?: default_container_registry]}:${params.container_version ?: version}\"",
            "publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir",
            "cpus params.cpus",
            "memory \"${params.mem} GB\""
        ],
        "when": "",
        "stub": ""
    },
    "gatkCollectOxogMetrics": {
        "name_process": "gatkCollectOxogMetrics",
        "string_process": "\nprocess gatkCollectOxogMetrics {\n  container \"quay.io/icgc-argo/gatk-collect-oxog-metrics:gatk-collect-oxog-metrics.${params.container_version ?: version}\"\n  cpus params.cpus\n  memory \"${params.mem} GB\"\n  publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: \"${params.publish_dir ? true : ''}\"\n\n  input:\n    path seq\n    path seq_idx\n    path ref_genome_fa\n    path ref_genome_secondary_file\n    path interval_file\n    val paired\n    val dependencies\n\n  output:\n    path \"*.oxog_metrics.tgz\", emit: oxog_metrics\n\n  script:\n    arg_interval_file = interval_file.name == 'NO_FILE' ? \"\" : \"-i ${interval_file}\"\n    arg_paired = paired == \"true\" ? \"-p\" : \"\"\n    \"\"\"\n    gatk-collect-oxog-metrics.py -s ${seq} \\\n                      -r ${ref_genome_fa} \\\n                      -m ${(int) (params.mem * 1000)} ${arg_interval_file} ${arg_paired}\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "    arg_interval_file = interval_file.name == 'NO_FILE' ? \"\" : \"-i ${interval_file}\"\n    arg_paired = paired == \"true\" ? \"-p\" : \"\"\n    \"\"\"\n    gatk-collect-oxog-metrics.py -s ${seq} \\\n                      -r ${ref_genome_fa} \\\n                      -m ${(int) (params.mem * 1000)} ${arg_interval_file} ${arg_paired}\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "seq",
            "seq_idx",
            "ref_genome_fa",
            "ref_genome_secondary_file",
            "interval_file",
            "paired",
            "dependencies"
        ],
        "nb_inputs": 7,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "container \"quay.io/icgc-argo/gatk-collect-oxog-metrics:gatk-collect-oxog-metrics.${params.container_version ?: version}\"",
            "cpus params.cpus",
            "memory \"${params.mem} GB\"",
            "publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: \"${params.publish_dir ? true : ''}\""
        ],
        "when": "",
        "stub": ""
    },
    "gatherOxogMetrics": {
        "name_process": "gatherOxogMetrics",
        "string_process": "\nprocess gatherOxogMetrics {\n  container \"quay.io/icgc-argo/gatk-collect-oxog-metrics:gatk-collect-oxog-metrics.${params.container_version ?: version}\"\n  cpus params.cpus\n  memory \"${params.mem} GB\"\n  publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: \"${params.publish_dir ? true : ''}\"\n\n  input:\n    path oxog_metrics_files\n\n  output:\n    path \"out/*.oxog_metrics.tgz\", emit: oxog_metrics\n\n  script:\n    \"\"\"\n    gather-oxog-metrics.py -m ${oxog_metrics_files}\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    gather-oxog-metrics.py -m ${oxog_metrics_files}\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "oxog_metrics_files"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "container \"quay.io/icgc-argo/gatk-collect-oxog-metrics:gatk-collect-oxog-metrics.${params.container_version ?: version}\"",
            "cpus params.cpus",
            "memory \"${params.mem} GB\"",
            "publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: \"${params.publish_dir ? true : ''}\""
        ],
        "when": "",
        "stub": ""
    },
    "payloadGenDnaSeqQc": {
        "name_process": "payloadGenDnaSeqQc",
        "string_process": "\nprocess payloadGenDnaSeqQc {\n  container \"quay.io/icgc-argo/payload-gen-dna-seq-qc:payload-gen-dna-seq-qc.${params.container_version ?: version}\"\n  cpus params.cpus\n  memory \"${params.mem} GB\"\n  publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: \"${params.publish_dir ? true : ''}\"\n\n  input:\n    path seq_experiment_analysis\n    path qc_files\n    val wf_name\n    val wf_version\n\n  output:\n    path \"*.dna_seq_qc.payload.json\", emit: payload\n    path \"out/*.tgz\", emit: qc_files\n\n  script:\n    \"\"\"\n    payload-gen-dna-seq-qc.py \\\n      -a ${seq_experiment_analysis} \\\n      -f ${qc_files} \\\n      -w \"${wf_name}\" \\\n      -r ${workflow.runName} \\\n      -s ${workflow.sessionId} \\\n      -v ${wf_version}\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "    \"\"\"\n    payload-gen-dna-seq-qc.py \\\n      -a ${seq_experiment_analysis} \\\n      -f ${qc_files} \\\n      -w \"${wf_name}\" \\\n      -r ${workflow.runName} \\\n      -s ${workflow.sessionId} \\\n      -v ${wf_version}\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "seq_experiment_analysis",
            "qc_files",
            "wf_name",
            "wf_version"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "container \"quay.io/icgc-argo/payload-gen-dna-seq-qc:payload-gen-dna-seq-qc.${params.container_version ?: version}\"",
            "cpus params.cpus",
            "memory \"${params.mem} GB\"",
            "publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: \"${params.publish_dir ? true : ''}\""
        ],
        "when": "",
        "stub": ""
    },
    "cleanupWorkdir": {
        "name_process": "cleanupWorkdir",
        "string_process": "\nprocess cleanupWorkdir {\n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"ubuntu:${params.container_version}\"\n\n    input:\n        path files_to_delete                                                                                       \n        val virtual_dep_flag                                                                                               \n\n    script:\n        \"\"\"\n        set -euxo pipefail\n\n        IFS=\" \"\n        read -a files <<< \"${files_to_delete}\"\n        for f in \"\\${files[@]}\"\n        do\n            dir_to_rm=\\$(dirname \\$(readlink -f \\$f))\n\n            if [[ \\$dir_to_rm != ${workflow.workDir}/* ]]; then  # skip dir not under workdir, like from input file dir\n                echo \"Not delete: \\$dir_to_rm/*\\\"\n                continue\n            fi\n\n            rm -fr \\$dir_to_rm/*  # delete all files and subdirs but not hidden ones\n            echo \"Deleted: \\$dir_to_rm/*\"\n        done\n        \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "        \"\"\"\n        set -euxo pipefail\n\n        IFS=\" \"\n        read -a files <<< \"${files_to_delete}\"\n        for f in \"\\${files[@]}\"\n        do\n            dir_to_rm=\\$(dirname \\$(readlink -f \\$f))\n\n            if [[ \\$dir_to_rm != ${workflow.workDir}/* ]]; then  # skip dir not under workdir, like from input file dir\n                echo \"Not delete: \\$dir_to_rm/*\\\"\n                continue\n            fi\n\n            rm -fr \\$dir_to_rm/*  # delete all files and subdirs but not hidden ones\n            echo \"Deleted: \\$dir_to_rm/*\"\n        done\n        \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "files_to_delete",
            "virtual_dep_flag"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "cpus params.cpus",
            "memory \"${params.mem} GB\"",
            "container \"ubuntu:${params.container_version}\""
        ],
        "when": "",
        "stub": ""
    },
    "scoreDownload": {
        "name_process": "scoreDownload",
        "string_process": "\nprocess scoreDownload {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/score:${params.container_version}\"\n    publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n    label \"scoreDownload\"\n    tag \"${analysis_id}\"\n\n    input:\n        path analysis\n        val study_id\n        val analysis_id\n\n    output:\n        path analysis, emit: analysis_json\n        path 'out/*', emit: files\n\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export METADATA_URL=${params.song_url}\n        export STORAGE_URL=${params.score_url}\n        export TRANSPORT_PARALLEL=${params.cpus}\n        export TRANSPORT_MEMORY=${params.transport_mem}\n        export ACCESSTOKEN=${accessToken}\n        \n        score-client download --analysis-id ${analysis_id} --study-id ${study_id} --output-dir ./out \n        \"\"\"\n}",
        "nb_lignes_process": 39,
        "string_script": "        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export METADATA_URL=${params.song_url}\n        export STORAGE_URL=${params.score_url}\n        export TRANSPORT_PARALLEL=${params.cpus}\n        export TRANSPORT_MEMORY=${params.transport_mem}\n        export ACCESSTOKEN=${accessToken}\n        \n        score-client download --analysis-id ${analysis_id} --study-id ${study_id} --output-dir ./out \n        \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "analysis",
            "study_id",
            "analysis_id"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "maxRetries params.max_retries",
            "errorStrategy { sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long); return params.max_retries ? 'retry' : 'finish' }",
            "pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]",
            "cpus params.cpus",
            "memory \"${params.mem} GB\"",
            "container \"overture/score:${params.container_version}\"",
            "publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir",
            "label \"scoreDownload\"",
            "tag \"${analysis_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "seqDataToLaneBam": {
        "name_process": "seqDataToLaneBam",
        "string_process": "\nprocess seqDataToLaneBam {\n  container \"quay.io/icgc-argo/seq-data-to-lane-bam:seq-data-to-lane-bam.${params.container_version ?: version}\"\n  cpus params.cpus\n  memory \"${params.mem} GB\"\n  publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", enabled: \"${params.publish_dir ? true : ''}\"\n\n  input:\n    path metadata_json\n    path seq_files\n\n  output:\n    path \"*.lane.bam\", emit: lane_bams\n\n  script:\n    \"\"\"\n    seq-data-to-lane-bam.py \\\n      -p ${metadata_json} \\\n      -s ${seq_files} \\\n      -d ${params.reads_max_discard_fraction} \\\n      -n ${params.cpus} \\\n      -m ${(int) (params.mem * 1000)}\n    \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "    \"\"\"\n    seq-data-to-lane-bam.py \\\n      -p ${metadata_json} \\\n      -s ${seq_files} \\\n      -d ${params.reads_max_discard_fraction} \\\n      -n ${params.cpus} \\\n      -m ${(int) (params.mem * 1000)}\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "metadata_json",
            "seq_files"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "container \"quay.io/icgc-argo/seq-data-to-lane-bam:seq-data-to-lane-bam.${params.container_version ?: version}\"",
            "cpus params.cpus",
            "memory \"${params.mem} GB\"",
            "publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", enabled: \"${params.publish_dir ? true : ''}\""
        ],
        "when": "",
        "stub": ""
    },
    "gatkSplitIntervals": {
        "name_process": "gatkSplitIntervals",
        "string_process": "\nprocess gatkSplitIntervals {\n  container \"quay.io/icgc-argo/gatk-split-intervals:gatk-split-intervals.${params.container_version ?: version}\"\n  cpus params.cpus\n  memory \"${params.mem} GB\"\n\n  input:\n    val scatter_count\n    path ref_genome_fa\n    path ref_genome_secondary_file\n    path intervals\n\n  output:\n    path \"*.interval_list\", emit: interval_files\n\n  script:\n    arg_intervals = intervals.name != 'NO_FILE' ? \"-L ${intervals}\" : \"\"\n    \"\"\"\n    gatk-split-intervals.py --scatter ${scatter_count} \\\n                      -R ${ref_genome_fa} \\\n                      -j ${(int) (params.mem * 1000)} ${arg_intervals}\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "    arg_intervals = intervals.name != 'NO_FILE' ? \"-L ${intervals}\" : \"\"\n    \"\"\"\n    gatk-split-intervals.py --scatter ${scatter_count} \\\n                      -R ${ref_genome_fa} \\\n                      -j ${(int) (params.mem * 1000)} ${arg_intervals}\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "scatter_count",
            "ref_genome_fa",
            "ref_genome_secondary_file",
            "intervals"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "container \"quay.io/icgc-argo/gatk-split-intervals:gatk-split-intervals.${params.container_version ?: version}\"",
            "cpus params.cpus",
            "memory \"${params.mem} GB\""
        ],
        "when": "",
        "stub": ""
    },
    "songManifest": {
        "name_process": "songManifest",
        "string_process": "\nprocess songManifest {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n\n    tag \"${analysis_id}\"\n\n    input:\n        val study_id\n        val analysis_id\n        path upload\n    \n    output:\n        path \"out/manifest.txt\"\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing manifest -a ${analysis_id} -d . -f ./out/manifest.txt\n        \"\"\"\n}",
        "nb_lignes_process": 35,
        "string_script": "        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing manifest -a ${analysis_id} -d . -f ./out/manifest.txt\n        \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [
            "Nursing"
        ],
        "tools_url": [
            "https://bio.tools/Nursing"
        ],
        "tools_dico": [
            {
                "name": "Nursing",
                "uri": "https://bio.tools/Nursing",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Medical informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Critical care medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3376",
                            "term": "Medicines research and development"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Biomedical informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Healthcare informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Health informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Health and disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Clinical informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Acute medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Emergency medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Intensive care medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3376",
                            "term": "Drug discovery and development"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The use and abuse of credentials.\n\nA Guide to Nursing Credentials and Degrees.\n\nComplete List of Common Nursing Certifications.\n\nFrom LPN to MSN, the jumble of letters following a nurse's name can be confusing. Learn what these nursing credentials mean and how they should be listed.\n\nView 183 commonly recognized nursing certifications along with links to their certifying organizations.\n\nThe confusing nature of nursing credentials has led to widespread use of the term \u201calphabet soup.\u201d The letters that follow a nurse\u2019s name can be perplexing to professionals in the medical field, and especially to patients and families. To solve this problem, nursing credentials need to be displayed properly.\n\nNever fear. Nurse.org has compiled an alphabetical list of 183 different nursing certifications along with the appropriate acronyms and links to their certifying organizations",
                "homepage": "https://online.alvernia.edu/program-resources/nursing-credentials/"
            }
        ],
        "inputs": [
            "study_id",
            "analysis_id",
            "upload"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "maxRetries params.max_retries",
            "errorStrategy { sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long); return params.max_retries ? 'retry' : 'finish' }",
            "publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir",
            "pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]",
            "cpus params.cpus",
            "memory \"${params.mem} GB\"",
            "container \"overture/song-client:${params.container_version}\"",
            "tag \"${analysis_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "bwaMemAligner": {
        "name_process": "bwaMemAligner",
        "string_process": "\nprocess bwaMemAligner {\n  container \"${params.container ?: container[params.container_registry ?: default_container_registry]}:${params.container_version ?: version}\"\n  publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n  cpus params.cpus\n  memory \"${params.mem} GB\"\n\n  tag \"${input_bam.size()}\"\n\n  input:\n    path input_bam\n    path ref_genome_gz\n    path ref_genome_gz_secondary_files\n    path sequencing_experiment_analysis\n    val tempdir\n    val dependencies\n\n  output:\n    path \"${params.aligned_lane_prefix}.${input_bam.baseName}.bam\", emit: aligned_bam\n\n  script:\n    metadata = sequencing_experiment_analysis ? \"-m \" + sequencing_experiment_analysis : \"\"\n    arg_tempdir = tempdir != 'NO_DIR' ? \"-t ${tempdir}\": \"\"\n    \"\"\"\n    main.py \\\n      -i ${input_bam} \\\n      -r ${ref_genome_gz} \\\n      -o ${params.aligned_lane_prefix} \\\n      -n ${task.cpus} ${metadata} ${arg_tempdir}\n    \"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "    metadata = sequencing_experiment_analysis ? \"-m \" + sequencing_experiment_analysis : \"\"\n    arg_tempdir = tempdir != 'NO_DIR' ? \"-t ${tempdir}\": \"\"\n    \"\"\"\n    main.py \\\n      -i ${input_bam} \\\n      -r ${ref_genome_gz} \\\n      -o ${params.aligned_lane_prefix} \\\n      -n ${task.cpus} ${metadata} ${arg_tempdir}\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "input_bam",
            "ref_genome_gz",
            "ref_genome_gz_secondary_files",
            "sequencing_experiment_analysis",
            "tempdir",
            "dependencies"
        ],
        "nb_inputs": 6,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "container \"${params.container ?: container[params.container_registry ?: default_container_registry]}:${params.container_version ?: version}\"",
            "publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir",
            "cpus params.cpus",
            "memory \"${params.mem} GB\"",
            "tag \"${input_bam.size()}\""
        ],
        "when": "",
        "stub": ""
    },
    "alignedSeqQC": {
        "name_process": "alignedSeqQC",
        "string_process": "\nprocess alignedSeqQC {\n  container \"quay.io/icgc-argo/aligned-seq-qc:aligned-seq-qc.${params.container_version ?: version}\"\n  cpus params.cpus\n  memory \"${params.mem} GB\"\n\n  input:\n    path seq\n    path ref_genome_gz\n    path ref_genome_gz_secondary_file\n    val dependencies\n\n  output:\n    path \"*.qc_metrics.tgz\", emit: metrics\n\n  script:\n    \"\"\"\n    aligned-seq-qc.py -s ${seq} \\\n                      -r ${ref_genome_gz} \\\n                      -n ${params.cpus}\n    \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "    \"\"\"\n    aligned-seq-qc.py -s ${seq} \\\n                      -r ${ref_genome_gz} \\\n                      -n ${params.cpus}\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "seq",
            "ref_genome_gz",
            "ref_genome_gz_secondary_file",
            "dependencies"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "container \"quay.io/icgc-argo/aligned-seq-qc:aligned-seq-qc.${params.container_version ?: version}\"",
            "cpus params.cpus",
            "memory \"${params.mem} GB\""
        ],
        "when": "",
        "stub": ""
    },
    "songPublish": {
        "name_process": "songPublish",
        "string_process": "\nprocess songPublish {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/song-client:${params.container_version}\"\n\n    tag \"${analysis_id}\"\n    \n    input:\n        val study_id\n        val analysis_id\n\n    output:\n        val analysis_id, emit: analysis_id\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing publish -a  ${analysis_id}\n        \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export CLIENT_SERVER_URL=${params.song_url}\n        export CLIENT_STUDY_ID=${study_id}\n        export CLIENT_ACCESS_TOKEN=${accessToken}\n\n        sing publish -a  ${analysis_id}\n        \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [
            "Nursing"
        ],
        "tools_url": [
            "https://bio.tools/Nursing"
        ],
        "tools_dico": [
            {
                "name": "Nursing",
                "uri": "https://bio.tools/Nursing",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Medical informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Critical care medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3376",
                            "term": "Medicines research and development"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Biomedical informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Healthcare informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Health informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Health and disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3063",
                            "term": "Clinical informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Acute medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Emergency medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3403",
                            "term": "Intensive care medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3376",
                            "term": "Drug discovery and development"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The use and abuse of credentials.\n\nA Guide to Nursing Credentials and Degrees.\n\nComplete List of Common Nursing Certifications.\n\nFrom LPN to MSN, the jumble of letters following a nurse's name can be confusing. Learn what these nursing credentials mean and how they should be listed.\n\nView 183 commonly recognized nursing certifications along with links to their certifying organizations.\n\nThe confusing nature of nursing credentials has led to widespread use of the term \u201calphabet soup.\u201d The letters that follow a nurse\u2019s name can be perplexing to professionals in the medical field, and especially to patients and families. To solve this problem, nursing credentials need to be displayed properly.\n\nNever fear. Nurse.org has compiled an alphabetical list of 183 different nursing certifications along with the appropriate acronyms and links to their certifying organizations",
                "homepage": "https://online.alvernia.edu/program-resources/nursing-credentials/"
            }
        ],
        "inputs": [
            "study_id",
            "analysis_id"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "maxRetries params.max_retries",
            "errorStrategy { sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long); return params.max_retries ? 'retry' : 'finish' }",
            "pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]",
            "cpus params.cpus",
            "memory \"${params.mem} GB\"",
            "container \"overture/song-client:${params.container_version}\"",
            "tag \"${analysis_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "readGroupUBamQC": {
        "name_process": "readGroupUBamQC",
        "string_process": "\nprocess readGroupUBamQC {\n  container \"quay.io/icgc-argo/read-group-ubam-qc:read-group-ubam-qc.${params.container_version ?: version}\"\n  cpus params.cpus\n  memory \"${params.mem} GB\"\n\n  input:\n    path ubam\n\n  output:\n    path \"*.ubam_qc_metrics.tgz\", emit: ubam_qc_metrics\n    path \"*.extra_info.json\", emit: ubam_info_json                            \n\n  script:\n    \"\"\"\n    read-group-ubam-qc.py -b ${ubam} -m ${(int) (params.mem * 1000)}\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    read-group-ubam-qc.py -b ${ubam} -m ${(int) (params.mem * 1000)}\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ubam"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "container \"quay.io/icgc-argo/read-group-ubam-qc:read-group-ubam-qc.${params.container_version ?: version}\"",
            "cpus params.cpus",
            "memory \"${params.mem} GB\""
        ],
        "when": "",
        "stub": ""
    },
    "payloadGenSeqExperiment": {
        "name_process": "payloadGenSeqExperiment",
        "string_process": "\nprocess payloadGenSeqExperiment {\n  container \"${params.container ?: container[params.container_registry ?: default_container_registry]}:${params.container_version ?: version}\"\n  publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir\n\n  cpus params.cpus\n  memory \"${params.mem} GB\"\n\n  input:\n    path experiment_info_tsv\n    path read_group_info_tsv\n    path file_info_tsv\n    path extra_info_tsv\n\n  output:\n    path \"*.sequencing_experiment.payload.json\", emit: payload\n\n  script:\n    args_experiment_info_tsv = !experiment_info_tsv.name.startsWith(\"NO_FILE\") ? \"-x ${experiment_info_tsv}\" : \"\"\n    args_read_group_info_tsv = !read_group_info_tsv.name.startsWith(\"NO_FILE\") ? \"-r ${read_group_info_tsv}\" : \"\"\n    args_file_info_tsv = !file_info_tsv.name.startsWith(\"NO_FILE\") ? \"-f ${file_info_tsv}\" : \"\"\n    args_extra_info_tsv = !extra_info_tsv.name.startsWith(\"NO_FILE\") ? \"-e ${extra_info_tsv}\" : \"\"\n\n    \"\"\"\n    main.py \\\n         ${args_experiment_info_tsv} \\\n         ${args_read_group_info_tsv} \\\n         ${args_file_info_tsv} \\\n         ${args_extra_info_tsv}\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "    args_experiment_info_tsv = !experiment_info_tsv.name.startsWith(\"NO_FILE\") ? \"-x ${experiment_info_tsv}\" : \"\"\n    args_read_group_info_tsv = !read_group_info_tsv.name.startsWith(\"NO_FILE\") ? \"-r ${read_group_info_tsv}\" : \"\"\n    args_file_info_tsv = !file_info_tsv.name.startsWith(\"NO_FILE\") ? \"-f ${file_info_tsv}\" : \"\"\n    args_extra_info_tsv = !extra_info_tsv.name.startsWith(\"NO_FILE\") ? \"-e ${extra_info_tsv}\" : \"\"\n\n    \"\"\"\n    main.py \\\n         ${args_experiment_info_tsv} \\\n         ${args_read_group_info_tsv} \\\n         ${args_file_info_tsv} \\\n         ${args_extra_info_tsv}\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "experiment_info_tsv",
            "read_group_info_tsv",
            "file_info_tsv",
            "extra_info_tsv"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "container \"${params.container ?: container[params.container_registry ?: default_container_registry]}:${params.container_version ?: version}\"",
            "publishDir \"${params.publish_dir}/${task.process.replaceAll(':', '_')}\", mode: \"copy\", enabled: params.publish_dir",
            "cpus params.cpus",
            "memory \"${params.mem} GB\""
        ],
        "when": "",
        "stub": ""
    },
    "scoreUpload": {
        "name_process": "scoreUpload",
        "string_process": "\nprocess scoreUpload {\n    maxRetries params.max_retries\n    errorStrategy {\n        sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long);                                           \n        return params.max_retries ? 'retry' : 'finish'\n    }\n\n    pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]\n    \n    cpus params.cpus\n    memory \"${params.mem} GB\"\n \n    container \"overture/score:${params.container_version}\"\n\n    tag \"${analysis_id}\"\n\n    input:\n        val analysis_id\n        path manifest\n        path upload\n\n    output:\n        val analysis_id, emit: ready_to_publish\n\n    script:\n        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export METADATA_URL=${params.song_url}\n        export STORAGE_URL=${params.score_url}\n        export TRANSPORT_PARALLEL=${params.cpus}\n        export TRANSPORT_MEMORY=${params.transport_mem}\n        export ACCESSTOKEN=${accessToken}\n        \n        score-client upload --manifest ${manifest}\n        \"\"\"\n}",
        "nb_lignes_process": 35,
        "string_script": "        accessToken = params.api_token ? params.api_token : \"`cat /tmp/rdpc_secret/secret`\"\n        \"\"\"\n        export METADATA_URL=${params.song_url}\n        export STORAGE_URL=${params.score_url}\n        export TRANSPORT_PARALLEL=${params.cpus}\n        export TRANSPORT_MEMORY=${params.transport_mem}\n        export ACCESSTOKEN=${accessToken}\n        \n        score-client upload --manifest ${manifest}\n        \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "analysis_id",
            "manifest",
            "upload"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "icgc-argo-workflows__dna-seq-processing-wfs",
        "directive": [
            "maxRetries params.max_retries",
            "errorStrategy { sleep(Math.pow(2, task.attempt) * params.first_retry_wait_time * 1000 as long); return params.max_retries ? 'retry' : 'finish' }",
            "pod = [secret: workflow.runName + \"-secret\", mountPath: \"/tmp/rdpc_secret\"]",
            "cpus params.cpus",
            "memory \"${params.mem} GB\"",
            "container \"overture/score:${params.container_version}\"",
            "tag \"${analysis_id}\""
        ],
        "when": "",
        "stub": ""
    }
}