{
    "KLEBORATE": {
        "name_process": "KLEBORATE",
        "string_process": "\nprocess KLEBORATE {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/kleborate:2.1.0--pyhdfd78af_1' :\n        'quay.io/biocontainers/kleborate:2.1.0--pyhdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(fastas)\n\n    output:\n    tuple val(meta), path(\"*.txt\"), emit: txt\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\",emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    kleborate \\\\\n        $options.args \\\\\n        --outfile ${prefix}.results.txt \\\\\n        --assemblies $fastas\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kleborate: \\$( echo \\$(kleborate --version | sed 's/Kleborate v//;'))\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    kleborate \\\\\n        $options.args \\\\\n        --outfile ${prefix}.results.txt \\\\\n        --assemblies $fastas\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kleborate: \\$( echo \\$(kleborate --version | sed 's/Kleborate v//;'))\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fastas"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/kleborate:2.1.0--pyhdfd78af_1' : 'quay.io/biocontainers/kleborate:2.1.0--pyhdfd78af_1' }\""
        ],
        "when": "",
        "stub": ""
    },
    "PIRATE": {
        "name_process": "PIRATE",
        "string_process": "\nprocess PIRATE {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pirate%3A1.0.4--hdfd78af_2' :\n        'quay.io/biocontainers/pirate:1.0.4--hdfd78af_2' }\"\n\n    input:\n    tuple val(meta), path(gff)\n\n    output:\n    tuple val(meta), path(\"results/*\")                        , emit: results\n    tuple val(meta), path(\"core-genome.aln.gz\")               , emit: aln, optional: true\n    tuple val(meta), path(\"results/gene_presence_absence.csv\"), emit: csv, optional: true\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    find . -name \"*.gff.gz\" | sed 's/.gz//' | xargs -I {} bash -c 'gzip -cdf {}.gz > {}'\n\n    PIRATE \\\\\n        $options.args \\\\\n        --align \\\\\n        --threads $task.cpus \\\\\n        --input ./ \\\\\n        --output results/\n    PIRATE_to_roary.pl -i results/PIRATE.*.tsv -o results/gene_presence_absence.csv\n    find . -name \"*.fasta\" | xargs -I {} -P $task.cpus -n 1 gzip {}\n\n    # Only copy files if they exist\n    if [[ -f \"results/core_alignment.fasta.gz\" ]]; then\n        cp results/core_alignment.fasta.gz ./core-genome.aln.gz\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pirate: \\$( echo \\$( PIRATE --version 2>&1) | sed 's/PIRATE //' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 46,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    find . -name \"*.gff.gz\" | sed 's/.gz//' | xargs -I {} bash -c 'gzip -cdf {}.gz > {}'\n\n    PIRATE \\\\\n        $options.args \\\\\n        --align \\\\\n        --threads $task.cpus \\\\\n        --input ./ \\\\\n        --output results/\n    PIRATE_to_roary.pl -i results/PIRATE.*.tsv -o results/gene_presence_absence.csv\n    find . -name \"*.fasta\" | xargs -I {} -P $task.cpus -n 1 gzip {}\n\n    # Only copy files if they exist\n    if [[ -f \"results/core_alignment.fasta.gz\" ]]; then\n        cp results/core_alignment.fasta.gz ./core-genome.aln.gz\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pirate: \\$( echo \\$( PIRATE --version 2>&1) | sed 's/PIRATE //' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [
            "PIRATE"
        ],
        "tools_url": [
            "https://bio.tools/PIRATE"
        ],
        "tools_dico": [
            {
                "name": "PIRATE",
                "uri": "https://bio.tools/PIRATE",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0623",
                            "term": "Gene and protein families"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype and phenotype"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0199",
                            "term": "Genetic variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0114",
                            "term": "Gene structure"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0623",
                            "term": "Genes, gene family or system"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype and phenotype resources"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype-phenotype analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype-phenotype"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0199",
                            "term": "DNA variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0114",
                            "term": "Gene features"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3359",
                                    "term": "Splitting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3432",
                                    "term": "Clustering"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3359",
                                    "term": "File splitting"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "PIRATE is a fast and scalable pangenomics toolbox for clustering diverged orthologues in bacteria.",
                "homepage": "https://github.com/SionBayliss/PIRATE"
            }
        ],
        "inputs": [
            "meta",
            "gff"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/pirate%3A1.0.4--hdfd78af_2' : 'quay.io/biocontainers/pirate:1.0.4--hdfd78af_2' }\""
        ],
        "when": "",
        "stub": ""
    },
    "HPSUISSERO": {
        "name_process": "HPSUISSERO",
        "string_process": "\nprocess HPSUISSERO {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/hpsuissero%3A1.0.1--hdfd78af_0' :\n        'quay.io/biocontainers/hpsuissero:1.0.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"*.{log,err}\"            , emit: logs, optional: true\n    path \".command.*\"             , emit: nf_logs\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    HpsuisSero.sh \\\\\n        -i $fasta_name \\\\\n        -o ./ \\\\\n        -s $prefix \\\\\n        -x fasta \\\\\n        -t $task.cpus\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        hpsuissero: $VERSION\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 42,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    HpsuisSero.sh \\\\\n        -i $fasta_name \\\\\n        -o ./ \\\\\n        -s $prefix \\\\\n        -x fasta \\\\\n        -t $task.cpus\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        hpsuissero: $VERSION\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 20,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/hpsuissero%3A1.0.1--hdfd78af_0' : 'quay.io/biocontainers/hpsuissero:1.0.1--hdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "ANNOTATE_GENOME": {
        "name_process": "ANNOTATE_GENOME",
        "string_process": "\nprocess ANNOTATE_GENOME {\n                                                                               \n    tag \"${meta.id}\"\n    label 'annotate_genome'\n\n    publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    input:\n    tuple val(meta), path(genome_size), path(fasta), path(total_contigs), path(prokka_proteins), path(prodigal_tf)\n\n    output:\n    tuple val(meta), path(\"${meta.id}.{ffn,ffn.gz}\"), path(\"${meta.id}.{faa,faa.gz}\"), emit: annotations\n    path \"results/*\", emit: results\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    shell:\n    contig_count = total_contigs.getName().replace('total_contigs_', '')\n    genus = \"Genus\"\n    species = \"species\"\n    proteins = \"\"\n    if (prokka_proteins.getName() != 'EMPTY_PROTEINS') {\n        proteins = \"--proteins ${prokka_proteins}\"\n        proteins_name = prokka_proteins.getName()\n        if(proteins_name.contains(\"-\")) {\n            genus = proteins_name.split('-')[0].capitalize()\n            species = proteins_name.split('-')[1]\n        } else {\n            genus = proteins_name.capitalize()\n            species = \"spp.\"\n        }\n    }\n\n    prodigal = \"\"\n    if (prodigal_tf.getName() != 'EMPTY_TF' && !params.skip_prodigal_tf) {\n        prodigal = \"--prodigaltf ${prodigal_tf}\"\n    }\n\n    compliant = params.compliant ? \"--compliant\" : \"\"\n    locustag = \"--locustag ${meta.id}\"\n    renamed = false\n                                      \n    if (\"gnl|${params.centre}|${meta.id}_${contig_count}\".length() > 37) {\n        locustag = \"\"\n        compliant = \"--compliant\"\n        renamed = true\n    }\n    addgenes = params.nogenes ? \"\" : \"--addgenes\"\n    addmrna = params.addmrna ? \"--addmrna\" : \"\"\n    rawproduct = params.rawproduct ? \"--rawproduct\" : \"\"\n    cdsrnaolap = params.cdsrnaolap ? \"--cdsrnaolap\" : \"\"\n    norrna = params.norrna ? \"--norrna\" : \"\"\n    notrna = params.notrna ? \"--notrna\" : \"\"\n    rnammer = params.rnammer ? \"--rnammer\" : \"\"\n    rfam = params.rnammer ? \"--rfam\" : \"\"\n    '''\n    if [ \"!{renamed}\" == \"true\" ]; then\n        echo \"Original sample name (!{meta.id}) not used due to creating a contig ID >37 characters\"\n    fi\n\n    if [[ \"!{params.skip_compression}\" == \"false\" ]]; then\n        gunzip -c !{fasta} > !{meta.id}.fna\n    fi\n\n    prokka --outdir results \\\n        --force \\\n        --prefix '!{meta.id}' \\\n        --genus '!{genus}' \\\n        --species '!{species}' \\\n        --evalue '!{params.prokka_evalue}' \\\n        --coverage !{params.prokka_coverage} \\\n        --cpus !{task.cpus} \\\n        --centre '!{params.centre}' \\\n        --mincontiglen !{params.min_contig_len} \\\n        !{locustag} \\\n        !{prodigal} \\\n        !{addgenes} \\\n        !{compliant} \\\n        !{proteins} \\\n        !{rawproduct} \\\n        !{cdsrnaolap} \\\n        !{addmrna} \\\n        !{norrna} \\\n        !{notrna} \\\n        !{rnammer} \\\n        !{rfam} \\\n        !{meta.id}.fna\n    mv results/!{meta.id}.err ./\n    mv results/!{meta.id}.log ./\n\n    if [[ \"!{params.skip_compression}\" == \"false\" ]]; then\n        find results/ -type f | \\\n            grep -v -E \"\\\\.err$|\\\\.log$|\\\\.tsv$|\\\\.txt$\"| \\\n            xargs -I {} pigz -n --best -p !{task.cpus} {}\n\n        # Files passed to other modules\n        ln -s results/!{meta.id}.faa.gz !{meta.id}.faa.gz\n        ln -s results/!{meta.id}.ffn.gz !{meta.id}.ffn.gz\n    else\n        # Files passed to other modules\n        ln -s results/!{meta.id}.faa !{meta.id}.faa\n        ln -s results/!{meta.id}.ffn !{meta.id}.ffn\n    fi\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        prokka:  $(echo $(prokka --version 2>&1) | sed 's/prokka //')\n    END_VERSIONS\n    '''\n}",
        "nb_lignes_process": 112,
        "string_script": "    contig_count = total_contigs.getName().replace('total_contigs_', '')\n    genus = \"Genus\"\n    species = \"species\"\n    proteins = \"\"\n    if (prokka_proteins.getName() != 'EMPTY_PROTEINS') {\n        proteins = \"--proteins ${prokka_proteins}\"\n        proteins_name = prokka_proteins.getName()\n        if(proteins_name.contains(\"-\")) {\n            genus = proteins_name.split('-')[0].capitalize()\n            species = proteins_name.split('-')[1]\n        } else {\n            genus = proteins_name.capitalize()\n            species = \"spp.\"\n        }\n    }\n\n    prodigal = \"\"\n    if (prodigal_tf.getName() != 'EMPTY_TF' && !params.skip_prodigal_tf) {\n        prodigal = \"--prodigaltf ${prodigal_tf}\"\n    }\n\n    compliant = params.compliant ? \"--compliant\" : \"\"\n    locustag = \"--locustag ${meta.id}\"\n    renamed = false\n                                      \n    if (\"gnl|${params.centre}|${meta.id}_${contig_count}\".length() > 37) {\n        locustag = \"\"\n        compliant = \"--compliant\"\n        renamed = true\n    }\n    addgenes = params.nogenes ? \"\" : \"--addgenes\"\n    addmrna = params.addmrna ? \"--addmrna\" : \"\"\n    rawproduct = params.rawproduct ? \"--rawproduct\" : \"\"\n    cdsrnaolap = params.cdsrnaolap ? \"--cdsrnaolap\" : \"\"\n    norrna = params.norrna ? \"--norrna\" : \"\"\n    notrna = params.notrna ? \"--notrna\" : \"\"\n    rnammer = params.rnammer ? \"--rnammer\" : \"\"\n    rfam = params.rnammer ? \"--rfam\" : \"\"\n    '''\n    if [ \"!{renamed}\" == \"true\" ]; then\n        echo \"Original sample name (!{meta.id}) not used due to creating a contig ID >37 characters\"\n    fi\n\n    if [[ \"!{params.skip_compression}\" == \"false\" ]]; then\n        gunzip -c !{fasta} > !{meta.id}.fna\n    fi\n\n    prokka --outdir results \\\n        --force \\\n        --prefix '!{meta.id}' \\\n        --genus '!{genus}' \\\n        --species '!{species}' \\\n        --evalue '!{params.prokka_evalue}' \\\n        --coverage !{params.prokka_coverage} \\\n        --cpus !{task.cpus} \\\n        --centre '!{params.centre}' \\\n        --mincontiglen !{params.min_contig_len} \\\n        !{locustag} \\\n        !{prodigal} \\\n        !{addgenes} \\\n        !{compliant} \\\n        !{proteins} \\\n        !{rawproduct} \\\n        !{cdsrnaolap} \\\n        !{addmrna} \\\n        !{norrna} \\\n        !{notrna} \\\n        !{rnammer} \\\n        !{rfam} \\\n        !{meta.id}.fna\n    mv results/!{meta.id}.err ./\n    mv results/!{meta.id}.log ./\n\n    if [[ \"!{params.skip_compression}\" == \"false\" ]]; then\n        find results/ -type f | \\\n            grep -v -E \"\\\\.err$|\\\\.log$|\\\\.tsv$|\\\\.txt$\"| \\\n            xargs -I {} pigz -n --best -p !{task.cpus} {}\n\n        # Files passed to other modules\n        ln -s results/!{meta.id}.faa.gz !{meta.id}.faa.gz\n        ln -s results/!{meta.id}.ffn.gz !{meta.id}.ffn.gz\n    else\n        # Files passed to other modules\n        ln -s results/!{meta.id}.faa !{meta.id}.faa\n        ln -s results/!{meta.id}.ffn !{meta.id}.ffn\n    fi\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        prokka:  $(echo $(prokka --version 2>&1) | sed 's/prokka //')\n    END_VERSIONS\n    '''",
        "nb_lignes_script": 92,
        "language_script": "bash",
        "tools": [
            "JSpecies",
            "ProteinsPlus",
            "RNAmmer",
            "Rfam",
            "Prokka"
        ],
        "tools_url": [
            "https://bio.tools/jspecies",
            "https://bio.tools/proteinsplus",
            "https://bio.tools/rnammer",
            "https://bio.tools/rfam",
            "https://bio.tools/prokka"
        ],
        "tools_dico": [
            {
                "name": "JSpecies",
                "uri": "https://bio.tools/jspecies",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3299",
                            "term": "Evolutionary biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0797",
                            "term": "Comparative genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3299",
                            "term": "Evolution"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3209",
                                    "term": "Genome comparison"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0362",
                                    "term": "Genome annotation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3209",
                                    "term": "Genomic region matching"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An easy to use, biologist-centric software designed to measure the probability if two genomes belonging to the same species or not.",
                "homepage": "http://www.imedea.uib-csic.es/jspecies/index.html"
            },
            {
                "name": "ProteinsPlus",
                "uri": "https://bio.tools/proteinsplus",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Structure analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3534",
                            "term": "Protein binding sites"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Structural bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Biomolecular structure"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0339",
                                    "term": "Structure database search"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2575",
                                    "term": "Binding site prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2406",
                                    "term": "Protein structure analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0570",
                                    "term": "Structure visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0321",
                                    "term": "Protein structure validation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2949",
                                    "term": "Protein-protein interaction analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2575",
                                    "term": "Protein binding site prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2575",
                                    "term": "Protein binding site detection"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2406",
                                    "term": "Structure analysis (protein)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0570",
                                    "term": "Structure rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0321",
                                    "term": "Protein model validation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2949",
                                    "term": "Protein interaction analysis"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_1460",
                                "term": "Protein structure"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_1772",
                                "term": "Score"
                            },
                            {
                                "uri": "http://edamontology.org/data_0897",
                                "term": "Protein property"
                            },
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_1460",
                                "term": "Protein structure"
                            },
                            {
                                "uri": "http://edamontology.org/data_2992",
                                "term": "Protein structure image"
                            }
                        ]
                    }
                ],
                "description": "It supports various commonly required tasks of structure-based molecular modeling. It provides functionalities for structure quality assessment , hydrogen placement, the search for alternative conformations, the generation of 2D-interaction diagrams, protein-protein interface classification as well as automatic pocket detection and druggablity assessment.",
                "homepage": "http://proteinsplus.zbh.uni-hamburg.de"
            },
            {
                "name": "RNAmmer",
                "uri": "https://bio.tools/rnammer",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0160",
                            "term": "Sequence sites, features and motifs"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0415",
                                    "term": "Nucleic acid feature detection"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0415",
                                    "term": "Sequence feature detection (nucleic acid)"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_1277",
                                "term": "Protein features"
                            }
                        ]
                    }
                ],
                "description": "Prediction of 5S/8S, 16S/18S, and 23S/28S ribosomal RNA in full genome sequences.",
                "homepage": "http://cbs.dtu.dk/services/RNAmmer/"
            },
            {
                "name": "Rfam",
                "uri": "https://bio.tools/rfam",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0099",
                            "term": "RNA"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0623",
                            "term": "Gene and protein families"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0659",
                            "term": "Functional, regulatory and non-coding RNA"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0623",
                            "term": "Genes, gene family or system"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0294",
                                    "term": "Structure-based sequence alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0362",
                                    "term": "Genome annotation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0294",
                                    "term": "Sequence alignment (structure-based)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Rfam database is a collection of RNA families, each represented by multiple sequence alignments, consensus secondary structures and covariance models (CMs)",
                "homepage": "http://rfam.org"
            },
            {
                "name": "Prokka",
                "uri": "https://bio.tools/prokka",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "Coding region prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0362",
                                    "term": "Genome annotation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "ORF prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "ORF finding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene finding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene calling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Software tool to annotate bacterial, archaeal and viral genomes quickly and produce standards-compliant output files.",
                "homepage": "https://github.com/tseemann/prokka"
            }
        ],
        "inputs": [
            "meta",
            "genome_size",
            "fasta",
            "total_contigs",
            "prokka_proteins",
            "prodigal_tf"
        ],
        "nb_inputs": 6,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"${meta.id}\"",
            "label 'annotate_genome'",
            "publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }"
        ],
        "when": "",
        "stub": ""
    },
    "HICAP": {
        "name_process": "HICAP",
        "string_process": "\nprocess HICAP {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/hicap:1.0.3--py_0' :\n        'quay.io/biocontainers/hicap:1.0.3--py_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path database_dir\n    path model_fp\n\n    output:\n    tuple val(meta), path(\"*.gbk\"), emit: gbk, optional: true\n    tuple val(meta), path(\"*.svg\"), emit: svg, optional: true\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\",emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def database_args = database_dir ? \"--database_dir ${database_dir}\" : \"\"\n    def model_args = model_fp ? \"--model_fp ${model_fp}\" : \"\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    hicap \\\\\n        --query_fp $fasta_name \\\\\n        $database_args \\\\\n        $model_args \\\\\n        $options.args \\\\\n        --threads $task.cpus \\\\\n        --debug \\\\\n        -o ./\n\n    if [ ! -f ${meta.id}.tsv ]; then\n        echo \"isolate<TAB>predicted_serotype<TAB>attributes<TAB>genes_identified<TAB>locus_location<TAB>region_I_genes<TAB>region_II_genes<TAB>region_III_genes<TAB>IS1016_hits\" | sed 's/<TAB>/\\t/g' > ${meta.id}.tsv\n        echo \"${meta.id}<TAB>cap_not_found<TAB>-<TAB>-<TAB>-<TAB>-<TAB>-<TAB>-<TAB>-\" | sed 's/<TAB>/\\t/g' >> ${meta.id}.tsv\n    else\n        sed -i 's/#isolate/isolate/' ${meta.id}.tsv\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        hicap: \\$( echo \\$( hicap --version 2>&1 ) | sed 's/^.*hicap //' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 56,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def database_args = database_dir ? \"--database_dir ${database_dir}\" : \"\"\n    def model_args = model_fp ? \"--model_fp ${model_fp}\" : \"\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    hicap \\\\\n        --query_fp $fasta_name \\\\\n        $database_args \\\\\n        $model_args \\\\\n        $options.args \\\\\n        --threads $task.cpus \\\\\n        --debug \\\\\n        -o ./\n\n    if [ ! -f ${meta.id}.tsv ]; then\n        echo \"isolate<TAB>predicted_serotype<TAB>attributes<TAB>genes_identified<TAB>locus_location<TAB>region_I_genes<TAB>region_II_genes<TAB>region_III_genes<TAB>IS1016_hits\" | sed 's/<TAB>/\\t/g' > ${meta.id}.tsv\n        echo \"${meta.id}<TAB>cap_not_found<TAB>-<TAB>-<TAB>-<TAB>-<TAB>-<TAB>-<TAB>-\" | sed 's/<TAB>/\\t/g' >> ${meta.id}.tsv\n    else\n        sed -i 's/#isolate/isolate/' ${meta.id}.tsv\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        hicap: \\$( echo \\$( hicap --version 2>&1 ) | sed 's/^.*hicap //' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 30,
        "language_script": "bash",
        "tools": [
            "HiCapTools"
        ],
        "tools_url": [
            "https://bio.tools/hicaptools"
        ],
        "tools_dico": [
            {
                "name": "HiCapTools",
                "uri": "https://bio.tools/hicaptools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2419",
                                    "term": "Primer and probe design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2419",
                                    "term": "Primer and probe prediction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Probe design and proximity detection for targeted chromosome conformation capture applications.",
                "homepage": "https://github.com/sahlenlab/HiCapTools"
            }
        ],
        "inputs": [
            "meta",
            "fasta",
            "database_dir",
            "model_fp"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/hicap:1.0.3--py_0' : 'quay.io/biocontainers/hicap:1.0.3--py_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "CUSTOM_DUMPSOFTWAREVERSIONS": {
        "name_process": "CUSTOM_DUMPSOFTWAREVERSIONS",
        "string_process": "\nprocess CUSTOM_DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n                                                                                                  \n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\", emit: yml\n    path \"software_versions_mqc.yml\", emit: mqc_yml\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n    import datetime\n    import yaml\n    import platform\n    from textwrap import dedent\n\n    def _make_versions_html(versions):\n        html = [\n            dedent(\n                '''\\\\\n                <style>\n                #nf-core-versions tbody:nth-child(even) {\n                    background-color: #f2f2f2;\n                }\n                </style>\n                <table class=\"table\" style=\"width:100%\" id=\"nf-core-versions\">\n                    <thead>\n                        <tr>\n                            <th> Process Name </th>\n                            <th> Software </th>\n                            <th> Version  </th>\n                        </tr>\n                    </thead>\n                '''\n            )\n        ]\n        for process, tmp_versions in sorted(versions.items()):\n            html.append(\"<tbody>\")\n            for i, (tool, version) in enumerate(sorted(tmp_versions.items())):\n                html.append(\n                    dedent(\n                        f'''\\\\\n                        <tr>\n                            <td><samp>{process if (i == 0) else ''}</samp></td>\n                            <td><samp>{tool}</samp></td>\n                            <td><samp>{version}</samp></td>\n                        </tr>\n                        '''\n                    )\n                )\n            html.append(\"</tbody>\")\n        html.append(\"</table>\")\n        return \"\\\\n\".join(html)\n\n    module_versions = {}\n    module_versions[\"custom_dumpsoftwareversions\"] = {\n        'python': platform.python_version(),\n        'yaml': yaml.__version__\n    }\n\n    with open(\"$versions\") as f:\n        workflow_versions = yaml.load(f, Loader=yaml.BaseLoader) | module_versions\n\n    workflow_versions[\"Workflow\"] = {\n        \"Nextflow\": \"$workflow.nextflow.version\",\n        \"$workflow.manifest.name\": \"$workflow.manifest.version\",\n        \"date\": datetime.datetime.now()\n    }\n\n    versions_mqc = {\n        'id': 'software_versions',\n        'section_name': '${workflow.manifest.name} Software Versions',\n        'section_href': 'https://github.com/${workflow.manifest.name}',\n        'plot_type': 'html',\n        'description': 'are collected at run time from the software output.',\n        'data': _make_versions_html(workflow_versions)\n    }\n\n    with open(\"software_versions.yml\", 'w') as f:\n        yaml.dump(workflow_versions, f, default_flow_style=False)\n    with open(\"software_versions_mqc.yml\", 'w') as f:\n        yaml.dump(versions_mqc, f, default_flow_style=False)\n\n    with open('versions.yml', 'w') as f:\n        yaml.dump(module_versions, f, default_flow_style=False)\n    \"\"\"\n}",
        "nb_lignes_process": 100,
        "string_script": "    \"\"\"\n    #!/usr/bin/env python\n    import datetime\n    import yaml\n    import platform\n    from textwrap import dedent\n\n    def _make_versions_html(versions):\n        html = [\n            dedent(\n                '''\\\\\n                <style>\n                #nf-core-versions tbody:nth-child(even) {\n                    background-color: #f2f2f2;\n                }\n                </style>\n                <table class=\"table\" style=\"width:100%\" id=\"nf-core-versions\">\n                    <thead>\n                        <tr>\n                            <th> Process Name </th>\n                            <th> Software </th>\n                            <th> Version  </th>\n                        </tr>\n                    </thead>\n                '''\n            )\n        ]\n        for process, tmp_versions in sorted(versions.items()):\n            html.append(\"<tbody>\")\n            for i, (tool, version) in enumerate(sorted(tmp_versions.items())):\n                html.append(\n                    dedent(\n                        f'''\\\\\n                        <tr>\n                            <td><samp>{process if (i == 0) else ''}</samp></td>\n                            <td><samp>{tool}</samp></td>\n                            <td><samp>{version}</samp></td>\n                        </tr>\n                        '''\n                    )\n                )\n            html.append(\"</tbody>\")\n        html.append(\"</table>\")\n        return \"\\\\n\".join(html)\n\n    module_versions = {}\n    module_versions[\"custom_dumpsoftwareversions\"] = {\n        'python': platform.python_version(),\n        'yaml': yaml.__version__\n    }\n\n    with open(\"$versions\") as f:\n        workflow_versions = yaml.load(f, Loader=yaml.BaseLoader) | module_versions\n\n    workflow_versions[\"Workflow\"] = {\n        \"Nextflow\": \"$workflow.nextflow.version\",\n        \"$workflow.manifest.name\": \"$workflow.manifest.version\",\n        \"date\": datetime.datetime.now()\n    }\n\n    versions_mqc = {\n        'id': 'software_versions',\n        'section_name': '${workflow.manifest.name} Software Versions',\n        'section_href': 'https://github.com/${workflow.manifest.name}',\n        'plot_type': 'html',\n        'description': 'are collected at run time from the software output.',\n        'data': _make_versions_html(workflow_versions)\n    }\n\n    with open(\"software_versions.yml\", 'w') as f:\n        yaml.dump(workflow_versions, f, default_flow_style=False)\n    with open(\"software_versions_mqc.yml\", 'w') as f:\n        yaml.dump(versions_mqc, f, default_flow_style=False)\n\n    with open('versions.yml', 'w') as f:\n        yaml.dump(module_versions, f, default_flow_style=False)\n    \"\"\"",
        "nb_lignes_script": 76,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "versions"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "label 'process_low'",
            "publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' : 'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MASH_DIST": {
        "name_process": "MASH_DIST",
        "string_process": "\nprocess MASH_DIST {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mash:2.3--he348c14_1' :\n        'quay.io/biocontainers/mash:2.3--he348c14_1' }\"\n\n    input:\n    tuple val(meta), path(query)\n    path reference\n\n    output:\n    tuple val(meta), path(\"*.txt\"), emit: dist\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    echo \"reference<TAB>query<TAB>distance<TAB>p-value<TAB>shared-hashes\" | sed 's/<TAB>/\\t/g' > ${prefix}-dist.txt\n    mash \\\\\n        dist \\\\\n        -p $task.cpus \\\\\n        $options.args \\\\\n        $reference \\\\\n        $query >> ${prefix}-dist.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mash: \\$(echo \\$(mash 2>&1) | sed 's/^.*Mash version //;s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 37,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    echo \"reference<TAB>query<TAB>distance<TAB>p-value<TAB>shared-hashes\" | sed 's/<TAB>/\\t/g' > ${prefix}-dist.txt\n    mash \\\\\n        dist \\\\\n        -p $task.cpus \\\\\n        $options.args \\\\\n        $reference \\\\\n        $query >> ${prefix}-dist.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mash: \\$(echo \\$(mash 2>&1) | sed 's/^.*Mash version //;s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [
            "Mash",
            "tqDist"
        ],
        "tools_url": [
            "https://bio.tools/mash",
            "https://bio.tools/tqdist"
        ],
        "tools_dico": [
            {
                "name": "Mash",
                "uri": "https://bio.tools/mash",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2533",
                            "term": "DNA mutation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix generation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Phylogenetic distance matrix generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Fast genome and metagenome distance estimation using MinHash.",
                "homepage": "https://github.com/marbl/mash"
            },
            {
                "name": "tqDist",
                "uri": "https://bio.tools/tqdist",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0325",
                                    "term": "Phylogenetic tree comparison"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Computation of the triplet- and Quartet-distance between evolutionary trees.",
                "homepage": "http://users-cs.au.dk/cstorm/software/tqdist/"
            }
        ],
        "inputs": [
            "meta",
            "query",
            "reference"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/mash:2.3--he348c14_1' : 'quay.io/biocontainers/mash:2.3--he348c14_1' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MERLIN_DIST": {
        "name_process": "MERLIN_DIST",
        "string_process": "\nprocess MERLIN_DIST {\n                                                     \n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mash:2.3--he348c14_1' :\n        'quay.io/biocontainers/mash:2.3--he348c14_1' }\"\n\n    input:\n    tuple val(meta), path(query), path(reads)\n    path reference\n\n    output:\n    tuple val(meta), path(\"*.txt\"), emit: dist\n    tuple val(meta), path(query), path(\"escherichia.*\")   , emit: escherichia, optional: true\n    tuple val(meta), path(query), path(\"haemophilus.*\")   , emit: haemophilus, optional: true\n    tuple val(meta), path(query), path(\"klebsiella.*\")    , emit: klebsiella, optional: true\n    tuple val(meta), path(query), path(\"legionella.*\")    , emit: legionella, optional: true\n    tuple val(meta), path(query), path(\"listeria.*\")      , emit: listeria, optional: true\n    tuple val(meta), path(query), path(\"mycobacterium.*\") , emit: mycobacterium, optional: true\n    tuple val(meta), path(reads), path(\"mycobacterium.*\") , emit: mycobacterium_fq, optional: true\n    tuple val(meta), path(query), path(\"neisseria.*\")     , emit: neisseria, optional: true\n    tuple val(meta), path(query), path(\"salmonella.*\")    , emit: salmonella, optional: true\n    tuple val(meta), path(query), path(\"staphylococcus.*\"), emit: staphylococcus, optional: true\n    tuple val(meta), path(query), path(\"streptococcus.*\") , emit: streptococcus, optional: true\n    path \"*.genus\", emit: genus, optional: true\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    echo \"reference<TAB>query<TAB>distance<TAB>p-value<TAB>shared-hashes\" | sed 's/<TAB>/\\t/g' > ${prefix}-dist.txt\n    mash \\\\\n        dist \\\\\n        -C \\\\\n        -p $task.cpus \\\\\n        $options.args \\\\\n        $reference \\\\\n        $query | sort -rn -k5,5 -t\\$'\\t' >> ${prefix}-dist.txt\n\n    # Extract genus with hits\n    declare -a GENUS=(\n        \"escherichia\" \"haemophilus\" \"glaesserella\" \"klebsiella\" \"legionella\" \"listeria\" \"mycobacterium\" \"neisseria\" \"salmonella\" \"shigella\" \"staphylococcus\" \"streptococcus\"\n    )\n    for i in \"\\${GENUS[@]}\"; do\n        if grep -q -i \"\\${i}\" ${prefix}-dist.txt; then\n            if [ \"\\${i}\" == \"shigella\" ]; then\n                touch escherichia.genus\n            elif [ \"\\${i}\" == \"glaesserella\" ]; then\n                touch haemophilus.genus\n            else\n                touch \\${i}.genus\n            fi\n        elif [ \"${params.full_merlin}\" == \"true\" ]; then\n            if [ \"\\${i}\" == \"shigella\" ]; then\n                touch escherichia.genus\n            elif [ \"\\${i}\" == \"glaesserella\" ]; then\n                touch haemophilus.genus\n            else\n                if [ \"\\${i}\" != \"listeria\" ]; then\n                    # lissero fails on non-Listeria samples\n                    touch \\${i}.genus\n                fi\n            fi\n        fi\n    done\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mash: \\$(echo \\$(mash 2>&1) | sed 's/^.*Mash version //;s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 78,
        "string_script": "    def prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    echo \"reference<TAB>query<TAB>distance<TAB>p-value<TAB>shared-hashes\" | sed 's/<TAB>/\\t/g' > ${prefix}-dist.txt\n    mash \\\\\n        dist \\\\\n        -C \\\\\n        -p $task.cpus \\\\\n        $options.args \\\\\n        $reference \\\\\n        $query | sort -rn -k5,5 -t\\$'\\t' >> ${prefix}-dist.txt\n\n    # Extract genus with hits\n    declare -a GENUS=(\n        \"escherichia\" \"haemophilus\" \"glaesserella\" \"klebsiella\" \"legionella\" \"listeria\" \"mycobacterium\" \"neisseria\" \"salmonella\" \"shigella\" \"staphylococcus\" \"streptococcus\"\n    )\n    for i in \"\\${GENUS[@]}\"; do\n        if grep -q -i \"\\${i}\" ${prefix}-dist.txt; then\n            if [ \"\\${i}\" == \"shigella\" ]; then\n                touch escherichia.genus\n            elif [ \"\\${i}\" == \"glaesserella\" ]; then\n                touch haemophilus.genus\n            else\n                touch \\${i}.genus\n            fi\n        elif [ \"${params.full_merlin}\" == \"true\" ]; then\n            if [ \"\\${i}\" == \"shigella\" ]; then\n                touch escherichia.genus\n            elif [ \"\\${i}\" == \"glaesserella\" ]; then\n                touch haemophilus.genus\n            else\n                if [ \"\\${i}\" != \"listeria\" ]; then\n                    # lissero fails on non-Listeria samples\n                    touch \\${i}.genus\n                fi\n            fi\n        fi\n    done\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mash: \\$(echo \\$(mash 2>&1) | sed 's/^.*Mash version //;s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 42,
        "language_script": "bash",
        "tools": [
            "Mash",
            "tqDist"
        ],
        "tools_url": [
            "https://bio.tools/mash",
            "https://bio.tools/tqdist"
        ],
        "tools_dico": [
            {
                "name": "Mash",
                "uri": "https://bio.tools/mash",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2533",
                            "term": "DNA mutation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix generation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Phylogenetic distance matrix generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Fast genome and metagenome distance estimation using MinHash.",
                "homepage": "https://github.com/marbl/mash"
            },
            {
                "name": "tqDist",
                "uri": "https://bio.tools/tqdist",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0325",
                                    "term": "Phylogenetic tree comparison"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Computation of the triplet- and Quartet-distance between evolutionary trees.",
                "homepage": "http://users-cs.au.dk/cstorm/software/tqdist/"
            }
        ],
        "inputs": [
            "meta",
            "query",
            "reads",
            "reference"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/mash:2.3--he348c14_1' : 'quay.io/biocontainers/mash:2.3--he348c14_1' }\""
        ],
        "when": "",
        "stub": ""
    },
    "BLAST": {
        "name_process": "BLAST",
        "string_process": "\nprocess BLAST {\n      \n                                                                 \n      \n    tag \"${meta.id} - ${query}\"\n    label \"blast\"\n\n    publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options, logs_subdir: query) }\n\n    input:\n    tuple val(meta), path(blastdb, stageAs: 'blastdb/*')\n    each path(query)\n\n    output:\n    path \"results/*\"\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    shell:\n    '''\n    OUTDIR=results/!{query}\n    mkdir -p ${OUTDIR}\n    for fasta in !{query}/*; do\n        name=$(basename \"${fasta%.*}\")\n        mkdir -p temp_json\n        if [ \"!{query}\" == \"genes\" ]; then\n            cat ${fasta} | sed -e 's/<[^>]*>//g' |\n            parallel --gnu --plain -j !{task.cpus} --recstart '>' -N 1 --pipe \\\n            blastn -db blastdb/!{meta.id} \\\n                -outfmt 15 \\\n                -evalue 1 \\\n                -perc_identity !{params.perc_identity} \\\n                -qcov_hsp_perc !{params.qcov_hsp_perc} \\\n                -query - \\\n                -out temp_json/${name}_{#}.json\n        elif [ \"!{query}\" == \"primers\" ]; then\n            cat ${fasta} | sed -e 's/<[^>]*>//g' |\n            parallel --gnu --plain -j !{task.cpus} --recstart '>' -N 1 --pipe \\\n            blastn -db blastdb/!{meta.id} \\\n                -outfmt 15 \\\n                -task blastn \\\n                -dust no \\\n                -word_size 7 \\\n                -perc_identity !{params.perc_identity} \\\n                -evalue 1 \\\n                -query - \\\n                -out temp_json/${name}_{#}.json\n        else\n            cat ${fasta} | sed -e 's/<[^>]*>//g' |\n            parallel --gnu --plain -j !{task.cpus} --recstart '>' -N 1 --pipe \\\n            tblastn -db blastdb/!{meta.id} \\\n                -outfmt 15 \\\n                -evalue 0.0001 \\\n                -qcov_hsp_perc !{params.qcov_hsp_perc} \\\n                -query - \\\n                -out temp_json/${name}_{#}.json\n        fi\n\n        merge-blast-json.py temp_json > ${OUTDIR}/${name}.json\n        rm -rf temp_json\n\n        if [[ !{params.skip_compression} == \"false\" ]]; then\n            pigz -n --best -p !{task.cpus} ${OUTDIR}/${name}.json\n        fi\n    done\n\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        blastn: $(echo $(blastn -version 2>&1) | sed 's/^.*blastn: //;s/ .*$//')\n        parallel: $(echo $(parallel --version 2>&1) | sed 's/^GNU parallel //;s/ .*$//')\n        pigz: $(echo $(pigz --version 2>&1) | sed 's/pigz //')\n        tblastn: $(echo $(tblastn -version 2>&1) | sed 's/^.*tblastn: //;s/ .*$//') \n    END_VERSIONS\n    '''\n}",
        "nb_lignes_process": 76,
        "string_script": "    '''\n    OUTDIR=results/!{query}\n    mkdir -p ${OUTDIR}\n    for fasta in !{query}/*; do\n        name=$(basename \"${fasta%.*}\")\n        mkdir -p temp_json\n        if [ \"!{query}\" == \"genes\" ]; then\n            cat ${fasta} | sed -e 's/<[^>]*>//g' |\n            parallel --gnu --plain -j !{task.cpus} --recstart '>' -N 1 --pipe \\\n            blastn -db blastdb/!{meta.id} \\\n                -outfmt 15 \\\n                -evalue 1 \\\n                -perc_identity !{params.perc_identity} \\\n                -qcov_hsp_perc !{params.qcov_hsp_perc} \\\n                -query - \\\n                -out temp_json/${name}_{#}.json\n        elif [ \"!{query}\" == \"primers\" ]; then\n            cat ${fasta} | sed -e 's/<[^>]*>//g' |\n            parallel --gnu --plain -j !{task.cpus} --recstart '>' -N 1 --pipe \\\n            blastn -db blastdb/!{meta.id} \\\n                -outfmt 15 \\\n                -task blastn \\\n                -dust no \\\n                -word_size 7 \\\n                -perc_identity !{params.perc_identity} \\\n                -evalue 1 \\\n                -query - \\\n                -out temp_json/${name}_{#}.json\n        else\n            cat ${fasta} | sed -e 's/<[^>]*>//g' |\n            parallel --gnu --plain -j !{task.cpus} --recstart '>' -N 1 --pipe \\\n            tblastn -db blastdb/!{meta.id} \\\n                -outfmt 15 \\\n                -evalue 0.0001 \\\n                -qcov_hsp_perc !{params.qcov_hsp_perc} \\\n                -query - \\\n                -out temp_json/${name}_{#}.json\n        fi\n\n        merge-blast-json.py temp_json > ${OUTDIR}/${name}.json\n        rm -rf temp_json\n\n        if [[ !{params.skip_compression} == \"false\" ]]; then\n            pigz -n --best -p !{task.cpus} ${OUTDIR}/${name}.json\n        fi\n    done\n\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        blastn: $(echo $(blastn -version 2>&1) | sed 's/^.*blastn: //;s/ .*$//')\n        parallel: $(echo $(parallel --version 2>&1) | sed 's/^GNU parallel //;s/ .*$//')\n        pigz: $(echo $(pigz --version 2>&1) | sed 's/pigz //')\n        tblastn: $(echo $(tblastn -version 2>&1) | sed 's/^.*tblastn: //;s/ .*$//') \n    END_VERSIONS\n    '''",
        "nb_lignes_script": 54,
        "language_script": "bash",
        "tools": [
            "parallelGWAS",
            "G-BLASTN"
        ],
        "tools_url": [
            "https://bio.tools/parallelgwas",
            "https://bio.tools/g-blastn"
        ],
        "tools_dico": [
            {
                "name": "parallelGWAS",
                "uri": "https://bio.tools/parallelgwas",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype and phenotype"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "GWAS study"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype and phenotype resources"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype-phenotype analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype-phenotype"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "GWAS analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "GWAS"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "Genome-wide association study"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0487",
                                    "term": "Haplotype mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0487",
                                    "term": "Haplotype reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0487",
                                    "term": "Haplotype map generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0487",
                                    "term": "Haplotype inference"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Developing parallel computing tools for genome-wide association studies.",
                "homepage": "https://en.osdn.jp/projects/parallelgwas/"
            },
            {
                "name": "G-BLASTN",
                "uri": "https://bio.tools/g-blastn",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acids"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0346",
                                    "term": "Sequence similarity search"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2976",
                                "term": "Protein sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0857",
                                "term": "Sequence search results"
                            }
                        ]
                    }
                ],
                "description": "GPU-accelerated nucleotide alignment tool based on the widely used NCBI-BLAST.",
                "homepage": "http://www.comp.hkbu.edu.hk/~chxw/software/G-BLASTN.html"
            }
        ],
        "inputs": [
            "meta",
            "blastdb",
            "query"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"${meta.id} - ${query}\"",
            "label \"blast\"",
            "publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options, logs_subdir: query) }"
        ],
        "when": "",
        "stub": ""
    },
    "RGI_HEATMAP": {
        "name_process": "RGI_HEATMAP",
        "string_process": "\nprocess RGI_HEATMAP {\n    tag \"$meta.id\"\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/rgi:5.2.1--pyha8f3691_2' :\n        'quay.io/biocontainers/rgi:5.2.1--pyha8f3691_2' }\"\n\n    input:\n    tuple val(meta), path(json, stageAs: 'json/*')\n\n    output:\n    tuple val(meta), path(\"*.{csv,eps,png}\"), emit: heatmap, optional: true\n    path \"*.{log,err}\"                      , emit: logs, optional: true\n    path \".command.*\"                       , emit: nf_logs\n    path \"versions.yml\"                     , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    NUM_SAMPLES=\\$(ls json/ | wc -l)\n    if [[ \"\\${NUM_SAMPLES}\" -gt 1 ]]; then\n        rgi \\\\\n            heatmap \\\\\n            $options.args \\\\\n            --output $prefix \\\\\n            --input json/\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        rgi: \\$(rgi main --version)\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 36,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    NUM_SAMPLES=\\$(ls json/ | wc -l)\n    if [[ \"\\${NUM_SAMPLES}\" -gt 1 ]]; then\n        rgi \\\\\n            heatmap \\\\\n            $options.args \\\\\n            --output $prefix \\\\\n            --input json/\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        rgi: \\$(rgi main --version)\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "Rgin",
            "heatmaps"
        ],
        "tools_url": [
            "https://bio.tools/rgin",
            "https://bio.tools/heatmaps"
        ],
        "tools_dico": [
            {
                "name": "Rgin",
                "uri": "https://bio.tools/rgin",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "GWAS study"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0199",
                            "term": "Genetic variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2885",
                            "term": "DNA polymorphism"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "GWAS analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "GWAS"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "Genome-wide association study"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0199",
                            "term": "DNA variation"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2497",
                                    "term": "Pathway or network analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "C++ implementation of SConES.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/Rgin.html"
            },
            {
                "name": "heatmaps",
                "uri": "https://bio.tools/heatmaps",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0085",
                            "term": "Functional genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3169",
                            "term": "ChIP-seq"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3169",
                            "term": "Chip-sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3169",
                            "term": "Chip Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3169",
                            "term": "ChIP-sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3169",
                            "term": "Chip sequencing"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2928",
                                    "term": "Alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2928",
                                    "term": "Alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2928",
                                    "term": "Alignment generation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package provides functions for plotting heatmaps of genome-wide data across genomic intervals, such as ChIP-seq signals at peaks or across promoters. Many functions are also provided for investigating sequence features.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/heatmaps.html"
            }
        ],
        "inputs": [
            "meta",
            "json"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/rgi:5.2.1--pyha8f3691_2' : 'quay.io/biocontainers/rgi:5.2.1--pyha8f3691_2' }\""
        ],
        "when": "",
        "stub": ""
    },
    "EGGNOG_DOWNLOAD": {
        "name_process": "EGGNOG_DOWNLOAD",
        "string_process": "\nprocess EGGNOG_DOWNLOAD {\n    label 'process_low'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/eggnog-mapper:2.1.6--pyhdfd78af_0' :\n        'quay.io/biocontainers/eggnog-mapper:2.1.6--pyhdfd78af_0' }\"\n\n    output:\n    path(\"eggnog/*\")                        , emit: db\n    path \"*.{log,err}\", emit: logs          , optional: true\n    path \".command.*\"                       , emit: nf_logs\n    path \"versions.yml\"                     , emit: versions\n\n    script:\n    \"\"\"\n    mkdir eggnog\n    download_eggnog_data.py \\\\\n        $options.args \\\\\n        -y \\\\\n        --data_dir eggnog/\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        eggnog-mapper: \\$( echo \\$(emapper.py --version 2>&1)| sed 's/.* emapper-//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "    \"\"\"\n    mkdir eggnog\n    download_eggnog_data.py \\\\\n        $options.args \\\\\n        -y \\\\\n        --data_dir eggnog/\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        eggnog-mapper: \\$( echo \\$(emapper.py --version 2>&1)| sed 's/.* emapper-//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "label 'process_low'",
            "publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/eggnog-mapper:2.1.6--pyhdfd78af_0' : 'quay.io/biocontainers/eggnog-mapper:2.1.6--pyhdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "ABRICATE_SUMMARY": {
        "name_process": "ABRICATE_SUMMARY",
        "string_process": "\nprocess ABRICATE_SUMMARY {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/abricate%3A1.0.1--ha8f3691_1' :\n        'quay.io/biocontainers/abricate:1.0.1--ha8f3691_1' }\"\n\n    input:\n    tuple val(meta), path(reports)\n\n    output:\n    tuple val(meta), path(\"*.txt\"), emit: report\n    path \"*.{log,err}\"            , emit: logs, optional: true\n    path \".command.*\"             , emit: nf_logs\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    abricate \\\\\n        --summary \\\\\n        $reports > ${prefix}.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        abricate: \\$(echo \\$(abricate --version 2>&1) | sed 's/^.*abricate //' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    abricate \\\\\n        --summary \\\\\n        $reports > ${prefix}.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        abricate: \\$(echo \\$(abricate --version 2>&1) | sed 's/^.*abricate //' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [
            "ABRicate"
        ],
        "tools_url": [
            "https://bio.tools/ABRicate"
        ],
        "tools_dico": [
            {
                "name": "ABRicate",
                "uri": "https://bio.tools/ABRicate",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3482",
                                    "term": "Antimicrobial resistance prediction"
                                }
                            ],
                            []
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_1234",
                                "term": "Sequence set (nucleic acid)"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0916",
                                "term": "Gene report"
                            }
                        ]
                    }
                ],
                "description": "Mass screening of contigs for antimicrobial resistance or virulence genes.",
                "homepage": "https://github.com/tseemann/abricate"
            }
        ],
        "inputs": [
            "meta",
            "reports"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/abricate%3A1.0.1--ha8f3691_1' : 'quay.io/biocontainers/abricate:1.0.1--ha8f3691_1' }\""
        ],
        "when": "",
        "stub": ""
    },
    "AMRFINDERPLUS_RUN": {
        "name_process": "AMRFINDERPLUS_RUN",
        "string_process": "\nprocess AMRFINDERPLUS_RUN {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ncbi-amrfinderplus%3A3.10.23--h17dc2d4_0' :\n        'quay.io/biocontainers/ncbi-amrfinderplus:3.10.23--h17dc2d4_0' }\"\n\n    input:\n    tuple val(meta), path(genes), path(proteins)\n    each path(db)\n\n    output:\n    tuple val(meta), path(\"${prefix}-genes.tsv\")                     , emit: gene_report\n    tuple val(meta), path(\"${prefix}-proteins.tsv\")                  , emit: protein_report\n    tuple val(meta), path(\"${prefix}-{genes,proteins}-mutations.tsv\"), emit: mutation_reports, optional: true\n    path \"*.{log,err}\"                                               , emit: logs, optional: true\n    path \".command.*\"                                                , emit: nf_logs\n    path \"versions.yml\"                                              , emit: versions\n\n    script:\n    def fna_is_compressed = genes.getName().endsWith(\".gz\") ? true : false\n    def faa_is_compressed = proteins.getName().endsWith(\".gz\") ? true : false\n    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    fna_organism_param = meta.containsKey(\"organism\") ? \"--organism ${meta.organism} --mutation_all ${prefix}-genes-mutations.tsv\" : \"\"\n    faa_organism_param = meta.containsKey(\"organism\") ? \"--organism ${meta.organism} --mutation_all ${prefix}-proteins-mutations.tsv\" : \"\"\n    fna_name = genes.getName().replace(\".gz\", \"\")\n    faa_name = proteins.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$fna_is_compressed\" == \"true\" ]; then\n        gzip -c -d $genes > $fna_name\n    fi\n\n    if [ \"$faa_is_compressed\" == \"true\" ]; then\n        gzip -c -d $proteins > $faa_name\n    fi\n\n    mkdir amrfinderdb\n    tar xzvf $db -C amrfinderdb\n\n    # Gene\n    amrfinder \\\\\n       -n $fna_name \\\\\n        $fna_organism_param \\\\\n        $options.args \\\\\n        --plus \\\\\n        --database amrfinderdb/ \\\\\n        --threads $task.cpus \\\\\n        --name $prefix > ${prefix}-genes.tsv\n\n    # Protein\n    amrfinder \\\\\n       -p $faa_name \\\\\n        $faa_organism_param \\\\\n        $options.args \\\\\n        --plus \\\\\n        --database amrfinderdb/ \\\\\n        --threads $task.cpus \\\\\n        --name $prefix > ${prefix}-proteins.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        amrfinderplus: \\$(amrfinder --version)\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 68,
        "string_script": "    def fna_is_compressed = genes.getName().endsWith(\".gz\") ? true : false\n    def faa_is_compressed = proteins.getName().endsWith(\".gz\") ? true : false\n    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    fna_organism_param = meta.containsKey(\"organism\") ? \"--organism ${meta.organism} --mutation_all ${prefix}-genes-mutations.tsv\" : \"\"\n    faa_organism_param = meta.containsKey(\"organism\") ? \"--organism ${meta.organism} --mutation_all ${prefix}-proteins-mutations.tsv\" : \"\"\n    fna_name = genes.getName().replace(\".gz\", \"\")\n    faa_name = proteins.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$fna_is_compressed\" == \"true\" ]; then\n        gzip -c -d $genes > $fna_name\n    fi\n\n    if [ \"$faa_is_compressed\" == \"true\" ]; then\n        gzip -c -d $proteins > $faa_name\n    fi\n\n    mkdir amrfinderdb\n    tar xzvf $db -C amrfinderdb\n\n    # Gene\n    amrfinder \\\\\n       -n $fna_name \\\\\n        $fna_organism_param \\\\\n        $options.args \\\\\n        --plus \\\\\n        --database amrfinderdb/ \\\\\n        --threads $task.cpus \\\\\n        --name $prefix > ${prefix}-genes.tsv\n\n    # Protein\n    amrfinder \\\\\n       -p $faa_name \\\\\n        $faa_organism_param \\\\\n        $options.args \\\\\n        --plus \\\\\n        --database amrfinderdb/ \\\\\n        --threads $task.cpus \\\\\n        --name $prefix > ${prefix}-proteins.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        amrfinderplus: \\$(amrfinder --version)\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 43,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "genes",
            "proteins",
            "db"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/ncbi-amrfinderplus%3A3.10.23--h17dc2d4_0' : 'quay.io/biocontainers/ncbi-amrfinderplus:3.10.23--h17dc2d4_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "STAPHOPIASCCMEC": {
        "name_process": "STAPHOPIASCCMEC",
        "string_process": "\nprocess STAPHOPIASCCMEC {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/staphopia-sccmec:1.0.0--hdfd78af_0' :\n        'quay.io/biocontainers/staphopia-sccmec:1.0.0--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    staphopia-sccmec --assembly $fasta $options.args > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        staphopia-sccmec: \\$(staphopia-sccmec --version 2>&1 | sed 's/^.*staphopia-sccmec //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    staphopia-sccmec --assembly $fasta $options.args > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        staphopia-sccmec: \\$(staphopia-sccmec --version 2>&1 | sed 's/^.*staphopia-sccmec //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/staphopia-sccmec:1.0.0--hdfd78af_0' : 'quay.io/biocontainers/staphopia-sccmec:1.0.0--hdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "EMMTYPER": {
        "name_process": "EMMTYPER",
        "string_process": "\nprocess EMMTYPER {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/emmtyper:0.2.0--py_0' :\n        'quay.io/biocontainers/emmtyper:0.2.0--py_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\")          , emit: tsv\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\"                       , emit: nf_logs\n    path \"versions.yml\"                     , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    emmtyper \\\\\n        $options.args \\\\\n        $fasta_name \\\\\n        > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        emmtyper: \\$( echo \\$(emmtyper --version 2>&1) | sed 's/^.*emmtyper v//' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 39,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    emmtyper \\\\\n        $options.args \\\\\n        $fasta_name \\\\\n        > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        emmtyper: \\$( echo \\$(emmtyper --version 2>&1) | sed 's/^.*emmtyper v//' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/emmtyper:0.2.0--py_0' : 'quay.io/biocontainers/emmtyper:0.2.0--py_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MINMER_QUERY": {
        "name_process": "MINMER_QUERY",
        "string_process": "\nprocess MINMER_QUERY {\n      \n                                                                      \n                                  \n      \n    tag \"${meta.id} - ${dataset_basename}\"\n    label \"base_mem_8gb\"\n    label \"minmer_query\"\n\n    publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    input:\n    tuple val(meta), path(fq), path(sourmash)\n    each path(dataset)\n\n    output:\n    path \"${meta.id}-${program}-${database}-${kmer}.txt\", emit: result\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    shell:\n    dataset_basename = dataset.getSimpleName()\n    dataset_info = dataset_basename.split(\"-\")                                         \n    program = dataset_info[0]\n    database = dataset_info[1]\n    kmer = dataset_info[2]\n    mash_w = params.no_winner_take_all ? \"\" : \"-w\"\n    fastq = meta.single_end ? \"${fq[0]}\" : \"${fq[0]} ${fq[1]}\"\n    '''\n    OUTPUT=\"!{meta.id}-!{program}-!{database}-!{kmer}.txt\"\n    if [ \"!{program}\" == \"mash\" ]; then\n        echo \"identity<TAB>shared-hashes<TAB>median-multiplicity<TAB>p-value<TAB>query-ID<TAB>query-comment\" | sed 's/<TAB>/\\t/g' > ${OUTPUT}\n        gzip -cd !{fastq} | \\\n            mash screen !{mash_w} -i !{params.screen_i} -p !{task.cpus} !{dataset} - | \\\n            sort -gr >> ${OUTPUT}\n    elif [ \"!{program}\" == \"sourmash\" ]; then\n        sourmash lca classify --query !{sourmash} --db !{dataset} > ${OUTPUT}\n    fi\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        mash: $(echo $(mash 2>&1) | sed 's/^.*Mash version //;s/ .*$//')\n        sourmash: $(echo $(sourmash --version 2>&1) | sed 's/sourmash //;')\n    END_VERSIONS\n    '''\n}",
        "nb_lignes_process": 48,
        "string_script": "    dataset_basename = dataset.getSimpleName()\n    dataset_info = dataset_basename.split(\"-\")                                         \n    program = dataset_info[0]\n    database = dataset_info[1]\n    kmer = dataset_info[2]\n    mash_w = params.no_winner_take_all ? \"\" : \"-w\"\n    fastq = meta.single_end ? \"${fq[0]}\" : \"${fq[0]} ${fq[1]}\"\n    '''\n    OUTPUT=\"!{meta.id}-!{program}-!{database}-!{kmer}.txt\"\n    if [ \"!{program}\" == \"mash\" ]; then\n        echo \"identity<TAB>shared-hashes<TAB>median-multiplicity<TAB>p-value<TAB>query-ID<TAB>query-comment\" | sed 's/<TAB>/\\t/g' > ${OUTPUT}\n        gzip -cd !{fastq} | \\\n            mash screen !{mash_w} -i !{params.screen_i} -p !{task.cpus} !{dataset} - | \\\n            sort -gr >> ${OUTPUT}\n    elif [ \"!{program}\" == \"sourmash\" ]; then\n        sourmash lca classify --query !{sourmash} --db !{dataset} > ${OUTPUT}\n    fi\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        mash: $(echo $(mash 2>&1) | sed 's/^.*Mash version //;s/ .*$//')\n        sourmash: $(echo $(sourmash --version 2>&1) | sed 's/sourmash //;')\n    END_VERSIONS\n    '''",
        "nb_lignes_script": 24,
        "language_script": "bash",
        "tools": [
            "GEO database",
            "Kmer-SSR",
            "FastQC",
            "Mash",
            "sourmash"
        ],
        "tools_url": [
            "https://bio.tools/GEO_database",
            "https://bio.tools/kmer-ssr",
            "https://bio.tools/fastqc",
            "https://bio.tools/mash",
            "https://bio.tools/sourmash"
        ],
        "tools_dico": [
            {
                "name": "GEO database",
                "uri": "https://bio.tools/GEO_database",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3489",
                            "term": "Database management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3810",
                            "term": "Agricultural science"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3324",
                            "term": "Infectious disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3489",
                            "term": "Database administration"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3324",
                            "term": "Transmissable disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3324",
                            "term": "Communicable disease"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phylogenetic inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2421",
                                    "term": "Database search"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phlyogenetic tree construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phylogenetic tree generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2421",
                                    "term": "Search"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Characterisation of Quorum Sensing System and Its Role in Global Regulation in Hafnia alvei.\n\nThis database stores curated gene expression DataSets, as well as original Series and Platform records in the Gene Expression Omnibus (GEO) repository. Enter search terms to locate experiments of interest. DataSet records contain additional resources including cluster tools and differential expression queries.\n\n(smok* OR diet) AND (mammals[organism] NOT human[organism]).\n\n||| COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/GSE33335 (NLM.NIH.GOV/gds), bio.tools/ncbi_resources (NLM.NIH.GOV), bio.tools/genbank (NLM.NIH.GOV).\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'homoserine lactones AHLs Hafnia alvei', 'AHLs Hafnia alvei', 'lactones AHLs Hafnia alvei', 'Hafnia alvei'",
                "homepage": "https://www.ncbi.nlm.nih.gov/gds"
            },
            {
                "name": "Kmer-SSR",
                "uri": "https://bio.tools/kmer-ssr",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0157",
                            "term": "Sequence composition, complexity and repeats"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0379",
                                    "term": "Repeat sequence detection"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Fast, Accurate, and Complete SSR Detection in Genomic Sequences.",
                "homepage": "https://github.com/ridgelab/Kmer-SSR"
            },
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            },
            {
                "name": "Mash",
                "uri": "https://bio.tools/mash",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2533",
                            "term": "DNA mutation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix generation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Phylogenetic distance matrix generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Fast genome and metagenome distance estimation using MinHash.",
                "homepage": "https://github.com/marbl/mash"
            },
            {
                "name": "sourmash",
                "uri": "https://bio.tools/sourmash",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3307",
                            "term": "Computational biology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0346",
                                    "term": "Sequence similarity search"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix generation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Phylogenetic distance matrix generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2975",
                                "term": "Nucleic acid sequence (raw)"
                            }
                        ],
                        "output": []
                    }
                ],
                "description": "Compute and compare MinHash signatures for DNA data sets.",
                "homepage": "https://sourmash.readthedocs.io/en/latest/"
            }
        ],
        "inputs": [
            "meta",
            "fq",
            "sourmash",
            "dataset"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"${meta.id} - ${dataset_basename}\"",
            "label \"base_mem_8gb\"",
            "label \"minmer_query\"",
            "publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }"
        ],
        "when": "",
        "stub": ""
    },
    "MAPPING_QUERY": {
        "name_process": "MAPPING_QUERY",
        "string_process": "\nprocess MAPPING_QUERY {\n                                                                       \n    tag \"${meta.id}\"\n    label \"mapping_query\"\n\n    publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force, saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    input:\n    tuple val(meta), path(fq)\n    each path(query)\n\n    when:\n    meta.runtype != \"ont\"\n\n    output:\n    path \"results/*\"\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    shell:\n    bwa_mem_opts = params.bwa_mem_opts ? params.bwa_mem_opts : \"\"\n    bwa_aln_opts = params.bwa_aln_opts ? params.bwa_aln_opts : \"\"\n    bwa_samse_opts = params.bwa_samse_opts ? params.bwa_samse_opts : \"\"\n    bwa_sampe_opts = params.bwa_sampe_opts ? params.bwa_sampe_opts : \"\"\n    '''\n    avg_len=`seqtk fqchk !{fq[0]} | head -n 1 | sed -r 's/.*avg_len: ([0-9]+).*;.*/\\\\1/'`\n    ls !{query}/* | xargs -I {} grep -H \"^>\" {} | awk '{print $1}' | sed 's/:>/\\\\t/; s=.*/==; s/\\\\..*\\\\t/\\\\t/' > mapping.txt\n    cat !{query}/* > multifasta.fa\n\n    bwa index multifasta.fa\n    if [ \"${avg_len}\" -gt \"70\" ]; then\n        bwa mem -M -t !{task.cpus} !{bwa_mem_opts} multifasta.fa !{fq} > bwa.sam\n    else\n        if [ \"!{meta.single_end}\" == \"true\" ]; then\n            bwa aln -f bwa.sai -t !{task.cpus} !{bwa_aln_opts} multifasta.fa !{fq[0]}\n            bwa samse -n !{params.bwa_n} !{bwa_samse_opts} multifasta.fa bwa.sai !{fq[0]} > bwa.sam\n        else\n            bwa aln -f r1.sai -t !{task.cpus} !{bwa_aln_opts} multifasta.fa !{fq[0]}\n            bwa aln -f r2.sai -t !{task.cpus} !{bwa_aln_opts} multifasta.fa !{fq[1]}\n            bwa sampe -n !{params.bwa_n} !{bwa_sampe_opts} multifasta.fa r1.sai r2.sai !{fq[0]} !{fq[1]} > bwa.sam\n        fi\n    fi\n\n    # Write per-base coverage\n    samtools view -bS bwa.sam | samtools sort -o cov.bam - \n    genomeCoverageBed -ibam cov.bam -d > cov.txt\n    split-coverages.py mapping.txt cov.txt --outdir results\n\n    if [[ !{params.skip_compression} == \"false\" ]]; then\n        pigz --best -n -p !{task.cpus} results/*.txt\n    fi\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        bedtools: $(echo $(bedtools --version 2>&1) | sed 's/bedtools v//')\n        bwa: $(echo $(bwa 2>&1) | sed 's/^.*Version: //;s/ .*$//')\n        pigz: $(echo $(pigz --version 2>&1) | sed 's/pigz //')\n        samtools: $(echo $(samtools --version 2>&1) |sed 's/^.*samtools //;s/ .*$//')\n    END_VERSIONS\n    '''\n}",
        "nb_lignes_process": 62,
        "string_script": "    bwa_mem_opts = params.bwa_mem_opts ? params.bwa_mem_opts : \"\"\n    bwa_aln_opts = params.bwa_aln_opts ? params.bwa_aln_opts : \"\"\n    bwa_samse_opts = params.bwa_samse_opts ? params.bwa_samse_opts : \"\"\n    bwa_sampe_opts = params.bwa_sampe_opts ? params.bwa_sampe_opts : \"\"\n    '''\n    avg_len=`seqtk fqchk !{fq[0]} | head -n 1 | sed -r 's/.*avg_len: ([0-9]+).*;.*/\\\\1/'`\n    ls !{query}/* | xargs -I {} grep -H \"^>\" {} | awk '{print $1}' | sed 's/:>/\\\\t/; s=.*/==; s/\\\\..*\\\\t/\\\\t/' > mapping.txt\n    cat !{query}/* > multifasta.fa\n\n    bwa index multifasta.fa\n    if [ \"${avg_len}\" -gt \"70\" ]; then\n        bwa mem -M -t !{task.cpus} !{bwa_mem_opts} multifasta.fa !{fq} > bwa.sam\n    else\n        if [ \"!{meta.single_end}\" == \"true\" ]; then\n            bwa aln -f bwa.sai -t !{task.cpus} !{bwa_aln_opts} multifasta.fa !{fq[0]}\n            bwa samse -n !{params.bwa_n} !{bwa_samse_opts} multifasta.fa bwa.sai !{fq[0]} > bwa.sam\n        else\n            bwa aln -f r1.sai -t !{task.cpus} !{bwa_aln_opts} multifasta.fa !{fq[0]}\n            bwa aln -f r2.sai -t !{task.cpus} !{bwa_aln_opts} multifasta.fa !{fq[1]}\n            bwa sampe -n !{params.bwa_n} !{bwa_sampe_opts} multifasta.fa r1.sai r2.sai !{fq[0]} !{fq[1]} > bwa.sam\n        fi\n    fi\n\n    # Write per-base coverage\n    samtools view -bS bwa.sam | samtools sort -o cov.bam - \n    genomeCoverageBed -ibam cov.bam -d > cov.txt\n    split-coverages.py mapping.txt cov.txt --outdir results\n\n    if [[ !{params.skip_compression} == \"false\" ]]; then\n        pigz --best -n -p !{task.cpus} results/*.txt\n    fi\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        bedtools: $(echo $(bedtools --version 2>&1) | sed 's/bedtools v//')\n        bwa: $(echo $(bwa 2>&1) | sed 's/^.*Version: //;s/ .*$//')\n        pigz: $(echo $(pigz --version 2>&1) | sed 's/pigz //')\n        samtools: $(echo $(samtools --version 2>&1) |sed 's/^.*samtools //;s/ .*$//')\n    END_VERSIONS\n    '''",
        "nb_lignes_script": 40,
        "language_script": "bash",
        "tools": [
            "BWA",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/bwa",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "meta",
            "fq",
            "query"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"${meta.id}\"",
            "label \"mapping_query\"",
            "publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force, saveAs: { filename -> saveFiles(filename:filename, opts:options) }"
        ],
        "when": "meta.runtype != \"ont\"",
        "stub": ""
    },
    "MASHTREE": {
        "name_process": "MASHTREE",
        "string_process": "\nprocess MASHTREE {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mashtree:1.2.0--pl526h516909a_0' :\n        'quay.io/biocontainers/mashtree:1.2.0--pl526h516909a_0' }\"\n\n    input:\n    tuple val(meta), path(seqs)\n\n    output:\n    tuple val(meta), path(\"*.dnd\"), emit: tree\n    tuple val(meta), path(\"*.tsv\"), emit: matrix\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\",emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    mashtree \\\\\n        $options.args \\\\\n        --numcpus $task.cpus \\\\\n        --outmatrix ${prefix}.tsv \\\\\n        --outtree ${prefix}.dnd \\\\\n        $seqs\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mash: \\$(echo \\$(mash 2>&1) | sed 's/^.*Mash version //;s/ .*\\$//')\n        mashtree: \\$( echo \\$( mashtree --version 2>&1 ) | sed 's/^.*Mashtree //' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 37,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    mashtree \\\\\n        $options.args \\\\\n        --numcpus $task.cpus \\\\\n        --outmatrix ${prefix}.tsv \\\\\n        --outtree ${prefix}.dnd \\\\\n        $seqs\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mash: \\$(echo \\$(mash 2>&1) | sed 's/^.*Mash version //;s/ .*\\$//')\n        mashtree: \\$( echo \\$( mashtree --version 2>&1 ) | sed 's/^.*Mashtree //' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "seqs"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/mashtree:1.2.0--pl526h516909a_0' : 'quay.io/biocontainers/mashtree:1.2.0--pl526h516909a_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "PHYLOFLASH": {
        "name_process": "PHYLOFLASH",
        "string_process": "\nprocess PHYLOFLASH  {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/phyloflash:3.4--hdfd78af_1' :\n        'quay.io/biocontainers/phyloflash:3.4--hdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  silva_db\n    path  univec_db\n\n    output:\n    tuple val(meta), path(\"${meta.id}/*\")     , emit: results\n    file \"${sample}/${sample}.toalign.fasta\"  , emit: aln, optional: true\n    file \"${sample}/${sample}.phyloFlash.json\", emit: summary, optional: true\n    path \"*.{log,err}\"                        , emit: logs, optional: true\n    path \".command.*\"                         , emit: nf_logs\n    path \"versions.yml\"                       , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def read_opts = meta.single_end ? \"-read1 ${reads[0]}\" : \"-read1 ${reads[0]} -read2 ${reads[1]}\"\n    \"\"\"\n    mkdir $prefix\n    phyloFlash.pl \\\\\n        $options.args \\\\\n        $read_opts \\\\\n        -lib $prefix \\\\\n        -dbhome . \\\\\n        -CPUs $task.cpus\n\n    jsonify-phyloflash.py ${prefix}.phyloFlash > ${prefix}.phyloFlash.json\n    mv ${prefix}.* $prefix\n\n\n    if phyloflash-summary.py ${prefix}/ | grep -q -c \"WARNING: Multiple SSUs were assembled by SPAdes\"; then\n        MULTI=\"1\"\n    fi\n\n    if [ \"${params.allow_multiple_16s}\" == \"true\" ]; then\n        MULTI=\"0\"\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        phyloFlash: \\$(echo \\$(phyloFlash.pl -version 2>&1) | sed \"s/^.*phyloFlash v//\")\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 53,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def read_opts = meta.single_end ? \"-read1 ${reads[0]}\" : \"-read1 ${reads[0]} -read2 ${reads[1]}\"\n    \"\"\"\n    mkdir $prefix\n    phyloFlash.pl \\\\\n        $options.args \\\\\n        $read_opts \\\\\n        -lib $prefix \\\\\n        -dbhome . \\\\\n        -CPUs $task.cpus\n\n    jsonify-phyloflash.py ${prefix}.phyloFlash > ${prefix}.phyloFlash.json\n    mv ${prefix}.* $prefix\n\n\n    if phyloflash-summary.py ${prefix}/ | grep -q -c \"WARNING: Multiple SSUs were assembled by SPAdes\"; then\n        MULTI=\"1\"\n    fi\n\n    if [ \"${params.allow_multiple_16s}\" == \"true\" ]; then\n        MULTI=\"0\"\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        phyloFlash: \\$(echo \\$(phyloFlash.pl -version 2>&1) | sed \"s/^.*phyloFlash v//\")\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 27,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads",
            "silva_db",
            "univec_db"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/phyloflash:3.4--hdfd78af_1' : 'quay.io/biocontainers/phyloflash:3.4--hdfd78af_1' }\""
        ],
        "when": "",
        "stub": ""
    },
    "AGRVATE": {
        "name_process": "AGRVATE",
        "string_process": "\nprocess AGRVATE {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/agrvate:1.0.2--hdfd78af_0' :\n        'quay.io/biocontainers/agrvate:1.0.2--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"results/${meta.id}-summary.tab\"), emit: summary\n    tuple val(meta), path(\"results/*\")                     , emit: results_dir\n    path \"*.{log,err}\"                                     , emit: logs, optional: true\n    path \".command.*\"                                      , emit: nf_logs\n    path \"versions.yml\"                                    , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    agrvate \\\\\n        $options.args \\\\\n        -i $fasta_name\n\n    mv ${meta.id}-results/ results/\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        agrvate: \\$(echo \\$(agrvate -v 2>&1) | sed 's/agrvate v//;')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 41,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    agrvate \\\\\n        $options.args \\\\\n        -i $fasta_name\n\n    mv ${meta.id}-results/ results/\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        agrvate: \\$(echo \\$(agrvate -v 2>&1) | sed 's/agrvate v//;')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/agrvate:1.0.2--hdfd78af_0' : 'quay.io/biocontainers/agrvate:1.0.2--hdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MLST": {
        "name_process": "MLST",
        "string_process": "\nprocess MLST {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mlst:2.19.0--hdfd78af_1' :\n        'quay.io/biocontainers/mlst:2.19.0--hdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"*.{log,err}\" , emit: logs, optional: true\n    path \".command.*\"  , emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    mlst \\\\\n        --threads $task.cpus \\\\\n        $options.args \\\\\n        $fasta \\\\\n        > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mlst: \\$( echo \\$(mlst --version 2>&1) | sed 's/mlst //' )\n    END_VERSIONS\n    \"\"\"\n\n}",
        "nb_lignes_process": 36,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    mlst \\\\\n        --threads $task.cpus \\\\\n        $options.args \\\\\n        $fasta \\\\\n        > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mlst: \\$( echo \\$(mlst --version 2>&1) | sed 's/mlst //' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "MLST"
        ],
        "tools_url": [
            "https://bio.tools/mlst"
        ],
        "tools_dico": [
            {
                "name": "MLST",
                "uri": "https://bio.tools/mlst",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2830",
                            "term": "Immunoproteins and antigens"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "Multi Locus Sequence Typing from an assembled genome or from a set of reads.",
                "homepage": "http://cge.cbs.dtu.dk/services/MLST/"
            }
        ],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/mlst:2.19.0--hdfd78af_1' : 'quay.io/biocontainers/mlst:2.19.0--hdfd78af_1' }\""
        ],
        "when": "",
        "stub": ""
    },
    "RGI_MAIN": {
        "name_process": "RGI_MAIN",
        "string_process": "\nprocess RGI_MAIN {\n    tag \"$meta.id\"\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/rgi:5.2.1--pyha8f3691_2' :\n        'quay.io/biocontainers/rgi:5.2.1--pyha8f3691_2' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.json\"), emit: json\n    tuple val(meta), path(\"*.txt\") , emit: tsv\n    path \"*.{log,err}\"             , emit: logs, optional: true\n    path \".command.*\"              , emit: nf_logs\n    path \"versions.yml\"            , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    rgi \\\\\n        main \\\\\n        $options.args \\\\\n        --clean \\\\\n        --data wgs \\\\\n        --num_threads $task.cpus \\\\\n        --output_file $prefix \\\\\n        --input_sequence $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        rgi: \\$(rgi main --version)\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 37,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    rgi \\\\\n        main \\\\\n        $options.args \\\\\n        --clean \\\\\n        --data wgs \\\\\n        --num_threads $task.cpus \\\\\n        --output_file $prefix \\\\\n        --input_sequence $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        rgi: \\$(rgi main --version)\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "Rgin",
            "Domainoid"
        ],
        "tools_url": [
            "https://bio.tools/rgin",
            "https://bio.tools/Domainoid"
        ],
        "tools_dico": [
            {
                "name": "Rgin",
                "uri": "https://bio.tools/rgin",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "GWAS study"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0199",
                            "term": "Genetic variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2885",
                            "term": "DNA polymorphism"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "GWAS analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "GWAS"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "Genome-wide association study"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0199",
                            "term": "DNA variation"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2497",
                                    "term": "Pathway or network analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "C++ implementation of SConES.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/Rgin.html"
            },
            {
                "name": "Domainoid",
                "uri": "https://bio.tools/Domainoid",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0736",
                            "term": "Protein folds and structural domains"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0130",
                            "term": "Protein folding, stability and design"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0303",
                                    "term": "Fold recognition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3435",
                                    "term": "Standardisation and normalisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0327",
                                    "term": "Phylogenetic footprinting"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0303",
                                    "term": "Protein domain prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0303",
                                    "term": "Fold prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0303",
                                    "term": "Protein fold recognition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0303",
                                    "term": "Domain prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0303",
                                    "term": "Protein fold prediction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "domain-oriented orthology inference.\n\nBACKGROUND:Orthology inference is normally based on full-length protein sequences. However, most proteins contain independently folding and recurring regions, domains. The domain architecture of a protein is vital for its function, and recombination events mean individual domains can have different evolutionary histories. It has previously been shown that orthologous proteins may differ in domain architecture, creating challenges for orthology inference methods operating on full-length sequences. We have developed Domainoid, a new tool aiming to overcome these challenges faced by full-length orthology methods by inferring orthology on the domain level. It employs the InParanoid algorithm on single domains separately, to infer groups of orthologous domains. RESULTS:This domain-oriented approach allows detection of discordant domain orthologs, cases where different domains on the same protein have different evolutionary histories",
                "homepage": "https://bitbucket.org/sonnhammergroup/domainoid/"
            }
        ],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/rgi:5.2.1--pyha8f3691_2' : 'quay.io/biocontainers/rgi:5.2.1--pyha8f3691_2' }\""
        ],
        "when": "",
        "stub": ""
    },
    "SEQUENCE_TYPE": {
        "name_process": "SEQUENCE_TYPE",
        "string_process": "\nprocess SEQUENCE_TYPE {\n                                                    \n    tag \"${meta.id} - ${schema}\"\n    label \"max_cpus_1\"\n    label \"sequence_type\"\n\n    publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options, logs_subdir: schema) }\n\n    input:\n    tuple val(meta), path(assembly), path(fq)\n    each path(dataset)\n\n    output:\n    path \"results/*\", emit: results\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    shell:\n    dataset_tarball = dataset.getName()\n    schema = dataset_tarball.replace('.tar.gz', '')\n    noclean = params.mlst_ariba_no_clean ? \"--noclean\" : \"\"\n    spades_options = params.mlst_spades_options ? \"--spades_options '${params.mlst_spades_options}'\" : \"\"\n    blast_opts = params.skip_compression ? \"\" : \"--compressed\"\n    '''\n    tar -xzvf !{dataset_tarball}\n\n    # Run BLAST\n    OUTDIR=\"results/!{schema}\"\n    mkdir -p ${OUTDIR}/blast\n    mlst-blast.py !{assembly} !{schema}/blastdb ${OUTDIR}/blast/!{meta.id}-blast.json --cpu !{task.cpus} !{blast_opts}\n\n    # Run Ariba\n    if [ \"!{meta.single_end}\" == \"false\" ]; then\n        mv !{schema}/ariba/ref_db ./\n        ariba run ref_db !{fq[0]} !{fq[1]} ariba \\\n            --nucmer_min_id !{params.mlst_nucmer_min_id} \\\n            --nucmer_min_len !{params.mlst_nucmer_min_len} \\\n            --nucmer_breaklen !{params.mlst_nucmer_breaklen} \\\n            --assembly_cov !{params.mlst_assembly_cov} \\\n            --min_scaff_depth !{params.mlst_min_scaff_depth} \\\n            --assembled_threshold !{params.mlst_assembled_threshold} \\\n            --gene_nt_extend !{params.mlst_gene_nt_extend} \\\n            --unique_threshold !{params.mlst_unique_threshold} \\\n            --threads 1 \\\n            --force \\\n            --verbose !{noclean} !{spades_options}\n        mv ariba ${OUTDIR}/\n    else\n        mkdir -p ${OUTDIR}/ariba\n        echo \"Ariba cannot be run on single end reads\" >${OUTDIR}/ariba/ariba-not-run.txt\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        ariba:  $(echo $(ariba version 2>&1) | sed 's/^.*ARIBA version: //;s/ .*$//')\n        blastn: $(echo $(blastn -version 2>&1) | sed 's/^.*blastn: //;s/ .*$//')\n    END_VERSIONS\n    '''\n}",
        "nb_lignes_process": 60,
        "string_script": "    dataset_tarball = dataset.getName()\n    schema = dataset_tarball.replace('.tar.gz', '')\n    noclean = params.mlst_ariba_no_clean ? \"--noclean\" : \"\"\n    spades_options = params.mlst_spades_options ? \"--spades_options '${params.mlst_spades_options}'\" : \"\"\n    blast_opts = params.skip_compression ? \"\" : \"--compressed\"\n    '''\n    tar -xzvf !{dataset_tarball}\n\n    # Run BLAST\n    OUTDIR=\"results/!{schema}\"\n    mkdir -p ${OUTDIR}/blast\n    mlst-blast.py !{assembly} !{schema}/blastdb ${OUTDIR}/blast/!{meta.id}-blast.json --cpu !{task.cpus} !{blast_opts}\n\n    # Run Ariba\n    if [ \"!{meta.single_end}\" == \"false\" ]; then\n        mv !{schema}/ariba/ref_db ./\n        ariba run ref_db !{fq[0]} !{fq[1]} ariba \\\n            --nucmer_min_id !{params.mlst_nucmer_min_id} \\\n            --nucmer_min_len !{params.mlst_nucmer_min_len} \\\n            --nucmer_breaklen !{params.mlst_nucmer_breaklen} \\\n            --assembly_cov !{params.mlst_assembly_cov} \\\n            --min_scaff_depth !{params.mlst_min_scaff_depth} \\\n            --assembled_threshold !{params.mlst_assembled_threshold} \\\n            --gene_nt_extend !{params.mlst_gene_nt_extend} \\\n            --unique_threshold !{params.mlst_unique_threshold} \\\n            --threads 1 \\\n            --force \\\n            --verbose !{noclean} !{spades_options}\n        mv ariba ${OUTDIR}/\n    else\n        mkdir -p ${OUTDIR}/ariba\n        echo \"Ariba cannot be run on single end reads\" >${OUTDIR}/ariba/ariba-not-run.txt\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        ariba:  $(echo $(ariba version 2>&1) | sed 's/^.*ARIBA version: //;s/ .*$//')\n        blastn: $(echo $(blastn -version 2>&1) | sed 's/^.*blastn: //;s/ .*$//')\n    END_VERSIONS\n    '''",
        "nb_lignes_script": 39,
        "language_script": "bash",
        "tools": [
            "SCHEMA"
        ],
        "tools_url": [
            "https://bio.tools/SCHEMA"
        ],
        "tools_dico": [
            {
                "name": "SCHEMA",
                "uri": "https://bio.tools/SCHEMA",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "Gene transcripts"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2830",
                            "term": "Immunoproteins and antigens"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "mRNA features"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3891",
                                    "term": "Essential dynamics"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3435",
                                    "term": "Standardisation and normalisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3561",
                                    "term": "Database comparison"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3891",
                                    "term": "PCA"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3891",
                                    "term": "Principal modes"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3891",
                                    "term": "ED"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Schema is a general algorithm for integrating heterogeneous data modalities, with application to multi-modal single-cell biological datasets.",
                "homepage": "http://schema.csail.mit.edu"
            }
        ],
        "inputs": [
            "meta",
            "assembly",
            "fq",
            "dataset"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"${meta.id} - ${schema}\"",
            "label \"max_cpus_1\"",
            "label \"sequence_type\"",
            "publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options, logs_subdir: schema) }"
        ],
        "when": "",
        "stub": ""
    },
    "SISTR": {
        "name_process": "SISTR",
        "string_process": "\nprocess SISTR {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/sistr_cmd:1.1.1--pyh864c0ab_2' :\n        'quay.io/biocontainers/sistr_cmd:1.1.1--pyh864c0ab_2' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\")            , emit: tsv\n    tuple val(meta), path(\"*-allele.fasta.gz\"), emit: allele_fasta\n    tuple val(meta), path(\"*-allele.json.gz\") , emit: allele_json\n    tuple val(meta), path(\"*-cgmlst.csv\")     , emit: cgmlst_csv\n    path \"*.{log,err}\"                        , emit: logs, optional: true\n    path \".command.*\"                         , emit: nf_logs\n    path \"versions.yml\"                       , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    sistr \\\\\n        --qc \\\\\n        $options.args \\\\\n        --threads $task.cpus \\\\\n        --alleles-output ${prefix}-allele.json \\\\\n        --novel-alleles ${prefix}-allele.fasta \\\\\n        --cgmlst-profiles ${prefix}-cgmlst.csv \\\\\n        --output-prediction ${prefix} \\\\\n        --output-format tab \\\\\n        $fasta_name\n\n    mv ${prefix}.tab ${prefix}.tsv\n    gzip ${prefix}-allele.json\n    gzip ${prefix}-allele.fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        sistr: \\$(echo \\$(sistr --version 2>&1) | sed 's/^.*sistr_cmd //; s/ .*\\$//' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 53,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    sistr \\\\\n        --qc \\\\\n        $options.args \\\\\n        --threads $task.cpus \\\\\n        --alleles-output ${prefix}-allele.json \\\\\n        --novel-alleles ${prefix}-allele.fasta \\\\\n        --cgmlst-profiles ${prefix}-cgmlst.csv \\\\\n        --output-prediction ${prefix} \\\\\n        --output-format tab \\\\\n        $fasta_name\n\n    mv ${prefix}.tab ${prefix}.tsv\n    gzip ${prefix}-allele.json\n    gzip ${prefix}-allele.fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        sistr: \\$(echo \\$(sistr --version 2>&1) | sed 's/^.*sistr_cmd //; s/ .*\\$//' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 28,
        "language_script": "bash",
        "tools": [
            "SISTR"
        ],
        "tools_url": [
            "https://bio.tools/SISTR"
        ],
        "tools_dico": [
            {
                "name": "SISTR",
                "uri": "https://bio.tools/SISTR",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3500",
                            "term": "Zoology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2830",
                            "term": "Immunoproteins and antigens"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Whole genome sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3500",
                            "term": "Metazoa"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3500",
                            "term": "Animal biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3500",
                            "term": "Animal"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3500",
                            "term": "Animals"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Genome sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "WGS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3840",
                                    "term": "Multilocus sequence typing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3840",
                                    "term": "MLST"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The Salmonella In Silico Typing Resource (SISTR) is an open-source and freely available web application for rapid in silico typing and serovar prediction from Salmonella genome assemblies using cgMLST and O and H antigen gene searching.",
                "homepage": "https://lfz.corefacility.ca/sistr-app/"
            }
        ],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/sistr_cmd:1.1.1--pyh864c0ab_2' : 'quay.io/biocontainers/sistr_cmd:1.1.1--pyh864c0ab_2' }\""
        ],
        "when": "",
        "stub": ""
    },
    "ASSEMBLE_GENOME": {
        "name_process": "ASSEMBLE_GENOME",
        "string_process": "\nprocess ASSEMBLE_GENOME {\n                                                                     \n    tag \"${meta.id}\"\n    label \"max_cpu_75\"\n    label \"assemble_genome\"\n\n    publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode,  overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    input:\n    tuple val(meta), path(fq), path(extra), path(genome_size)\n\n    output:\n    tuple val(meta), path(genome_size), path(\"results/${meta.id}.{fna,fna.gz}\"), file(\"total_contigs_*\"), emit: fna, optional: true\n    tuple val(meta), path(\"results/${meta.id}.{fna,fna.gz}\"), emit: fna_only, optional: true\n    tuple val(meta), path(\"results/${meta.id}.{fna,fna.gz}\"), path(fq), emit: fna_fastq, optional: true\n    tuple val(meta), path(\"blastdb/*\"), emit: blastdb, optional: true\n    path \"results/*\"\n    path \"${meta.id}-assembly-error.txt\", optional: true\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    shell:\n                \n    no_miniasm = params.no_miniasm ? \"--no_miniasm\" : \"\"\n    no_rotate = params.no_rotate ? \"--no_rotate\" : \"\"\n    no_pilon = params.no_polish ? \"--no_pilon\" : \"\"\n    keep = params.keep_all_files ? \"--keep 3\" : \"--keep 1\"\n    is_hybrid = meta.runtype == \"hybrid\" ? \"-l ${extra}\" : \"\"\n    unicycler_opts = \"${no_miniasm} ${no_rotate} ${no_pilon} ${is_hybrid} ${keep}\"\n\n              \n    contig_namefmt = params.contig_namefmt ? params.contig_namefmt : \"${meta.id}_%05d\"\n    shovill_ram = task.memory.toString().split(' ')[0].toInteger()-1\n    opts = params.shovill_opts ? \"--opts '${params.shovill_opts}'\" : \"\"\n    kmers = params.shovill_kmers ? \"--kmers '${params.shovill_kmers}'\" : \"\"\n    nostitch = params.no_stitch ? \"--nostitch\" : \"\"\n    nocorr = params.no_corr ? \"--nocorr\" : \"\"\n    shovill_mode = meta.single_end == false ? \"shovill --R1 ${fq[0]} --R2 ${fq[1]} ${nostitch}\" : \"shovill-se --SE ${fq[0]}\"\n    shovill_opts = \"--assembler ${params.shovill_assembler} --depth 0 --noreadcorr ${opts} ${kmers} ${nocorr}\"\n    \n                 \n    nopolish = params.no_polish ? \"--nopolish\" : \"\"\n    medaka_model = params.medaka_model ? \"--model ${params.medaka_model}\" : \"\"\n    dragonflye_opts = \"--assembler ${params.dragonflye_assembler} --depth 0 --minreadlen 0 --minquality 0 --racon ${params.racon_steps} --medaka ${params.medaka_steps} ${medaka_model} ${nopolish}\"\n\n                                   \n    assembler_wf = meta.runtype == \"ont\" ? \"dragonflye\" : \"shovill\"\n    assembler_mode = meta.runtype == \"ont\" ? \"dragonflye --reads ${fq[0]}\" : shovill_mode\n    assemnber_opts = meta.runtype == \"ont\" ? dragonflye_opts : shovill_opts\n\n                      \n    use_original_assembly = null\n    if (meta.runtype.startsWith('assembly')) {\n        use_original_assembly = params.reassemble ? false : true\n    }\n    '''\n    OUTDIR=results\n    GENOME_SIZE=`head -n 1 !{genome_size}`\n    if [ \"!{use_original_assembly}\" == \"true\" ]; then\n        mkdir ${OUTDIR}\n        gzip -cd !{extra} > ${OUTDIR}/!{meta.id}.fna\n    elif [[ \"!{meta.runtype}\" == \"hybrid\"  || \"!{params.use_unicycler}\" == \"true\" ]]; then\n        unicycler -1 !{fq[0]} -2 !{fq[1]} -o ${OUTDIR}/ --no_correct \\\n            --min_fasta_length !{params.min_contig_len} \\\n            --threads !{task.cpus} \\\n            --mode !{params.unicycler_mode} \\\n            --min_polish_size !{params.min_polish_size} \\\n            --min_component_size !{params.min_component_size} \\\n            --min_dead_end_size !{params.min_dead_end_size} !{unicycler_opts}\n        sed -r 's/^>([0-9]+)(.*)/>!{meta.id}_\\\\1\\\\2/' ${OUTDIR}/assembly.fasta > ${OUTDIR}/!{meta.id}.fna\n        mv ${OUTDIR}/assembly.gfa ${OUTDIR}/unicycler.gfa\n    else\n        # Shovill or Dragonflye\n        !{assembler_mode} --gsize ${GENOME_SIZE} \\\n            --outdir ${OUTDIR} \\\n            --force \\\n            --minlen !{params.min_contig_len} \\\n            --mincov !{params.min_contig_cov} \\\n            --namefmt \"!{contig_namefmt}\" \\\n            --keepfiles \\\n            --cpus !{task.cpus} \\\n            --ram !{shovill_ram} !{assemnber_opts}\n        mv ${OUTDIR}/contigs.fa ${OUTDIR}/!{meta.id}.fna\n\n        # Rename Graphs\n        if [ \"!{assembler_wf}\" == \"shovill\" ]; then \n            if [ -f \"${OUTDIR}/contigs.gfa\" ]; then\n                mv ${OUTDIR}/contigs.gfa ${OUTDIR}/!{params.shovill_assembler}-unpolished.gfa\n            elif [ -f \"${OUTDIR}/contigs.fastg\" ]; then\n                mv ${OUTDIR}/contigs.fastg ${OUTDIR}/!{params.shovill_assembler}-unpolished.gfa\n            elif [ -f \"${OUTDIR}/contigs.LastGraph\" ]; then\n                mv ${OUTDIR}/contigs.LastGraph ${OUTDIR}/!{params.shovill_assembler}-unpolished.gfa\n            fi\n        fi\n\n        if [ -f \"${OUTDIR}/flye-info.txt\" ]; then\n            mv ${OUTDIR}/flye-info.txt ${OUTDIR}/flye.log\n        fi\n    fi\n\n\n    # Check quality of assembly\n    TOTAL_CONTIGS=`grep -c \"^>\" ${OUTDIR}/!{meta.id}.fna || true`\n    touch \"total_contigs_${TOTAL_CONTIGS}\"\n    if [ \"${TOTAL_CONTIGS}\" -gt 0 ]; then\n        assembly-scan ${OUTDIR}/!{meta.id}.fna > ${OUTDIR}/!{meta.id}.json\n        TOTAL_CONTIG_SIZE=$(grep \"total_contig_length\" ${OUTDIR}/!{meta.id}.json | sed -r 's/.*: ([0-9]+),/\\\\1/')\n        if [ \"${TOTAL_CONTIG_SIZE}\" -lt !{params.min_genome_size} ]; then\n            mv ${OUTDIR}/!{meta.id}.fna ${OUTDIR}/!{meta.id}-error.fna\n            mv ${OUTDIR}/!{meta.id}.json ${OUTDIR}/!{meta.id}-error.json\n            echo \"!{meta.id} assembled size (${TOTAL_CONTIG_SIZE} bp) is less than the minimum allowed genome\n                    size (!{params.min_genome_size} bp). If this is unexpected, please investigate !{meta.id} to\n                    determine a cause (e.g. metagenomic, contaminants, etc...) for the poor assembly.\n                    Otherwise, adjust the --min_genome_size parameter to fit your need. Further assembly\n                    based analysis of !{meta.id} will be discontinued.\" | \\\n            sed 's/^\\\\s*//' > !{meta.id}-assembly-error.txt\n        else\n            # Make BLASTDB\n            mkdir blastdb\n            cat ${OUTDIR}/!{meta.id}.fna | makeblastdb -dbtype \"nucl\" -title \"Assembled contigs for !{meta.id}\" -out blastdb/!{meta.id}\n        fi\n    else\n        mv ${OUTDIR}/!{meta.id}.fna ${OUTDIR}/!{meta.id}-error.fna\n        echo \"!{meta.id} assembled successfully, but 0 contigs were formed. Please investigate\n                !{meta.id} to determine a cause (e.g. metagenomic, contaminants, etc...) for this\n                outcome. Further assembly-based analysis of !{meta.id} will be discontinued.\" | \\\n        sed 's/^\\\\s*//' > !{meta.id}-assembly-error.txt\n    fi\n\n    # Cleanup and compress\n    if [ \"!{params.keep_all_files}\" == \"false\" ]; then\n        # Remove intermediate files\n        rm -rfv ${OUTDIR}/shovill.bam* ${OUTDIR}/shovill-se.bam* ${OUTDIR}/flash.extendedFrags*  \\\n                ${OUTDIR}/flash.notCombined* ${OUTDIR}/skesa.fasta* ${OUTDIR}/*.fq.gz ${OUTDIR}/00*.gfa \\\n                ${OUTDIR}/pilon_polish* ${OUTDIR}/flye/ ${OUTDIR}/flye.fasta* ${OUTDIR}/raven/  \\\n                ${OUTDIR}/raven.fasta* ${OUTDIR}/raven.cereal ${OUTDIR}/miniasm/ ${OUTDIR}/miniasm.fasta* \\\n                ${OUTDIR}/spades/ ${OUTDIR}/spades.fasta* ${OUTDIR}/megahit/ ${OUTDIR}/megahit.fasta* \\\n                ${OUTDIR}/velvet.fasta* ${OUTDIR}/velvet/\n    fi\n\n    if [[ !{params.skip_compression} == \"false\" ]]; then\n        # Compress based on matched extensions\n        find ${OUTDIR}/ -type f | \\\n            grep -E \"\\\\.fna$|\\\\.fasta$|\\\\.fa$|\\\\.gfa$\" | \\\n            xargs -I {} pigz -n --best -p !{task.cpus} {}\n    fi\n    find ${OUTDIR} -maxdepth 1 -name \"*.log\" | xargs -I {} mv {} ./\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        any2fasta:  $(echo $(any2fasta -v 2>&1) | sed 's/any2fasta //')\n        assembly-scan: $(echo $(assembly-scan --version 2>&1) | sed 's/assembly-scan //')\n        bwa: $(echo $(bwa 2>&1) | sed 's/^.*Version: //;s/ .*$//')\n        flash: $(echo $(flash --version 2>&1) | sed 's/^.*FLASH v//;s/ .*$//')\n        flye: $(echo $(flye --version))\n        makeblastdb: $(echo $(makeblastdb -version 2>&1) | sed 's/^.*makeblastdb: //;s/ .*$//')\n        medaka: $(echo $(medaka --version 2>&1) | sed 's/medaka //')\n        megahit: $(echo $(megahit --version 2>&1) | sed 's/MEGAHIT v//')\n        miniasm: $(echo $(miniasm -V))\n        minimap2: $(echo $(minimap2 --version))\n        nanoq: $(echo $(nanoq --version 2>&1) | sed 's/nanoq //')\n        pigz: $(echo $(pigz --version 2>&1) | sed 's/pigz //')\n        pilon: $(echo $(pilon --version 2>&1) | sed 's/^.*Pilon version //;s/ .*$//')\n        racon: $(echo $(racon --version 2>&1) | sed 's/v//')\n        rasusa: $(echo $(rasusa --version 2>&1) | sed 's/rasusa //')\n        raven: $(echo $(raven --version))\n        samclip: $(echo $(samclip --version 2>&1) | sed 's/samclip //')\n        samtools: $(echo $(samtools --version 2>&1) |sed 's/^.*samtools //;s/ .*$//')\n        shovill: $(echo $(shovill --version 2>&1) | sed 's/shovill //')\n        shovill-se: $(echo $(shovill-se --version 2>&1) | sed 's/shovill-se //')\n        skesa: $(echo $(skesa --version 2>&1) | sed 's/^.*SKESA //;s/ .*$//')\n        spades.py: $(echo $(spades.py --version 2>&1) | sed 's/SPAdes genome assembler v//')\n        velvetg: $(echo $(velvetg 2>&1) | sed 's/^.*Version //;s/ .*$//')\n        velveth: $(echo $(velveth 2>&1) | sed 's/^.*Version //;s/ .*$//')\n        unicycler: $(echo $(unicycler --version 2>&1) | sed 's/^.*Unicycler v//;s/ .*$//')\n    END_VERSIONS\n    '''\n}",
        "nb_lignes_process": 180,
        "string_script": "    no_miniasm = params.no_miniasm ? \"--no_miniasm\" : \"\"\n    no_rotate = params.no_rotate ? \"--no_rotate\" : \"\"\n    no_pilon = params.no_polish ? \"--no_pilon\" : \"\"\n    keep = params.keep_all_files ? \"--keep 3\" : \"--keep 1\"\n    is_hybrid = meta.runtype == \"hybrid\" ? \"-l ${extra}\" : \"\"\n    unicycler_opts = \"${no_miniasm} ${no_rotate} ${no_pilon} ${is_hybrid} ${keep}\"\n\n              \n    contig_namefmt = params.contig_namefmt ? params.contig_namefmt : \"${meta.id}_%05d\"\n    shovill_ram = task.memory.toString().split(' ')[0].toInteger()-1\n    opts = params.shovill_opts ? \"--opts '${params.shovill_opts}'\" : \"\"\n    kmers = params.shovill_kmers ? \"--kmers '${params.shovill_kmers}'\" : \"\"\n    nostitch = params.no_stitch ? \"--nostitch\" : \"\"\n    nocorr = params.no_corr ? \"--nocorr\" : \"\"\n    shovill_mode = meta.single_end == false ? \"shovill --R1 ${fq[0]} --R2 ${fq[1]} ${nostitch}\" : \"shovill-se --SE ${fq[0]}\"\n    shovill_opts = \"--assembler ${params.shovill_assembler} --depth 0 --noreadcorr ${opts} ${kmers} ${nocorr}\"\n    \n                 \n    nopolish = params.no_polish ? \"--nopolish\" : \"\"\n    medaka_model = params.medaka_model ? \"--model ${params.medaka_model}\" : \"\"\n    dragonflye_opts = \"--assembler ${params.dragonflye_assembler} --depth 0 --minreadlen 0 --minquality 0 --racon ${params.racon_steps} --medaka ${params.medaka_steps} ${medaka_model} ${nopolish}\"\n\n                                   \n    assembler_wf = meta.runtype == \"ont\" ? \"dragonflye\" : \"shovill\"\n    assembler_mode = meta.runtype == \"ont\" ? \"dragonflye --reads ${fq[0]}\" : shovill_mode\n    assemnber_opts = meta.runtype == \"ont\" ? dragonflye_opts : shovill_opts\n\n                      \n    use_original_assembly = null\n    if (meta.runtype.startsWith('assembly')) {\n        use_original_assembly = params.reassemble ? false : true\n    }\n    '''\n    OUTDIR=results\n    GENOME_SIZE=`head -n 1 !{genome_size}`\n    if [ \"!{use_original_assembly}\" == \"true\" ]; then\n        mkdir ${OUTDIR}\n        gzip -cd !{extra} > ${OUTDIR}/!{meta.id}.fna\n    elif [[ \"!{meta.runtype}\" == \"hybrid\"  || \"!{params.use_unicycler}\" == \"true\" ]]; then\n        unicycler -1 !{fq[0]} -2 !{fq[1]} -o ${OUTDIR}/ --no_correct \\\n            --min_fasta_length !{params.min_contig_len} \\\n            --threads !{task.cpus} \\\n            --mode !{params.unicycler_mode} \\\n            --min_polish_size !{params.min_polish_size} \\\n            --min_component_size !{params.min_component_size} \\\n            --min_dead_end_size !{params.min_dead_end_size} !{unicycler_opts}\n        sed -r 's/^>([0-9]+)(.*)/>!{meta.id}_\\\\1\\\\2/' ${OUTDIR}/assembly.fasta > ${OUTDIR}/!{meta.id}.fna\n        mv ${OUTDIR}/assembly.gfa ${OUTDIR}/unicycler.gfa\n    else\n        # Shovill or Dragonflye\n        !{assembler_mode} --gsize ${GENOME_SIZE} \\\n            --outdir ${OUTDIR} \\\n            --force \\\n            --minlen !{params.min_contig_len} \\\n            --mincov !{params.min_contig_cov} \\\n            --namefmt \"!{contig_namefmt}\" \\\n            --keepfiles \\\n            --cpus !{task.cpus} \\\n            --ram !{shovill_ram} !{assemnber_opts}\n        mv ${OUTDIR}/contigs.fa ${OUTDIR}/!{meta.id}.fna\n\n        # Rename Graphs\n        if [ \"!{assembler_wf}\" == \"shovill\" ]; then \n            if [ -f \"${OUTDIR}/contigs.gfa\" ]; then\n                mv ${OUTDIR}/contigs.gfa ${OUTDIR}/!{params.shovill_assembler}-unpolished.gfa\n            elif [ -f \"${OUTDIR}/contigs.fastg\" ]; then\n                mv ${OUTDIR}/contigs.fastg ${OUTDIR}/!{params.shovill_assembler}-unpolished.gfa\n            elif [ -f \"${OUTDIR}/contigs.LastGraph\" ]; then\n                mv ${OUTDIR}/contigs.LastGraph ${OUTDIR}/!{params.shovill_assembler}-unpolished.gfa\n            fi\n        fi\n\n        if [ -f \"${OUTDIR}/flye-info.txt\" ]; then\n            mv ${OUTDIR}/flye-info.txt ${OUTDIR}/flye.log\n        fi\n    fi\n\n\n    # Check quality of assembly\n    TOTAL_CONTIGS=`grep -c \"^>\" ${OUTDIR}/!{meta.id}.fna || true`\n    touch \"total_contigs_${TOTAL_CONTIGS}\"\n    if [ \"${TOTAL_CONTIGS}\" -gt 0 ]; then\n        assembly-scan ${OUTDIR}/!{meta.id}.fna > ${OUTDIR}/!{meta.id}.json\n        TOTAL_CONTIG_SIZE=$(grep \"total_contig_length\" ${OUTDIR}/!{meta.id}.json | sed -r 's/.*: ([0-9]+),/\\\\1/')\n        if [ \"${TOTAL_CONTIG_SIZE}\" -lt !{params.min_genome_size} ]; then\n            mv ${OUTDIR}/!{meta.id}.fna ${OUTDIR}/!{meta.id}-error.fna\n            mv ${OUTDIR}/!{meta.id}.json ${OUTDIR}/!{meta.id}-error.json\n            echo \"!{meta.id} assembled size (${TOTAL_CONTIG_SIZE} bp) is less than the minimum allowed genome\n                    size (!{params.min_genome_size} bp). If this is unexpected, please investigate !{meta.id} to\n                    determine a cause (e.g. metagenomic, contaminants, etc...) for the poor assembly.\n                    Otherwise, adjust the --min_genome_size parameter to fit your need. Further assembly\n                    based analysis of !{meta.id} will be discontinued.\" | \\\n            sed 's/^\\\\s*//' > !{meta.id}-assembly-error.txt\n        else\n            # Make BLASTDB\n            mkdir blastdb\n            cat ${OUTDIR}/!{meta.id}.fna | makeblastdb -dbtype \"nucl\" -title \"Assembled contigs for !{meta.id}\" -out blastdb/!{meta.id}\n        fi\n    else\n        mv ${OUTDIR}/!{meta.id}.fna ${OUTDIR}/!{meta.id}-error.fna\n        echo \"!{meta.id} assembled successfully, but 0 contigs were formed. Please investigate\n                !{meta.id} to determine a cause (e.g. metagenomic, contaminants, etc...) for this\n                outcome. Further assembly-based analysis of !{meta.id} will be discontinued.\" | \\\n        sed 's/^\\\\s*//' > !{meta.id}-assembly-error.txt\n    fi\n\n    # Cleanup and compress\n    if [ \"!{params.keep_all_files}\" == \"false\" ]; then\n        # Remove intermediate files\n        rm -rfv ${OUTDIR}/shovill.bam* ${OUTDIR}/shovill-se.bam* ${OUTDIR}/flash.extendedFrags*  \\\n                ${OUTDIR}/flash.notCombined* ${OUTDIR}/skesa.fasta* ${OUTDIR}/*.fq.gz ${OUTDIR}/00*.gfa \\\n                ${OUTDIR}/pilon_polish* ${OUTDIR}/flye/ ${OUTDIR}/flye.fasta* ${OUTDIR}/raven/  \\\n                ${OUTDIR}/raven.fasta* ${OUTDIR}/raven.cereal ${OUTDIR}/miniasm/ ${OUTDIR}/miniasm.fasta* \\\n                ${OUTDIR}/spades/ ${OUTDIR}/spades.fasta* ${OUTDIR}/megahit/ ${OUTDIR}/megahit.fasta* \\\n                ${OUTDIR}/velvet.fasta* ${OUTDIR}/velvet/\n    fi\n\n    if [[ !{params.skip_compression} == \"false\" ]]; then\n        # Compress based on matched extensions\n        find ${OUTDIR}/ -type f | \\\n            grep -E \"\\\\.fna$|\\\\.fasta$|\\\\.fa$|\\\\.gfa$\" | \\\n            xargs -I {} pigz -n --best -p !{task.cpus} {}\n    fi\n    find ${OUTDIR} -maxdepth 1 -name \"*.log\" | xargs -I {} mv {} ./\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        any2fasta:  $(echo $(any2fasta -v 2>&1) | sed 's/any2fasta //')\n        assembly-scan: $(echo $(assembly-scan --version 2>&1) | sed 's/assembly-scan //')\n        bwa: $(echo $(bwa 2>&1) | sed 's/^.*Version: //;s/ .*$//')\n        flash: $(echo $(flash --version 2>&1) | sed 's/^.*FLASH v//;s/ .*$//')\n        flye: $(echo $(flye --version))\n        makeblastdb: $(echo $(makeblastdb -version 2>&1) | sed 's/^.*makeblastdb: //;s/ .*$//')\n        medaka: $(echo $(medaka --version 2>&1) | sed 's/medaka //')\n        megahit: $(echo $(megahit --version 2>&1) | sed 's/MEGAHIT v//')\n        miniasm: $(echo $(miniasm -V))\n        minimap2: $(echo $(minimap2 --version))\n        nanoq: $(echo $(nanoq --version 2>&1) | sed 's/nanoq //')\n        pigz: $(echo $(pigz --version 2>&1) | sed 's/pigz //')\n        pilon: $(echo $(pilon --version 2>&1) | sed 's/^.*Pilon version //;s/ .*$//')\n        racon: $(echo $(racon --version 2>&1) | sed 's/v//')\n        rasusa: $(echo $(rasusa --version 2>&1) | sed 's/rasusa //')\n        raven: $(echo $(raven --version))\n        samclip: $(echo $(samclip --version 2>&1) | sed 's/samclip //')\n        samtools: $(echo $(samtools --version 2>&1) |sed 's/^.*samtools //;s/ .*$//')\n        shovill: $(echo $(shovill --version 2>&1) | sed 's/shovill //')\n        shovill-se: $(echo $(shovill-se --version 2>&1) | sed 's/shovill-se //')\n        skesa: $(echo $(skesa --version 2>&1) | sed 's/^.*SKESA //;s/ .*$//')\n        spades.py: $(echo $(spades.py --version 2>&1) | sed 's/SPAdes genome assembler v//')\n        velvetg: $(echo $(velvetg 2>&1) | sed 's/^.*Version //;s/ .*$//')\n        velveth: $(echo $(velveth 2>&1) | sed 's/^.*Version //;s/ .*$//')\n        unicycler: $(echo $(unicycler --version 2>&1) | sed 's/^.*Unicycler v//;s/ .*$//')\n    END_VERSIONS\n    '''",
        "nb_lignes_script": 154,
        "language_script": "bash",
        "tools": [
            "KmerStream",
            "nanopolish",
            "Unicycler",
            "ssize",
            "MBASED",
            "SPAdes"
        ],
        "tools_url": [
            "https://bio.tools/kmerstream",
            "https://bio.tools/nanopolish",
            "https://bio.tools/unicycler",
            "https://bio.tools/ssize",
            "https://bio.tools/mbased",
            "https://bio.tools/spades"
        ],
        "tools_dico": [
            {
                "name": "KmerStream",
                "uri": "https://bio.tools/kmerstream",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Streaming algorithm for estimating the number of distinct k-mers present in high throughput sequencing data.",
                "homepage": "https://github.com/pmelsted/KmerStream"
            },
            {
                "name": "nanopolish",
                "uri": "https://bio.tools/nanopolish",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0199",
                            "term": "Genetic variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0199",
                            "term": "DNA variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3197",
                                    "term": "Genetic variation analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3204",
                                    "term": "Methylation analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0310",
                                    "term": "Sequence assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3197",
                                    "term": "Sequence variation analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3197",
                                    "term": "Variant analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3197",
                                    "term": "Genetic variation annotation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3204",
                                    "term": "Methylation profile analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Cartography"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2093",
                                "term": "Data reference"
                            },
                            {
                                "uri": "http://edamontology.org/data_0849",
                                "term": "Sequence record"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0867",
                                "term": "Sequence alignment report"
                            }
                        ]
                    }
                ],
                "description": "A package for detecting cytosine methylations and genetic variations from nanopore MinION sequencing data.",
                "homepage": "https://github.com/jts/nanopolish"
            },
            {
                "name": "Unicycler",
                "uri": "https://bio.tools/unicycler",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3436",
                                    "term": "Aggregation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0925",
                                "term": "Sequence assembly"
                            }
                        ]
                    }
                ],
                "description": "A tool for assembling bacterial genomes from a combination of short (2nd generation) and long (3rd generation) sequencing reads.",
                "homepage": "https://github.com/rrwick/Unicycler"
            },
            {
                "name": "ssize",
                "uri": "https://bio.tools/ssize",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene expression profiling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differentially expressed gene identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential expression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene expression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Functions for computing and displaying sample size information for gene expression arrays.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/ssize.html"
            },
            {
                "name": "MBASED",
                "uri": "https://bio.tools/mbased",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3800",
                                    "term": "RNA-Seq quantification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3800",
                                    "term": "RNA-Seq quantitation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The package implements the corresponding algorithm for detecting allele-specific gene expression from RNA count data, where allele counts at individual loci (SNVs) are integrated into a gene-specific measure of ASE, and utilizes simulations to appropriately assess the statistical significance of observed ASE.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/MBASED.html"
            },
            {
                "name": "SPAdes",
                "uri": "https://bio.tools/spades",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_0006",
                                "term": "Data"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0006",
                                "term": "Data"
                            },
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            }
                        ]
                    }
                ],
                "description": "St. Petersburg genome assembler \u2013 is intended for both standard isolates and single-cell MDA bacteria assemblies. SPAdes 3.9 works with Illumina or IonTorrent reads and is capable of providing hybrid assemblies using PacBio, Oxford Nanopore and Sanger reads. Additional contigs can be provided and can be used as long reads.",
                "homepage": "http://cab.spbu.ru/software/spades/"
            }
        ],
        "inputs": [
            "meta",
            "fq",
            "extra",
            "genome_size"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"${meta.id}\"",
            "label \"max_cpu_75\"",
            "label \"assemble_genome\"",
            "publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }"
        ],
        "when": "",
        "stub": ""
    },
    "ANTIMICROBIAL_RESISTANCE": {
        "name_process": "ANTIMICROBIAL_RESISTANCE",
        "string_process": "\nprocess ANTIMICROBIAL_RESISTANCE {\n      \n                                                                                                     \n                                          \n      \n    tag \"${meta.id}\"\n    label \"antimicrobial_resistance\"\n\n    publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    input:\n    tuple val(meta), path(genes), path(proteins)\n    each path(amrdb)\n\n    output:\n    tuple val(meta), path(\"*{gene,protein}-{point-mutations,report}.txt\"), emit: results\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    shell:\n    plus = params.amr_plus ? \"--plus\" : \"\"\n    report_common = params.amr_report_common ? \"--report_common\" : \"\"\n    organism_gene = \"\"\n    organism_protein = \"\"\n    if (params.amr_organism) {\n        organism_gene = \"-O ${params.amr_organism} --point_mut_all ${meta.id}-gene-point-mutations.txt\"\n        organism_protein = \"-O ${params.amr_organism} --point_mut_all ${meta.id}-protein-point-mutations.txt\"\n    }\n    '''\n    if [[ !{params.skip_compression} == \"false\" ]]; then\n        # Files passed to other modules\n        gunzip -c !{meta.id}.faa.gz > !{meta.id}.faa\n        gunzip -c !{meta.id}.ffn.gz > !{meta.id}.ffn\n    fi\n\n    tar -xzvf !{amrdb}\n    amrfinder -n !{meta.id}.ffn \\\n            -d amrfinderdb/ \\\n            -o !{meta.id}-gene-report.txt \\\n            --ident_min !{params.amr_ident_min} \\\n            --coverage_min !{params.amr_coverage_min} \\\n            --translation_table !{params.amr_translation_table} \\\n            --threads !{task.cpus} !{organism_gene} !{plus} !{report_common}\n\n    amrfinder -p !{meta.id}.faa \\\n            -d amrfinderdb/ \\\n            -o !{meta.id}-protein-report.txt \\\n            --ident_min !{params.amr_ident_min} \\\n            --coverage_min !{params.amr_coverage_min} \\\n            --translation_table !{params.amr_translation_table} \\\n            --threads !{task.cpus} !{organism_protein} !{plus} !{report_common}\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        amrfinder:  $(echo $(amrfinder --version 2>&1))\n    END_VERSIONS\n    '''\n}",
        "nb_lignes_process": 60,
        "string_script": "    plus = params.amr_plus ? \"--plus\" : \"\"\n    report_common = params.amr_report_common ? \"--report_common\" : \"\"\n    organism_gene = \"\"\n    organism_protein = \"\"\n    if (params.amr_organism) {\n        organism_gene = \"-O ${params.amr_organism} --point_mut_all ${meta.id}-gene-point-mutations.txt\"\n        organism_protein = \"-O ${params.amr_organism} --point_mut_all ${meta.id}-protein-point-mutations.txt\"\n    }\n    '''\n    if [[ !{params.skip_compression} == \"false\" ]]; then\n        # Files passed to other modules\n        gunzip -c !{meta.id}.faa.gz > !{meta.id}.faa\n        gunzip -c !{meta.id}.ffn.gz > !{meta.id}.ffn\n    fi\n\n    tar -xzvf !{amrdb}\n    amrfinder -n !{meta.id}.ffn \\\n            -d amrfinderdb/ \\\n            -o !{meta.id}-gene-report.txt \\\n            --ident_min !{params.amr_ident_min} \\\n            --coverage_min !{params.amr_coverage_min} \\\n            --translation_table !{params.amr_translation_table} \\\n            --threads !{task.cpus} !{organism_gene} !{plus} !{report_common}\n\n    amrfinder -p !{meta.id}.faa \\\n            -d amrfinderdb/ \\\n            -o !{meta.id}-protein-report.txt \\\n            --ident_min !{params.amr_ident_min} \\\n            --coverage_min !{params.amr_coverage_min} \\\n            --translation_table !{params.amr_translation_table} \\\n            --threads !{task.cpus} !{organism_protein} !{plus} !{report_common}\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        amrfinder:  $(echo $(amrfinder --version 2>&1))\n    END_VERSIONS\n    '''",
        "nb_lignes_script": 37,
        "language_script": "bash",
        "tools": [
            "OCplus"
        ],
        "tools_url": [
            "https://bio.tools/ocplus"
        ],
        "tools_dico": [
            {
                "name": "OCplus",
                "uri": "https://bio.tools/ocplus",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0204",
                            "term": "Gene regulation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarray experiment"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarrays"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression data analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package allows to characterize the operating characteristics of a microarray experiment, i.e. the trade-off between false discovery rate and the power to detect truly regulated genes. The package includes tools both for planned experiments (for sample size assessment) and for already collected data (identification of differentially expressed genes).",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/OCplus.html"
            }
        ],
        "inputs": [
            "meta",
            "genes",
            "proteins",
            "amrdb"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"${meta.id}\"",
            "label \"antimicrobial_resistance\"",
            "publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }"
        ],
        "when": "",
        "stub": ""
    },
    "GTDBTK_CLASSIFYWF": {
        "name_process": "GTDBTK_CLASSIFYWF",
        "string_process": "\nprocess GTDBTK_CLASSIFYWF {\n    tag \"${meta.id}\"\n    label 'process_high'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gtdbtk:1.7.0--pyhdfd78af_0' :\n        'quay.io/biocontainers/gtdbtk:1.7.0--pyhdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(fna, stageAs: 'fna-tmp/*')\n    path db, stageAs: 'gtdb/*'\n\n    output:\n    path \"results/*\"   , emit: results\n    path \"*.{log,err}\" , emit: logs, optional: true\n    path \".command.*\"  , emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    export GTDBTK_DATA_PATH=\"${params.gtdb}\"\n    mkdir fna\n    cp -L fna-tmp/* fna/\n    find fna/ -name \"*.fna.gz\" | xargs gunzip\n\n    gtdbtk classify_wf \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --pplacer_cpus $task.cpus \\\\\n        --genome_dir ./fna \\\\\n        --out_dir results \\\\\n        --prefix ${prefix}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gtdb-tk: \\$(echo \\$(gtdbtk --version -v 2>&1) | sed \"s/gtdbtk: version //; s/ Copyright.*//\")\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 42,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    export GTDBTK_DATA_PATH=\"${params.gtdb}\"\n    mkdir fna\n    cp -L fna-tmp/* fna/\n    find fna/ -name \"*.fna.gz\" | xargs gunzip\n\n    gtdbtk classify_wf \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --pplacer_cpus $task.cpus \\\\\n        --genome_dir ./fna \\\\\n        --out_dir results \\\\\n        --prefix ${prefix}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gtdb-tk: \\$(echo \\$(gtdbtk --version -v 2>&1) | sed \"s/gtdbtk: version //; s/ Copyright.*//\")\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 19,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fna",
            "db"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"${meta.id}\"",
            "label 'process_high'",
            "publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/gtdbtk:1.7.0--pyhdfd78af_0' : 'quay.io/biocontainers/gtdbtk:1.7.0--pyhdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "GTDBTK_SETUPDB": {
        "name_process": "GTDBTK_SETUPDB",
        "string_process": "\nprocess GTDBTK_SETUPDB {\n    label 'process_high'\n    publishDir \"${params.gtdb}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gtdbtk:1.7.0--pyhdfd78af_0' :\n        'quay.io/biocontainers/gtdbtk:1.7.0--pyhdfd78af_0' }\"\n\n    output:\n    path(\"${params.gtdb}/*\"), emit: db\n    path \"*.{log,err}\"      , emit: logs, optional: true\n    path \".command.*\"       , emit: nf_logs\n    path \"versions.yml\"     , emit: versions\n\n    script:\n    \"\"\"\n    export GTDBTK_DATA_PATH=\"${params.gtdb}\"\n    if [ \"${params.download_gtdb}\" == \"true\" ]; then\n        rm -rf !{params.gtdb}/*.tar.gz\n        download-db.sh\n    else\n        echo \"skipping GTDB database download\"\n    fi\n\n    if [ \"${params.skip_check}\" == \"false\" ]; then\n        gtdbtk check_install && touch gtdb-setup.txt\n    else\n        echo \"skipping GTDB database checks\"\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gtdbtk: \\$(echo \\$(gtdbtk --version -v 2>&1) | sed \"s/gtdbtk: version //; s/ Copyright.*//\")\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 37,
        "string_script": "    \"\"\"\n    export GTDBTK_DATA_PATH=\"${params.gtdb}\"\n    if [ \"${params.download_gtdb}\" == \"true\" ]; then\n        rm -rf !{params.gtdb}/*.tar.gz\n        download-db.sh\n    else\n        echo \"skipping GTDB database download\"\n    fi\n\n    if [ \"${params.skip_check}\" == \"false\" ]; then\n        gtdbtk check_install && touch gtdb-setup.txt\n    else\n        echo \"skipping GTDB database checks\"\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gtdbtk: \\$(echo \\$(gtdbtk --version -v 2>&1) | sed \"s/gtdbtk: version //; s/ Copyright.*//\")\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 19,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "label 'process_high'",
            "publishDir \"${params.gtdb}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/gtdbtk:1.7.0--pyhdfd78af_0' : 'quay.io/biocontainers/gtdbtk:1.7.0--pyhdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MINMER_SKETCH": {
        "name_process": "MINMER_SKETCH",
        "string_process": "\nprocess MINMER_SKETCH {\n      \n                                                                    \n                                              \n      \n    tag \"${meta.id}\"\n    label \"base_mem_8gb\"\n    label \"minmer_sketch\"\n\n    publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    input:\n    tuple val(meta), path(fq)\n\n    output:\n    tuple val(meta), path(fq), path(\"${meta.id}.sig\"), emit: sketch\n    path(\"${meta.id}*.{msh,sig}\")\n    path(\"${meta.id}.ctx\"), optional: true\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    shell:\n    fastq = meta.single_end ? fq[0] : \"${fq[0]} ${fq[1]}\"\n    mccortex_fq = meta.single_end ? \"-1 ${fq[0]}\" : \"-2 ${fq[0]}:${fq[1]}\"\n    m = task.memory.toString().split(' ')[0].toInteger() * 1000 - 500\n    '''\n    gzip -cd !{fastq} | mash sketch -o !{meta.id}-k21 -k 21 -s !{params.sketch_size} -r -I !{meta.id} -\n    gzip -cd !{fastq} | mash sketch -o !{meta.id}-k31 -k 31 -s !{params.sketch_size} -r -I !{meta.id} -\n    sourmash sketch dna -p k=21,k=31,k=51,abund,scaled=!{params.sourmash_scale} --merge !{meta.id} -o !{meta.id}.sig !{fastq}\n\n    if [[ \"!{params.count_31mers}\" == \"true\" ]]; then\n        mccortex31 build -f -k 31 -s !{meta.id} !{mccortex_fq} -t !{task.cpus} -m !{m}mb -q temp_counts\n        if [ \"!{params.keep_singletons}\" == \"false\" ]; then\n            # Clean up Cortex file (mostly remove singletons)\n            mccortex31 clean -q -B 2 -U2 -T2 -m !{m}mb -o !{meta.id}.ctx temp_counts\n            rm temp_counts\n        else\n            mv temp_counts !{meta.id}.ctx\n        fi\n    fi\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        mash: $(echo $(mash 2>&1) | sed 's/^.*Mash version //;s/ .*$//')\n        mccortex: $(echo $(mccortex31 2>&1) | sed 's/^.*mccortex=v//;s/ .*$//')\n        sourmash: $(echo $(sourmash --version 2>&1) | sed 's/sourmash //;')\n    END_VERSIONS\n    '''\n}",
        "nb_lignes_process": 51,
        "string_script": "    fastq = meta.single_end ? fq[0] : \"${fq[0]} ${fq[1]}\"\n    mccortex_fq = meta.single_end ? \"-1 ${fq[0]}\" : \"-2 ${fq[0]}:${fq[1]}\"\n    m = task.memory.toString().split(' ')[0].toInteger() * 1000 - 500\n    '''\n    gzip -cd !{fastq} | mash sketch -o !{meta.id}-k21 -k 21 -s !{params.sketch_size} -r -I !{meta.id} -\n    gzip -cd !{fastq} | mash sketch -o !{meta.id}-k31 -k 31 -s !{params.sketch_size} -r -I !{meta.id} -\n    sourmash sketch dna -p k=21,k=31,k=51,abund,scaled=!{params.sourmash_scale} --merge !{meta.id} -o !{meta.id}.sig !{fastq}\n\n    if [[ \"!{params.count_31mers}\" == \"true\" ]]; then\n        mccortex31 build -f -k 31 -s !{meta.id} !{mccortex_fq} -t !{task.cpus} -m !{m}mb -q temp_counts\n        if [ \"!{params.keep_singletons}\" == \"false\" ]; then\n            # Clean up Cortex file (mostly remove singletons)\n            mccortex31 clean -q -B 2 -U2 -T2 -m !{m}mb -o !{meta.id}.ctx temp_counts\n            rm temp_counts\n        else\n            mv temp_counts !{meta.id}.ctx\n        fi\n    fi\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        mash: $(echo $(mash 2>&1) | sed 's/^.*Mash version //;s/ .*$//')\n        mccortex: $(echo $(mccortex31 2>&1) | sed 's/^.*mccortex=v//;s/ .*$//')\n        sourmash: $(echo $(sourmash --version 2>&1) | sed 's/sourmash //;')\n    END_VERSIONS\n    '''",
        "nb_lignes_script": 26,
        "language_script": "bash",
        "tools": [
            "FastQC",
            "Mash",
            "sourmash"
        ],
        "tools_url": [
            "https://bio.tools/fastqc",
            "https://bio.tools/mash",
            "https://bio.tools/sourmash"
        ],
        "tools_dico": [
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            },
            {
                "name": "Mash",
                "uri": "https://bio.tools/mash",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2533",
                            "term": "DNA mutation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix generation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Phylogenetic distance matrix generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Fast genome and metagenome distance estimation using MinHash.",
                "homepage": "https://github.com/marbl/mash"
            },
            {
                "name": "sourmash",
                "uri": "https://bio.tools/sourmash",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3307",
                            "term": "Computational biology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0346",
                                    "term": "Sequence similarity search"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix generation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Phylogenetic distance matrix generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2975",
                                "term": "Nucleic acid sequence (raw)"
                            }
                        ],
                        "output": []
                    }
                ],
                "description": "Compute and compare MinHash signatures for DNA data sets.",
                "homepage": "https://sourmash.readthedocs.io/en/latest/"
            }
        ],
        "inputs": [
            "meta",
            "fq"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"${meta.id}\"",
            "label \"base_mem_8gb\"",
            "label \"minmer_sketch\"",
            "publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }"
        ],
        "when": "",
        "stub": ""
    },
    "ASSEMBLY_QC": {
        "name_process": "ASSEMBLY_QC",
        "string_process": "\nprocess ASSEMBLY_QC {\n                                                                   \n    tag \"${meta.id}\"\n    label \"max_cpu_75\"\n    label \"process_medium\"\n    label \"assembly_qc\"\n\n    publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    input:\n    tuple val(meta), path(genome_size), path(fasta), path(total_contigs)\n\n    output:\n    path \"results/*\"\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    shell:\n                    \n    full_tree = params.full_tree ? '' : '--reduced_tree'\n    checkm_ali = params.checkm_ali ? '--ali' : ''\n    checkm_nt = params.checkm_nt ? '--nt' : ''\n    force_domain = params.force_domain ? '--force_domain' : ''\n    no_refinement = params.no_refinement ? '--no_refinement' : ''\n    individual_markers = params.individual_markers ? '--individual_markers' : ''\n    skip_adj_correction = params.skip_adj_correction ? '--skip_adj_correction' : ''\n    skip_pseudogene_correction = params.skip_pseudogene_correction ? '--skip_pseudogene_correction' : ''\n    ignore_thresholds = params.ignore_thresholds ? '--ignore_thresholds' : ''\n    checkm_opts = [full_tree, checkm_ali, checkm_nt, force_domain, no_refinement, individual_markers, \n                   skip_adj_correction, skip_pseudogene_correction, ignore_thresholds].join(\" \")\n    is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    fasta_name = fasta.getName().replace(\".gz\", \"\")\n    '''\n    if [ \"!{is_compressed}\" == \"true\" ]; then\n        gzip -c -d !{fasta} > !{fasta_name}\n    fi\n\n    RAN_CHECKM=\"0\"\n    # CheckM\n    mkdir checkm/\n    if [ \"$(uname)\" = Darwin ]; then\n        echo \"checkm is not available due to pplacer not being available on MacOSX (via BioConda)\" > checkm/checkm-not-available-on-macosx.txt\n    elif [[ \"!{params.run_checkm}\" == \"true\" ]]; then\n        RAN_CHECKM=\"1\"\n        checkm lineage_wf ./ checkm/ \\\n            --alignment_file checkm/checkm-genes.aln \\\n            --tab_table \\\n            --file checkm/checkm-results.txt \\\n            --threads !{task.cpus} \\\n            --pplacer_threads !{task.cpus} \\\n            --unique !{params.checkm_unique} \\\n            --multi !{params.checkm_multi} \\\n            --aai_strain !{params.aai_strain} \\\n            --length !{params.checkm_length} !{checkm_opts}\n        mv checkm/checkm.log ./\n\n        if [[ !{params.skip_compression} == \"false\" ]]; then\n            find . -name \"*.faa\" -or -name \"*hmmer.analyze.txt\" | xargs -I {} pigz -n --best -p !{task.cpus} {}\n        fi\n    else\n        echo \"checkm was skipped due to not including '--run_checkm'\" > checkm/checkm-was-skipped.txt\n    fi\n\n    # QUAST\n    GENOME_SIZE=`head -n 1 !{genome_size}`\n    est_ref_size=\"\"\n    if [ \"${GENOME_SIZE}\" != \"0\" ]; then\n        est_ref_size=\"--est-ref-size ${GENOME_SIZE}\"\n    fi\n\n    quast !{fasta_name} ${est_ref_size} \\\n        -o quast \\\n        --threads !{task.cpus} \\\n        --glimmer \\\n        --contig-thresholds !{params.contig_thresholds} \\\n        --plots-format !{params.plots_format}\n    mv quast/quast.log ./\n\n    # Results dir\n    mkdir results\n    mv checkm/ results/\n    mv quast/ results/\n\n    # Capture versions (no indent or it'll break)\n    if [ \"${RAN_CHECKM}\" == \"1\" ]; then\n\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        checkm: $(echo $(checkm -h 2>&1) | sed 's/.*CheckM v//;s/ .*$//')\n        quast: $(echo $(quast --version 2>&1) | sed 's/.*QUAST v//;s/ .*$//')\n    END_VERSIONS\n\n    else\n\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        quast: $(echo $(quast --version 2>&1) | sed 's/.*QUAST v//;s/ .*$//')\n    END_VERSIONS\n\n    fi\n    '''\n}",
        "nb_lignes_process": 103,
        "string_script": "    full_tree = params.full_tree ? '' : '--reduced_tree'\n    checkm_ali = params.checkm_ali ? '--ali' : ''\n    checkm_nt = params.checkm_nt ? '--nt' : ''\n    force_domain = params.force_domain ? '--force_domain' : ''\n    no_refinement = params.no_refinement ? '--no_refinement' : ''\n    individual_markers = params.individual_markers ? '--individual_markers' : ''\n    skip_adj_correction = params.skip_adj_correction ? '--skip_adj_correction' : ''\n    skip_pseudogene_correction = params.skip_pseudogene_correction ? '--skip_pseudogene_correction' : ''\n    ignore_thresholds = params.ignore_thresholds ? '--ignore_thresholds' : ''\n    checkm_opts = [full_tree, checkm_ali, checkm_nt, force_domain, no_refinement, individual_markers, \n                   skip_adj_correction, skip_pseudogene_correction, ignore_thresholds].join(\" \")\n    is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    fasta_name = fasta.getName().replace(\".gz\", \"\")\n    '''\n    if [ \"!{is_compressed}\" == \"true\" ]; then\n        gzip -c -d !{fasta} > !{fasta_name}\n    fi\n\n    RAN_CHECKM=\"0\"\n    # CheckM\n    mkdir checkm/\n    if [ \"$(uname)\" = Darwin ]; then\n        echo \"checkm is not available due to pplacer not being available on MacOSX (via BioConda)\" > checkm/checkm-not-available-on-macosx.txt\n    elif [[ \"!{params.run_checkm}\" == \"true\" ]]; then\n        RAN_CHECKM=\"1\"\n        checkm lineage_wf ./ checkm/ \\\n            --alignment_file checkm/checkm-genes.aln \\\n            --tab_table \\\n            --file checkm/checkm-results.txt \\\n            --threads !{task.cpus} \\\n            --pplacer_threads !{task.cpus} \\\n            --unique !{params.checkm_unique} \\\n            --multi !{params.checkm_multi} \\\n            --aai_strain !{params.aai_strain} \\\n            --length !{params.checkm_length} !{checkm_opts}\n        mv checkm/checkm.log ./\n\n        if [[ !{params.skip_compression} == \"false\" ]]; then\n            find . -name \"*.faa\" -or -name \"*hmmer.analyze.txt\" | xargs -I {} pigz -n --best -p !{task.cpus} {}\n        fi\n    else\n        echo \"checkm was skipped due to not including '--run_checkm'\" > checkm/checkm-was-skipped.txt\n    fi\n\n    # QUAST\n    GENOME_SIZE=`head -n 1 !{genome_size}`\n    est_ref_size=\"\"\n    if [ \"${GENOME_SIZE}\" != \"0\" ]; then\n        est_ref_size=\"--est-ref-size ${GENOME_SIZE}\"\n    fi\n\n    quast !{fasta_name} ${est_ref_size} \\\n        -o quast \\\n        --threads !{task.cpus} \\\n        --glimmer \\\n        --contig-thresholds !{params.contig_thresholds} \\\n        --plots-format !{params.plots_format}\n    mv quast/quast.log ./\n\n    # Results dir\n    mkdir results\n    mv checkm/ results/\n    mv quast/ results/\n\n    # Capture versions (no indent or it'll break)\n    if [ \"${RAN_CHECKM}\" == \"1\" ]; then\n\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        checkm: $(echo $(checkm -h 2>&1) | sed 's/.*CheckM v//;s/ .*$//')\n        quast: $(echo $(quast --version 2>&1) | sed 's/.*QUAST v//;s/ .*$//')\n    END_VERSIONS\n\n    else\n\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        quast: $(echo $(quast --version 2>&1) | sed 's/.*QUAST v//;s/ .*$//')\n    END_VERSIONS\n\n    fi\n    '''",
        "nb_lignes_script": 81,
        "language_script": "bash",
        "tools": [
            "QUAST"
        ],
        "tools_url": [
            "https://bio.tools/quast"
        ],
        "tools_dico": [
            {
                "name": "QUAST",
                "uri": "https://bio.tools/quast",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly validation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly quality evaluation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Assembly QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Assembly quality evaluation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "QUAST stands for QUality ASsessment Tool.  \nIt evaluates a quality of genome assemblies by computing various metrics and providing nice reports.",
                "homepage": "http://quast.sourceforge.net/quast"
            }
        ],
        "inputs": [
            "meta",
            "genome_size",
            "fasta",
            "total_contigs"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"${meta.id}\"",
            "label \"max_cpu_75\"",
            "label \"process_medium\"",
            "label \"assembly_qc\"",
            "publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }"
        ],
        "when": "",
        "stub": ""
    },
    "ECTYPER": {
        "name_process": "ECTYPER",
        "string_process": "\nprocess ECTYPER {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}/${meta.id}\",  mode: params.publish_dir_mode,  overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ectyper:1.0.0--pyhdfd78af_1' :\n        'quay.io/biocontainers/ectyper:1.0.0--pyhdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    tuple val(meta), path(\"*.txt\"), emit: txt\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    ectyper \\\\\n        $options.args \\\\\n        --cores $task.cpus \\\\\n        --output ./ \\\\\n        --input $fasta_name\n    mv output.tsv ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ectyper: \\$(echo \\$(ectyper --version 2>&1)  | sed 's/.*ectyper //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 42,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    ectyper \\\\\n        $options.args \\\\\n        --cores $task.cpus \\\\\n        --output ./ \\\\\n        --input $fasta_name\n    mv output.tsv ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ectyper: \\$(echo \\$(ectyper --version 2>&1)  | sed 's/.*ectyper //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 19,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/ectyper:1.0.0--pyhdfd78af_1' : 'quay.io/biocontainers/ectyper:1.0.0--pyhdfd78af_1' }\""
        ],
        "when": "",
        "stub": ""
    },
    "ARIBA_ANALYSIS": {
        "name_process": "ARIBA_ANALYSIS",
        "string_process": "\nprocess ARIBA_ANALYSIS {\n                                                                 \n    tag \"${meta.id} - ${dataset_name}\"\n    label \"ariba_analysis\"\n\n    publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options, logs_subdir:dataset_name) }\n\n    input:\n    tuple val(meta), path(fq)\n    each path(dataset)\n\n    output:\n    path \"${dataset_name}/*\", emit: results\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    when:\n    meta.single_end == false\n\n    shell:\n    dataset_tarball = dataset.getName()\n    dataset_name = dataset_tarball.replace('.tar.gz', '')\n    spades_options = params.spades_options ? \"--spades_options '${params.spades_options}'\" : \"\"\n    noclean = params.ariba_no_clean ? \"--noclean\" : \"\"\n    '''\n    tar -xzvf !{dataset_tarball}\n    mv !{dataset_name} !{dataset_name}db\n    ariba run !{dataset_name}db !{fq} !{dataset_name} \\\n            --nucmer_min_id !{params.nucmer_min_id} \\\n            --nucmer_min_len !{params.nucmer_min_len} \\\n            --nucmer_breaklen !{params.nucmer_breaklen} \\\n            --assembly_cov !{params.assembly_cov} \\\n            --min_scaff_depth !{params.min_scaff_depth} \\\n            --assembled_threshold !{params.assembled_threshold} \\\n            --gene_nt_extend !{params.gene_nt_extend} \\\n            --unique_threshold !{params.unique_threshold} \\\n            --threads !{task.cpus} \\\n            --force \\\n            --verbose !{noclean} !{spades_options}\n\n    ariba summary !{dataset_name}/summary !{dataset_name}/report.tsv \\\n            --cluster_cols assembled,match,known_var,pct_id,ctg_cov,novel_var \\\n            --col_filter n --row_filter n\n\n    # Cleanup\n    rm -rf ariba.tmp*\n    rm -rf !{dataset_name}db\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        ariba:  $(echo $(ariba version 2>&1) | sed 's/^.*ARIBA version: //;s/ .*$//')\n    END_VERSIONS\n    '''\n}",
        "nb_lignes_process": 56,
        "string_script": "    dataset_tarball = dataset.getName()\n    dataset_name = dataset_tarball.replace('.tar.gz', '')\n    spades_options = params.spades_options ? \"--spades_options '${params.spades_options}'\" : \"\"\n    noclean = params.ariba_no_clean ? \"--noclean\" : \"\"\n    '''\n    tar -xzvf !{dataset_tarball}\n    mv !{dataset_name} !{dataset_name}db\n    ariba run !{dataset_name}db !{fq} !{dataset_name} \\\n            --nucmer_min_id !{params.nucmer_min_id} \\\n            --nucmer_min_len !{params.nucmer_min_len} \\\n            --nucmer_breaklen !{params.nucmer_breaklen} \\\n            --assembly_cov !{params.assembly_cov} \\\n            --min_scaff_depth !{params.min_scaff_depth} \\\n            --assembled_threshold !{params.assembled_threshold} \\\n            --gene_nt_extend !{params.gene_nt_extend} \\\n            --unique_threshold !{params.unique_threshold} \\\n            --threads !{task.cpus} \\\n            --force \\\n            --verbose !{noclean} !{spades_options}\n\n    ariba summary !{dataset_name}/summary !{dataset_name}/report.tsv \\\n            --cluster_cols assembled,match,known_var,pct_id,ctg_cov,novel_var \\\n            --col_filter n --row_filter n\n\n    # Cleanup\n    rm -rf ariba.tmp*\n    rm -rf !{dataset_name}db\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        ariba:  $(echo $(ariba version 2>&1) | sed 's/^.*ARIBA version: //;s/ .*$//')\n    END_VERSIONS\n    '''",
        "nb_lignes_script": 33,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fq",
            "dataset"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"${meta.id} - ${dataset_name}\"",
            "label \"ariba_analysis\"",
            "publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options, logs_subdir:dataset_name) }"
        ],
        "when": "meta.single_end == false",
        "stub": ""
    },
    "SNPDISTS": {
        "name_process": "SNPDISTS",
        "string_process": "\nprocess SNPDISTS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/snp-dists:0.8.2--h5bf99c6_0' :\n        'quay.io/biocontainers/snp-dists:0.8.2--h5bf99c6_0' }\"\n\n    input:\n    tuple val(meta), path(alignment)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    snp-dists \\\\\n        $options.args \\\\\n        $alignment > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        snp-dists: \\$(snp-dists -v 2>&1 | sed 's/snp-dists //;')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    snp-dists \\\\\n        $options.args \\\\\n        $alignment > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        snp-dists: \\$(snp-dists -v 2>&1 | sed 's/snp-dists //;')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "alignment"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/snp-dists:0.8.2--h5bf99c6_0' : 'quay.io/biocontainers/snp-dists:0.8.2--h5bf99c6_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "CALL_VARIANTS": {
        "name_process": "CALL_VARIANTS",
        "string_process": "\nprocess CALL_VARIANTS {\n      \n                                                                      \n                 \n\n                                                                                                       \n\n                                                                                        \n                                                                                        \n                                                                     \n      \n    tag \"${meta.id} - ${reference_name}\"\n    label \"base_mem_4gb\"\n    label \"max_cpu_75\"\n    label \"call_variants\"\n\n    publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options, logs_subdir:reference_name) }\n\n    input:\n    tuple val(meta), path(fq)\n    each path(reference)\n\n    when:\n    meta.runtype != \"ont\"\n\n    output:\n    path \"results/*\"\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    shell:\n    snippy_ram = task.memory.toString().split(' ')[0].toInteger()-1\n    reference_name = reference.getSimpleName()\n    fastq = meta.single_end ? \"--se ${fq[0]}\" : \"--R1 ${fq[0]} --R2 ${fq[1]}\"\n    bwaopt = params.bwaopt ? \"--bwaopt 'params.bwaopt'\" : \"\"\n    fbopt = params.fbopt ? \"--fbopt 'params.fbopt'\" : \"\"\n    no_cache = params.no_cache ? '-N' : ''\n    tie_break = params.random_tie_break ? \"--random_tie_break\" : \"\"\n    '''\n    REFERENCE=\"!{reference}\"\n    REFERENCE_NAME=\"!{reference_name}\"\n    if [[ \"!{reference}\" == \"refseq-genomes.msh\" ]]; then\n        # Get Mash distance (For paired-end, use just R1)\n        mash dist -k 31 -s 10000 -t !{fq[0]} ${REFERENCE} | grep -v \"query\" | sort -k 2,2 > distances.txt\n\n        # Pick top genome and download\n        printf \"accession\\\\tdistance\\\\tlatest_accession\\\\tupdated\\\\n\" > mash-dist.txt\n        select-references.py distances.txt 1 !{tie_break} >> mash-dist.txt\n        grep -v distance mash-dist.txt | cut -f3 > download-list.txt\n        ncbi-genome-download bacteria -l complete -o ./ -F genbank -p !{task.cpus} -A download-list.txt \\\n            -r !{params.max_retry} !{no_cache}\n\n        # Move and uncompress genomes\n        mkdir genbank_temp\n        find refseq -name \"*.gbff.gz\" | xargs -I {} mv {} genbank_temp/\n        rename 's/(GC[AF]_\\\\d+).*/$1/' genbank_temp/*\n        ls genbank_temp/ | xargs -I {} sh -c 'gzip -cd genbank_temp/{} > {}.gbk'\n        rm -rf genbank_temp/ refseq/\n\n        # Update variables\n        REFERENCE=$(ls *.gbk)\n        REFERENCE_NAME=$(basename ${REFERENCE} .gbk)\n\n        # Capture version\n        mash --version > mash.version.txt 2>&1\n        ncbi-genome-download --version > ncbi-genome-download.version.txt 2>&1\n    fi\n\n    snippy !{fastq} \\\n        --ref ${REFERENCE} \\\n        --cpus !{task.cpus} \\\n        --ram !{snippy_ram} \\\n        --outdir ${REFERENCE_NAME} \\\n        --prefix !{meta.id} \\\n        --mapqual !{params.mapqual} \\\n        --basequal !{params.basequal} \\\n        --mincov !{params.mincov} \\\n        --minfrac !{params.minfrac} \\\n        --minqual !{params.minqual} \\\n        --maxsoft !{params.maxsoft} !{bwaopt} !{fbopt}\n    mv ${REFERENCE_NAME}/!{meta.id}.log ./\n\n    # Add GenBank annotations\n    vcf-annotator ${REFERENCE_NAME}/!{meta.id}.vcf ${REFERENCE} > ${REFERENCE_NAME}/!{meta.id}.annotated.vcf\n\n    # Get per-base coverage\n    grep \"^##contig\" ${REFERENCE_NAME}/!{meta.id}.vcf > ${REFERENCE_NAME}/!{meta.id}.full-coverage.txt\n    genomeCoverageBed -ibam ${REFERENCE_NAME}/!{meta.id}.bam -d >> ${REFERENCE_NAME}/!{meta.id}.full-coverage.txt\n    cleanup-coverage.py ${REFERENCE_NAME}/!{meta.id}.full-coverage.txt > ${REFERENCE_NAME}/!{meta.id}.coverage.txt\n    rm ${REFERENCE_NAME}/!{meta.id}.full-coverage.txt\n\n    # Mask low coverage regions\n    mask-consensus.py !{meta.id} ${REFERENCE_NAME} \\\n        ${REFERENCE_NAME}/!{meta.id}.consensus.subs.fa \\\n        ${REFERENCE_NAME}/!{meta.id}.subs.vcf \\\n        ${REFERENCE_NAME}/!{meta.id}.coverage.txt \\\n        --mincov !{params.mincov} > ${REFERENCE_NAME}/!{meta.id}.consensus.subs.masked.fa\n\n    # Clean Up\n    rm -rf ${REFERENCE_NAME}/reference ${REFERENCE_NAME}/ref.fa* ${REFERENCE_NAME}/!{meta.id}.vcf.gz*\n\n    if [[ \"!{reference}\" == \"refseq-genomes.msh\" ]]; then\n        mv distances.txt ${REFERENCE_NAME}/mash-distances.txt\n        mv mash-dist.txt ${REFERENCE_NAME}/mash-selected.txt\n        mv download-list.txt ${REFERENCE_NAME}/mash-downloaded.txt\n        mv ${REFERENCE} ${REFERENCE_NAME}/\n    fi\n\n    if [[ !{params.skip_compression} == \"false\" ]]; then\n        find ${REFERENCE_NAME}/ -type f | \\\n            grep -v -E \"\\\\.bam$|\\\\.bai$|\\\\.log$|\\\\.txt$|\\\\.html$|\\\\.tab$\" | \\\n            xargs -I {} pigz -n --best -p !{task.cpus} {}\n        pigz -n --best -p !{task.cpus} ${REFERENCE_NAME}/!{meta.id}.coverage.txt\n    fi\n\n    mkdir results\n    mv ${REFERENCE_NAME}/ results/\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        bedtools: $(echo $(bedtools --version 2>&1) | sed 's/bedtools v//')\n        mash: $(echo $(mash --version 2>&1))\n        ncbi-genome-download: $(echo $(ncbi-genome-download --version 2>&1))\n        pigz: $(echo $(pigz --version 2>&1) | sed 's/pigz //')\n        snippy: $(echo $(snippy --version 2>&1) | sed 's/snippy //')\n        vcf-annotator: $(echo $(vcf-annotator --version 2>&1) | sed 's/vcf-annotator.py //')\n    END_VERSIONS\n    '''\n}",
        "nb_lignes_process": 131,
        "string_script": "    snippy_ram = task.memory.toString().split(' ')[0].toInteger()-1\n    reference_name = reference.getSimpleName()\n    fastq = meta.single_end ? \"--se ${fq[0]}\" : \"--R1 ${fq[0]} --R2 ${fq[1]}\"\n    bwaopt = params.bwaopt ? \"--bwaopt 'params.bwaopt'\" : \"\"\n    fbopt = params.fbopt ? \"--fbopt 'params.fbopt'\" : \"\"\n    no_cache = params.no_cache ? '-N' : ''\n    tie_break = params.random_tie_break ? \"--random_tie_break\" : \"\"\n    '''\n    REFERENCE=\"!{reference}\"\n    REFERENCE_NAME=\"!{reference_name}\"\n    if [[ \"!{reference}\" == \"refseq-genomes.msh\" ]]; then\n        # Get Mash distance (For paired-end, use just R1)\n        mash dist -k 31 -s 10000 -t !{fq[0]} ${REFERENCE} | grep -v \"query\" | sort -k 2,2 > distances.txt\n\n        # Pick top genome and download\n        printf \"accession\\\\tdistance\\\\tlatest_accession\\\\tupdated\\\\n\" > mash-dist.txt\n        select-references.py distances.txt 1 !{tie_break} >> mash-dist.txt\n        grep -v distance mash-dist.txt | cut -f3 > download-list.txt\n        ncbi-genome-download bacteria -l complete -o ./ -F genbank -p !{task.cpus} -A download-list.txt \\\n            -r !{params.max_retry} !{no_cache}\n\n        # Move and uncompress genomes\n        mkdir genbank_temp\n        find refseq -name \"*.gbff.gz\" | xargs -I {} mv {} genbank_temp/\n        rename 's/(GC[AF]_\\\\d+).*/$1/' genbank_temp/*\n        ls genbank_temp/ | xargs -I {} sh -c 'gzip -cd genbank_temp/{} > {}.gbk'\n        rm -rf genbank_temp/ refseq/\n\n        # Update variables\n        REFERENCE=$(ls *.gbk)\n        REFERENCE_NAME=$(basename ${REFERENCE} .gbk)\n\n        # Capture version\n        mash --version > mash.version.txt 2>&1\n        ncbi-genome-download --version > ncbi-genome-download.version.txt 2>&1\n    fi\n\n    snippy !{fastq} \\\n        --ref ${REFERENCE} \\\n        --cpus !{task.cpus} \\\n        --ram !{snippy_ram} \\\n        --outdir ${REFERENCE_NAME} \\\n        --prefix !{meta.id} \\\n        --mapqual !{params.mapqual} \\\n        --basequal !{params.basequal} \\\n        --mincov !{params.mincov} \\\n        --minfrac !{params.minfrac} \\\n        --minqual !{params.minqual} \\\n        --maxsoft !{params.maxsoft} !{bwaopt} !{fbopt}\n    mv ${REFERENCE_NAME}/!{meta.id}.log ./\n\n    # Add GenBank annotations\n    vcf-annotator ${REFERENCE_NAME}/!{meta.id}.vcf ${REFERENCE} > ${REFERENCE_NAME}/!{meta.id}.annotated.vcf\n\n    # Get per-base coverage\n    grep \"^##contig\" ${REFERENCE_NAME}/!{meta.id}.vcf > ${REFERENCE_NAME}/!{meta.id}.full-coverage.txt\n    genomeCoverageBed -ibam ${REFERENCE_NAME}/!{meta.id}.bam -d >> ${REFERENCE_NAME}/!{meta.id}.full-coverage.txt\n    cleanup-coverage.py ${REFERENCE_NAME}/!{meta.id}.full-coverage.txt > ${REFERENCE_NAME}/!{meta.id}.coverage.txt\n    rm ${REFERENCE_NAME}/!{meta.id}.full-coverage.txt\n\n    # Mask low coverage regions\n    mask-consensus.py !{meta.id} ${REFERENCE_NAME} \\\n        ${REFERENCE_NAME}/!{meta.id}.consensus.subs.fa \\\n        ${REFERENCE_NAME}/!{meta.id}.subs.vcf \\\n        ${REFERENCE_NAME}/!{meta.id}.coverage.txt \\\n        --mincov !{params.mincov} > ${REFERENCE_NAME}/!{meta.id}.consensus.subs.masked.fa\n\n    # Clean Up\n    rm -rf ${REFERENCE_NAME}/reference ${REFERENCE_NAME}/ref.fa* ${REFERENCE_NAME}/!{meta.id}.vcf.gz*\n\n    if [[ \"!{reference}\" == \"refseq-genomes.msh\" ]]; then\n        mv distances.txt ${REFERENCE_NAME}/mash-distances.txt\n        mv mash-dist.txt ${REFERENCE_NAME}/mash-selected.txt\n        mv download-list.txt ${REFERENCE_NAME}/mash-downloaded.txt\n        mv ${REFERENCE} ${REFERENCE_NAME}/\n    fi\n\n    if [[ !{params.skip_compression} == \"false\" ]]; then\n        find ${REFERENCE_NAME}/ -type f | \\\n            grep -v -E \"\\\\.bam$|\\\\.bai$|\\\\.log$|\\\\.txt$|\\\\.html$|\\\\.tab$\" | \\\n            xargs -I {} pigz -n --best -p !{task.cpus} {}\n        pigz -n --best -p !{task.cpus} ${REFERENCE_NAME}/!{meta.id}.coverage.txt\n    fi\n\n    mkdir results\n    mv ${REFERENCE_NAME}/ results/\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        bedtools: $(echo $(bedtools --version 2>&1) | sed 's/bedtools v//')\n        mash: $(echo $(mash --version 2>&1))\n        ncbi-genome-download: $(echo $(ncbi-genome-download --version 2>&1))\n        pigz: $(echo $(pigz --version 2>&1) | sed 's/pigz //')\n        snippy: $(echo $(snippy --version 2>&1) | sed 's/snippy //')\n        vcf-annotator: $(echo $(vcf-annotator --version 2>&1) | sed 's/vcf-annotator.py //')\n    END_VERSIONS\n    '''",
        "nb_lignes_script": 97,
        "language_script": "bash",
        "tools": [
            "FastQC",
            "Mash",
            "snippy"
        ],
        "tools_url": [
            "https://bio.tools/fastqc",
            "https://bio.tools/mash",
            "https://bio.tools/snippy"
        ],
        "tools_dico": [
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            },
            {
                "name": "Mash",
                "uri": "https://bio.tools/mash",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2533",
                            "term": "DNA mutation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix generation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Phylogenetic distance matrix generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Fast genome and metagenome distance estimation using MinHash.",
                "homepage": "https://github.com/marbl/mash"
            },
            {
                "name": "snippy",
                "uri": "https://bio.tools/snippy",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2885",
                            "term": "DNA polymorphism"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phylogenetic inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3227",
                                    "term": "Variant calling"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phlyogenetic tree construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phylogenetic tree generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3227",
                                    "term": "Variant mapping"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Rapid haploid variant calling and core SNP phylogeny generation.",
                "homepage": "https://github.com/tseemann/snippy"
            }
        ],
        "inputs": [
            "meta",
            "fq",
            "reference"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"${meta.id} - ${reference_name}\"",
            "label \"base_mem_4gb\"",
            "label \"max_cpu_75\"",
            "label \"call_variants\"",
            "publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options, logs_subdir:reference_name) }"
        ],
        "when": "meta.runtype != \"ont\"",
        "stub": ""
    },
    "MENINGOTYPE": {
        "name_process": "MENINGOTYPE",
        "string_process": "\nprocess MENINGOTYPE {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/meningotype:0.8.5--pyhdfd78af_0' :\n        'quay.io/biocontainers/meningotype:0.8.5--pyhdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    meningotype \\\\\n        $options.args \\\\\n        $fasta_name \\\\\n        > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        meningotype: \\$( echo \\$(meningotype --version 2>&1) | sed 's/^.*meningotype v//' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 39,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    meningotype \\\\\n        $options.args \\\\\n        $fasta_name \\\\\n        > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        meningotype: \\$( echo \\$(meningotype --version 2>&1) | sed 's/^.*meningotype v//' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/meningotype:0.8.5--pyhdfd78af_0' : 'quay.io/biocontainers/meningotype:0.8.5--pyhdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "NCBIGENOMEDOWNLOAD": {
        "name_process": "NCBIGENOMEDOWNLOAD",
        "string_process": "\nprocess NCBIGENOMEDOWNLOAD {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container 'quay.io/bactopia/bactopia:2.0.1'\n\n    input:\n    val meta\n    path accessions\n\n    output:\n    path(\"*.gz\")                            , emit: all\n    path(\"*_genomic.gbff.gz\")               , emit: gbk     , optional: true\n    path(\"*_genomic.fna.gz\")                , emit: fna     , optional: true\n    path(\"*_rm.out.gz\")                     , emit: rm      , optional: true\n    path(\"*_feature_table.txt.gz\")          , emit: features, optional: true\n    path(\"*_genomic.gff.gz\")                , emit: gff     , optional: true\n    path(\"*_protein.faa.gz\")                , emit: faa     , optional: true\n    path(\"*_protein.gpff.gz\")               , emit: gpff    , optional: true\n    path(\"*_wgsmaster.gbff.gz\")             , emit: wgs_gbk , optional: true\n    path(\"*_cds_from_genomic.fna.gz\")       , emit: cds     , optional: true\n    path(\"*_rna.fna.gz\")                    , emit: rna     , optional: true\n    path(\"*_rna_from_genomic.fna.gz\")       , emit: rna_fna , optional: true\n    path(\"*_assembly_report.txt\")           , emit: report  , optional: true\n    path(\"*_assembly_stats.txt\")            , emit: stats   , optional: true\n    path \"*.{log,err}\"                      , emit: logs    , optional: true\n    path \".command.*\"                       , emit: nf_logs\n    path \"versions.yml\"                     , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def has_accessions = accessions ? true : false\n    def opts = \"${options.args} --output-folder ./ --flat-output -p ${task.cpus} -r ${params.max_retry}\"\n    \"\"\"\n    if [ \"${meta.species}\" != \"null\" ]; then\n        if [ \"${meta.limit}\" != \"null\" ]; then\n            ncbi-genome-download $opts -g \"${meta.species}\" --dry-run | grep -v \"Considering\" > accession-list.txt\n            shuf accession-list.txt | head -n ${meta.limit} | cut -f 1,1  > accession-subset.txt\n            ncbi-genome-download $opts -A accession-subset.txt\n        else\n            ncbi-genome-download $opts -g \"${meta.species}\"\n        fi\n    fi\n\n    if [ \"${meta.accession}\" != \"null\" ]; then\n        ncbi-genome-download $opts -A ${meta.accession}\n    fi\n\n    if [ \"${meta.has_accessions}\" == \"true\" ]; then\n        ncbi-genome-download $opts -A ${accessions}\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ncbi-genome-download: \\$( ncbi-genome-download --version )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 60,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def has_accessions = accessions ? true : false\n    def opts = \"${options.args} --output-folder ./ --flat-output -p ${task.cpus} -r ${params.max_retry}\"\n    \"\"\"\n    if [ \"${meta.species}\" != \"null\" ]; then\n        if [ \"${meta.limit}\" != \"null\" ]; then\n            ncbi-genome-download $opts -g \"${meta.species}\" --dry-run | grep -v \"Considering\" > accession-list.txt\n            shuf accession-list.txt | head -n ${meta.limit} | cut -f 1,1  > accession-subset.txt\n            ncbi-genome-download $opts -A accession-subset.txt\n        else\n            ncbi-genome-download $opts -g \"${meta.species}\"\n        fi\n    fi\n\n    if [ \"${meta.accession}\" != \"null\" ]; then\n        ncbi-genome-download $opts -A ${meta.accession}\n    fi\n\n    if [ \"${meta.has_accessions}\" == \"true\" ]; then\n        ncbi-genome-download $opts -A ${accessions}\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ncbi-genome-download: \\$( ncbi-genome-download --version )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 26,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "accessions"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container 'quay.io/bactopia/bactopia:2.0.1'"
        ],
        "when": "",
        "stub": ""
    },
    "SPATYPER": {
        "name_process": "SPATYPER",
        "string_process": "\nprocess SPATYPER {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/spatyper%3A0.3.3--pyhdfd78af_3' :\n        'quay.io/biocontainers/spatyper:0.3.3--pyhdfd78af_3' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path repeats\n    path repeat_order\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def input_args = repeats && repeat_order ? \"-r ${repeats} -o ${repeat_order}\" : \"\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    spaTyper \\\\\n        $options.args \\\\\n        $input_args \\\\\n        --fasta $fasta_name \\\\\n        --output ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        spatyper: \\$( echo \\$(spaTyper --version 2>&1) | sed 's/^.*spaTyper //' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 43,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def input_args = repeats && repeat_order ? \"-r ${repeats} -o ${repeat_order}\" : \"\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    spaTyper \\\\\n        $options.args \\\\\n        $input_args \\\\\n        --fasta $fasta_name \\\\\n        --output ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        spatyper: \\$( echo \\$(spaTyper --version 2>&1) | sed 's/^.*spaTyper //' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 19,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta",
            "repeats",
            "repeat_order"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/spatyper%3A0.3.3--pyhdfd78af_3' : 'quay.io/biocontainers/spatyper:0.3.3--pyhdfd78af_3' }\""
        ],
        "when": "",
        "stub": ""
    },
    "CLONALFRAMEML": {
        "name_process": "CLONALFRAMEML",
        "string_process": "\nprocess CLONALFRAMEML {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-f5c68f1508671d5744655da9b0e8b609098f4138:7e089189af7822a6a18245830639dbfe11a4c277-0' :\n        'quay.io/biocontainers/mulled-v2-f5c68f1508671d5744655da9b0e8b609098f4138:7e089189af7822a6a18245830639dbfe11a4c277-0' }\"\n\n    input:\n    tuple val(meta), path(msa), path(newick)\n\n    output:\n    tuple val(meta), path(\"*.emsim.txt\")                   , emit: emsim, optional: true\n    tuple val(meta), path(\"*.em.txt\")                      , emit: em\n    tuple val(meta), path(\"*.importation_status.txt\")      , emit: status\n    tuple val(meta), path(\"*.labelled_tree.newick\")        , emit: newick\n    tuple val(meta), path(\"*.ML_sequence.fasta\")           , emit: fasta\n    tuple val(meta), path(\"*.position_cross_reference.txt\"), emit: pos_ref\n    tuple val(meta), path(\"*.masked.aln.gz\")               , emit: masked_aln\n    path \"*.{log,err}\"                                     , emit: logs, optional: true\n    path \".command.*\"                                      , emit: nf_logs\n    path \"versions.yml\"                                    , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = msa.getName().endsWith(\".gz\") ? true : false\n    def msa_name = msa.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $msa > $msa_name\n    fi\n\n    ClonalFrameML \\\\\n        $newick \\\\\n        $msa_name \\\\\n        $prefix \\\\\n        $options.args\n\n    maskrc-svg.py $prefix --aln ${msa_name} --symbol '-' --out ${prefix}.masked.aln\n    gzip ${prefix}.masked.aln\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        clonalframeml: \\$( echo \\$(ClonalFrameML -version 2>&1) | sed 's/^.*ClonalFrameML v//' )\n        maskrc-svg: \\$( echo \\$(maskrc-svg.py --version 2>&1) | sed 's/^.*maskrc-svg.py //' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 50,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = msa.getName().endsWith(\".gz\") ? true : false\n    def msa_name = msa.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $msa > $msa_name\n    fi\n\n    ClonalFrameML \\\\\n        $newick \\\\\n        $msa_name \\\\\n        $prefix \\\\\n        $options.args\n\n    maskrc-svg.py $prefix --aln ${msa_name} --symbol '-' --out ${prefix}.masked.aln\n    gzip ${prefix}.masked.aln\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        clonalframeml: \\$( echo \\$(ClonalFrameML -version 2>&1) | sed 's/^.*ClonalFrameML v//' )\n        maskrc-svg: \\$( echo \\$(maskrc-svg.py --version 2>&1) | sed 's/^.*maskrc-svg.py //' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [
            "ClonalFrameML"
        ],
        "tools_url": [
            "https://bio.tools/clonalframeml"
        ],
        "tools_dico": [
            {
                "name": "ClonalFrameML",
                "uri": "https://bio.tools/clonalframeml",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acids"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0194",
                            "term": "Phylogenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2403",
                                    "term": "Sequence analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0539",
                                    "term": "Phylogenetic inference (method centric)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Nucleic acid sequence analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2403",
                                    "term": "Sequence analysis (general)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0539",
                                    "term": "Phylogenetic tree construction (method centric)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0539",
                                    "term": "Phylogenetic tree generation (method centric)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Sequence analysis (nucleic acid)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "ClonalFrameML is a maximum likelihood implementation of the Bayesian software ClonalFrame which was previously described by Didelot and Falush (2007). The recombination model underpinning ClonalFrameML is exactly the same as for ClonalFrame, but this new implementation is a lot faster, is able to deal with much larger genomic dataset, and does not suffer from MCMC convergence issues",
                "homepage": "https://github.com/xavierdidelot/ClonalFrameML"
            }
        ],
        "inputs": [
            "meta",
            "msa",
            "newick"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/mulled-v2-f5c68f1508671d5744655da9b0e8b609098f4138:7e089189af7822a6a18245830639dbfe11a4c277-0' : 'quay.io/biocontainers/mulled-v2-f5c68f1508671d5744655da9b0e8b609098f4138:7e089189af7822a6a18245830639dbfe11a4c277-0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MOBSUITE_RECON": {
        "name_process": "MOBSUITE_RECON",
        "string_process": "\nprocess MOBSUITE_RECON {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mob_suite%3A3.0.3--pyhdfd78af_0' :\n        'quay.io/biocontainers/mob_suite:3.0.3--pyhdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"results/chromosome.fasta\") , emit: chromosome\n    tuple val(meta), path(\"results/contig_report.txt\"), emit: contig_report\n    tuple val(meta), path(\"results/plasmid_*.fasta\")  , emit: plasmids        , optional: true\n    tuple val(meta), path(\"results/${prefix}-mobtyper.txt\") , emit: mobtyper_results, optional: true\n    path \"*.{log,err}\"                                , emit: logs, optional: true\n    path \".command.*\"                                 , emit: nf_logs\n    path \"versions.yml\"                               , emit: versions\n\n    script:\n    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    mob_recon \\\\\n        --infile $fasta_name \\\\\n        $options.args \\\\\n        --num_threads $task.cpus \\\\\n        --outdir results \\\\\n        --sample_id $prefix\n\n    if [[ -f \"results/mobtyper_results.txt\" ]]; then\n        mv results/mobtyper_results.txt results/${prefix}-mobtyper.txt\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mobsuite: \\$(echo \\$(mob_recon --version 2>&1) | sed 's/^.*mob_recon //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 48,
        "string_script": "    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    mob_recon \\\\\n        --infile $fasta_name \\\\\n        $options.args \\\\\n        --num_threads $task.cpus \\\\\n        --outdir results \\\\\n        --sample_id $prefix\n\n    if [[ -f \"results/mobtyper_results.txt\" ]]; then\n        mv results/mobtyper_results.txt results/${prefix}-mobtyper.txt\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        mobsuite: \\$(echo \\$(mob_recon --version 2>&1) | sed 's/^.*mob_recon //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 23,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/mob_suite%3A3.0.3--pyhdfd78af_0' : 'quay.io/biocontainers/mob_suite:3.0.3--pyhdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "IQTREE": {
        "name_process": "IQTREE",
        "string_process": "\nprocess IQTREE {\n    tag \"$prefix\"\n    label 'process_medium'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/iqtree:2.1.4_beta--hdcc8f71_0' :\n        'quay.io/biocontainers/iqtree:2.1.4_beta--hdcc8f71_0' }\"\n\n    input:\n    tuple val(meta), path(alignment)\n\n    output:\n    tuple val(meta), path(\"${prefix}*\")                         , emit: results\n    tuple val(meta), path(\"${prefix}.treefile\")                 , emit: phylogeny\n    tuple val(meta), path(alignment), path(\"${prefix}.treefile\"), emit: aln_tree\n    path \"*.{log,err}\"                                          , emit: logs, optional: true\n    path \".command.*\"                                           , emit: nf_logs\n    path \"versions.yml\"                                         , emit: versions\n\n    script:\n    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def memory = task.memory.toString().replaceAll(' ', '')\n    \"\"\"\n    iqtree \\\\\n        $options.args \\\\\n        -s $alignment \\\\\n        -nt AUTO \\\\\n        -ntmax $task.cpus \\\\\n        -pre $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        iqtree: \\$(echo \\$(iqtree -version 2>&1) | sed 's/^IQ-TREE multicore version //;s/ .*//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def memory = task.memory.toString().replaceAll(' ', '')\n    \"\"\"\n    iqtree \\\\\n        $options.args \\\\\n        -s $alignment \\\\\n        -nt AUTO \\\\\n        -ntmax $task.cpus \\\\\n        -pre $prefix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        iqtree: \\$(echo \\$(iqtree -version 2>&1) | sed 's/^IQ-TREE multicore version //;s/ .*//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "alignment"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$prefix\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/iqtree:2.1.4_beta--hdcc8f71_0' : 'quay.io/biocontainers/iqtree:2.1.4_beta--hdcc8f71_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "KRAKEN2": {
        "name_process": "KRAKEN2",
        "string_process": "\nprocess KRAKEN2 {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0' :\n        'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path db\n\n    output:\n    tuple val(meta), path('*classified*')  , emit: classified\n    tuple val(meta), path('*unclassified*'), emit: unclassified\n    tuple val(meta), path('*report.txt')   , emit: report\n    path \"*.{log,err}\" , emit: logs, optional: true\n    path \".command.*\"  , emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $options.args \\\\\n        $reads > /dev/null\n\n    pigz -p $task.cpus *.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kraken2: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 48,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def paired       = meta.single_end ? \"\" : \"--paired\"\n    def classified   = meta.single_end ? \"${prefix}.classified.fastq\"   : \"${prefix}.classified#.fastq\"\n    def unclassified = meta.single_end ? \"${prefix}.unclassified.fastq\" : \"${prefix}.unclassified#.fastq\"\n    \"\"\"\n    kraken2 \\\\\n        --db $db \\\\\n        --threads $task.cpus \\\\\n        --unclassified-out $unclassified \\\\\n        --classified-out $classified \\\\\n        --report ${prefix}.kraken2.report.txt \\\\\n        --gzip-compressed \\\\\n        $paired \\\\\n        $options.args \\\\\n        $reads > /dev/null\n\n    pigz -p $task.cpus *.fastq\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        kraken2: \\$(echo \\$(kraken2 --version 2>&1) | sed 's/^.*Kraken version //; s/ .*\\$//')\n        pigz: \\$( pigz --version 2>&1 | sed 's/pigz //g' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 23,
        "language_script": "bash",
        "tools": [
            "kraken2",
            "NullSeq"
        ],
        "tools_url": [
            "https://bio.tools/kraken2",
            "https://bio.tools/nullseq"
        ],
        "tools_dico": [
            {
                "name": "kraken2",
                "uri": "https://bio.tools/kraken2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3028",
                                "term": "Taxonomy"
                            }
                        ]
                    }
                ],
                "description": "Kraken 2 is the newest version of Kraken, a taxonomic classification system using exact k-mer matches to achieve high accuracy and fast classification speeds. This classifier matches each k-mer within a query sequence to the lowest common ancestor (LCA) of all genomes containing the given k-mer. The k-mer assignments inform the classification algorithm.",
                "homepage": "https://ccb.jhu.edu/software/kraken2/"
            },
            {
                "name": "NullSeq",
                "uri": "https://bio.tools/nullseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0364",
                                    "term": "Random sequence generation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Creates Random Coding Sequences with specified GC content and Amino Acid usage.",
                "homepage": "https://github.com/amarallab/NullSeq"
            }
        ],
        "inputs": [
            "meta",
            "reads",
            "db"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0' : 'quay.io/biocontainers/mulled-v2-5799ab18b5fc681e75923b2450abaa969907ec98:941789bd7fe00db16531c26de8bf3c5c985242a5-0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "SEQSERO2": {
        "name_process": "SEQSERO2",
        "string_process": "\nprocess SEQSERO2 {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/seqsero2:1.2.1--py_0' :\n        'quay.io/biocontainers/seqsero2:1.2.1--py_0' }\"\n\n    input:\n    tuple val(meta), path(seqs)\n\n    output:\n    tuple val(meta), path(\"results/*_log.txt\")   , emit: log\n    tuple val(meta), path(\"results/*_result.tsv\"), emit: tsv\n    tuple val(meta), path(\"results/*_result.txt\"), emit: txt\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed_fna = seqs[0].getName().endsWith(\"fna.gz\") ? true : false\n    def seq_name = is_compressed_fna ? seqs[0].getName().replace(\".gz\", \"\") : \"${seqs}\"\n    \"\"\"\n    if [ \"$is_compressed_fna\" == \"true\" ]; then\n        gzip -c -d ${seqs[0]} > $seq_name\n    fi\n    SeqSero2_package.py \\\\\n        $options.args \\\\\n        -d results/ \\\\\n        -n $prefix \\\\\n        -p $task.cpus \\\\\n        -i $seq_name\n\n    mv results/SeqSero_log.txt results/${prefix}_log.txt\n    mv results/SeqSero_result.tsv results/${prefix}_result.tsv\n    mv results/SeqSero_result.txt results/${prefix}_result.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        seqsero2: \\$( echo \\$( SeqSero2_package.py --version 2>&1) | sed 's/^.*SeqSero2_package.py //' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 46,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed_fna = seqs[0].getName().endsWith(\"fna.gz\") ? true : false\n    def seq_name = is_compressed_fna ? seqs[0].getName().replace(\".gz\", \"\") : \"${seqs}\"\n    \"\"\"\n    if [ \"$is_compressed_fna\" == \"true\" ]; then\n        gzip -c -d ${seqs[0]} > $seq_name\n    fi\n    SeqSero2_package.py \\\\\n        $options.args \\\\\n        -d results/ \\\\\n        -n $prefix \\\\\n        -p $task.cpus \\\\\n        -i $seq_name\n\n    mv results/SeqSero_log.txt results/${prefix}_log.txt\n    mv results/SeqSero_result.tsv results/${prefix}_result.tsv\n    mv results/SeqSero_result.txt results/${prefix}_result.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        seqsero2: \\$( echo \\$( SeqSero2_package.py --version 2>&1) | sed 's/^.*SeqSero2_package.py //' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "seqs"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/seqsero2:1.2.1--py_0' : 'quay.io/biocontainers/seqsero2:1.2.1--py_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "NGMASTER": {
        "name_process": "NGMASTER",
        "string_process": "\nprocess NGMASTER {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ngmaster:0.5.8--pyhdfd78af_1' :\n        'quay.io/biocontainers/ngmaster:0.5.8--pyhdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    ngmaster \\\\\n        $options.args \\\\\n        $fasta_name \\\\\n        > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ngmaster: \\$( echo \\$(ngmaster --version 2>&1) | sed 's/^.*ngmaster //' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 39,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    ngmaster \\\\\n        $options.args \\\\\n        $fasta_name \\\\\n        > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ngmaster: \\$( echo \\$(ngmaster --version 2>&1) | sed 's/^.*ngmaster //' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/ngmaster:0.5.8--pyhdfd78af_1' : 'quay.io/biocontainers/ngmaster:0.5.8--pyhdfd78af_1' }\""
        ],
        "when": "",
        "stub": ""
    },
    "CHECKM_LINEAGEWF": {
        "name_process": "CHECKM_LINEAGEWF",
        "string_process": "\nprocess CHECKM_LINEAGEWF {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/checkm-genome:1.1.3--py_1' :\n        'quay.io/biocontainers/checkm-genome:1.1.3--py_1' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"results/*\")    , emit: results\n    tuple val(meta), path(\"results/${prefix}-results.txt\"), emit: tsv\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    checkm \\\\\n        lineage_wf ./ results/ \\\\\n        --tab_table \\\\\n        --threads $task.cpus \\\\\n        --pplacer_threads $task.cpus \\\\\n        --alignment_file results/${prefix}-genes.aln \\\\\n        --file results/${prefix}-results.txt \\\\\n        $options.args\n\n    find ./results/ -name \"*.faa\" -or -name \"*hmmer.analyze.txt\" -or -name \"*.fasta\" | xargs gzip\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        checkm: \\$(echo \\$(checkm -h 2>&1) | sed 's/.*CheckM v//;s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 46,
        "string_script": "    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    checkm \\\\\n        lineage_wf ./ results/ \\\\\n        --tab_table \\\\\n        --threads $task.cpus \\\\\n        --pplacer_threads $task.cpus \\\\\n        --alignment_file results/${prefix}-genes.aln \\\\\n        --file results/${prefix}-results.txt \\\\\n        $options.args\n\n    find ./results/ -name \"*.faa\" -or -name \"*hmmer.analyze.txt\" -or -name \"*.fasta\" | xargs gzip\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        checkm: \\$(echo \\$(checkm -h 2>&1) | sed 's/.*CheckM v//;s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 23,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/checkm-genome:1.1.3--py_1' : 'quay.io/biocontainers/checkm-genome:1.1.3--py_1' }\""
        ],
        "when": "",
        "stub": ""
    },
    "SSUISSERO": {
        "name_process": "SSUISSERO",
        "string_process": "\nprocess SSUISSERO {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ssuissero%3A1.0.1--hdfd78af_0' :\n        'quay.io/biocontainers/ssuissero:1.0.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"*.{log,err}\"            , emit: logs, optional: true\n    path \".command.*\"             , emit: nf_logs\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    SsuisSero.sh \\\\\n        -i $fasta_name \\\\\n        -o ./ \\\\\n        -s $prefix \\\\\n        -x fasta \\\\\n        -t $task.cpus\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ssuissero: $VERSION\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 42,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    SsuisSero.sh \\\\\n        -i $fasta_name \\\\\n        -o ./ \\\\\n        -s $prefix \\\\\n        -x fasta \\\\\n        -t $task.cpus\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ssuissero: $VERSION\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 20,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/ssuissero%3A1.0.1--hdfd78af_0' : 'quay.io/biocontainers/ssuissero:1.0.1--hdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "PROKKA": {
        "name_process": "PROKKA",
        "string_process": "\nprocess PROKKA {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0' :\n        'quay.io/biocontainers/prokka:1.14.6--pl526_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path proteins\n    path prodigal_tf\n\n    output:\n    tuple val(meta), path(\"${prefix}/*.{gff,gff.gz}\"), emit: gff\n    tuple val(meta), path(\"${prefix}/*.{gbk,gbk.gz}\"), emit: gbk\n    tuple val(meta), path(\"${prefix}/*.{fna,fna.gz}\"), emit: fna\n    tuple val(meta), path(\"${prefix}/*.{faa,faa.gz}\"), emit: faa\n    tuple val(meta), path(\"${prefix}/*.{ffn,ffn.gz}\"), emit: ffn\n    tuple val(meta), path(\"${prefix}/*.{sqn,sqn.gz}\"), emit: sqn\n    tuple val(meta), path(\"${prefix}/*.{fsa,fsa.gz}\"), emit: fsa\n    tuple val(meta), path(\"${prefix}/*.{tbl,tbl.gz}\"), emit: tbl\n    tuple val(meta), path(\"${prefix}/*.txt\"), emit: txt\n    tuple val(meta), path(\"${prefix}/*.tsv\"), emit: tsv\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    prokka \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta_name\n\n    if [[ \"${params.skip_compression}\" == \"false\" ]]; then\n        gzip ${prefix}/*.gff\n        gzip ${prefix}/*.gbk\n        gzip ${prefix}/*.fna\n        gzip ${prefix}/*.faa\n        gzip ${prefix}/*.ffn\n        gzip ${prefix}/*.sqn\n        gzip ${prefix}/*.fsa\n        gzip ${prefix}/*.tbl\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        prokka: \\$( echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 66,
        "string_script": "    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    prokka \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta_name\n\n    if [[ \"${params.skip_compression}\" == \"false\" ]]; then\n        gzip ${prefix}/*.gff\n        gzip ${prefix}/*.gbk\n        gzip ${prefix}/*.fna\n        gzip ${prefix}/*.faa\n        gzip ${prefix}/*.ffn\n        gzip ${prefix}/*.sqn\n        gzip ${prefix}/*.fsa\n        gzip ${prefix}/*.tbl\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        prokka: \\$( echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 33,
        "language_script": "bash",
        "tools": [
            "Prokka"
        ],
        "tools_url": [
            "https://bio.tools/prokka"
        ],
        "tools_dico": [
            {
                "name": "Prokka",
                "uri": "https://bio.tools/prokka",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "Coding region prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0362",
                                    "term": "Genome annotation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "ORF prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "ORF finding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene finding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene calling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Software tool to annotate bacterial, archaeal and viral genomes quickly and produce standards-compliant output files.",
                "homepage": "https://github.com/tseemann/prokka"
            }
        ],
        "inputs": [
            "meta",
            "fasta",
            "proteins",
            "prodigal_tf"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0' : 'quay.io/biocontainers/prokka:1.14.6--pl526_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "LISSERO": {
        "name_process": "LISSERO",
        "string_process": "\nprocess LISSERO {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/lissero:0.4.9--py_0' :\n        'quay.io/biocontainers/lissero:0.4.9--py_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    lissero \\\\\n        $options.args \\\\\n        $fasta_name \\\\\n        > ${prefix}.tsv\n    sed -i 's/^.*${fasta_name}/${fasta_name}/' ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        lissero: \\$( echo \\$(lissero --version 2>&1) | sed 's/^.*LisSero //' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 40,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    lissero \\\\\n        $options.args \\\\\n        $fasta_name \\\\\n        > ${prefix}.tsv\n    sed -i 's/^.*${fasta_name}/${fasta_name}/' ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        lissero: \\$( echo \\$(lissero --version 2>&1) | sed 's/^.*LisSero //' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/lissero:0.4.9--py_0' : 'quay.io/biocontainers/lissero:0.4.9--py_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "LEGSTA": {
        "name_process": "LEGSTA",
        "string_process": "\nprocess LEGSTA {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/legsta%3A0.5.1--hdfd78af_2' :\n        'quay.io/biocontainers/legsta:0.5.1--hdfd78af_2' }\"\n\n    input:\n    tuple val(meta), path(seqs)\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    path \"*.{log,err}\"            , emit: logs, optional: true\n    path \".command.*\"             , emit: nf_logs\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    legsta \\\\\n        $options.args \\\\\n        $seqs | sed 's/.fna//; s/.gz//' > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        legsta: \\$(echo \\$(legsta --version 2>&1) | sed 's/^.*legsta //; s/ .*\\$//;')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    legsta \\\\\n        $options.args \\\\\n        $seqs | sed 's/.fna//; s/.gz//' > ${prefix}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        legsta: \\$(echo \\$(legsta --version 2>&1) | sed 's/^.*legsta //; s/ .*\\$//;')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "seqs"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/legsta%3A0.5.1--hdfd78af_2' : 'quay.io/biocontainers/legsta:0.5.1--hdfd78af_2' }\""
        ],
        "when": "",
        "stub": ""
    },
    "ABRICATE_RUN": {
        "name_process": "ABRICATE_RUN",
        "string_process": "\nprocess ABRICATE_RUN {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/abricate%3A1.0.1--ha8f3691_1' :\n        'quay.io/biocontainers/abricate:1.0.1--ha8f3691_1' }\"\n\n    input:\n    tuple val(meta), path(assembly)\n\n    output:\n    tuple val(meta), path(\"*.txt\"), emit: report\n    path \"*.{log,err}\"            , emit: logs, optional: true\n    path \".command.*\"             , emit: nf_logs\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    abricate \\\\\n        $assembly \\\\\n        $options.args \\\\\n        --threads $task.cpus > ${prefix}.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        abricate: \\$(echo \\$(abricate --version 2>&1) | sed 's/^.*abricate //' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    abricate \\\\\n        $assembly \\\\\n        $options.args \\\\\n        --threads $task.cpus > ${prefix}.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        abricate: \\$(echo \\$(abricate --version 2>&1) | sed 's/^.*abricate //' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "ABRicate"
        ],
        "tools_url": [
            "https://bio.tools/ABRicate"
        ],
        "tools_dico": [
            {
                "name": "ABRicate",
                "uri": "https://bio.tools/ABRicate",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3482",
                                    "term": "Antimicrobial resistance prediction"
                                }
                            ],
                            []
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_1234",
                                "term": "Sequence set (nucleic acid)"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0916",
                                "term": "Gene report"
                            }
                        ]
                    }
                ],
                "description": "Mass screening of contigs for antimicrobial resistance or virulence genes.",
                "homepage": "https://github.com/tseemann/abricate"
            }
        ],
        "inputs": [
            "meta",
            "assembly"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/abricate%3A1.0.1--ha8f3691_1' : 'quay.io/biocontainers/abricate:1.0.1--ha8f3691_1' }\""
        ],
        "when": "",
        "stub": ""
    },
    "GATHER_SAMPLES": {
        "name_process": "GATHER_SAMPLES",
        "string_process": "\nprocess GATHER_SAMPLES {\n                                              \n    tag \"${meta.id}\"\n    label \"gather_samples\"\n\n    publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    input:\n    tuple val(meta), path(r1, stageAs: '*???-r1'), path(r2, stageAs: '*???-r2'), path(extra)\n\n    output:\n    tuple val(meta), path(\"fastqs/${meta.id}*.fastq.gz\"), path(\"extra/*.gz\"), path(\"${meta.id}-genome-size.txt\"), emit: raw_fastq, optional: true\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n    path \"*-{error,merged}.txt\", optional: true\n\n    shell:\n    meta.original_runtype = meta.runtype\n    runtype = meta.original_runtype\n    is_assembly = runtype.startsWith('assembly') ? true : false\n    is_compressed = extra ? (extra.getName().endsWith('gz') ? true : false) : false\n    no_cache = params.no_cache ? '-N' : ''\n    archive = params.use_ena ? (task.attempt >= 4 ? \"SRA\" : \"ENA\") : \"SRA\"\n    section = runtype == 'assembly_accession' ? (meta.id.startsWith('GCF') ? 'refseq' : 'genbank') : null\n    fcov = params.coverage.toInteger() == 0 ? 150 : Math.round(params.coverage.toInteger() * 1.5)\n    if (runtype == 'hybrid-merge-pe') {\n        meta.runtype = 'hybrid'\n    } else if (runtype == 'merge-pe') {\n        meta.runtype = 'paired-end'\n    } else if (runtype == 'merge-se') {\n        meta.runtype = 'single-end'\n    }\n    qin = is_assembly ? 'qin=33' : 'qin=auto'\n    '''\n    MERGED=\"multiple-read-sets-merged.txt\"\n    mkdir -p fastqs\n    mkdir -p extra\n\n    if [ \"!{runtype}\" == \"paired-end\" ]; then\n        # Paired-End Reads\n        cp -L !{r1[0]} fastqs/!{meta.id}_R1.fastq.gz\n        cp -L !{r2[0]} fastqs/!{meta.id}_R2.fastq.gz\n        touch extra/empty.fna.gz\n    elif [ \"!{runtype}\" == \"single-end\" ]; then\n        # Single-End Reads\n        cp -L !{r1[0]} fastqs/!{meta.id}.fastq.gz\n        touch extra/empty.fna.gz\n    elif [ \"!{runtype}\" == \"ont\" ]; then\n        # Nanopore reads\n        cp -L !{r1[0]} fastqs/!{meta.id}.fastq.gz\n        touch extra/empty.fna.gz\n    elif  [ \"!{runtype}\" == \"hybrid\" ]; then\n        # Paired-End Reads\n        cp -L !{r1[0]} fastqs/!{meta.id}_R1.fastq.gz\n        cp -L !{r2[0]} fastqs/!{meta.id}_R2.fastq.gz\n        cp -L !{extra} extra/!{meta.id}.fastq.gz\n    elif [ \"!{runtype}\" == \"merge-pe\" ] || [ \"!{runtype}\" == \"hybrid-merge-pe\" ]; then \n        # Merge Paired-End Reads\n        echo \"This sample had reads merged.\" > ${MERGED}\n        echo \"R1:\" >> ${MERGED}\n        find -name \"*r1\" | sort | xargs -I {} readlink {} | xargs -I {} ls -l {} | awk '{print $5\"\\t\"$9}' >> ${MERGED}\n        find -name \"*r1\" | sort | xargs -I {} readlink {} | xargs -I {} cat {} > fastqs/!{meta.id}_R1.fastq.gz\n        echo \"Merged R1:\" >> ${MERGED}\n        ls -l fastqs/!{meta.id}_R1.fastq.gz | awk '{print $5\"\\t\"$9}' >> ${MERGED}\n\n        echo \"R2:\" >> ${MERGED}\n        find -name \"*r2\" | sort | xargs -I {} readlink {} | xargs -I {} ls -l {} | awk '{print $5\"\\t\"$9}' >> ${MERGED}\n        find -name \"*r2\" | sort | xargs -I {} readlink {} | xargs -I {} cat {} > fastqs/!{meta.id}_R2.fastq.gz\n        echo \"Merged R2:\" >> ${MERGED}\n        ls -l fastqs/!{meta.id}_R2.fastq.gz | awk '{print $5\"\\t\"$9}' >> ${MERGED}\n\n        if [ \"!{runtype}\" == \"hybrid-merge-pe\" ]; then\n            cp -L !{extra} extra/!{meta.id}.fastq.gz\n        else\n            touch extra/empty.fna.gz\n        fi\n    elif [ \"!{runtype}\" == \"merge-se\" ]; then \n        # Merge Single-End Reads\n        echo \"This sample had reads merged.\" > ${MERGED}\n        echo \"SE:\" >> ${MERGED}\n        find -name \"*r1\" | sort | xargs -I {} readlink {} | xargs -I {} ls -l {} | awk '{print $5\"\\t\"$9}' >> ${MERGED}\n        find -name \"*r1\" | sort | xargs -I {} readlink {} | xargs -I {} cat {} > fastqs/!{meta.id}.fastq.gz\n        echo \"Merged SE:\" >> ${MERGED}\n        ls -l fastqs/!{meta.id}.fastq.gz | awk '{print $5\"\\t\"$9}' >> ${MERGED}\n\n        touch extra/empty.fna.gz\n    elif [ \"!{runtype}\" == \"sra_accession\" ]; then\n        if [ \"!{task.attempt}\" == \"!{params.max_retry}\" ]; then\n            echo \"Unable to download !{meta.id} from both SRA and ENA !{params.max_retry} times. This may or may \n                not be a temporary connection issue. Rather than stop the whole Bactopia run, \n                further analysis of !{meta.id} will be discontinued.\" | \\\n            sed 's/^\\\\s*//' > !{meta.id}-fastq-download-error.txt\n            exit\n        else\n            # Download accession from ENA/SRA\n            fastq-dl !{meta.id} !{archive} \\\n                --cpus !{task.cpus} \\\n                --outdir fastqs/ \\\n                --group_by_experiment \\\n                --is_experiment \\\n                --ftp_only\n            touch extra/empty.fna.gz\n        fi \n    elif [ \"!{is_assembly}\" == \"true\" ]; then\n        if [ \"!{runtype}\" == \"assembly_accession\" ]; then\n            if [ \"!{task.attempt}\" == \"!{params.max_retry}\" ]; then\n                touch extra/empty.fna.gz\n                echo \"Unable to download !{meta.id} from NCBI Assembly !{params.max_retry} times. This may or may\n                    not be a temporary connection issue. Rather than stop the whole Bactopia run, \n                    further analysis of !{meta.id} will be discontinued.\" | \\\n                sed 's/^\\\\s*//' > !{meta.id}-assembly-download-error.txt\n                exit\n            else\n                # Verify Assembly accession\n                check-assembly-accession.py !{meta.id} > accession.txt 2> check-assembly-accession.txt\n\n                if [ -s \"accession.txt\" ]; then\n                    # Download from NCBI assembly and simulate reads\n                    mkdir fasta/\n                    ncbi-genome-download bacteria -o ./ -F fasta -p !{task.cpus} \\\n                                                -s !{section} -A accession.txt -r 50 !{no_cache}\n                    find . -name \"*!{meta.id}*.fna.gz\" | xargs -I {} mv {} fasta/\n                    rename 's/(GC[AF]_\\\\d+).*/$1.fna.gz/' fasta/*\n                    gzip -cd fasta/!{meta.id}.fna.gz > !{meta.id}-art.fna\n                    rm check-assembly-accession.txt\n                else\n                    mv check-assembly-accession.txt !{meta.id}-assembly-accession-error.txt\n                    exit\n                fi\n            fi\n        elif [ \"!{runtype}\" == \"assembly\" ]; then\n            if [ \"!{is_compressed}\" == \"true\" ]; then\n                gzip -cd !{extra} > !{meta.id}-art.fna\n            else \n                cat !{extra} > !{meta.id}-art.fna\n            fi\n        fi\n\n        # Simulate reads from assembly, reads are 250bp without errors\n        art_illumina -p -ss MSv3 -l 250 -m 400 -s 30 --fcov !{fcov} -ir 0 -ir2 0 -dr 0 -dr2 0 -rs !{params.sampleseed} \\\n                        -na -qL 33 -qU 40 -o !{meta.id}_R --id !{meta.id} -i !{meta.id}-art.fna\n\n        mv !{meta.id}_R1.fq fastqs/!{meta.id}_R1.fastq\n        mv !{meta.id}_R2.fq fastqs/!{meta.id}_R2.fastq\n        pigz -p !{task.cpus} --fast fastqs/*.fastq\n        cp !{meta.id}-art.fna extra/!{meta.id}.fna\n        pigz -p !{task.cpus} --best extra/!{meta.id}.fna\n    fi\n\n    # Validate input FASTQs\n    if [ \"!{params.skip_fastq_check}\" == \"false\" ]; then\n        ERROR=0\n        # Check paired-end reads have same read counts\n        OPTS=\"--sample !{meta.id} --min_basepairs !{params.min_basepairs} --min_reads !{params.min_reads} --min_proportion !{params.min_proportion}\"\n        if [ -f  \"fastqs/!{meta.id}_R2.fastq.gz\" ]; then\n            # Paired-end\n            gzip -cd fastqs/!{meta.id}_R1.fastq.gz | fastq-scan > r1.json\n            gzip -cd fastqs/!{meta.id}_R2.fastq.gz | fastq-scan > r2.json\n            if ! reformat.sh in1=fastqs/!{meta.id}_R1.fastq.gz in2=fastqs/!{meta.id}_R2.fastq.gz !{qin} out=/dev/null 2> !{meta.id}-paired-end-error.txt; then\n                ERROR=1\n                echo \"!{meta.id} FASTQs contains an error. Please check the input FASTQs.\n                    Further analysis is discontinued.\" | \\\n                sed 's/^\\\\s*//' >> !{meta.id}-paired-end-error.txt\n            else\n                rm -f !{meta.id}-paired-end-error.txt\n            fi\n\n            if ! check-fastqs.py --fq1 r1.json --fq2 r2.json ${OPTS}; then\n                ERROR=1\n            fi\n            rm r1.json r2.json\n        else\n            # Single-end\n            gzip -cd fastqs/!{meta.id}.fastq.gz | fastq-scan > r1.json\n            if ! check-fastqs.py --fq1 r1.json ${OPTS}; then\n                ERROR=1\n            fi\n            rm r1.json\n        fi\n\n        # Failed validations so, let's keep them from continuing\n        if [ \"${ERROR}\" -eq \"1\" ]; then\n            mv fastqs/ failed-tests-fastqs/\n        fi\n    fi\n\n    # Estimate Genome Size\n    GENOME_SIZE_OUTPUT=\"!{meta.id}-genome-size.txt\"\n    if [ \"!{meta.genome_size}\" == \"0\" ]; then\n        if [ \"!{is_assembly}\" == \"true\" ]; then\n            # Use the total assembly size as the genome size\n            stats.sh in=extra/!{meta.id}.fna.gz | grep All | awk '{print $5}' | sed 's/,//g' > ${GENOME_SIZE_OUTPUT}\n        else\n            FASTQS=\"\"\n            if [ -f  \"fastqs/!{meta.id}_R2.fastq.gz\" ]; then\n                FASTQS=\"-r fastqs/!{meta.id}_R1.fastq.gz fastqs/!{meta.id}_R2.fastq.gz\"\n            else\n                FASTQS=\"fastqs/!{meta.id}.fastq.gz\"\n            fi\n\n            # First Pass\n            mash sketch -o test -k 31 -m 3 ${FASTQS} 2>&1 | \\\n                grep \"Estimated genome size:\" | \\\n                awk '{if($4){printf(\"%d\\\\n\", $4)}} END {if (!NR) print \"0\"}' > ${GENOME_SIZE_OUTPUT}\n            rm -rf test.msh\n            ESTIMATED_GENOME_SIZE=`head -n1 ${GENOME_SIZE_OUTPUT}`\n\n            # Check if second pass is needed\n            if [ ${ESTIMATED_GENOME_SIZE} -gt \"!{params.max_genome_size}\" ] || [ ${ESTIMATED_GENOME_SIZE} -lt \"!{params.min_genome_size}\" ]; then\n                # Probably high coverage, try increasing number of kmer copies to 10\n                M=\"-m 10\"\n                if [ ${ESTIMATED_GENOME_SIZE} -lt \"!{params.min_genome_size}\" ]; then\n                    # Probably low coverage, try decreasing the number of kmer copies to 1\n                    M=\"-m 1\"\n                fi\n                mash sketch -o test -k 31 ${M} ${FASTQS} 2>&1 | \\\n                    grep \"Estimated genome size:\" | \\\n                    awk '{if($4){printf(\"%d\\\\n\", $4)}} END {if (!NR) print \"0\"}' > ${GENOME_SIZE_OUTPUT}\n                rm -rf test.msh\n            fi\n        fi\n\n        # Check final estimate\n        ESTIMATED_GENOME_SIZE=`head -n1 ${GENOME_SIZE_OUTPUT}`\n        if [ ${ESTIMATED_GENOME_SIZE} -gt \"!{params.max_genome_size}\" ]; then\n            rm ${GENOME_SIZE_OUTPUT}\n            echo \"!{meta.id} estimated genome size (${ESTIMATED_GENOME_SIZE} bp) exceeds the maximum\n                    allowed genome size (!{params.max_genome_size} bp). If this is unexpected, please\n                    investigate !{meta.id} to determine a cause (e.g. metagenomic, contaminants, etc...).\n                    Otherwise, adjust the --max_genome_size parameter to fit your need. Further analysis\n                    of !{meta.id} will be discontinued.\" | \\\n            sed 's/^\\\\s*//' > !{meta.id}-genome-size-error.txt\n        elif [ ${ESTIMATED_GENOME_SIZE} -lt \"!{params.min_genome_size}\" ]; then\n            rm ${GENOME_SIZE_OUTPUT}\n            echo \"!{meta.id} estimated genome size (${ESTIMATED_GENOME_SIZE} bp) is less than the minimum\n                    allowed genome size (!{params.min_genome_size} bp). If this is unexpected, please\n                    investigate !{meta.id} to determine a cause (e.g. metagenomic, contaminants, etc...).\n                    Otherwise, adjust the --min_genome_size parameter to fit your need. Further analysis\n                    of !{meta.id} will be discontinued.\" | \\\n            sed 's/^\\\\s*//' > !{meta.id}-genome-size-error.txt\n        fi\n    else\n        # Use the genome size given by the user. (Should be >= 0)\n        echo \"!{meta.genome_size}\" > ${GENOME_SIZE_OUTPUT}\n    fi\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        art: $(echo $(art_illumina --help 2>&1) | sed 's/^.*Version //;s/ .*$//')\n        fastq-dl: $(echo $(fastq-dl --version 2>&1) | sed 's/fastq-dl //')\n        fastq-scan: $(echo $(fastq-scan -v 2>&1) | sed 's/fastq-scan //')\n        mash: $(echo $(mash --version 2>&1))\n        ncbi-genome-download: $(echo $(ncbi-genome-download --version 2>&1))\n        pigz: $(echo $(pigz --version 2>&1) | sed 's/pigz //')\n    END_VERSIONS\n    '''\n}",
        "nb_lignes_process": 259,
        "string_script": "    meta.original_runtype = meta.runtype\n    runtype = meta.original_runtype\n    is_assembly = runtype.startsWith('assembly') ? true : false\n    is_compressed = extra ? (extra.getName().endsWith('gz') ? true : false) : false\n    no_cache = params.no_cache ? '-N' : ''\n    archive = params.use_ena ? (task.attempt >= 4 ? \"SRA\" : \"ENA\") : \"SRA\"\n    section = runtype == 'assembly_accession' ? (meta.id.startsWith('GCF') ? 'refseq' : 'genbank') : null\n    fcov = params.coverage.toInteger() == 0 ? 150 : Math.round(params.coverage.toInteger() * 1.5)\n    if (runtype == 'hybrid-merge-pe') {\n        meta.runtype = 'hybrid'\n    } else if (runtype == 'merge-pe') {\n        meta.runtype = 'paired-end'\n    } else if (runtype == 'merge-se') {\n        meta.runtype = 'single-end'\n    }\n    qin = is_assembly ? 'qin=33' : 'qin=auto'\n    '''\n    MERGED=\"multiple-read-sets-merged.txt\"\n    mkdir -p fastqs\n    mkdir -p extra\n\n    if [ \"!{runtype}\" == \"paired-end\" ]; then\n        # Paired-End Reads\n        cp -L !{r1[0]} fastqs/!{meta.id}_R1.fastq.gz\n        cp -L !{r2[0]} fastqs/!{meta.id}_R2.fastq.gz\n        touch extra/empty.fna.gz\n    elif [ \"!{runtype}\" == \"single-end\" ]; then\n        # Single-End Reads\n        cp -L !{r1[0]} fastqs/!{meta.id}.fastq.gz\n        touch extra/empty.fna.gz\n    elif [ \"!{runtype}\" == \"ont\" ]; then\n        # Nanopore reads\n        cp -L !{r1[0]} fastqs/!{meta.id}.fastq.gz\n        touch extra/empty.fna.gz\n    elif  [ \"!{runtype}\" == \"hybrid\" ]; then\n        # Paired-End Reads\n        cp -L !{r1[0]} fastqs/!{meta.id}_R1.fastq.gz\n        cp -L !{r2[0]} fastqs/!{meta.id}_R2.fastq.gz\n        cp -L !{extra} extra/!{meta.id}.fastq.gz\n    elif [ \"!{runtype}\" == \"merge-pe\" ] || [ \"!{runtype}\" == \"hybrid-merge-pe\" ]; then \n        # Merge Paired-End Reads\n        echo \"This sample had reads merged.\" > ${MERGED}\n        echo \"R1:\" >> ${MERGED}\n        find -name \"*r1\" | sort | xargs -I {} readlink {} | xargs -I {} ls -l {} | awk '{print $5\"\\t\"$9}' >> ${MERGED}\n        find -name \"*r1\" | sort | xargs -I {} readlink {} | xargs -I {} cat {} > fastqs/!{meta.id}_R1.fastq.gz\n        echo \"Merged R1:\" >> ${MERGED}\n        ls -l fastqs/!{meta.id}_R1.fastq.gz | awk '{print $5\"\\t\"$9}' >> ${MERGED}\n\n        echo \"R2:\" >> ${MERGED}\n        find -name \"*r2\" | sort | xargs -I {} readlink {} | xargs -I {} ls -l {} | awk '{print $5\"\\t\"$9}' >> ${MERGED}\n        find -name \"*r2\" | sort | xargs -I {} readlink {} | xargs -I {} cat {} > fastqs/!{meta.id}_R2.fastq.gz\n        echo \"Merged R2:\" >> ${MERGED}\n        ls -l fastqs/!{meta.id}_R2.fastq.gz | awk '{print $5\"\\t\"$9}' >> ${MERGED}\n\n        if [ \"!{runtype}\" == \"hybrid-merge-pe\" ]; then\n            cp -L !{extra} extra/!{meta.id}.fastq.gz\n        else\n            touch extra/empty.fna.gz\n        fi\n    elif [ \"!{runtype}\" == \"merge-se\" ]; then \n        # Merge Single-End Reads\n        echo \"This sample had reads merged.\" > ${MERGED}\n        echo \"SE:\" >> ${MERGED}\n        find -name \"*r1\" | sort | xargs -I {} readlink {} | xargs -I {} ls -l {} | awk '{print $5\"\\t\"$9}' >> ${MERGED}\n        find -name \"*r1\" | sort | xargs -I {} readlink {} | xargs -I {} cat {} > fastqs/!{meta.id}.fastq.gz\n        echo \"Merged SE:\" >> ${MERGED}\n        ls -l fastqs/!{meta.id}.fastq.gz | awk '{print $5\"\\t\"$9}' >> ${MERGED}\n\n        touch extra/empty.fna.gz\n    elif [ \"!{runtype}\" == \"sra_accession\" ]; then\n        if [ \"!{task.attempt}\" == \"!{params.max_retry}\" ]; then\n            echo \"Unable to download !{meta.id} from both SRA and ENA !{params.max_retry} times. This may or may \n                not be a temporary connection issue. Rather than stop the whole Bactopia run, \n                further analysis of !{meta.id} will be discontinued.\" | \\\n            sed 's/^\\\\s*//' > !{meta.id}-fastq-download-error.txt\n            exit\n        else\n            # Download accession from ENA/SRA\n            fastq-dl !{meta.id} !{archive} \\\n                --cpus !{task.cpus} \\\n                --outdir fastqs/ \\\n                --group_by_experiment \\\n                --is_experiment \\\n                --ftp_only\n            touch extra/empty.fna.gz\n        fi \n    elif [ \"!{is_assembly}\" == \"true\" ]; then\n        if [ \"!{runtype}\" == \"assembly_accession\" ]; then\n            if [ \"!{task.attempt}\" == \"!{params.max_retry}\" ]; then\n                touch extra/empty.fna.gz\n                echo \"Unable to download !{meta.id} from NCBI Assembly !{params.max_retry} times. This may or may\n                    not be a temporary connection issue. Rather than stop the whole Bactopia run, \n                    further analysis of !{meta.id} will be discontinued.\" | \\\n                sed 's/^\\\\s*//' > !{meta.id}-assembly-download-error.txt\n                exit\n            else\n                # Verify Assembly accession\n                check-assembly-accession.py !{meta.id} > accession.txt 2> check-assembly-accession.txt\n\n                if [ -s \"accession.txt\" ]; then\n                    # Download from NCBI assembly and simulate reads\n                    mkdir fasta/\n                    ncbi-genome-download bacteria -o ./ -F fasta -p !{task.cpus} \\\n                                                -s !{section} -A accession.txt -r 50 !{no_cache}\n                    find . -name \"*!{meta.id}*.fna.gz\" | xargs -I {} mv {} fasta/\n                    rename 's/(GC[AF]_\\\\d+).*/$1.fna.gz/' fasta/*\n                    gzip -cd fasta/!{meta.id}.fna.gz > !{meta.id}-art.fna\n                    rm check-assembly-accession.txt\n                else\n                    mv check-assembly-accession.txt !{meta.id}-assembly-accession-error.txt\n                    exit\n                fi\n            fi\n        elif [ \"!{runtype}\" == \"assembly\" ]; then\n            if [ \"!{is_compressed}\" == \"true\" ]; then\n                gzip -cd !{extra} > !{meta.id}-art.fna\n            else \n                cat !{extra} > !{meta.id}-art.fna\n            fi\n        fi\n\n        # Simulate reads from assembly, reads are 250bp without errors\n        art_illumina -p -ss MSv3 -l 250 -m 400 -s 30 --fcov !{fcov} -ir 0 -ir2 0 -dr 0 -dr2 0 -rs !{params.sampleseed} \\\n                        -na -qL 33 -qU 40 -o !{meta.id}_R --id !{meta.id} -i !{meta.id}-art.fna\n\n        mv !{meta.id}_R1.fq fastqs/!{meta.id}_R1.fastq\n        mv !{meta.id}_R2.fq fastqs/!{meta.id}_R2.fastq\n        pigz -p !{task.cpus} --fast fastqs/*.fastq\n        cp !{meta.id}-art.fna extra/!{meta.id}.fna\n        pigz -p !{task.cpus} --best extra/!{meta.id}.fna\n    fi\n\n    # Validate input FASTQs\n    if [ \"!{params.skip_fastq_check}\" == \"false\" ]; then\n        ERROR=0\n        # Check paired-end reads have same read counts\n        OPTS=\"--sample !{meta.id} --min_basepairs !{params.min_basepairs} --min_reads !{params.min_reads} --min_proportion !{params.min_proportion}\"\n        if [ -f  \"fastqs/!{meta.id}_R2.fastq.gz\" ]; then\n            # Paired-end\n            gzip -cd fastqs/!{meta.id}_R1.fastq.gz | fastq-scan > r1.json\n            gzip -cd fastqs/!{meta.id}_R2.fastq.gz | fastq-scan > r2.json\n            if ! reformat.sh in1=fastqs/!{meta.id}_R1.fastq.gz in2=fastqs/!{meta.id}_R2.fastq.gz !{qin} out=/dev/null 2> !{meta.id}-paired-end-error.txt; then\n                ERROR=1\n                echo \"!{meta.id} FASTQs contains an error. Please check the input FASTQs.\n                    Further analysis is discontinued.\" | \\\n                sed 's/^\\\\s*//' >> !{meta.id}-paired-end-error.txt\n            else\n                rm -f !{meta.id}-paired-end-error.txt\n            fi\n\n            if ! check-fastqs.py --fq1 r1.json --fq2 r2.json ${OPTS}; then\n                ERROR=1\n            fi\n            rm r1.json r2.json\n        else\n            # Single-end\n            gzip -cd fastqs/!{meta.id}.fastq.gz | fastq-scan > r1.json\n            if ! check-fastqs.py --fq1 r1.json ${OPTS}; then\n                ERROR=1\n            fi\n            rm r1.json\n        fi\n\n        # Failed validations so, let's keep them from continuing\n        if [ \"${ERROR}\" -eq \"1\" ]; then\n            mv fastqs/ failed-tests-fastqs/\n        fi\n    fi\n\n    # Estimate Genome Size\n    GENOME_SIZE_OUTPUT=\"!{meta.id}-genome-size.txt\"\n    if [ \"!{meta.genome_size}\" == \"0\" ]; then\n        if [ \"!{is_assembly}\" == \"true\" ]; then\n            # Use the total assembly size as the genome size\n            stats.sh in=extra/!{meta.id}.fna.gz | grep All | awk '{print $5}' | sed 's/,//g' > ${GENOME_SIZE_OUTPUT}\n        else\n            FASTQS=\"\"\n            if [ -f  \"fastqs/!{meta.id}_R2.fastq.gz\" ]; then\n                FASTQS=\"-r fastqs/!{meta.id}_R1.fastq.gz fastqs/!{meta.id}_R2.fastq.gz\"\n            else\n                FASTQS=\"fastqs/!{meta.id}.fastq.gz\"\n            fi\n\n            # First Pass\n            mash sketch -o test -k 31 -m 3 ${FASTQS} 2>&1 | \\\n                grep \"Estimated genome size:\" | \\\n                awk '{if($4){printf(\"%d\\\\n\", $4)}} END {if (!NR) print \"0\"}' > ${GENOME_SIZE_OUTPUT}\n            rm -rf test.msh\n            ESTIMATED_GENOME_SIZE=`head -n1 ${GENOME_SIZE_OUTPUT}`\n\n            # Check if second pass is needed\n            if [ ${ESTIMATED_GENOME_SIZE} -gt \"!{params.max_genome_size}\" ] || [ ${ESTIMATED_GENOME_SIZE} -lt \"!{params.min_genome_size}\" ]; then\n                # Probably high coverage, try increasing number of kmer copies to 10\n                M=\"-m 10\"\n                if [ ${ESTIMATED_GENOME_SIZE} -lt \"!{params.min_genome_size}\" ]; then\n                    # Probably low coverage, try decreasing the number of kmer copies to 1\n                    M=\"-m 1\"\n                fi\n                mash sketch -o test -k 31 ${M} ${FASTQS} 2>&1 | \\\n                    grep \"Estimated genome size:\" | \\\n                    awk '{if($4){printf(\"%d\\\\n\", $4)}} END {if (!NR) print \"0\"}' > ${GENOME_SIZE_OUTPUT}\n                rm -rf test.msh\n            fi\n        fi\n\n        # Check final estimate\n        ESTIMATED_GENOME_SIZE=`head -n1 ${GENOME_SIZE_OUTPUT}`\n        if [ ${ESTIMATED_GENOME_SIZE} -gt \"!{params.max_genome_size}\" ]; then\n            rm ${GENOME_SIZE_OUTPUT}\n            echo \"!{meta.id} estimated genome size (${ESTIMATED_GENOME_SIZE} bp) exceeds the maximum\n                    allowed genome size (!{params.max_genome_size} bp). If this is unexpected, please\n                    investigate !{meta.id} to determine a cause (e.g. metagenomic, contaminants, etc...).\n                    Otherwise, adjust the --max_genome_size parameter to fit your need. Further analysis\n                    of !{meta.id} will be discontinued.\" | \\\n            sed 's/^\\\\s*//' > !{meta.id}-genome-size-error.txt\n        elif [ ${ESTIMATED_GENOME_SIZE} -lt \"!{params.min_genome_size}\" ]; then\n            rm ${GENOME_SIZE_OUTPUT}\n            echo \"!{meta.id} estimated genome size (${ESTIMATED_GENOME_SIZE} bp) is less than the minimum\n                    allowed genome size (!{params.min_genome_size} bp). If this is unexpected, please\n                    investigate !{meta.id} to determine a cause (e.g. metagenomic, contaminants, etc...).\n                    Otherwise, adjust the --min_genome_size parameter to fit your need. Further analysis\n                    of !{meta.id} will be discontinued.\" | \\\n            sed 's/^\\\\s*//' > !{meta.id}-genome-size-error.txt\n        fi\n    else\n        # Use the genome size given by the user. (Should be >= 0)\n        echo \"!{meta.genome_size}\" > ${GENOME_SIZE_OUTPUT}\n    fi\n\n    # Capture versions\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        art: $(echo $(art_illumina --help 2>&1) | sed 's/^.*Version //;s/ .*$//')\n        fastq-dl: $(echo $(fastq-dl --version 2>&1) | sed 's/fastq-dl //')\n        fastq-scan: $(echo $(fastq-scan -v 2>&1) | sed 's/fastq-scan //')\n        mash: $(echo $(mash --version 2>&1))\n        ncbi-genome-download: $(echo $(ncbi-genome-download --version 2>&1))\n        pigz: $(echo $(pigz --version 2>&1) | sed 's/pigz //')\n    END_VERSIONS\n    '''",
        "nb_lignes_script": 239,
        "language_script": "bash",
        "tools": [
            "RefCov",
            "SEQing",
            "Annot",
            "Mash",
            "MOFs"
        ],
        "tools_url": [
            "https://bio.tools/refcov",
            "https://bio.tools/SEQing",
            "https://bio.tools/Annot",
            "https://bio.tools/mash",
            "https://bio.tools/MOFs"
        ],
        "tools_dico": [
            {
                "name": "RefCov",
                "uri": "https://bio.tools/refcov",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "WashU Reference Coverage tool for analyzing the depth, breadth, and topology of sequencing coverage.",
                "homepage": "http://gmt.genome.wustl.edu/packages/refcov/"
            },
            {
                "name": "SEQing",
                "uri": "https://bio.tools/SEQing",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "Gene transcripts"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plant biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3794",
                            "term": "RNA immunoprecipitation"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "mRNA features"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plant science"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plants"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Botany"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plant"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3794",
                            "term": "RIP"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2499",
                                    "term": "Splicing analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2499",
                                    "term": "Splicing model analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "web-based visualization of Arabidopsis thaliana iCLIP and RNA-seq data in an interactive python framework.\n\nSEQing: interactive web-based tool for visualization of iCLIP and RNA-seq data.\n\nThe file requirements.txt can be used to install all needed needed dependencies for the project. Python 3.5 or higher is required and we recommend to setup a virtual environment for this project. If your current python points to a python2 version, please put python3 instead of just python before running SEQing",
                "homepage": "https://github.com/malewins/SEQing"
            },
            {
                "name": "Annot",
                "uri": "https://bio.tools/Annot",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarray experiment"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0089",
                            "term": "Ontology and terminology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0219",
                            "term": "Data submission, annotation and curation"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarrays"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0558",
                                    "term": "Phylogenetic tree annotation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0361",
                                    "term": "Sequence annotation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3553",
                                    "term": "Image annotation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data extraction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Retrieval"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "a Django-based sample, reagent, and experiment metadata tracking system.\n\nAnnot and information about what Annot is can be found here: https://gitlab.com/biotransistor/annot . This here is just tutorial material for the tutorial described here: http://annot.readthedocs.io/en/latest/man_tutorial.html .\n\nWelcome to Annotamentum\u2019s Documentation. \u2014 Annot 5 documentation.\n\nWelcome to Annotamentum\u2019s Documentation.\u00b6.\n\nDjango admin based sample, reagent and experiment metadata tracking.\n\nSummary: Annot is a web application, developed for biological wetlab experiment layout, sample and reagent logging, so that data is ready for sharing and analysis. On its core annot makes use of the acpipe_anjson library and acjson - assay coordinate json - file format. The use of controlled vocabulary from ontologies for sample and reagent annotation is enforced. Annot\u2019s modular implementation can be adapted to a variety of experimental paradigms",
                "homepage": "https://gitlab.com/biotransistor/annot"
            },
            {
                "name": "Mash",
                "uri": "https://bio.tools/mash",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2533",
                            "term": "DNA mutation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix generation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Phylogenetic distance matrix generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Fast genome and metagenome distance estimation using MinHash.",
                "homepage": "https://github.com/marbl/mash"
            },
            {
                "name": "MOFs",
                "uri": "https://bio.tools/MOFs",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2275",
                            "term": "Molecular modelling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2258",
                            "term": "Cheminformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3314",
                            "term": "Chemistry"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3068",
                            "term": "Literature and language"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_2258",
                            "term": "Chemoinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2258",
                            "term": "Chemical informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3068",
                            "term": "Language"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3068",
                            "term": "Literature"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2476",
                                    "term": "Molecular dynamics"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3891",
                                    "term": "Essential dynamics"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3454",
                                    "term": "Phasing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2476",
                                    "term": "Molecular dynamics simulation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3891",
                                    "term": "PCA"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3891",
                                    "term": "Principal modes"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3891",
                                    "term": "ED"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The Role of Molecular Modeling & Simulation in the Discovery and Deployment of Metal-Organic Frameworks for Gas Storage and Separation.\n\nWelcome to the Computation-Ready, Experimental (CoRE) Metal-Organic Frameworks Database!.\n\nHigh-throughput computational screening of metal-organic frameworks rely on the availability of disorder-free atomic coordinate files which can be used as input to simulation software packages.\n\nWe have created CoRE MOF database and its variants which contains almost all MOFs that have been reported in the literature. We expect the database to continue to grow so please check back for new updates!.\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'adsorption', 'MOFs adsorption-based'",
                "homepage": "http://gregchung.github.io/CoRE-MOFs/"
            }
        ],
        "inputs": [
            "meta",
            "r1",
            "r2",
            "extra"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"${meta.id}\"",
            "label \"gather_samples\"",
            "publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }"
        ],
        "when": "",
        "stub": ""
    },
    "TBPROFILER_PROFILE": {
        "name_process": "TBPROFILER_PROFILE",
        "string_process": "\nprocess TBPROFILER_PROFILE {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/tb-profiler:3.0.8--pypyh5e36f6f_0' :\n        'quay.io/biocontainers/tb-profiler:3.0.8--pypyh5e36f6f_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"bam/*.bam\")     , emit: bam\n    tuple val(meta), path(\"results/*.csv\") , emit: csv, optional: true\n    tuple val(meta), path(\"results/*.json\"), emit: json\n    tuple val(meta), path(\"results/*.txt\") , emit: txt, optional: true\n    tuple val(meta), path(\"vcf/*.vcf.gz\")  , emit: vcf\n    path \"*.{log,err}\"                     , emit: logs, optional: true\n    path \".command.*\"                      , emit: nf_logs\n    path \"versions.yml\"                    , emit: versions\n\n    script:\n    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"--read1 $reads\" : \"--read1 ${reads[0]} --read2 ${reads[1]}\"\n    def platform = meta.runtype == \"ont\" ? \"--platform nanopore\" : \"--platform illumina\"\n    \"\"\"\n    tb-profiler \\\\\n        profile \\\\\n        $options.args \\\\\n        $platform \\\\\n        --csv \\\\\n        --txt \\\\\n        --prefix ${prefix} \\\\\n        --threads $task.cpus \\\\\n        --no_trim \\\\\n        $input_reads\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        tb-profiler:  \\$( echo \\$(tb-profiler --version 2>&1) | sed 's/TBProfiler version //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 45,
        "string_script": "    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"--read1 $reads\" : \"--read1 ${reads[0]} --read2 ${reads[1]}\"\n    def platform = meta.runtype == \"ont\" ? \"--platform nanopore\" : \"--platform illumina\"\n    \"\"\"\n    tb-profiler \\\\\n        profile \\\\\n        $options.args \\\\\n        $platform \\\\\n        --csv \\\\\n        --txt \\\\\n        --prefix ${prefix} \\\\\n        --threads $task.cpus \\\\\n        --no_trim \\\\\n        $input_reads\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        tb-profiler:  \\$( echo \\$(tb-profiler --version 2>&1) | sed 's/TBProfiler version //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 19,
        "language_script": "bash",
        "tools": [
            "gProfileR"
        ],
        "tools_url": [
            "https://bio.tools/gprofile_r"
        ],
        "tools_dico": [
            {
                "name": "gProfileR",
                "uri": "https://bio.tools/gprofile_r",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0602",
                            "term": "Molecular interactions, pathways and networks"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_1775",
                            "term": "Function analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0089",
                            "term": "Ontology and terminology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_1775",
                            "term": "Functional analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3672",
                                    "term": "Gene functional annotation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2436",
                                    "term": "Gene-set enrichment analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2497",
                                    "term": "Pathway or network analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3672",
                                    "term": "Sequence functional annotation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data extraction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2436",
                                    "term": "GSEA"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2436",
                                    "term": "Functional enrichment analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2436",
                                    "term": "Gene-set over-represenation analysis"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2295",
                                "term": "Gene ID"
                            },
                            {
                                "uri": "http://edamontology.org/data_3021",
                                "term": "UniProt accession"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2600",
                                "term": "Pathway or network"
                            }
                        ]
                    }
                ],
                "description": "gProfileR performs functional enrichment analysis, gene identifier conversion and mapping homologous genes across related organisms via the 'g:Profiler' toolkit. The tool performs statistical enrichment analysis to find over-representation of information like Gene Ontology terms, biological pathways, regulatory DNA elements, human disease gene annotations, and protein-protein interaction networks. The basic input is a list of genes.",
                "homepage": "https://cran.r-project.org/web/packages/gProfileR/index.html"
            }
        ],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/tb-profiler:3.0.8--pypyh5e36f6f_0' : 'quay.io/biocontainers/tb-profiler:3.0.8--pypyh5e36f6f_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "SCOARY": {
        "name_process": "SCOARY",
        "string_process": "\nprocess SCOARY {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/scoary:1.6.16--py_2' :\n        'quay.io/biocontainers/scoary:1.6.16--py_2' }\"\n\n    input:\n    tuple val(meta), path(genes)\n    path(traits)\n\n    output:\n    tuple val(meta), path(\"*.csv\"), emit: csv\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"scoary\"\n    \"\"\"\n    scoary \\\\\n        $options.args \\\\\n        --no-time \\\\\n        --threads $task.cpus \\\\\n        --traits $traits \\\\\n        --genes $genes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        scoary: \\$( scoary --version 2>&1 )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 36,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"scoary\"\n    \"\"\"\n    scoary \\\\\n        $options.args \\\\\n        --no-time \\\\\n        --threads $task.cpus \\\\\n        --traits $traits \\\\\n        --genes $genes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        scoary: \\$( scoary --version 2>&1 )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "Scoary"
        ],
        "tools_url": [
            "https://bio.tools/scoary"
        ],
        "tools_dico": [
            {
                "name": "Scoary",
                "uri": "https://bio.tools/scoary",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype and phenotype"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "GWAS study"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0085",
                            "term": "Functional genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype and phenotype resources"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype-phenotype analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype-phenotype"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "GWAS analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "GWAS"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3517",
                            "term": "Genome-wide association study"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2945",
                                    "term": "Analysis"
                                }
                            ],
                            []
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0916",
                                "term": "Gene report"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_1669",
                                "term": "P-value"
                            }
                        ]
                    }
                ],
                "description": "Pan-genome wide association studies and  is designed to take the gene_presence_absence.csv file from Roary as well as a traits file created by the user and calculate the assocations between all genes in the accessory genome (all genes that are present in i genomes where 1 < i < N) and the traits. It reports a list of genes sorted by strength of association per trait.",
                "homepage": "https://github.com/AdmiralenOla/Scoary"
            }
        ],
        "inputs": [
            "meta",
            "genes",
            "traits"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/scoary:1.6.16--py_2' : 'quay.io/biocontainers/scoary:1.6.16--py_2' }\""
        ],
        "when": "",
        "stub": ""
    },
    "AMRFINDERPLUS_UPDATE": {
        "name_process": "AMRFINDERPLUS_UPDATE",
        "string_process": "\nprocess AMRFINDERPLUS_UPDATE {\n    tag \"update\"\n    label 'process_low'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ncbi-amrfinderplus%3A3.10.23--h17dc2d4_0' :\n        'quay.io/biocontainers/ncbi-amrfinderplus:3.10.23--h17dc2d4_0' }\"\n\n    output:\n    path \"amrfinderdb.tar.gz\", emit: db\n    path \"*.{log,err}\"       , emit: logs, optional: true\n    path \".command.*\"        , emit: nf_logs\n    path \"versions.yml\"      , emit: versions\n\n    script:\n    \"\"\"\n    mkdir amrfinderdb\n    amrfinder_update -d amrfinderdb\n    tar czvf amrfinderdb.tar.gz -C \\$(readlink amrfinderdb/latest) ./\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        amrfinderplus: \\$(amrfinder --version)\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "    \"\"\"\n    mkdir amrfinderdb\n    amrfinder_update -d amrfinderdb\n    tar czvf amrfinderdb.tar.gz -C \\$(readlink amrfinderdb/latest) ./\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        amrfinderplus: \\$(amrfinder --version)\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"update\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/ncbi-amrfinderplus%3A3.10.23--h17dc2d4_0' : 'quay.io/biocontainers/ncbi-amrfinderplus:3.10.23--h17dc2d4_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "ROARY": {
        "name_process": "ROARY",
        "string_process": "\nprocess ROARY {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/roary:3.13.0--pl526h516909a_0' :\n        'quay.io/biocontainers/roary:3.13.0--pl526h516909a_0' }\"\n\n    input:\n    tuple val(meta), path(gff, stageAs: 'gff-tmp/*')\n\n    output:\n    tuple val(meta), path(\"results/*\")                        , emit: results\n    tuple val(meta), path(\"core-genome.aln.gz\")               , emit: aln, optional: true\n    tuple val(meta), path(\"results/gene_presence_absence.csv\"), emit: csv, optional: true\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    mkdir gff\n    cp -L gff-tmp/* gff/\n    find gff/ -name \"*.gff.gz\" | xargs gunzip\n    roary \\\\\n        $options.args \\\\\n        -p $task.cpus \\\\\n        -f results/ \\\\\n        gff/*.gff\n\n    gzip results/*.aln\n    gzip results/*.fa\n\n    if [[ -f \"results/core_gene_alignment.aln.gz\" ]]; then\n        cp results/core_gene_alignment.aln.gz ./core-genome.aln.gz\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        roary: \\$( roary --version )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 46,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    mkdir gff\n    cp -L gff-tmp/* gff/\n    find gff/ -name \"*.gff.gz\" | xargs gunzip\n    roary \\\\\n        $options.args \\\\\n        -p $task.cpus \\\\\n        -f results/ \\\\\n        gff/*.gff\n\n    gzip results/*.aln\n    gzip results/*.fa\n\n    if [[ -f \"results/core_gene_alignment.aln.gz\" ]]; then\n        cp results/core_gene_alignment.aln.gz ./core-genome.aln.gz\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        roary: \\$( roary --version )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [
            "Roary"
        ],
        "tools_url": [
            "https://bio.tools/roary"
        ],
        "tools_dico": [
            {
                "name": "Roary",
                "uri": "https://bio.tools/roary",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A high speed stand alone pan genome pipeline, which takes annotated assemblies in GFF3 format (produced by Prokka (Seemann, 2014)) and calculates the pan genome.",
                "homepage": "http://sanger-pathogens.github.io/Roary/"
            }
        ],
        "inputs": [
            "meta",
            "gff"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/roary:3.13.0--pl526h516909a_0' : 'quay.io/biocontainers/roary:3.13.0--pl526h516909a_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "CSVTK_CONCAT": {
        "name_process": "CSVTK_CONCAT",
        "string_process": "\nprocess CSVTK_CONCAT {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/csvtk:0.23.0--h9ee0642_0' :\n        'quay.io/biocontainers/csvtk:0.23.0--h9ee0642_0' }\"\n\n    input:\n    tuple val(meta), path(csv)\n    val in_format\n    val out_format\n\n    output:\n    tuple val(meta), path(\"${prefix}.${out_extension}\"), emit: csv\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\",emit: versions\n\n    script:\n    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def delimiter = in_format == \"tsv\" ? \"\\t\" : (in_format == \"csv\" ? \",\" : in_format)\n    def out_delimiter = out_format == \"tsv\" ? \"\\t\" : (out_format == \"csv\" ? \",\" : out_format)\n    out_extension = out_format == \"tsv\" ? 'tsv' : 'csv'\n    \"\"\"\n    csvtk \\\\\n        concat \\\\\n        $options.args \\\\\n        --num-cpus $task.cpus \\\\\n        --delimiter \"${delimiter}\" \\\\\n        --out-delimiter \"${out_delimiter}\" \\\\\n        --out-file ${prefix}.${out_extension} \\\\\n        $csv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        csvtk: \\$(echo \\$( csvtk version | sed -e \"s/csvtk v//g\" ))\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 42,
        "string_script": "    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def delimiter = in_format == \"tsv\" ? \"\\t\" : (in_format == \"csv\" ? \",\" : in_format)\n    def out_delimiter = out_format == \"tsv\" ? \"\\t\" : (out_format == \"csv\" ? \",\" : out_format)\n    out_extension = out_format == \"tsv\" ? 'tsv' : 'csv'\n    \"\"\"\n    csvtk \\\\\n        concat \\\\\n        $options.args \\\\\n        --num-cpus $task.cpus \\\\\n        --delimiter \"${delimiter}\" \\\\\n        --out-delimiter \"${out_delimiter}\" \\\\\n        --out-file ${prefix}.${out_extension} \\\\\n        $csv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        csvtk: \\$(echo \\$( csvtk version | sed -e \"s/csvtk v//g\" ))\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "csv",
            "in_format",
            "out_format"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/csvtk:0.23.0--h9ee0642_0' : 'quay.io/biocontainers/csvtk:0.23.0--h9ee0642_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "BAKTA": {
        "name_process": "BAKTA",
        "string_process": "\nprocess BAKTA {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n    \n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bakta:1.4.0--pyhdfd78af_1' :\n        'quay.io/biocontainers/bakta:1.4.0--pyhdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path db\n    path proteins\n    path prodigal_tf\n    path replicons\n\n    output:\n    tuple val(meta), path(\"${prefix}.embl\")             , emit: embl\n    tuple val(meta), path(\"${prefix}.faa\")              , emit: faa\n    tuple val(meta), path(\"${prefix}.ffn\")              , emit: ffn\n    tuple val(meta), path(\"${prefix}.fna\")              , emit: fna\n    tuple val(meta), path(\"${prefix}.gbff\")             , emit: gbff\n    tuple val(meta), path(\"${prefix}.gff3\")             , emit: gff\n    tuple val(meta), path(\"${prefix}.hypotheticals.tsv\"), emit: hypotheticals_tsv\n    tuple val(meta), path(\"${prefix}.hypotheticals.faa\"), emit: hypotheticals_faa\n    tuple val(meta), path(\"${prefix}.tsv\")              , emit: tsv\n    tuple val(meta), path(\"${prefix}.txt\")              , emit: txt\n    path \"*.{log,err}\"                                  , emit: logs, optional: true\n    path \".command.*\"                                   , emit: nf_logs\n    path \"versions.yml\"                                 , emit: versions\n\n\n    script:\n    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigal-tf ${prodigal_tf[0]}\" : \"\"\n    def replicons_opt = replicons ? \"--replicons ${replicons[0]}\" : \"\"\n    \"\"\"\n    bakta \\\\\n        $options.args \\\\\n        --threads $task.cpus \\\\\n        --prefix ${prefix} \\\\\n        --db $db \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $replicons_opt \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bakta: \\$( echo \\$(bakta --version 2>&1) | sed 's/^.*bakta //' )\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.embl\n    touch ${prefix}.faa\n    touch ${prefix}.ffn\n    touch ${prefix}.fna\n    touch ${prefix}.gbff\n    touch ${prefix}.gff3\n    touch ${prefix}.hypotheticals.tsv\n    touch ${prefix}.hypotheticals.faa\n    touch ${prefix}.tsv\n    touch ${prefix}.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bakta: \\$( echo \\$(bakta --version 2>&1) | sed 's/^.*bakta //' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 75,
        "string_script": "    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigal-tf ${prodigal_tf[0]}\" : \"\"\n    def replicons_opt = replicons ? \"--replicons ${replicons[0]}\" : \"\"\n    \"\"\"\n    bakta \\\\\n        $options.args \\\\\n        --threads $task.cpus \\\\\n        --prefix ${prefix} \\\\\n        --db $db \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $replicons_opt \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bakta: \\$( echo \\$(bakta --version 2>&1) | sed 's/^.*bakta //' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 19,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta",
            "db",
            "proteins",
            "prodigal_tf",
            "replicons"
        ],
        "nb_inputs": 6,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bakta:1.4.0--pyhdfd78af_1' : 'quay.io/biocontainers/bakta:1.4.0--pyhdfd78af_1' }\""
        ],
        "when": "",
        "stub": "\n    prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.embl\n    touch ${prefix}.faa\n    touch ${prefix}.ffn\n    touch ${prefix}.fna\n    touch ${prefix}.gbff\n    touch ${prefix}.gff3\n    touch ${prefix}.hypotheticals.tsv\n    touch ${prefix}.hypotheticals.faa\n    touch ${prefix}.tsv\n    touch ${prefix}.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bakta: \\$( echo \\$(bakta --version 2>&1) | sed 's/^.*bakta //' )\n    END_VERSIONS\n    \"\"\""
    },
    "FASTANI": {
        "name_process": "FASTANI",
        "string_process": "\nprocess FASTANI {\n    tag \"${reference_name}\"\n    label 'process_medium'\n    publishDir \"${publish_dir}/${reference_name}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastani:1.32--he1c1bb9_0' :\n        'quay.io/biocontainers/fastani:1.32--he1c1bb9_0' }\"\n\n\n    input:\n    tuple val(meta), path(query, stageAs: 'query-tmp/*')\n    each path(reference)\n\n    output:\n    tuple val(meta), path(\"*.tsv\")          , emit: tsv\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\"                       , emit: nf_logs\n    path \"versions.yml\"                     , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = reference.getName().endsWith(\".gz\") ? true : false\n    reference_fasta = reference.getName().replace(\".gz\", \"\")\n    reference_name = reference_fasta.replace(\".fna\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $reference > $reference_fasta\n    fi\n\n    mkdir query\n    cp -L query-tmp/* query/\n    find query/ -name \"*.gz\" | xargs gunzip\n    find query/ -name \"*\" -type f > query-list.txt\n\n    fastANI \\\\\n        --ql query-list.txt \\\\\n        -r $reference_fasta \\\\\n        -o fastani-result.tmp\n\n    echo \"query<TAB>reference<TAB>ani<TAB>mapped_fragments<TAB>total_fragments\" | sed 's/<TAB>/\\t/g' > ${reference_name}.tsv\n    sed 's=^query/==' fastani-result.tmp>> ${reference_name}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        fastani: \\$(fastANI --version 2>&1 | sed 's/version//;')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 50,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = reference.getName().endsWith(\".gz\") ? true : false\n    reference_fasta = reference.getName().replace(\".gz\", \"\")\n    reference_name = reference_fasta.replace(\".fna\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $reference > $reference_fasta\n    fi\n\n    mkdir query\n    cp -L query-tmp/* query/\n    find query/ -name \"*.gz\" | xargs gunzip\n    find query/ -name \"*\" -type f > query-list.txt\n\n    fastANI \\\\\n        --ql query-list.txt \\\\\n        -r $reference_fasta \\\\\n        -o fastani-result.tmp\n\n    echo \"query<TAB>reference<TAB>ani<TAB>mapped_fragments<TAB>total_fragments\" | sed 's/<TAB>/\\t/g' > ${reference_name}.tsv\n    sed 's=^query/==' fastani-result.tmp>> ${reference_name}.tsv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        fastani: \\$(fastANI --version 2>&1 | sed 's/version//;')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 26,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "query",
            "reference"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"${reference_name}\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}/${reference_name}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/fastani:1.32--he1c1bb9_0' : 'quay.io/biocontainers/fastani:1.32--he1c1bb9_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "EGGNOG_MAPPER": {
        "name_process": "EGGNOG_MAPPER",
        "string_process": "\nprocess EGGNOG_MAPPER {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/eggnog-mapper:2.1.6--pyhdfd78af_0' :\n        'quay.io/biocontainers/eggnog-mapper:2.1.6--pyhdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path(db)\n\n    output:\n    tuple val(meta), path(\"*.emapper.hits\")                , emit: hits\n    tuple val(meta), path(\"*.emapper.seed_orthologs\")      , emit: seed_orthologs\n    tuple val(meta), path(\"*.emapper.annotations\")         , emit: annotations\n    tuple val(meta), path(\"*.emapper.annotations.xlsx\")    , emit: xlsx     , optional: true\n    tuple val(meta), path(\"*.emapper.orthologs\")           , emit: orthologs, optional: true\n    tuple val(meta), path(\"*.emapper.genepred.fasta\")      , emit: genepred , optional: true\n    tuple val(meta), path(\"*.emapper.gff\")                 , emit: gff      , optional: true\n    tuple val(meta), path(\"*.emapper.no_annotations.fasta\"), emit: no_anno  , optional: true\n    tuple val(meta), path(\"*.emapper.pfam\")                , emit: pfam     , optional: true\n    path \"*.{log,err}\"                                     , emit: logs     , optional: true\n    path \".command.*\"                                      , emit: nf_logs\n    path \"versions.yml\"                                    , emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    emapper.py \\\\\n        $options.args \\\\\n        --cpu $task.cpus \\\\\n        --data_dir ./ \\\\\n        --output $prefix \\\\\n        -i $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        eggnog-mapper: \\$( echo \\$(emapper.py --version 2>&1)| sed 's/.* emapper-//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 44,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    emapper.py \\\\\n        $options.args \\\\\n        --cpu $task.cpus \\\\\n        --data_dir ./ \\\\\n        --output $prefix \\\\\n        -i $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        eggnog-mapper: \\$( echo \\$(emapper.py --version 2>&1)| sed 's/.* emapper-//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta",
            "db"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/eggnog-mapper:2.1.6--pyhdfd78af_0' : 'quay.io/biocontainers/eggnog-mapper:2.1.6--pyhdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "QC_READS": {
        "name_process": "QC_READS",
        "string_process": "\nprocess QC_READS {\n                                 \n    tag \"${meta.id}\"\n    label \"base_mem_4gb\"\n    label \"qc_reads\"\n\n    publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force, saveAs: { filename -> saveFiles(filename:filename, opts: options) }\n\n    input:\n    tuple val(meta), path(fq), path(extra), path(genome_size)\n\n    output:\n    tuple val(meta), path(\"results/${meta.id}*.fastq.gz\"), emit: fastq, optional: true\n    tuple val(meta), path(\"results/${meta.id}*.fastq.gz\"), path(extra), path(genome_size), emit: fastq_assembly, optional: true\n    path \"results/*\"\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\", emit: versions\n    path \"*-error.txt\", optional: true\n\n    shell:\n    options.ignore = [ '-genome-size.txt', extra]\n    meta.single_end = fq[1] == null ? true : false\n    is_assembly = meta.runtype.startsWith('assembly') ? true : false\n    qin = meta.runtype.startsWith('assembly') ? 'qin=33' : 'qin=auto'\n    adapters = params.adapters ? path(params.adapters) : 'adapters'\n    phix = params.phix ? path(params.phix) : 'phix'\n    adapter_opts = meta.single_end ? \"\" : \"in2=${fq[1]} out2=adapter-r2.fq\"\n    phix_opts = meta.single_end ? \"\" : \"in2=adapter-r2.fq out2=phix-r2.fq\"\n    lighter_opts = meta.single_end ? \"\" : \"-r phix-r2.fq\"\n    reformat_opts = meta.single_end ? \"\" : \"in2=phix-r2.cor.fq out2=subsample-r2.fq\"\n\n                                                                \n    xmx = Math.round(task.memory.toBytes()*0.95)\n    '''\n    mkdir -p results\n    ERROR=0\n    GENOME_SIZE=`head -n 1 !{genome_size}`\n    MIN_COVERAGE=$(( !{params.min_coverage}*${GENOME_SIZE} ))\n    TOTAL_BP=$(( !{params.coverage}*${GENOME_SIZE} ))\n\n    if [ \"!{params.skip_qc}\" == \"true\" ]; then\n        echo \"Sequence QC was skipped for !{meta.id}\" > results/!{meta.id}-qc-skipped.txt\n        if [ \"!{meta.single_end}\" == \"false\" ]; then\n            # Paired-End Reads\n            cp !{fq[0]} results/!{meta.id}_R1.fastq.gz\n            cp !{fq[1]} results/!{meta.id}_R2.fastq.gz\n        else\n            # Single-End Reads\n            cp !{fq[0]} results/!{meta.id}.fastq.gz\n        fi\n    else\n        if [[ \"!{meta.runtype}\" == \"ont\" ]]; then\n            # Remove Adapters\n            porechop --input !{fq[0]} !{params.porechop_opts} \\\n                --format fastq \\\n                --threads !{task.cpus} > adapter-r1.fq\n\n            # Quality filter\n            nanoq --min-len !{params.ont_minlength} \\\n                  --min-qual !{params.ont_minqual} \\\n                  --input adapter-r1.fq 1> filt-r1.fq\n        else\n            # Illumina Reads\n            # Remove Adapters\n            bbduk.sh -Xmx!{xmx} \\\n                in=!{fq[0]} out=adapter-r1.fq !{adapter_opts} \\\n                ref=!{adapters} \\\n                k=!{params.adapter_k} \\\n                ktrim=!{params.ktrim} \\\n                mink=!{params.mink} \\\n                hdist=!{params.hdist} \\\n                tpe=!{params.tpe} \\\n                tbo=!{params.tbo} \\\n                threads=!{task.cpus} \\\n                ftm=!{params.ftm} \\\n                !{qin} ordered=t !{params.bbduk_opts}\n\n            # Remove PhiX\n            bbduk.sh -Xmx!{xmx} \\\n                in=adapter-r1.fq out=phix-r1.fq !{phix_opts} \\\n                ref=!{phix} \\\n                k=!{params.phix_k} \\\n                hdist=!{params.hdist} \\\n                tpe=!{params.tpe} \\\n                tbo=!{params.tbo} \\\n                qtrim=!{params.qtrim} \\\n                trimq=!{params.trimq} \\\n                minlength=!{params.minlength} \\\n                minavgquality=!{params.maq} \\\n                !{qin} qout=!{params.qout} \\\n                tossjunk=!{params.tossjunk} \\\n                threads=!{task.cpus} \\\n                ordered=t !{params.bbduk_opts}\n        fi\n\n        # Error Correction\n        if [[ \"!{meta.runtype}\" == \"ont\" ]]; then\n            echo \"Skipping error correction. Have a recommended ONT error corrector? Let me know!\"\n        else\n            if [ \"!{params.skip_error_correction}\" == \"false\" ]; then\n                lighter -od . -r phix-r1.fq !{lighter_opts} -K 31 ${GENOME_SIZE} -maxcor 1 -zlib 0 -t !{task.cpus}\n            else\n                echo \"Skipping error correction\"\n                ln -s phix-r1.fq phix-r1.cor.fq\n                if [ \"!{meta.single_end}\" == \"false\" ]; then\n                    ln -s phix-r2.fq phix-r2.cor.fq\n                fi\n            fi\n        fi\n\n        # Reduce Coverage\n        if (( ${TOTAL_BP} > 0 )); then\n            if [[ \"!{meta.runtype}\" == \"ont\" ]]; then\n                rasusa -i filt-r1.fq \\\n                       -c !{params.coverage} \\\n                       -g ${GENOME_SIZE} \\\n                       -s !{params.sampleseed} 1> subsample-r1.fq\n            else\n                reformat.sh -Xmx!{xmx} \\\n                    in=phix-r1.cor.fq out=subsample-r1.fq !{reformat_opts} \\\n                    samplebasestarget=${TOTAL_BP} \\\n                    sampleseed=!{params.sampleseed} \\\n                    overwrite=t\n            fi\n        else\n            echo \"Skipping coverage reduction\"\n            ln -s phix-r1.cor.fq subsample-r1.fq\n            if [ \"!{meta.single_end}\" == \"false\" ]; then\n                ln -s phix-r2.cor.fq subsample-r2.fq\n            fi\n        fi\n\n        # Compress\n        if [ \"!{meta.single_end}\" == \"false\" ]; then\n            pigz -p !{task.cpus} -c -n subsample-r1.fq > results/!{meta.id}_R1.fastq.gz\n            pigz -p !{task.cpus} -c -n subsample-r2.fq > results/!{meta.id}_R2.fastq.gz\n        else\n            pigz -p !{task.cpus} -c -n subsample-r1.fq > results/!{meta.id}.fastq.gz\n        fi\n\n        if [ \"!{params.keep_all_files}\" == \"false\" ]; then\n            # Remove intermediate FASTQ files\n            rm *.fq\n        fi\n    fi\n\n    # Quality stats before and after QC\n    mkdir results/summary/\n    # fastq-scan\n    if [ \"!{meta.single_end}\" == \"false\" ]; then\n        # Paired-End Reads\n        gzip -cd !{fq[0]} | fastq-scan -g ${GENOME_SIZE} > results/summary/!{meta.id}_R1-original.json\n        gzip -cd !{fq[1]} | fastq-scan -g ${GENOME_SIZE} > results/summary/!{meta.id}_R2-original.json\n        gzip -cd results/!{meta.id}_R1.fastq.gz | fastq-scan -g ${GENOME_SIZE} > results/summary/!{meta.id}_R1-final.json\n        gzip -cd results/!{meta.id}_R2.fastq.gz | fastq-scan -g ${GENOME_SIZE} > results/summary/!{meta.id}_R2-final.json\n    else\n        # Single-End Reads\n        gzip -cd !{fq[0]} | fastq-scan -g ${GENOME_SIZE} > results/summary/!{meta.id}-original.json\n        gzip -cd results/!{meta.id}.fastq.gz | fastq-scan -g ${GENOME_SIZE} > results/summary/!{meta.id}-final.json\n    fi\n\n    # FastQC and NanoPlot\n    if [[ \"!{params.skip_qc_plots}\" == \"false\" ]]; then\n        if [[ \"!{meta.runtype}\" == \"ont\" ]]; then\n            mkdir results/summary/!{meta.id}-original results/summary/!{meta.id}-final\n            NanoPlot !{params.nanoplot_opts} \\\n                --threads !{task.cpus} \\\n                --fastq !{fq[0]} \\\n                --outdir results/summary/!{meta.id}-original/ \\\n                --prefix !{meta.id}-original_\n            cp results/summary/!{meta.id}-original/!{meta.id}-original_NanoPlot-report.html results/summary/!{meta.id}-original_NanoPlot-report.html\n            tar -cvf - results/summary/!{meta.id}-original/ | pigz --best -p !{task.cpus} > results/summary/!{meta.id}-original_NanoPlot.tar.gz\n\n            NanoPlot !{params.nanoplot_opts} \\\n                --threads !{task.cpus} \\\n                --fastq results/!{meta.id}.fastq.gz \\\n                --outdir results/summary/!{meta.id}-final/ \\\n                --prefix !{meta.id}-final_\n            cp results/summary/!{meta.id}-final/!{meta.id}-final_NanoPlot-report.html results/summary/!{meta.id}-final_NanoPlot-report.html\n            tar -cvf - results/summary/!{meta.id}-final/ | pigz --best -p !{task.cpus} > results/summary/!{meta.id}-final_NanoPlot.tar.gz\n            rm -rf results/summary/!{meta.id}-original/ results/summary/!{meta.id}-final/\n        else\n            if [ \"!{meta.single_end}\" == \"false\" ]; then\n                # Paired-End Reads\n                ln -s !{fq[0]} !{meta.id}_R1-original.fastq.gz\n                ln -s !{fq[1]} !{meta.id}_R2-original.fastq.gz\n                ln -s results/!{meta.id}_R1.fastq.gz !{meta.id}_R1-final.fastq.gz\n                ln -s results/!{meta.id}_R2.fastq.gz !{meta.id}_R2-final.fastq.gz\n                fastqc --noextract -f fastq -t !{task.cpus} !{meta.id}_R1-original.fastq.gz !{meta.id}_R2-original.fastq.gz !{meta.id}_R1-final.fastq.gz !{meta.id}_R2-final.fastq.gz\n            else\n                # Single-End Reads\n                ln -s !{fq[0]} !{meta.id}-original.fastq.gz\n                ln -s results/!{meta.id}.fastq.gz !{meta.id}-final.fastq.gz\n                fastqc --noextract -f fastq -t !{task.cpus} !{meta.id}-original.fastq.gz !{meta.id}-final.fastq.gz\n            fi\n            mv *_fastqc.html *_fastqc.zip results/summary/\n        fi\n    fi\n\n    # Final QC check\n    gzip -cd results/*.fastq.gz | fastq-scan -g ${GENOME_SIZE} > temp.json\n    FINAL_BP=$(grep \"total_bp\" temp.json | sed -r 's/.*:[ ]*([0-9]+),/\\\\1/')\n    FINAL_READS=$(grep \"read_total\" temp.json | sed -r 's/.*:[ ]*([0-9]+),/\\\\1/')\n    rm temp.json\n\n    if [ ${FINAL_BP} -lt ${MIN_COVERAGE} ]; then\n        ERROR=1\n        echo \"After QC, !{meta.id} FASTQ(s) contain ${FINAL_BP} total basepairs. This does\n                not exceed the required minimum ${MIN_COVERAGE} bp (!{params.min_coverage}x coverage). Further analysis \n                is discontinued.\" | \\\n        sed 's/^\\\\s*//' > !{meta.id}-low-sequence-depth-error.txt\n    elif [ ${FINAL_BP} -lt \"!{params.min_basepairs}\" ]; then\n        ERROR=1\n        echo \"After QC, !{meta.id} FASTQ(s) contain ${FINAL_BP} total basepairs. This does\n                not exceed the required minimum !{params.min_basepairs} bp. Further analysis\n                is discontinued.\" | \\\n        sed 's/^\\\\s*//' > !{meta.id}-low-sequence-depth-error.txt\n    fi\n\n    if [ ${FINAL_READS} -lt \"!{params.min_reads}\" ]; then\n        # Prevent ONT samples from being caught by this\n        if [ ${FINAL_BP} -lt ${MIN_COVERAGE} ]; then\n            ERROR=1\n            echo \"After QC, !{meta.id} FASTQ(s) contain ${FINAL_READS} total reads. This does\n                    not exceed the required minimum !{params.min_reads} reads count. Further analysis\n                    is discontinued.\" | \\\n            sed 's/^\\\\s*//' > !{meta.id}-low-read-count-error.txt\n        fi\n    fi\n\n    if [ \"!{is_assembly}\" == \"true\" ]; then\n        touch results/reads-simulated-from-assembly.txt\n    fi\n\n    if [ \"${ERROR}\" -eq \"1\" ]; then\n        if [ \"!{meta.single_end}\" == \"false\" ]; then\n            mv results/!{meta.id}_R1.fastq.gz results/!{meta.id}_R1.error-fastq.gz\n            mv results/!{meta.id}_R2.fastq.gz results/!{meta.id}_R2.error-fastq.gz\n        else\n            mv results/!{meta.id}.fastq.gz results/!{meta.id}.error-fastq.gz\n        fi\n    fi\n\n    # Capture versions\n    if [[ \"!{params.skip_qc_plots}\" == \"false\" ]]; then\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        bbduk: $(echo $(bbduk.sh --version 2>&1) | sed 's/^.*BBMap version //;s/ .*$//')\n        fastqc: $(echo $(fastqc --version 2>&1) | sed 's/^.*FastQC v//')\n        fastq-scan: $(echo $(fastq-scan -v 2>&1) | sed 's/fastq-scan //')\n        lighter: $(echo $(lighter -v 2>&1) | sed 's/Lighter v//')\n        nanoplot: $(echo $(NanoPlot -v 2>&1) | sed 's/NanoPlot //')\n        nanoq: $(echo $(nanoq --version 2>&1) | sed 's/nanoq //')\n        pigz: $(echo $(pigz --version 2>&1) | sed 's/pigz //')\n        porechop: $(echo $(porechop --version 2>&1))\n        rasusa: $(echo $(rasusa --version 2>&1) | sed 's/rasusa //')\n    END_VERSIONS\n    else\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        bbduk: $(echo $(bbduk.sh --version 2>&1) | sed 's/^.*BBMap version //;s/ .*$//')\n        fastq-scan: $(echo $(fastq-scan -v 2>&1) | sed 's/fastq-scan //')\n        lighter: $(echo $(lighter -v 2>&1) | sed 's/Lighter v//')\n        nanoq: $(echo $(nanoq --version 2>&1) | sed 's/nanoq //')\n        pigz: $(echo $(pigz --version 2>&1) | sed 's/pigz //')\n        porechop: $(echo $(porechop --version 2>&1))\n        rasusa: $(echo $(rasusa --version 2>&1) | sed 's/rasusa //')\n    END_VERSIONS\n    fi\n    '''\n}",
        "nb_lignes_process": 271,
        "string_script": "    options.ignore = [ '-genome-size.txt', extra]\n    meta.single_end = fq[1] == null ? true : false\n    is_assembly = meta.runtype.startsWith('assembly') ? true : false\n    qin = meta.runtype.startsWith('assembly') ? 'qin=33' : 'qin=auto'\n    adapters = params.adapters ? path(params.adapters) : 'adapters'\n    phix = params.phix ? path(params.phix) : 'phix'\n    adapter_opts = meta.single_end ? \"\" : \"in2=${fq[1]} out2=adapter-r2.fq\"\n    phix_opts = meta.single_end ? \"\" : \"in2=adapter-r2.fq out2=phix-r2.fq\"\n    lighter_opts = meta.single_end ? \"\" : \"-r phix-r2.fq\"\n    reformat_opts = meta.single_end ? \"\" : \"in2=phix-r2.cor.fq out2=subsample-r2.fq\"\n\n                                                                \n    xmx = Math.round(task.memory.toBytes()*0.95)\n    '''\n    mkdir -p results\n    ERROR=0\n    GENOME_SIZE=`head -n 1 !{genome_size}`\n    MIN_COVERAGE=$(( !{params.min_coverage}*${GENOME_SIZE} ))\n    TOTAL_BP=$(( !{params.coverage}*${GENOME_SIZE} ))\n\n    if [ \"!{params.skip_qc}\" == \"true\" ]; then\n        echo \"Sequence QC was skipped for !{meta.id}\" > results/!{meta.id}-qc-skipped.txt\n        if [ \"!{meta.single_end}\" == \"false\" ]; then\n            # Paired-End Reads\n            cp !{fq[0]} results/!{meta.id}_R1.fastq.gz\n            cp !{fq[1]} results/!{meta.id}_R2.fastq.gz\n        else\n            # Single-End Reads\n            cp !{fq[0]} results/!{meta.id}.fastq.gz\n        fi\n    else\n        if [[ \"!{meta.runtype}\" == \"ont\" ]]; then\n            # Remove Adapters\n            porechop --input !{fq[0]} !{params.porechop_opts} \\\n                --format fastq \\\n                --threads !{task.cpus} > adapter-r1.fq\n\n            # Quality filter\n            nanoq --min-len !{params.ont_minlength} \\\n                  --min-qual !{params.ont_minqual} \\\n                  --input adapter-r1.fq 1> filt-r1.fq\n        else\n            # Illumina Reads\n            # Remove Adapters\n            bbduk.sh -Xmx!{xmx} \\\n                in=!{fq[0]} out=adapter-r1.fq !{adapter_opts} \\\n                ref=!{adapters} \\\n                k=!{params.adapter_k} \\\n                ktrim=!{params.ktrim} \\\n                mink=!{params.mink} \\\n                hdist=!{params.hdist} \\\n                tpe=!{params.tpe} \\\n                tbo=!{params.tbo} \\\n                threads=!{task.cpus} \\\n                ftm=!{params.ftm} \\\n                !{qin} ordered=t !{params.bbduk_opts}\n\n            # Remove PhiX\n            bbduk.sh -Xmx!{xmx} \\\n                in=adapter-r1.fq out=phix-r1.fq !{phix_opts} \\\n                ref=!{phix} \\\n                k=!{params.phix_k} \\\n                hdist=!{params.hdist} \\\n                tpe=!{params.tpe} \\\n                tbo=!{params.tbo} \\\n                qtrim=!{params.qtrim} \\\n                trimq=!{params.trimq} \\\n                minlength=!{params.minlength} \\\n                minavgquality=!{params.maq} \\\n                !{qin} qout=!{params.qout} \\\n                tossjunk=!{params.tossjunk} \\\n                threads=!{task.cpus} \\\n                ordered=t !{params.bbduk_opts}\n        fi\n\n        # Error Correction\n        if [[ \"!{meta.runtype}\" == \"ont\" ]]; then\n            echo \"Skipping error correction. Have a recommended ONT error corrector? Let me know!\"\n        else\n            if [ \"!{params.skip_error_correction}\" == \"false\" ]; then\n                lighter -od . -r phix-r1.fq !{lighter_opts} -K 31 ${GENOME_SIZE} -maxcor 1 -zlib 0 -t !{task.cpus}\n            else\n                echo \"Skipping error correction\"\n                ln -s phix-r1.fq phix-r1.cor.fq\n                if [ \"!{meta.single_end}\" == \"false\" ]; then\n                    ln -s phix-r2.fq phix-r2.cor.fq\n                fi\n            fi\n        fi\n\n        # Reduce Coverage\n        if (( ${TOTAL_BP} > 0 )); then\n            if [[ \"!{meta.runtype}\" == \"ont\" ]]; then\n                rasusa -i filt-r1.fq \\\n                       -c !{params.coverage} \\\n                       -g ${GENOME_SIZE} \\\n                       -s !{params.sampleseed} 1> subsample-r1.fq\n            else\n                reformat.sh -Xmx!{xmx} \\\n                    in=phix-r1.cor.fq out=subsample-r1.fq !{reformat_opts} \\\n                    samplebasestarget=${TOTAL_BP} \\\n                    sampleseed=!{params.sampleseed} \\\n                    overwrite=t\n            fi\n        else\n            echo \"Skipping coverage reduction\"\n            ln -s phix-r1.cor.fq subsample-r1.fq\n            if [ \"!{meta.single_end}\" == \"false\" ]; then\n                ln -s phix-r2.cor.fq subsample-r2.fq\n            fi\n        fi\n\n        # Compress\n        if [ \"!{meta.single_end}\" == \"false\" ]; then\n            pigz -p !{task.cpus} -c -n subsample-r1.fq > results/!{meta.id}_R1.fastq.gz\n            pigz -p !{task.cpus} -c -n subsample-r2.fq > results/!{meta.id}_R2.fastq.gz\n        else\n            pigz -p !{task.cpus} -c -n subsample-r1.fq > results/!{meta.id}.fastq.gz\n        fi\n\n        if [ \"!{params.keep_all_files}\" == \"false\" ]; then\n            # Remove intermediate FASTQ files\n            rm *.fq\n        fi\n    fi\n\n    # Quality stats before and after QC\n    mkdir results/summary/\n    # fastq-scan\n    if [ \"!{meta.single_end}\" == \"false\" ]; then\n        # Paired-End Reads\n        gzip -cd !{fq[0]} | fastq-scan -g ${GENOME_SIZE} > results/summary/!{meta.id}_R1-original.json\n        gzip -cd !{fq[1]} | fastq-scan -g ${GENOME_SIZE} > results/summary/!{meta.id}_R2-original.json\n        gzip -cd results/!{meta.id}_R1.fastq.gz | fastq-scan -g ${GENOME_SIZE} > results/summary/!{meta.id}_R1-final.json\n        gzip -cd results/!{meta.id}_R2.fastq.gz | fastq-scan -g ${GENOME_SIZE} > results/summary/!{meta.id}_R2-final.json\n    else\n        # Single-End Reads\n        gzip -cd !{fq[0]} | fastq-scan -g ${GENOME_SIZE} > results/summary/!{meta.id}-original.json\n        gzip -cd results/!{meta.id}.fastq.gz | fastq-scan -g ${GENOME_SIZE} > results/summary/!{meta.id}-final.json\n    fi\n\n    # FastQC and NanoPlot\n    if [[ \"!{params.skip_qc_plots}\" == \"false\" ]]; then\n        if [[ \"!{meta.runtype}\" == \"ont\" ]]; then\n            mkdir results/summary/!{meta.id}-original results/summary/!{meta.id}-final\n            NanoPlot !{params.nanoplot_opts} \\\n                --threads !{task.cpus} \\\n                --fastq !{fq[0]} \\\n                --outdir results/summary/!{meta.id}-original/ \\\n                --prefix !{meta.id}-original_\n            cp results/summary/!{meta.id}-original/!{meta.id}-original_NanoPlot-report.html results/summary/!{meta.id}-original_NanoPlot-report.html\n            tar -cvf - results/summary/!{meta.id}-original/ | pigz --best -p !{task.cpus} > results/summary/!{meta.id}-original_NanoPlot.tar.gz\n\n            NanoPlot !{params.nanoplot_opts} \\\n                --threads !{task.cpus} \\\n                --fastq results/!{meta.id}.fastq.gz \\\n                --outdir results/summary/!{meta.id}-final/ \\\n                --prefix !{meta.id}-final_\n            cp results/summary/!{meta.id}-final/!{meta.id}-final_NanoPlot-report.html results/summary/!{meta.id}-final_NanoPlot-report.html\n            tar -cvf - results/summary/!{meta.id}-final/ | pigz --best -p !{task.cpus} > results/summary/!{meta.id}-final_NanoPlot.tar.gz\n            rm -rf results/summary/!{meta.id}-original/ results/summary/!{meta.id}-final/\n        else\n            if [ \"!{meta.single_end}\" == \"false\" ]; then\n                # Paired-End Reads\n                ln -s !{fq[0]} !{meta.id}_R1-original.fastq.gz\n                ln -s !{fq[1]} !{meta.id}_R2-original.fastq.gz\n                ln -s results/!{meta.id}_R1.fastq.gz !{meta.id}_R1-final.fastq.gz\n                ln -s results/!{meta.id}_R2.fastq.gz !{meta.id}_R2-final.fastq.gz\n                fastqc --noextract -f fastq -t !{task.cpus} !{meta.id}_R1-original.fastq.gz !{meta.id}_R2-original.fastq.gz !{meta.id}_R1-final.fastq.gz !{meta.id}_R2-final.fastq.gz\n            else\n                # Single-End Reads\n                ln -s !{fq[0]} !{meta.id}-original.fastq.gz\n                ln -s results/!{meta.id}.fastq.gz !{meta.id}-final.fastq.gz\n                fastqc --noextract -f fastq -t !{task.cpus} !{meta.id}-original.fastq.gz !{meta.id}-final.fastq.gz\n            fi\n            mv *_fastqc.html *_fastqc.zip results/summary/\n        fi\n    fi\n\n    # Final QC check\n    gzip -cd results/*.fastq.gz | fastq-scan -g ${GENOME_SIZE} > temp.json\n    FINAL_BP=$(grep \"total_bp\" temp.json | sed -r 's/.*:[ ]*([0-9]+),/\\\\1/')\n    FINAL_READS=$(grep \"read_total\" temp.json | sed -r 's/.*:[ ]*([0-9]+),/\\\\1/')\n    rm temp.json\n\n    if [ ${FINAL_BP} -lt ${MIN_COVERAGE} ]; then\n        ERROR=1\n        echo \"After QC, !{meta.id} FASTQ(s) contain ${FINAL_BP} total basepairs. This does\n                not exceed the required minimum ${MIN_COVERAGE} bp (!{params.min_coverage}x coverage). Further analysis \n                is discontinued.\" | \\\n        sed 's/^\\\\s*//' > !{meta.id}-low-sequence-depth-error.txt\n    elif [ ${FINAL_BP} -lt \"!{params.min_basepairs}\" ]; then\n        ERROR=1\n        echo \"After QC, !{meta.id} FASTQ(s) contain ${FINAL_BP} total basepairs. This does\n                not exceed the required minimum !{params.min_basepairs} bp. Further analysis\n                is discontinued.\" | \\\n        sed 's/^\\\\s*//' > !{meta.id}-low-sequence-depth-error.txt\n    fi\n\n    if [ ${FINAL_READS} -lt \"!{params.min_reads}\" ]; then\n        # Prevent ONT samples from being caught by this\n        if [ ${FINAL_BP} -lt ${MIN_COVERAGE} ]; then\n            ERROR=1\n            echo \"After QC, !{meta.id} FASTQ(s) contain ${FINAL_READS} total reads. This does\n                    not exceed the required minimum !{params.min_reads} reads count. Further analysis\n                    is discontinued.\" | \\\n            sed 's/^\\\\s*//' > !{meta.id}-low-read-count-error.txt\n        fi\n    fi\n\n    if [ \"!{is_assembly}\" == \"true\" ]; then\n        touch results/reads-simulated-from-assembly.txt\n    fi\n\n    if [ \"${ERROR}\" -eq \"1\" ]; then\n        if [ \"!{meta.single_end}\" == \"false\" ]; then\n            mv results/!{meta.id}_R1.fastq.gz results/!{meta.id}_R1.error-fastq.gz\n            mv results/!{meta.id}_R2.fastq.gz results/!{meta.id}_R2.error-fastq.gz\n        else\n            mv results/!{meta.id}.fastq.gz results/!{meta.id}.error-fastq.gz\n        fi\n    fi\n\n    # Capture versions\n    if [[ \"!{params.skip_qc_plots}\" == \"false\" ]]; then\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        bbduk: $(echo $(bbduk.sh --version 2>&1) | sed 's/^.*BBMap version //;s/ .*$//')\n        fastqc: $(echo $(fastqc --version 2>&1) | sed 's/^.*FastQC v//')\n        fastq-scan: $(echo $(fastq-scan -v 2>&1) | sed 's/fastq-scan //')\n        lighter: $(echo $(lighter -v 2>&1) | sed 's/Lighter v//')\n        nanoplot: $(echo $(NanoPlot -v 2>&1) | sed 's/NanoPlot //')\n        nanoq: $(echo $(nanoq --version 2>&1) | sed 's/nanoq //')\n        pigz: $(echo $(pigz --version 2>&1) | sed 's/pigz //')\n        porechop: $(echo $(porechop --version 2>&1))\n        rasusa: $(echo $(rasusa --version 2>&1) | sed 's/rasusa //')\n    END_VERSIONS\n    else\n    cat <<-END_VERSIONS > versions.yml\n    \"!{task.process}\":\n        bbduk: $(echo $(bbduk.sh --version 2>&1) | sed 's/^.*BBMap version //;s/ .*$//')\n        fastq-scan: $(echo $(fastq-scan -v 2>&1) | sed 's/fastq-scan //')\n        lighter: $(echo $(lighter -v 2>&1) | sed 's/Lighter v//')\n        nanoq: $(echo $(nanoq --version 2>&1) | sed 's/nanoq //')\n        pigz: $(echo $(pigz --version 2>&1) | sed 's/pigz //')\n        porechop: $(echo $(porechop --version 2>&1))\n        rasusa: $(echo $(rasusa --version 2>&1) | sed 's/rasusa //')\n    END_VERSIONS\n    fi\n    '''",
        "nb_lignes_script": 249,
        "language_script": "bash",
        "tools": [
            "SEQing",
            "Lighter",
            "FastQC",
            "Annot",
            "GIS"
        ],
        "tools_url": [
            "https://bio.tools/SEQing",
            "https://bio.tools/lighter",
            "https://bio.tools/fastqc",
            "https://bio.tools/Annot",
            "https://bio.tools/gis"
        ],
        "tools_dico": [
            {
                "name": "SEQing",
                "uri": "https://bio.tools/SEQing",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "Gene transcripts"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plant biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3794",
                            "term": "RNA immunoprecipitation"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "mRNA features"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plant science"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plants"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Botany"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plant"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3794",
                            "term": "RIP"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2499",
                                    "term": "Splicing analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2499",
                                    "term": "Splicing model analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "web-based visualization of Arabidopsis thaliana iCLIP and RNA-seq data in an interactive python framework.\n\nSEQing: interactive web-based tool for visualization of iCLIP and RNA-seq data.\n\nThe file requirements.txt can be used to install all needed needed dependencies for the project. Python 3.5 or higher is required and we recommend to setup a virtual environment for this project. If your current python points to a python2 version, please put python3 instead of just python before running SEQing",
                "homepage": "https://github.com/malewins/SEQing"
            },
            {
                "name": "Lighter",
                "uri": "https://bio.tools/lighter",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Whole genome sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Genome sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "WGS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3195",
                                    "term": "Sequencing error detection"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3195",
                                    "term": "Short-read error correction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3195",
                                    "term": "Short read error correction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Kmer-based error correction method for whole genome sequencing data. Lighter uses sampling (rather than counting) to obtain a set of kmers that are likely from the genome. Using this information, Lighter can correct the reads containing sequence errors.",
                "homepage": "https://github.com/mourisl/Lighter"
            },
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            },
            {
                "name": "Annot",
                "uri": "https://bio.tools/Annot",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarray experiment"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0089",
                            "term": "Ontology and terminology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0219",
                            "term": "Data submission, annotation and curation"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarrays"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0558",
                                    "term": "Phylogenetic tree annotation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0361",
                                    "term": "Sequence annotation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3553",
                                    "term": "Image annotation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data extraction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Retrieval"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "a Django-based sample, reagent, and experiment metadata tracking system.\n\nAnnot and information about what Annot is can be found here: https://gitlab.com/biotransistor/annot . This here is just tutorial material for the tutorial described here: http://annot.readthedocs.io/en/latest/man_tutorial.html .\n\nWelcome to Annotamentum\u2019s Documentation. \u2014 Annot 5 documentation.\n\nWelcome to Annotamentum\u2019s Documentation.\u00b6.\n\nDjango admin based sample, reagent and experiment metadata tracking.\n\nSummary: Annot is a web application, developed for biological wetlab experiment layout, sample and reagent logging, so that data is ready for sharing and analysis. On its core annot makes use of the acpipe_anjson library and acjson - assay coordinate json - file format. The use of controlled vocabulary from ontologies for sample and reagent annotation is enforced. Annot\u2019s modular implementation can be adapted to a variety of experimental paradigms",
                "homepage": "https://gitlab.com/biotransistor/annot"
            },
            {
                "name": "GIS",
                "uri": "https://bio.tools/gis",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0623",
                            "term": "Gene and protein families"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2814",
                            "term": "Protein structure analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0736",
                            "term": "Protein folds and structural domains"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0082",
                            "term": "Structure prediction"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0623",
                            "term": "Genes, gene family or system"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2814",
                            "term": "Protein structure"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2487",
                                    "term": "Protein structure comparison"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0503",
                                    "term": "Pairwise structure alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2483",
                                    "term": "Structure comparison"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2406",
                                    "term": "Protein structure analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2487",
                                    "term": "Structure comparison (protein)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0503",
                                    "term": "Structure alignment (pairwise)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2406",
                                    "term": "Structure analysis (protein)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Allows users to analyze protein structure families according to the SCOP classification scheme. Users can upload their own protein structures for pair-wise protein structure comparison, structure alignment or symmetry analysis.",
                "homepage": "http://agknapp.chemie.fu-berlin.de/gplus"
            }
        ],
        "inputs": [
            "meta",
            "fq",
            "extra",
            "genome_size"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"${meta.id}\"",
            "label \"base_mem_4gb\"",
            "label \"qc_reads\"",
            "publishDir \"${params.outdir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force, saveAs: { filename -> saveFiles(filename:filename, opts: options) }"
        ],
        "when": "",
        "stub": ""
    },
    "MAKEDB": {
        "name_process": "MAKEDB",
        "string_process": "\nprocess MAKEDB {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/phyloflash:3.4--hdfd78af_1' :\n        'quay.io/biocontainers/phyloflash:3.4--hdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"${meta.id}-summary.tab\"), emit: summary\n    path \"results/\", emit: results_dir\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\",emit: versions\n\n    script:\n    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    #TODO\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        phyloFlash: \\$(echo \\$(phyloFlash.pl -version 2>&1) | sed \"s/^.*phyloFlash v//\")\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 37,
        "string_script": "    def prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def is_compressed = fasta.getName().endsWith(\".gz\") ? true : false\n    def fasta_name = fasta.getName().replace(\".gz\", \"\")\n    \"\"\"\n    if [ \"$is_compressed\" == \"true\" ]; then\n        gzip -c -d $fasta > $fasta_name\n    fi\n\n    #TODO\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        phyloFlash: \\$(echo \\$(phyloFlash.pl -version 2>&1) | sed \"s/^.*phyloFlash v//\")\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/phyloflash:3.4--hdfd78af_1' : 'quay.io/biocontainers/phyloflash:3.4--hdfd78af_1' }\""
        ],
        "when": "",
        "stub": ""
    },
    "ISMAPPER": {
        "name_process": "ISMAPPER",
        "string_process": "\nprocess ISMAPPER {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force,\n        saveAs: { filename -> saveFiles(filename:filename, opts:options, logs_subdir: query_base) }\n\n    conda (params.enable_conda ? conda_env : null)\n    container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ismapper:2.0.2--pyhdfd78af_1' :\n        'quay.io/biocontainers/ismapper:2.0.2--pyhdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path(reference)\n    path(query)\n\n    output:\n    tuple val(meta), path(\"${query_base}/*\"), emit: results\n    path \"*.{log,err}\", emit: logs, optional: true\n    path \".command.*\", emit: nf_logs\n    path \"versions.yml\",emit: versions\n\n    script:\n    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def ref_compressed = reference.getName().endsWith(\".gz\") ? true : false\n    def reference_name = reference.getName().replace(\".gz\", \"\")\n    def query_compressed = query.getName().endsWith(\".gz\") ? true : false\n    def query_name = query.getName().replace(\".gz\", \"\")\n    query_base = query.getSimpleName()\n    \"\"\"\n    if [ \"$ref_compressed\" == \"true\" ]; then\n        gzip -c -d $reference > $reference_name\n    fi\n    if [ \"$query_compressed\" == \"true\" ]; then\n        gzip -c -d $query > $query_name\n    fi\n    \n    ismap \\\\\n        $options.args \\\\\n        --t $task.cpus \\\\\n        --output_dir results/ \\\\\n        --queries $query_name \\\\\n        --log ${prefix} \\\\\n        --reference $reference_name \\\\\n        --reads $reads\n\n    mkdir ${query_base}\n    mv results/${meta.id}/* ${query_base}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ismapper: \\$( echo \\$( ismap --version 2>&1 ) | sed 's/^.*ismap //' )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 54,
        "string_script": "    prefix = options.suffix ? \"${options.suffix}\" : \"${meta.id}\"\n    def ref_compressed = reference.getName().endsWith(\".gz\") ? true : false\n    def reference_name = reference.getName().replace(\".gz\", \"\")\n    def query_compressed = query.getName().endsWith(\".gz\") ? true : false\n    def query_name = query.getName().replace(\".gz\", \"\")\n    query_base = query.getSimpleName()\n    \"\"\"\n    if [ \"$ref_compressed\" == \"true\" ]; then\n        gzip -c -d $reference > $reference_name\n    fi\n    if [ \"$query_compressed\" == \"true\" ]; then\n        gzip -c -d $query > $query_name\n    fi\n    \n    ismap \\\\\n        $options.args \\\\\n        --t $task.cpus \\\\\n        --output_dir results/ \\\\\n        --queries $query_name \\\\\n        --log ${prefix} \\\\\n        --reference $reference_name \\\\\n        --reads $reads\n\n    mkdir ${query_base}\n    mv results/${meta.id}/* ${query_base}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ismapper: \\$( echo \\$( ismap --version 2>&1 ) | sed 's/^.*ismap //' )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 30,
        "language_script": "bash",
        "tools": [
            "VISMapper"
        ],
        "tools_url": [
            "https://bio.tools/vismapper"
        ],
        "tools_dico": [
            {
                "name": "VISMapper",
                "uri": "https://bio.tools/vismapper",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3303",
                            "term": "Medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3303",
                            "term": "Experimental medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3303",
                            "term": "Clinical medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3303",
                            "term": "Biomedical research"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Ultra-fast exhaustive cartography of viral insertion sites for gene therapy.",
                "homepage": "http://vismapper.babelomics.org/"
            }
        ],
        "inputs": [
            "meta",
            "reads",
            "reference",
            "query"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "bactopia__bactopia",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${publish_dir}/${meta.id}\", mode: params.publish_dir_mode, overwrite: params.force , saveAs: { filename -> saveFiles(filename:filename, opts:options, logs_subdir: query_base) }",
            "conda (params.enable_conda ? conda_env : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/ismapper:2.0.2--pyhdfd78af_1' : 'quay.io/biocontainers/ismapper:2.0.2--pyhdfd78af_1' }\""
        ],
        "when": "",
        "stub": ""
    }
}