{
    "Gappa_KRD_1t1": {
        "name_process": "Gappa_KRD_1t1",
        "string_process": "\nprocess Gappa_KRD_1t1 {\n    container = \"${container__gappa}\"\n    label = 'io_limited'\n\n    input:\n        path new_jplace\n        path old_jplaces\n    output:\n        path \"v${new_jplace.name.replace('.jplace.gz', '')}.krd_long.csv\"\n\n    \n    \"\"\"\n    interval_krd.py \\\n    --new-jplace ${new_jplace} \\\n    --old-jplaces \"${old_jplaces}\"\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "\"\"\"\n    interval_krd.py \\\n    --new-jplace ${new_jplace} \\\n    --old-jplaces \"${old_jplaces}\"\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "new_jplace",
            "old_jplaces"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__gappa}\"",
            "label = 'io_limited'"
        ],
        "when": "",
        "stub": ""
    },
    "CombineKRDLong": {
        "name_process": "CombineKRDLong",
        "string_process": "\nprocess CombineKRDLong {\n    container = \"${container__gappa}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/\", mode: 'copy'\n\n    input:\n        path krd_longs\n    output:\n        path \"interval_krd_long.csv.gz\"\n    \n\"\"\"\n#!/usr/bin/env python3\nimport csv\nimport gzip\n\nkrd_long_fn = \"${krd_longs}\".split()\n\nwith gzip.open(\"interval_krd_long.csv.gz\", 'wt') as out_h:\n    w = csv.DictWriter(out_h, fieldnames=['specimen_1', 'specimen_2', 'krd'])\n    w.writeheader()\n    for fn in krd_long_fn:\n        with open(fn, 'rt') as in_h:\n            r = csv.DictReader(in_h)\n            w.writerows([\n                row for row in r\n            ])\n\"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport csv\nimport gzip\n\nkrd_long_fn = \"${krd_longs}\".split()\n\nwith gzip.open(\"interval_krd_long.csv.gz\", 'wt') as out_h:\n    w = csv.DictWriter(out_h, fieldnames=['specimen_1', 'specimen_2', 'krd'])\n    w.writeheader()\n    for fn in krd_long_fn:\n        with open(fn, 'rt') as in_h:\n            r = csv.DictReader(in_h)\n            w.writerows([\n                row for row in r\n            ])\n\"\"\"",
        "nb_lignes_script": 16,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "krd_longs"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__gappa}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "AlignSV": {
        "name_process": "AlignSV",
        "string_process": "\nprocess AlignSV {\n    container = \"${container__infernal}\"\n    label = 'mem_veryhigh'\n\n    input:\n        path sv_fasta_f\n        path cm\n    \n    output:\n        path \"sv.aln.sto\"\n        path \"sv.aln.scores\"\n        \n    \n    \"\"\"\n    cmalign \\\n    --cpu ${task.cpus} --noprob --dnaout --mxsize ${params.cmalign_mxsize} \\\n    --sfile sv.aln.scores -o sv.aln.sto \\\n    ${cm} ${sv_fasta_f}\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "\"\"\"\n    cmalign \\\n    --cpu ${task.cpus} --noprob --dnaout --mxsize ${params.cmalign_mxsize} \\\n    --sfile sv.aln.scores -o sv.aln.sto \\\n    ${cm} ${sv_fasta_f}\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sv_fasta_f",
            "cm"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__infernal}\"",
            "label = 'mem_veryhigh'"
        ],
        "when": "",
        "stub": ""
    },
    "CombineAln_SV_refpkg": {
        "name_process": "CombineAln_SV_refpkg",
        "string_process": "\nprocess CombineAln_SV_refpkg {\n    container = \"${container__easel}\"\n    label = 'mem_veryhigh'\n\n    input:\n        file sv_aln_sto_f \n        file refpkg_aln_sto_f\n        \n    \n    output:\n        file \"sv_refpkg.aln.sto\"\n    \n    \"\"\"\n    esl-alimerge --dna \\\n     -o sv_refpkg.aln.sto \\\n     ${sv_aln_sto_f} ${refpkg_aln_sto_f}\n    \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "\"\"\"\n    esl-alimerge --dna \\\n     -o sv_refpkg.aln.sto \\\n     ${sv_aln_sto_f} ${refpkg_aln_sto_f}\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sv_aln_sto_f",
            "refpkg_aln_sto_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__easel}\"",
            "label = 'mem_veryhigh'"
        ],
        "when": "",
        "stub": ""
    },
    "PplacerPlacement": {
        "name_process": "PplacerPlacement",
        "string_process": "\nprocess PplacerPlacement {\n    container = \"${container__pplacer}\"\n    label = 'mem_veryhigh'\n\n    publishDir \"${params.output}/placement\", mode: 'copy'\n\n    input:\n        file sv_refpkg_aln_sto_f\n        file refpkg_tgz_f\n    output:\n        file 'dedup.jplace'\n    \n    afterScript \"rm -rf refpkg/\"\n    \"\"\"\n    mkdir -p refpkg/ &&\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C ./refpkg &&\n    pplacer -p -j ${task.cpus} \\\n    --inform-prior --prior-lower ${params.pplacer_prior_lower} --map-identity \\\n    -c refpkg/ ${sv_refpkg_aln_sto_f} \\\n    -o dedup.jplace\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\n    mkdir -p refpkg/ &&\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C ./refpkg &&\n    pplacer -p -j ${task.cpus} \\\n    --inform-prior --prior-lower ${params.pplacer_prior_lower} --map-identity \\\n    -c refpkg/ ${sv_refpkg_aln_sto_f} \\\n    -o dedup.jplace\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sv_refpkg_aln_sto_f",
            "refpkg_tgz_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'mem_veryhigh'",
            "publishDir \"${params.output}/placement\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "PplacerReduplicate": {
        "name_process": "PplacerReduplicate",
        "string_process": "\nprocess PplacerReduplicate {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n\n    publishDir \"${params.output}/placement\", mode: 'copy'\n\n    input:\n        file dedup_jplace_f\n        file sv_weights_f\n    output:\n        file 'redup.jplace.gz'\n    \n    \"\"\"\n    guppy redup -m \\\n    -o /dev/stdout \\\n    -d ${sv_weights_f} \\\n    ${dedup_jplace_f} \\\n    | gzip > redup.jplace.gz\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "\"\"\"\n    guppy redup -m \\\n    -o /dev/stdout \\\n    -d ${sv_weights_f} \\\n    ${dedup_jplace_f} \\\n    | gzip > redup.jplace.gz\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "GUPPY"
        ],
        "tools_url": [
            "https://bio.tools/guppy"
        ],
        "tools_dico": [
            {
                "name": "GUPPY",
                "uri": "https://bio.tools/guppy",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Program to visualize sequence annotation data of the genetic sequence data with graphical layout. This highly interactive program allows scrolling and zooming from the genomic landscape to discrete nucleic acid sequences. Its main function is quick rendering of data on a standalone personal computer to save the layout as a parsonal file. With optional link to the internet resources, and printing support, this program tries to make more greater use of computational media in research actitiviy.",
                "homepage": "http://staff.aist.go.jp/yutaka.ueno/guppy/"
            }
        ],
        "inputs": [
            "dedup_jplace_f",
            "sv_weights_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/placement\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "PplacerADCL": {
        "name_process": "PplacerADCL",
        "string_process": "\nprocess PplacerADCL {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n\n    publishDir \"${params.output}/placement\", mode: 'copy'\n\n    input:\n        file dedup_jplace_f\n    output:\n        file 'adcl.csv.gz'\n    \n    \"\"\"\n    (echo name,adcl,weight && \n    guppy adcl --no-collapse ${dedup_jplace_f} -o /dev/stdout) | \n    gzip > adcl.csv.gz\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "\"\"\"\n    (echo name,adcl,weight && \n    guppy adcl --no-collapse ${dedup_jplace_f} -o /dev/stdout) | \n    gzip > adcl.csv.gz\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "GUPPY"
        ],
        "tools_url": [
            "https://bio.tools/guppy"
        ],
        "tools_dico": [
            {
                "name": "GUPPY",
                "uri": "https://bio.tools/guppy",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Program to visualize sequence annotation data of the genetic sequence data with graphical layout. This highly interactive program allows scrolling and zooming from the genomic landscape to discrete nucleic acid sequences. Its main function is quick rendering of data on a standalone personal computer to save the layout as a parsonal file. With optional link to the internet resources, and printing support, this program tries to make more greater use of computational media in research actitiviy.",
                "homepage": "http://staff.aist.go.jp/yutaka.ueno/guppy/"
            }
        ],
        "inputs": [
            "dedup_jplace_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/placement\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "PplacerEDPL": {
        "name_process": "PplacerEDPL",
        "string_process": "\nprocess PplacerEDPL {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n\n    publishDir \"${params.output}/placement\", mode: 'copy'\n\n    input:\n        file dedup_jplace_f\n    output:\n        file 'edpl.csv.gz'\n    \n    \"\"\"\n    (echo name,edpl && guppy edpl --csv ${dedup_jplace_f} -o /dev/stdout) | \n    gzip > edpl.csv.gz\n    \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "\"\"\"\n    (echo name,edpl && guppy edpl --csv ${dedup_jplace_f} -o /dev/stdout) | \n    gzip > edpl.csv.gz\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "dedup_jplace_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/placement\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "PplacerPCA": {
        "name_process": "PplacerPCA",
        "string_process": "\nprocess PplacerPCA {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n    afterScript \"rm -r refpkg/\"\n    publishDir \"${params.output}/placement\", mode: 'copy'\n    errorStrategy = 'ignore'\n\n    input:\n        file refpkg_tgz_f\n        file dedup_jplace_f\n        file sv_map_f\n    output:\n        file 'pca/epca.proj'\n        file 'pca/epca.xml'\n        file 'pca/epca.trans'\n        file 'pca/lpca.proj'\n        file 'pca/lpca.xml'\n        file 'pca/lpca.trans'\n    \n    \"\"\"\n    mkdir -p refpkg/ && mkdir -p pca/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/ &&\n    guppy epca ${dedup_jplace_f}:${sv_map_f} -c refpkg/ --out-dir pca/ --prefix epca &&\n    guppy lpca ${dedup_jplace_f}:${sv_map_f} -c refpkg/ --out-dir pca/ --prefix lpca\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n    mkdir -p refpkg/ && mkdir -p pca/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/ &&\n    guppy epca ${dedup_jplace_f}:${sv_map_f} -c refpkg/ --out-dir pca/ --prefix epca &&\n    guppy lpca ${dedup_jplace_f}:${sv_map_f} -c refpkg/ --out-dir pca/ --prefix lpca\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "GUPPY"
        ],
        "tools_url": [
            "https://bio.tools/guppy"
        ],
        "tools_dico": [
            {
                "name": "GUPPY",
                "uri": "https://bio.tools/guppy",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Program to visualize sequence annotation data of the genetic sequence data with graphical layout. This highly interactive program allows scrolling and zooming from the genomic landscape to discrete nucleic acid sequences. Its main function is quick rendering of data on a standalone personal computer to save the layout as a parsonal file. With optional link to the internet resources, and printing support, this program tries to make more greater use of computational media in research actitiviy.",
                "homepage": "http://staff.aist.go.jp/yutaka.ueno/guppy/"
            }
        ],
        "inputs": [
            "refpkg_tgz_f",
            "dedup_jplace_f",
            "sv_map_f"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "afterScript \"rm -r refpkg/\"",
            "publishDir \"${params.output}/placement\", mode: 'copy'",
            "errorStrategy = 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "PplacerAlphaDiversity": {
        "name_process": "PplacerAlphaDiversity",
        "string_process": "\nprocess PplacerAlphaDiversity {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n\n    input:\n        path jplace_f\n    output:\n        path \"${jplace_f.getBaseName()}.ad.csv\"\n\n    \n    \"\"\"\n    guppy fpd --csv --include-pendant --chao-d 0,1,1.00001,2,3,4,5 \\\n    ${jplace_f} > ${jplace_f.getBaseName()}.ad.csv\n    \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "\"\"\"\n    guppy fpd --csv --include-pendant --chao-d 0,1,1.00001,2,3,4,5 \\\n    ${jplace_f} > ${jplace_f.getBaseName()}.ad.csv\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "GUPPY"
        ],
        "tools_url": [
            "https://bio.tools/guppy"
        ],
        "tools_dico": [
            {
                "name": "GUPPY",
                "uri": "https://bio.tools/guppy",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Program to visualize sequence annotation data of the genetic sequence data with graphical layout. This highly interactive program allows scrolling and zooming from the genomic landscape to discrete nucleic acid sequences. Its main function is quick rendering of data on a standalone personal computer to save the layout as a parsonal file. With optional link to the internet resources, and printing support, this program tries to make more greater use of computational media in research actitiviy.",
                "homepage": "http://staff.aist.go.jp/yutaka.ueno/guppy/"
            }
        ],
        "inputs": [
            "jplace_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'"
        ],
        "when": "",
        "stub": ""
    },
    "PplacerKR": {
        "name_process": "PplacerKR",
        "string_process": "\nprocess PplacerKR {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n    afterScript \"rm -r refpkg/\"\n    publishDir \"${params.output}/placement\", mode: 'copy'\n\n    input:\n        file refpkg_tgz_f\n        file dedup_jplace_f\n        file sv_map_f\n    output:\n        file 'kr_distance.csv.gz'\n\n    \n    \"\"\"\n    mkdir -p refpkg/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/\n    guppy kr --list-out -c refpkg/ ${dedup_jplace_f}:${sv_map_f} |\n    gzip > kr_distance.csv.gz\n    \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "\"\"\"\n    mkdir -p refpkg/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/\n    guppy kr --list-out -c refpkg/ ${dedup_jplace_f}:${sv_map_f} |\n    gzip > kr_distance.csv.gz\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "GUPPY"
        ],
        "tools_url": [
            "https://bio.tools/guppy"
        ],
        "tools_dico": [
            {
                "name": "GUPPY",
                "uri": "https://bio.tools/guppy",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Program to visualize sequence annotation data of the genetic sequence data with graphical layout. This highly interactive program allows scrolling and zooming from the genomic landscape to discrete nucleic acid sequences. Its main function is quick rendering of data on a standalone personal computer to save the layout as a parsonal file. With optional link to the internet resources, and printing support, this program tries to make more greater use of computational media in research actitiviy.",
                "homepage": "http://staff.aist.go.jp/yutaka.ueno/guppy/"
            }
        ],
        "inputs": [
            "refpkg_tgz_f",
            "dedup_jplace_f",
            "sv_map_f"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "afterScript \"rm -r refpkg/\"",
            "publishDir \"${params.output}/placement\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "ClassifyDB_Prep": {
        "name_process": "ClassifyDB_Prep",
        "string_process": "\nprocess ClassifyDB_Prep {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n    afterScript \"rm -r refpkg/\"\n    cache = false\n\n    input:\n        file refpkg_tgz_f\n        file sv_map_f\n    \n    output:\n        file 'classify.prep.db'\n    \n\n    \"\"\"\n    mkdir -p refpkg/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/\n    rppr prep_db -c refpkg/ --sqlite classify.prep.db\n    (echo \"name,specimen\"; cat ${sv_map_f}) |\n    csvsql --table seq_info --insert --snifflimit 1000 --db sqlite:///classify.prep.db\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\n    mkdir -p refpkg/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/\n    rppr prep_db -c refpkg/ --sqlite classify.prep.db\n    (echo \"name,specimen\"; cat ${sv_map_f}) |\n    csvsql --table seq_info --insert --snifflimit 1000 --db sqlite:///classify.prep.db\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "refpkg_tgz_f",
            "sv_map_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "afterScript \"rm -r refpkg/\"",
            "cache = false"
        ],
        "when": "",
        "stub": ""
    },
    "ClassifySV": {
        "name_process": "ClassifySV",
        "string_process": "\nprocess ClassifySV {\n    container = \"${container__pplacer}\"\n    label = 'mem_veryhigh'\n    afterScript \"rm -r refpkg/\"\n    cache = false\n\n    input:\n        file refpkg_tgz_f\n        file classify_db_prepped\n        file dedup_jplace_f\n        file sv_refpkg_aln_sto_f\n    \n    output:\n        file 'classify.classified.db'\n\n    \"\"\"\n    mkdir -p refpkg/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/\n    guppy classify --pp \\\n    --classifier ${params.pp_classifer} \\\n    -j ${task.cpus} \\\n    -c refpkg/ \\\n    --nbc-sequences ${sv_refpkg_aln_sto_f} \\\n    --sqlite ${classify_db_prepped} \\\n    --seed ${params.pp_seed} \\\n    --cutoff ${params.pp_likelihood_cutoff} \\\n    --bayes-cutoff ${params.pp_bayes_cutoff} \\\n    --multiclass-min ${params.pp_multiclass_min} \\\n    --bootstrap-cutoff ${params.pp_bootstrap_cutoff} \\\n    --bootstrap-extension-cutoff ${params.pp_bootstrap_extension_cutoff} \\\n    --word-length ${params.pp_nbc_word_length} \\\n    --nbc-rank ${params.pp_nbc_target_rank} \\\n    --n-boot ${params.pp_nbc_boot} \\\n    ${dedup_jplace_f}\n    cp ${classify_db_prepped} classify.classified.db\n    \"\"\"\n}",
        "nb_lignes_process": 36,
        "string_script": "\"\"\"\n    mkdir -p refpkg/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/\n    guppy classify --pp \\\n    --classifier ${params.pp_classifer} \\\n    -j ${task.cpus} \\\n    -c refpkg/ \\\n    --nbc-sequences ${sv_refpkg_aln_sto_f} \\\n    --sqlite ${classify_db_prepped} \\\n    --seed ${params.pp_seed} \\\n    --cutoff ${params.pp_likelihood_cutoff} \\\n    --bayes-cutoff ${params.pp_bayes_cutoff} \\\n    --multiclass-min ${params.pp_multiclass_min} \\\n    --bootstrap-cutoff ${params.pp_bootstrap_cutoff} \\\n    --bootstrap-extension-cutoff ${params.pp_bootstrap_extension_cutoff} \\\n    --word-length ${params.pp_nbc_word_length} \\\n    --nbc-rank ${params.pp_nbc_target_rank} \\\n    --n-boot ${params.pp_nbc_boot} \\\n    ${dedup_jplace_f}\n    cp ${classify_db_prepped} classify.classified.db\n    \"\"\"",
        "nb_lignes_script": 20,
        "language_script": "bash",
        "tools": [
            "GUPPY"
        ],
        "tools_url": [
            "https://bio.tools/guppy"
        ],
        "tools_dico": [
            {
                "name": "GUPPY",
                "uri": "https://bio.tools/guppy",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Program to visualize sequence annotation data of the genetic sequence data with graphical layout. This highly interactive program allows scrolling and zooming from the genomic landscape to discrete nucleic acid sequences. Its main function is quick rendering of data on a standalone personal computer to save the layout as a parsonal file. With optional link to the internet resources, and printing support, this program tries to make more greater use of computational media in research actitiviy.",
                "homepage": "http://staff.aist.go.jp/yutaka.ueno/guppy/"
            }
        ],
        "inputs": [
            "refpkg_tgz_f",
            "classify_db_prepped",
            "dedup_jplace_f",
            "sv_refpkg_aln_sto_f"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'mem_veryhigh'",
            "afterScript \"rm -r refpkg/\"",
            "cache = false"
        ],
        "when": "",
        "stub": ""
    },
    "ClassifyMCC": {
        "name_process": "ClassifyMCC",
        "string_process": "\nprocess ClassifyMCC {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n    cache = false\n    publishDir \"${params.output}/classify\", mode: 'copy'\n\n    input:\n        file classifyDB_classified\n        file sv_weights_f\n\n    output:\n        file 'classify.mcc.db'\n\n    \"\"\"\n    multiclass_concat.py -k \\\n    --dedup-info ${sv_weights_f} ${classifyDB_classified}\n    cp ${classifyDB_classified} classify.mcc.db\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    multiclass_concat.py -k \\\n    --dedup-info ${sv_weights_f} ${classifyDB_classified}\n    cp ${classifyDB_classified} classify.mcc.db\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "classifyDB_classified",
            "sv_weights_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "cache = false",
            "publishDir \"${params.output}/classify\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "ClassifyTables": {
        "name_process": "ClassifyTables",
        "string_process": "\nprocess ClassifyTables {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/classify\", mode: 'copy'\n\n    input:\n        tuple val(rank), file(classifyDB_mcc), file(sv_map_for_tables_f)\n\n    output:\n        tuple val(rank), file(\"tables/by_specimen.${rank}.csv\"), file(\"tables/by_taxon.${rank}.csv\"), file(\"tables/tallies_wide.${rank}.csv\")\n\n    \"\"\"\n    mkdir -p tables/\n    classif_table.py ${classifyDB_mcc} \\\n    tables/by_taxon.${rank}.csv \\\n    --rank ${rank} \\\n    --specimen-map ${sv_map_for_tables_f} \\\n    --by-specimen tables/by_specimen.${rank}.csv \\\n    --tallies-wide tables/tallies_wide.${rank}.csv\n    \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "\"\"\"\n    mkdir -p tables/\n    classif_table.py ${classifyDB_mcc} \\\n    tables/by_taxon.${rank}.csv \\\n    --rank ${rank} \\\n    --specimen-map ${sv_map_for_tables_f} \\\n    --by-specimen tables/by_specimen.${rank}.csv \\\n    --tallies-wide tables/tallies_wide.${rank}.csv\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "rank",
            "classifyDB_mcc",
            "sv_map_for_tables_f"
        ],
        "nb_inputs": 3,
        "outputs": [
            "rank"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/classify\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "SharetableToMapWeight": {
        "name_process": "SharetableToMapWeight",
        "string_process": "\nprocess SharetableToMapWeight {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/sv\", mode: 'copy'\n\n    input:\n        path (sharetable)\n\n    output:\n        path (\"sv_sp_map.csv\"), emit: sv_map\n        path (\"sv_weights.csv\"), emit: sv_weights\n        path (\"sp_sv_long.csv\"), emit: sp_sv_long\n\n\"\"\"\n#!/usr/bin/env python\nimport csv\n\nsp_count = {}\nwith open('${sharetable}', 'rt') as st_h:\n    st_r = csv.reader(st_h, delimiter='\\\\t')\n    header = next(st_r)\n    sv_name = header[3:]\n    for r in st_r:\n        sp_count[r[0]] = [int(c) for c in r[3:]]\nweightsL = []\nmapL = []\nsv_long = []\nfor sv_i, sv in enumerate(sv_name):\n    sv_counts = [\n        (sp, c[sv_i]) for sp, c in sp_count.items()\n        if c[sv_i] > 0\n    ]\n    if len(sv_counts) == 0:\n        continue\n    # Implicit else\n    shared_sv = \"{}:{}\".format(sv, sorted(sv_counts, key=lambda v: -1*v[1])[0][0])\n    sv_long += [\n        (sp, shared_sv, c[sv_i]) for sp, c in sp_count.items()\n        if c[sv_i] > 0\n    ]    \n    weightsL += [\n        (shared_sv, \"{}:{}\".format(sv, sp), c)\n        for sp, c in \n        sv_counts\n    ]\n    mapL += [\n        (\"{}:{}\".format(sv, sp), sp)\n        for sp, c in \n        sv_counts\n    ]\nwith open(\"sv_sp_map.csv\", \"w\") as map_h:\n    map_w = csv.writer(map_h)\n    map_w.writerows(mapL)\nwith open(\"sv_weights.csv\", \"w\") as weights_h:\n    weights_w = csv.writer(weights_h)\n    weights_w.writerows(weightsL)\nwith open(\"sp_sv_long.csv\", 'wt') as svl_h:\n    svl_w = csv.writer(svl_h)\n    svl_w.writerow((\n        'specimen',\n        'sv',\n        'count'\n    ))\n    svl_w.writerows(sv_long)\n\"\"\"\n}",
        "nb_lignes_process": 65,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport csv\n\nsp_count = {}\nwith open('${sharetable}', 'rt') as st_h:\n    st_r = csv.reader(st_h, delimiter='\\\\t')\n    header = next(st_r)\n    sv_name = header[3:]\n    for r in st_r:\n        sp_count[r[0]] = [int(c) for c in r[3:]]\nweightsL = []\nmapL = []\nsv_long = []\nfor sv_i, sv in enumerate(sv_name):\n    sv_counts = [\n        (sp, c[sv_i]) for sp, c in sp_count.items()\n        if c[sv_i] > 0\n    ]\n    if len(sv_counts) == 0:\n        continue\n    # Implicit else\n    shared_sv = \"{}:{}\".format(sv, sorted(sv_counts, key=lambda v: -1*v[1])[0][0])\n    sv_long += [\n        (sp, shared_sv, c[sv_i]) for sp, c in sp_count.items()\n        if c[sv_i] > 0\n    ]    \n    weightsL += [\n        (shared_sv, \"{}:{}\".format(sv, sp), c)\n        for sp, c in \n        sv_counts\n    ]\n    mapL += [\n        (\"{}:{}\".format(sv, sp), sp)\n        for sp, c in \n        sv_counts\n    ]\nwith open(\"sv_sp_map.csv\", \"w\") as map_h:\n    map_w = csv.writer(map_h)\n    map_w.writerows(mapL)\nwith open(\"sv_weights.csv\", \"w\") as weights_h:\n    weights_w = csv.writer(weights_h)\n    weights_w.writerows(weightsL)\nwith open(\"sp_sv_long.csv\", 'wt') as svl_h:\n    svl_w = csv.writer(svl_h)\n    svl_w.writerow((\n        'specimen',\n        'sv',\n        'count'\n    ))\n    svl_w.writerows(sv_long)\n\"\"\"",
        "nb_lignes_script": 51,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sharetable"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/sv\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "Dada2_convert_output": {
        "name_process": "Dada2_convert_output",
        "string_process": "\nprocess Dada2_convert_output {\n    container \"${container__dada2pplacer}\"\n    label 'io_mem'\n    publishDir \"${params.output}/sv/\", mode: 'copy'\n    errorStrategy \"retry\"\n\n    input:\n        file(final_seqtab_csv)\n\n    output:\n        file \"dada2.sv.fasta\"\n        file \"dada2.sv.map.csv\"\n        file \"dada2.sv.weights.csv\"\n\n    \"\"\"\n    dada2-seqtab-to-pplacer \\\n    -s ${final_seqtab_csv} \\\n    -f dada2.sv.fasta \\\n    -m dada2.sv.map.csv \\\n    -w dada2.sv.weights.csv \\\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\n    dada2-seqtab-to-pplacer \\\n    -s ${final_seqtab_csv} \\\n    -f dada2.sv.fasta \\\n    -m dada2.sv.map.csv \\\n    -w dada2.sv.weights.csv \\\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "final_seqtab_csv"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2pplacer}\"",
            "label 'io_mem'",
            "publishDir \"${params.output}/sv/\", mode: 'copy'",
            "errorStrategy \"retry\""
        ],
        "when": "",
        "stub": ""
    },
    "Extract_Taxonomy": {
        "name_process": "Extract_Taxonomy",
        "string_process": "\nprocess Extract_Taxonomy {\n    container \"${container__dada2pplacer}\"\n    label 'io_mem'\n    publishDir \"${params.output}/classify\", mode: 'copy'\n    errorStrategy \"ignore\"\n\n    input:\n        file (weights_csv)\n        file (taxonomy_db)\n\n    output:\n        file \"sv_taxonomy.csv\"\n\n\"\"\"\n#!/usr/bin/env python\nimport csv\nimport pandas as pd\nimport sqlite3\n\nsv = {\n    r[0] for r in \n    csv.reader(open(\n        \"${weights_csv}\",\n        'rt'\n    ))\n}\n\ntax_db_conn = sqlite3.connect(\"${taxonomy_db}\")\ntax_db_cur = tax_db_conn.cursor()\ntax_db_cur.execute(\"CREATE INDEX IF NOT EXISTS idx_mcc_name_tax  ON multiclass_concat(name, tax_id)\")\n\nsv_classification = pd.concat([\n    pd.read_sql(\"SELECT name, want_rank, taxa.tax_id, taxa.tax_name, taxa.rank, likelihood FROM multiclass_concat JOIN taxa ON multiclass_concat.tax_id = taxa.tax_id WHERE name=(?)\",\n        con = tax_db_conn,\n        params=(\n            sv,\n        ))    \n    for sv in sv\n], ignore_index=True)\n\nsv_classification.to_csv(\"sv_taxonomy.csv\", index=False)\n\n\"\"\"\n}",
        "nb_lignes_process": 43,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport csv\nimport pandas as pd\nimport sqlite3\n\nsv = {\n    r[0] for r in \n    csv.reader(open(\n        \"${weights_csv}\",\n        'rt'\n    ))\n}\n\ntax_db_conn = sqlite3.connect(\"${taxonomy_db}\")\ntax_db_cur = tax_db_conn.cursor()\ntax_db_cur.execute(\"CREATE INDEX IF NOT EXISTS idx_mcc_name_tax  ON multiclass_concat(name, tax_id)\")\n\nsv_classification = pd.concat([\n    pd.read_sql(\"SELECT name, want_rank, taxa.tax_id, taxa.tax_name, taxa.rank, likelihood FROM multiclass_concat JOIN taxa ON multiclass_concat.tax_id = taxa.tax_id WHERE name=(?)\",\n        con = tax_db_conn,\n        params=(\n            sv,\n        ))    \n    for sv in sv\n], ignore_index=True)\n\nsv_classification.to_csv(\"sv_taxonomy.csv\", index=False)\n\n\"\"\"",
        "nb_lignes_script": 29,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "weights_csv",
            "taxonomy_db"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2pplacer}\"",
            "label 'io_mem'",
            "publishDir \"${params.output}/classify\", mode: 'copy'",
            "errorStrategy \"ignore\""
        ],
        "when": "",
        "stub": ""
    },
    "ExtractRefpkg": {
        "name_process": "ExtractRefpkg",
        "string_process": "\nprocess ExtractRefpkg {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    \n    input:\n        file refpkg_tgz_f\n\n    output:\n        path 'refpkg_tree.nwk', emit: tree\n        path 'refpkg.aln.fasta', emit: ref_aln_fasta\n        path 'refpkg.aln.sto', emit: ref_aln_sto\n        path 'model.txt', emit: model\n        path 'leaf_info.csv', emit: leaf_info\n        path 'taxonomy.csv', emit: taxonomy\n        path 'refpkg.cm', emit: cm\n\n\"\"\"\n#!/usr/bin/env python\nimport tarfile\nimport json\nimport os\nimport re\n\ntar_h = tarfile.open('${refpkg_tgz_f}')\ntar_contents_dict = {os.path.basename(f.name): f for f in tar_h.getmembers()}\ncontents = json.loads(\n    tar_h.extractfile(\n        tar_contents_dict['CONTENTS.json']\n    ).read().decode('utf-8')\n)\n\nwith open('refpkg.cm', 'wb') as cm_h:\n    cm_h.write(\n        tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('profile')]\n        ).read()\n    )\n\nwith open('refpkg_tree.nwk', 'wt') as tree_h:\n    tree_h.writelines(\n        tar_h.extractfile(\n                tar_contents_dict[contents['files'].get('tree')]\n            ).read().decode('utf-8')\n    )\n\naln_fasta_intgz = contents['files'].get('aln_fasta')\naln_sto_intgz = contents['files'].get('aln_sto')\n\nif aln_fasta_intgz and aln_sto_intgz:\n    # Both version of the alignment are in the refpkg\n    with open('refpkg.aln.fasta','w') as out_aln_fasta_h:\n        out_aln_fasta_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_fasta_intgz]\n            ).read().decode('utf-8')\n        )\n    with open('refpkg.aln.sto','w') as out_aln_sto_h:\n        out_aln_sto_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_sto_intgz]\n            ).read().decode('utf-8')\n        )\nelif aln_fasta_intgz:\n    # Only fasta exists\n    with open('refpkg.aln.fasta','w') as out_aln_fasta_h:\n        out_aln_fasta_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_fasta_intgz]\n            ).read().decode('utf-8')\n        )\n    # And convert to sto format\n    with open('refpkg.aln.sto','w') as out_aln_sto_h:\n        AlignIO.write(\n            AlignIO.read(\n                tar_h.extractfile(tar_contents_dict[aln_fasta_intgz]),\n                'fasta'),\n            out_aln_sto_h,\n            'stockholm'\n        )\nelif aln_sto_intgz:\n    # Only STO exists\n    with open('refpkg.aln.sto','w') as out_aln_sto_h:\n        out_aln_sto_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_sto_intgz]\n            ).read().decode('utf-8')\n        )\n    with sopen('refpkg.aln.fasta','w') as out_aln_fasta_h:\n        AlignIO.write(\n            AlignIO.read(\n                            tar_h.extractfile(tar_contents_dict[aln_sto_intgz]),\n                            'stockholm'),\n            out_aln_fasta_h,\n            'fasta'\n        )\n# Model\nif 'raxml_ng_model' in contents['files']:\n    with open('model.txt', 'wt') as out_h:\n        out_h.write(tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('raxml_ng_model')]\n        ).read().decode('utf-8'))\n\nelse:\n    phylo_model = json.loads(tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('phylo_model')]\n        ).read().decode('utf-8')\n    ).get('subs_rates')\n\n    re_basefreq = re.compile(r'Base frequencies: (?P<A>0\\\\.\\\\d+) (?P<C>0\\\\.\\\\d+) (?P<G>0\\\\.\\\\d+) (?P<T>0\\\\.\\\\d+)')\n    bf_m = re_basefreq.search(tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('tree_stats')]\n        ).read().decode('utf-8'))\n    with open('model.txt', 'wt') as model_h:\n        model_h.writelines( \n            \"GTR{\"+\n            \"{}/{}/{}/{}/{}/{}\".format(\n                phylo_model['ac'],\n                phylo_model['ag'],\n                phylo_model['at'],\n                phylo_model['cg'],\n                phylo_model['ct'],\n                phylo_model['gt'],\n            )\n            +\"}\"+\"+FU{\"+\n            \"{}/{}/{}/{}\".format(\n                bf_m['A'],\n                bf_m['C'],\n                bf_m['G'],\n                bf_m['T'],\n            )\n            +\"}\"\n        )\n\nwith open('leaf_info.csv', 'wt') as leaf_h:\n    leaf_h.write(tar_h.extractfile(\n        tar_contents_dict[contents['files'].get('seq_info')]\n    ).read().decode('utf-8'))\n\nwith open('taxonomy.csv', 'wt') as leaf_h:\n    leaf_h.write(tar_h.extractfile(\n        tar_contents_dict[contents['files'].get('taxonomy')]\n    ).read().decode('utf-8'))\n\n\n\"\"\"\n}",
        "nb_lignes_process": 145,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport tarfile\nimport json\nimport os\nimport re\n\ntar_h = tarfile.open('${refpkg_tgz_f}')\ntar_contents_dict = {os.path.basename(f.name): f for f in tar_h.getmembers()}\ncontents = json.loads(\n    tar_h.extractfile(\n        tar_contents_dict['CONTENTS.json']\n    ).read().decode('utf-8')\n)\n\nwith open('refpkg.cm', 'wb') as cm_h:\n    cm_h.write(\n        tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('profile')]\n        ).read()\n    )\n\nwith open('refpkg_tree.nwk', 'wt') as tree_h:\n    tree_h.writelines(\n        tar_h.extractfile(\n                tar_contents_dict[contents['files'].get('tree')]\n            ).read().decode('utf-8')\n    )\n\naln_fasta_intgz = contents['files'].get('aln_fasta')\naln_sto_intgz = contents['files'].get('aln_sto')\n\nif aln_fasta_intgz and aln_sto_intgz:\n    # Both version of the alignment are in the refpkg\n    with open('refpkg.aln.fasta','w') as out_aln_fasta_h:\n        out_aln_fasta_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_fasta_intgz]\n            ).read().decode('utf-8')\n        )\n    with open('refpkg.aln.sto','w') as out_aln_sto_h:\n        out_aln_sto_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_sto_intgz]\n            ).read().decode('utf-8')\n        )\nelif aln_fasta_intgz:\n    # Only fasta exists\n    with open('refpkg.aln.fasta','w') as out_aln_fasta_h:\n        out_aln_fasta_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_fasta_intgz]\n            ).read().decode('utf-8')\n        )\n    # And convert to sto format\n    with open('refpkg.aln.sto','w') as out_aln_sto_h:\n        AlignIO.write(\n            AlignIO.read(\n                tar_h.extractfile(tar_contents_dict[aln_fasta_intgz]),\n                'fasta'),\n            out_aln_sto_h,\n            'stockholm'\n        )\nelif aln_sto_intgz:\n    # Only STO exists\n    with open('refpkg.aln.sto','w') as out_aln_sto_h:\n        out_aln_sto_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_sto_intgz]\n            ).read().decode('utf-8')\n        )\n    with sopen('refpkg.aln.fasta','w') as out_aln_fasta_h:\n        AlignIO.write(\n            AlignIO.read(\n                            tar_h.extractfile(tar_contents_dict[aln_sto_intgz]),\n                            'stockholm'),\n            out_aln_fasta_h,\n            'fasta'\n        )\n# Model\nif 'raxml_ng_model' in contents['files']:\n    with open('model.txt', 'wt') as out_h:\n        out_h.write(tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('raxml_ng_model')]\n        ).read().decode('utf-8'))\n\nelse:\n    phylo_model = json.loads(tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('phylo_model')]\n        ).read().decode('utf-8')\n    ).get('subs_rates')\n\n    re_basefreq = re.compile(r'Base frequencies: (?P<A>0\\\\.\\\\d+) (?P<C>0\\\\.\\\\d+) (?P<G>0\\\\.\\\\d+) (?P<T>0\\\\.\\\\d+)')\n    bf_m = re_basefreq.search(tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('tree_stats')]\n        ).read().decode('utf-8'))\n    with open('model.txt', 'wt') as model_h:\n        model_h.writelines( \n            \"GTR{\"+\n            \"{}/{}/{}/{}/{}/{}\".format(\n                phylo_model['ac'],\n                phylo_model['ag'],\n                phylo_model['at'],\n                phylo_model['cg'],\n                phylo_model['ct'],\n                phylo_model['gt'],\n            )\n            +\"}\"+\"+FU{\"+\n            \"{}/{}/{}/{}\".format(\n                bf_m['A'],\n                bf_m['C'],\n                bf_m['G'],\n                bf_m['T'],\n            )\n            +\"}\"\n        )\n\nwith open('leaf_info.csv', 'wt') as leaf_h:\n    leaf_h.write(tar_h.extractfile(\n        tar_contents_dict[contents['files'].get('seq_info')]\n    ).read().decode('utf-8'))\n\nwith open('taxonomy.csv', 'wt') as leaf_h:\n    leaf_h.write(tar_h.extractfile(\n        tar_contents_dict[contents['files'].get('taxonomy')]\n    ).read().decode('utf-8'))\n\n\n\"\"\"",
        "nb_lignes_script": 128,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "refpkg_tgz_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'"
        ],
        "when": "",
        "stub": ""
    },
    "ConvertAlnToFasta": {
        "name_process": "ConvertAlnToFasta",
        "string_process": "\nprocess ConvertAlnToFasta {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    errorStrategy \"retry\"\n\n    input: \n        file recruits_aln_sto_f\n    \n    output:\n        file \"recruits.aln.fasta\"\n    \n    \"\"\"\n    #!/usr/bin/env python\n    from Bio import AlignIO\n\n    with open('recruits.aln.fasta', 'wt') as out_h:\n        AlignIO.write(\n            AlignIO.read(\n                open('${recruits_aln_sto_f}', 'rt'),\n                'stockholm'\n            ),\n            out_h,\n            'fasta'\n        )\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n    #!/usr/bin/env python\n    from Bio import AlignIO\n\n    with open('recruits.aln.fasta', 'wt') as out_h:\n        AlignIO.write(\n            AlignIO.read(\n                open('${recruits_aln_sto_f}', 'rt'),\n                'stockholm'\n            ),\n            out_h,\n            'fasta'\n        )\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "recruits_aln_sto_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "errorStrategy \"retry\""
        ],
        "when": "",
        "stub": ""
    },
    "EPAngPlacement": {
        "name_process": "EPAngPlacement",
        "string_process": "\nprocess EPAngPlacement {\n    container = \"${container__epang}\"\n    label = 'mem_veryhigh'\n    publishDir \"${params.output}/placement\", mode: 'copy'\n    input:\n        file refpkg_aln_fasta\n        file combined_aln_fasta\n        file model\n        file ref_tree\n\n    output:\n        file 'dedup.jplace'\n    \"\"\"\n    set -e\n\n    epa-ng --split ${refpkg_aln_fasta} ${combined_aln_fasta}\n    model=`cat ${model}`\n    \n    epa-ng -t ${ref_tree} \\\n    -s reference.fasta -q query.fasta \\\n    -m \\$model -T ${task.cpus} \\\n    --baseball-heur\n\n    mv epa_result.jplace dedup.jplace\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n    set -e\n\n    epa-ng --split ${refpkg_aln_fasta} ${combined_aln_fasta}\n    model=`cat ${model}`\n    \n    epa-ng -t ${ref_tree} \\\n    -s reference.fasta -q query.fasta \\\n    -m \\$model -T ${task.cpus} \\\n    --baseball-heur\n\n    mv epa_result.jplace dedup.jplace\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "EPA-ng"
        ],
        "tools_url": [
            "https://bio.tools/EPA-ng"
        ],
        "tools_dico": [
            {
                "name": "EPA-ng",
                "uri": "https://bio.tools/EPA-ng",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0310",
                                    "term": "Sequence assembly"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0362",
                                    "term": "Genome annotation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression data analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Complete reimplementation of the evolutionary placement algorithm (EPA) that is substantially faster, offers a distributed memory parallelization, and integrates concepts from both, RAxML-EPA and PPLACER.",
                "homepage": "https://github.com/Pbdas/epa-ng"
            }
        ],
        "inputs": [
            "refpkg_aln_fasta",
            "combined_aln_fasta",
            "model",
            "ref_tree"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__epang}\"",
            "label = 'mem_veryhigh'",
            "publishDir \"${params.output}/placement\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "MakeEPAngTaxonomy": {
        "name_process": "MakeEPAngTaxonomy",
        "string_process": "\nprocess MakeEPAngTaxonomy {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/refpkg\", mode: 'copy'\n\n    input:\n        path leaf_info_f\n        path taxonomy_f\n\n    output:\n        path 'epang_taxon_file.tsv'\n\n\"\"\"\n#!/usr/bin/env python\nimport csv\n\ntax_dict = {\n    r['tax_id']: r for r in\n    csv.DictReader(open('${taxonomy_f}', 'rt'))\n}\ntax_names = {\n    tax_id: r['tax_name']\n    for tax_id, r in tax_dict.items()\n}\nRANKS = [\n    'superkingdom',\n    'phylum',\n    'class',\n    'order',\n    'family',\n    'genus',\n    'species',\n]\nwith open('epang_taxon_file.tsv', 'wt') as tf_h:\n    tf_w = csv.writer(tf_h, delimiter='\\\\t')\n    for row in csv.DictReader(open('${leaf_info_f}', 'rt')):\n        tax_id = row.get('tax_id', None)\n        if tax_id is None:\n            continue\n        # Implicit else\n        tax_lineage = tax_dict.get(tax_id, None)\n        if tax_lineage is None:\n            continue\n        lineage_str = \";\".join([\n            tax_names.get(tax_lineage.get(rank, \"\"), \"\")\n            for rank in RANKS\n        ])\n        tf_w.writerow([row['seqname'], lineage_str])\n\n\"\"\"\n\n}",
        "nb_lignes_process": 51,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport csv\n\ntax_dict = {\n    r['tax_id']: r for r in\n    csv.DictReader(open('${taxonomy_f}', 'rt'))\n}\ntax_names = {\n    tax_id: r['tax_name']\n    for tax_id, r in tax_dict.items()\n}\nRANKS = [\n    'superkingdom',\n    'phylum',\n    'class',\n    'order',\n    'family',\n    'genus',\n    'species',\n]\nwith open('epang_taxon_file.tsv', 'wt') as tf_h:\n    tf_w = csv.writer(tf_h, delimiter='\\\\t')\n    for row in csv.DictReader(open('${leaf_info_f}', 'rt')):\n        tax_id = row.get('tax_id', None)\n        if tax_id is None:\n            continue\n        # Implicit else\n        tax_lineage = tax_dict.get(tax_id, None)\n        if tax_lineage is None:\n            continue\n        lineage_str = \";\".join([\n            tax_names.get(tax_lineage.get(rank, \"\"), \"\")\n            for rank in RANKS\n        ])\n        tf_w.writerow([row['seqname'], lineage_str])\n\n\"\"\"",
        "nb_lignes_script": 37,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "leaf_info_f",
            "taxonomy_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/refpkg\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "Gappa_Classify": {
        "name_process": "Gappa_Classify",
        "string_process": "\nprocess Gappa_Classify {\n    container = \"${container__gappa}\"\n    label = 'mem_veryhigh'\n    publishDir \"${params.output}/classify\", mode: 'copy'\n    errorStrategy 'ignore'\n\n    input:\n        path dedup_jplace\n        path taxon_file\n    \n    output:\n        path 'per_query.tsv'\n\n    \"\"\"\n    set -e\n\n\n    gappa examine assign \\\n    --per-query-results \\\n    --verbose \\\n    --threads ${task.cpus} \\\n    --jplace-path ${dedup_jplace} \\\n    --taxon-file ${taxon_file} \\\n    \n    \"\"\"\n\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n    set -e\n\n\n    gappa examine assign \\\n    --per-query-results \\\n    --verbose \\\n    --threads ${task.cpus} \\\n    --jplace-path ${dedup_jplace} \\\n    --taxon-file ${taxon_file} \\\n    \n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "GAPPA"
        ],
        "tools_url": [
            "https://bio.tools/GAPPA"
        ],
        "tools_dico": [
            {
                "name": "GAPPA",
                "uri": "https://bio.tools/GAPPA",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0325",
                                    "term": "Phylogenetic tree comparison"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic tree reconstruction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Collection of commands for working with phylogenetic data.",
                "homepage": "http://github.com/lczech/gappa"
            }
        ],
        "inputs": [
            "dedup_jplace",
            "taxon_file"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__gappa}\"",
            "label = 'mem_veryhigh'",
            "publishDir \"${params.output}/classify\", mode: 'copy'",
            "errorStrategy 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "Make_Wide_Tax_Table": {
        "name_process": "Make_Wide_Tax_Table",
        "string_process": "\nprocess Make_Wide_Tax_Table {\n    container \"${container__dada2pplacer}\"\n    label 'io_mem'\n    publishDir \"${params.output}/classify\", mode: 'copy'\n                            \n\n    input:\n        path sv_long\n        path sv_taxonomy\n        val want_rank\n\n    output:\n        path \"tables/taxon_wide_ra.${want_rank}.csv\", emit: ra\n        path \"tables/taxon_wide_nreads.${want_rank}.csv\", emit: nreads\n\n\"\"\"\n#!/usr/bin/env python\nimport pandas as pd\nimport os\n\ntry:\n    os.makedirs('tables')\nexcept:\n    pass\n\nsv_long = pd.read_csv(\"${sv_long}\").rename({\n    'count': 'nreads'\n}, axis=1)\n# Add in rel abund\nfor sp, sp_sv in sv_long.groupby('specimen'):\n    sv_long.loc[sp_sv.index, 'fract'] = sp_sv.nreads / sp_sv.nreads.sum()\n\nsv_taxonomy = pd.read_csv('${sv_taxonomy}')\nsv_long_tax = pd.merge(\n    sv_long,\n    sv_taxonomy[sv_taxonomy.want_rank == '${want_rank}'],\n    on='sv',\n    how='left'\n)\n\nsp_tax = sv_long_tax.groupby(['specimen', 'tax_name']).sum().reset_index()[[\n    'specimen',\n    'tax_name',\n    'nreads',\n    'fract'\n]]\n\nsp_tax_wide_ra = sp_tax.pivot(\n    index='specimen',\n    columns='tax_name',\n    values='fract'\n).fillna(0)\n# Sort by mean RA\nsp_tax_wide_ra = sp_tax_wide_ra[sp_tax_wide_ra.mean().sort_values(ascending=False).index]\n\nsp_tax_wide_ra.to_csv(\"tables/taxon_wide_ra.${want_rank}.csv\")\n\nsp_tax_wide_nreads = sp_tax.pivot(\n    index='specimen',\n    columns='tax_name',\n    values='nreads'\n).fillna(0)\n# Sort by mean RA\nsp_tax_wide_nreads = sp_tax_wide_nreads[sp_tax_wide_ra.mean().sort_values(ascending=False).index].astype(int)\nsp_tax_wide_nreads.to_csv(\"tables/taxon_wide_nreads.${want_rank}.csv\")\n\n\"\"\"\n}",
        "nb_lignes_process": 67,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport pandas as pd\nimport os\n\ntry:\n    os.makedirs('tables')\nexcept:\n    pass\n\nsv_long = pd.read_csv(\"${sv_long}\").rename({\n    'count': 'nreads'\n}, axis=1)\n# Add in rel abund\nfor sp, sp_sv in sv_long.groupby('specimen'):\n    sv_long.loc[sp_sv.index, 'fract'] = sp_sv.nreads / sp_sv.nreads.sum()\n\nsv_taxonomy = pd.read_csv('${sv_taxonomy}')\nsv_long_tax = pd.merge(\n    sv_long,\n    sv_taxonomy[sv_taxonomy.want_rank == '${want_rank}'],\n    on='sv',\n    how='left'\n)\n\nsp_tax = sv_long_tax.groupby(['specimen', 'tax_name']).sum().reset_index()[[\n    'specimen',\n    'tax_name',\n    'nreads',\n    'fract'\n]]\n\nsp_tax_wide_ra = sp_tax.pivot(\n    index='specimen',\n    columns='tax_name',\n    values='fract'\n).fillna(0)\n# Sort by mean RA\nsp_tax_wide_ra = sp_tax_wide_ra[sp_tax_wide_ra.mean().sort_values(ascending=False).index]\n\nsp_tax_wide_ra.to_csv(\"tables/taxon_wide_ra.${want_rank}.csv\")\n\nsp_tax_wide_nreads = sp_tax.pivot(\n    index='specimen',\n    columns='tax_name',\n    values='nreads'\n).fillna(0)\n# Sort by mean RA\nsp_tax_wide_nreads = sp_tax_wide_nreads[sp_tax_wide_ra.mean().sort_values(ascending=False).index].astype(int)\nsp_tax_wide_nreads.to_csv(\"tables/taxon_wide_nreads.${want_rank}.csv\")\n\n\"\"\"",
        "nb_lignes_script": 51,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sv_long",
            "sv_taxonomy",
            "want_rank"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2pplacer}\"",
            "label 'io_mem'",
            "publishDir \"${params.output}/classify\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "Gappa_Extract_Taxonomy": {
        "name_process": "Gappa_Extract_Taxonomy",
        "string_process": "\nprocess Gappa_Extract_Taxonomy {\n    container \"${container__dada2pplacer}\"\n    label 'io_mem'\n    publishDir \"${params.output}/classify\", mode: 'copy'\n                            \n\n    input:\n        path gappa_taxonomy\n        path refpkg_taxtable\n\n    output:\n        path \"sv_taxonomy.csv\"\n        path refpkg_taxtable\n\n\"\"\"\n#!/usr/bin/env python\nimport pandas as pd\n\nMIN_AFRACT = 0\nRANKS = [\n    'superkingdom',\n    'phylum',\n    'class',\n    'order',\n    'family',\n    'genus',\n    'species',\n]\nRANK_DEPTH = {\n    i+1: r for (i, r) in enumerate(RANKS)\n}\nrefpkg_taxtable = pd.read_csv(\"${refpkg_taxtable}\")\ntax_name_to_id = {\n    row.tax_name: row.tax_id for\n    idx, row in refpkg_taxtable.iterrows()\n}\nepa_tax = pd.read_csv('${gappa_taxonomy}', sep='\\t')\nepa_tax['lineage']=epa_tax.taxopath.apply(lambda tp: tp.split(';'))\nepa_tax['rank_depth']=epa_tax.lineage.apply(len)\nsv_tax_list = []\nfor sv, sv_c in epa_tax[epa_tax.taxopath != 'DISTANT'].groupby('name'):\n    sv_tax = pd.DataFrame()\n    rank = None\n    tax_name = None\n    lineage = None\n    afract = None    \n    for rank_depth, want_rank in RANK_DEPTH.items():\n        sv_depth = sv_c[sv_c.rank_depth == rank_depth]\n        if len(sv_depth) > 0 and sv_depth.afract.sum() >= MIN_AFRACT:\n            # Something at this depth, and the cumulative fract likelihood is above our threshold\n            rank = want_rank\n            tax_name = \" / \".join(sv_depth.lineage.apply(lambda L: L[-1]))\n            ncbi_tax_id = \",\".join([str(tax_name_to_id.get(tn, -1)) for tn in sv_depth.lineage.apply(lambda L: L[-1])])\n            lineage = \";\".join(sv_depth.lineage.iloc[0][:-1] + [tax_name])\n            afract = sv_depth.afract.sum()\n            \n            \n        sv_tax.loc[rank, 'sv'] = sv\n        sv_tax.loc[rank, 'want_rank'] = want_rank\n        sv_tax.loc[rank, 'rank'] = rank\n        sv_tax.loc[rank, 'rank_depth'] = rank_depth\n        sv_tax.loc[rank, 'tax_name'] = tax_name\n        sv_tax.loc[rank, 'ncbi_tax_id'] = ncbi_tax_id\n        sv_tax.loc[rank, 'lineage'] = lineage\n        sv_tax.loc[rank, 'afract'] = afract\n        sv_tax.loc[rank, 'ambiguous'] = len(sv_depth) != 1\n    sv_tax_list.append(sv_tax)\n\n\nsv_taxonomy = pd.concat(sv_tax_list, ignore_index=True)\nsv_taxonomy['rank_depth'] = sv_taxonomy.rank_depth.astype(int)\nsv_taxonomy.to_csv('sv_taxonomy.csv', index=None)\n\n\"\"\"\n}",
        "nb_lignes_process": 74,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport pandas as pd\n\nMIN_AFRACT = 0\nRANKS = [\n    'superkingdom',\n    'phylum',\n    'class',\n    'order',\n    'family',\n    'genus',\n    'species',\n]\nRANK_DEPTH = {\n    i+1: r for (i, r) in enumerate(RANKS)\n}\nrefpkg_taxtable = pd.read_csv(\"${refpkg_taxtable}\")\ntax_name_to_id = {\n    row.tax_name: row.tax_id for\n    idx, row in refpkg_taxtable.iterrows()\n}\nepa_tax = pd.read_csv('${gappa_taxonomy}', sep='\\t')\nepa_tax['lineage']=epa_tax.taxopath.apply(lambda tp: tp.split(';'))\nepa_tax['rank_depth']=epa_tax.lineage.apply(len)\nsv_tax_list = []\nfor sv, sv_c in epa_tax[epa_tax.taxopath != 'DISTANT'].groupby('name'):\n    sv_tax = pd.DataFrame()\n    rank = None\n    tax_name = None\n    lineage = None\n    afract = None    \n    for rank_depth, want_rank in RANK_DEPTH.items():\n        sv_depth = sv_c[sv_c.rank_depth == rank_depth]\n        if len(sv_depth) > 0 and sv_depth.afract.sum() >= MIN_AFRACT:\n            # Something at this depth, and the cumulative fract likelihood is above our threshold\n            rank = want_rank\n            tax_name = \" / \".join(sv_depth.lineage.apply(lambda L: L[-1]))\n            ncbi_tax_id = \",\".join([str(tax_name_to_id.get(tn, -1)) for tn in sv_depth.lineage.apply(lambda L: L[-1])])\n            lineage = \";\".join(sv_depth.lineage.iloc[0][:-1] + [tax_name])\n            afract = sv_depth.afract.sum()\n            \n            \n        sv_tax.loc[rank, 'sv'] = sv\n        sv_tax.loc[rank, 'want_rank'] = want_rank\n        sv_tax.loc[rank, 'rank'] = rank\n        sv_tax.loc[rank, 'rank_depth'] = rank_depth\n        sv_tax.loc[rank, 'tax_name'] = tax_name\n        sv_tax.loc[rank, 'ncbi_tax_id'] = ncbi_tax_id\n        sv_tax.loc[rank, 'lineage'] = lineage\n        sv_tax.loc[rank, 'afract'] = afract\n        sv_tax.loc[rank, 'ambiguous'] = len(sv_depth) != 1\n    sv_tax_list.append(sv_tax)\n\n\nsv_taxonomy = pd.concat(sv_tax_list, ignore_index=True)\nsv_taxonomy['rank_depth'] = sv_taxonomy.rank_depth.astype(int)\nsv_taxonomy.to_csv('sv_taxonomy.csv', index=None)\n\n\"\"\"",
        "nb_lignes_script": 59,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "gappa_taxonomy",
            "refpkg_taxtable"
        ],
        "nb_inputs": 2,
        "outputs": [
            "refpkg_taxtable"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2pplacer}\"",
            "label 'io_mem'",
            "publishDir \"${params.output}/classify\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "EDPL": {
        "name_process": "EDPL",
        "string_process": "\nprocess EDPL {\n    container = \"${container__gappa}\"\n    label = 'multithread'\n    publishDir \"${params.output}/placement\", mode: 'copy'\n    errorStrategy 'ignore' \n\n    input:\n        path dedup_jplace\n    \n    output:\n        path 'edpl_list.csv'\n\n    \"\"\"\n    set -e\n\n\n    gappa examine edpl \\\n    --jplace-path ${dedup_jplace} \\\n    --verbose \\\n    --threads ${task.cpus}\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\n    set -e\n\n\n    gappa examine edpl \\\n    --jplace-path ${dedup_jplace} \\\n    --verbose \\\n    --threads ${task.cpus}\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "GAPPA"
        ],
        "tools_url": [
            "https://bio.tools/GAPPA"
        ],
        "tools_dico": [
            {
                "name": "GAPPA",
                "uri": "https://bio.tools/GAPPA",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0325",
                                    "term": "Phylogenetic tree comparison"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic tree reconstruction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Collection of commands for working with phylogenetic data.",
                "homepage": "http://github.com/lczech/gappa"
            }
        ],
        "inputs": [
            "dedup_jplace"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__gappa}\"",
            "label = 'multithread'",
            "publishDir \"${params.output}/placement\", mode: 'copy'",
            "errorStrategy 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "MakeSplit": {
        "name_process": "MakeSplit",
        "string_process": "\nprocess MakeSplit {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/sv\", mode: 'copy'\n\n    input:\n        path sv_long_f\n    output:\n        path 'sv_multiplicity.csv'\n\n\"\"\"\n#!/usr/bin/env python\nimport csv\n\nwith open('${sv_long_f}', 'rt') as in_h, open('sv_multiplicity.csv', 'wt') as out_h:\n    svl = csv.DictReader(in_h)\n    t_svl = csv.writer(\n        out_h,\n    )\n    for r in svl:\n        t_svl.writerow([\n            r['sv'],\n            r['specimen'],\n            r['count']\n        ])\n\"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport csv\n\nwith open('${sv_long_f}', 'rt') as in_h, open('sv_multiplicity.csv', 'wt') as out_h:\n    svl = csv.DictReader(in_h)\n    t_svl = csv.writer(\n        out_h,\n    )\n    for r in svl:\n        t_svl.writerow([\n            r['sv'],\n            r['specimen'],\n            r['count']\n        ])\n\"\"\"",
        "nb_lignes_script": 15,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sv_long_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/sv\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "GappaSplit": {
        "name_process": "GappaSplit",
        "string_process": "\nprocess GappaSplit {\n    container = \"${container__gappa}\"\n    label = 'multithread'\n    publishDir \"${params.output}/placement/\", mode: 'copy'\n\n    input:\n        path dedup_jplace\n        path split_csv\n    \n    output:\n        path 'specimen_jplace/*.jplace.gz'\n\n    \"\"\"\n    set -e\n\n    mkdir specimen_jplace\n\n    gappa edit split \\\n    --jplace-path ${dedup_jplace} \\\n    --split-file ${split_csv} \\\n    --compress \\\n    --verbose \\\n    --threads ${task.cpus} \\\n    --out-dir specimen_jplace\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n    set -e\n\n    mkdir specimen_jplace\n\n    gappa edit split \\\n    --jplace-path ${dedup_jplace} \\\n    --split-file ${split_csv} \\\n    --compress \\\n    --verbose \\\n    --threads ${task.cpus} \\\n    --out-dir specimen_jplace\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "GAPPA"
        ],
        "tools_url": [
            "https://bio.tools/GAPPA"
        ],
        "tools_dico": [
            {
                "name": "GAPPA",
                "uri": "https://bio.tools/GAPPA",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0325",
                                    "term": "Phylogenetic tree comparison"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic tree reconstruction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Collection of commands for working with phylogenetic data.",
                "homepage": "http://github.com/lczech/gappa"
            }
        ],
        "inputs": [
            "dedup_jplace",
            "split_csv"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__gappa}\"",
            "label = 'multithread'",
            "publishDir \"${params.output}/placement/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "Gappa_KRD": {
        "name_process": "Gappa_KRD",
        "string_process": "\nprocess Gappa_KRD {\n    container = \"${container__gappa}\"\n    label = 'mem_veryhigh'\n    publishDir \"${params.output}/placement/\", mode: 'copy'\n    errorStrategy 'ignore'\n\n    input:\n        path specimen_jplace\n    \n    output:\n        path 'krd/krd_matrix.csv.gz'\n\n    \"\"\"\n    set -e\n\n    gappa analyze krd \\\n    --jplace-path ${specimen_jplace} \\\n    --krd-out-dir krd/ \\\n    --krd-compress \\\n    --verbose \\\n    --threads ${task.cpus}\n\n    \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "\"\"\"\n    set -e\n\n    gappa analyze krd \\\n    --jplace-path ${specimen_jplace} \\\n    --krd-out-dir krd/ \\\n    --krd-compress \\\n    --verbose \\\n    --threads ${task.cpus}\n\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [
            "GAPPA"
        ],
        "tools_url": [
            "https://bio.tools/GAPPA"
        ],
        "tools_dico": [
            {
                "name": "GAPPA",
                "uri": "https://bio.tools/GAPPA",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0325",
                                    "term": "Phylogenetic tree comparison"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic tree reconstruction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Collection of commands for working with phylogenetic data.",
                "homepage": "http://github.com/lczech/gappa"
            }
        ],
        "inputs": [
            "specimen_jplace"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__gappa}\"",
            "label = 'mem_veryhigh'",
            "publishDir \"${params.output}/placement/\", mode: 'copy'",
            "errorStrategy 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "Gappa_ePCA": {
        "name_process": "Gappa_ePCA",
        "string_process": "\nprocess Gappa_ePCA {\n    container = \"${container__gappa}\"\n    label = 'mem_veryhigh'\n    publishDir \"${params.output}/placement/\", mode: 'copy'\n    errorStrategy 'ignore'\n\n    input:\n        path specimen_jplace\n    \n    output:\n        path 'ePCA/projection.csv'\n        path 'ePCA/transformation.csv'\n\n    \"\"\"\n    set -e\n\n    gappa analyze edgepca \\\n    --jplace-path ${specimen_jplace} \\\n    --out-dir ePCA/ \\\n    --verbose \\\n    --threads ${task.cpus}\n\n    ls -l ePCA\n\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n    set -e\n\n    gappa analyze edgepca \\\n    --jplace-path ${specimen_jplace} \\\n    --out-dir ePCA/ \\\n    --verbose \\\n    --threads ${task.cpus}\n\n    ls -l ePCA\n\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "GAPPA"
        ],
        "tools_url": [
            "https://bio.tools/GAPPA"
        ],
        "tools_dico": [
            {
                "name": "GAPPA",
                "uri": "https://bio.tools/GAPPA",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0325",
                                    "term": "Phylogenetic tree comparison"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic tree reconstruction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Collection of commands for working with phylogenetic data.",
                "homepage": "http://github.com/lczech/gappa"
            }
        ],
        "inputs": [
            "specimen_jplace"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__gappa}\"",
            "label = 'mem_veryhigh'",
            "publishDir \"${params.output}/placement/\", mode: 'copy'",
            "errorStrategy 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "CombineSpAd": {
        "name_process": "CombineSpAd",
        "string_process": "\nprocess CombineSpAd {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/placement\", mode: 'copy'\n\n    input:\n        path specimen_ad_csv\n    \n    output:\n        path \"alpha_diversity.csv.gz\"\n    \n\"\"\"\n#!/usr/bin/env python\nimport csv\nimport gzip\nfiles_to_combine = \"${specimen_ad_csv}\".split()\ncombined_data = [\n    row\n    for fn in files_to_combine\n    for row in csv.DictReader(\n        open(fn, 'rt')\n    )\n]\n\nwith gzip.open('alpha_diversity.csv.gz', 'wt') as out_h:\n    out_w = csv.DictWriter(out_h,fieldnames=combined_data[0].keys())\n    out_w.writeheader()\n    out_w.writerows(combined_data)\n\n\"\"\"\n\n}",
        "nb_lignes_process": 31,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport csv\nimport gzip\nfiles_to_combine = \"${specimen_ad_csv}\".split()\ncombined_data = [\n    row\n    for fn in files_to_combine\n    for row in csv.DictReader(\n        open(fn, 'rt')\n    )\n]\n\nwith gzip.open('alpha_diversity.csv.gz', 'wt') as out_h:\n    out_w = csv.DictWriter(out_h,fieldnames=combined_data[0].keys())\n    out_w.writeheader()\n    out_w.writerows(combined_data)\n\n\"\"\"",
        "nb_lignes_script": 18,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen_ad_csv"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/placement\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "WeightMaptoLong": {
        "name_process": "WeightMaptoLong",
        "string_process": "\nprocess WeightMaptoLong {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/sv\", mode: 'copy'\n\n    input:\n        path (weight)\n        path (map)\n\n    output:\n        path (\"sp_sv_long.csv\")\n\n\"\"\"\n#!/usr/bin/env python\nimport csv\n\nspecimen_comSV = {\n    r[0]: r[1]\n    for r in \n    csv.reader(\n        open('${map}', 'rt')\n    )\n}\nwith open('${weight}', 'rt') as w_h, open(\"sp_sv_long.csv\", 'wt') as sv_long_h:\n    w_r = csv.reader(w_h)\n    svl_w = csv.writer(sv_long_h)\n    svl_w.writerow((\n        'specimen',\n        'sv',\n        'count'\n    ))    \n    for row in w_r:\n        svl_w.writerow((\n            specimen_comSV[row[1]],\n            row[0],\n            int(row[2])\n        ))\n    \n\"\"\"\n}",
        "nb_lignes_process": 39,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport csv\n\nspecimen_comSV = {\n    r[0]: r[1]\n    for r in \n    csv.reader(\n        open('${map}', 'rt')\n    )\n}\nwith open('${weight}', 'rt') as w_h, open(\"sp_sv_long.csv\", 'wt') as sv_long_h:\n    w_r = csv.reader(w_h)\n    svl_w = csv.writer(sv_long_h)\n    svl_w.writerow((\n        'specimen',\n        'sv',\n        'count'\n    ))    \n    for row in w_r:\n        svl_w.writerow((\n            specimen_comSV[row[1]],\n            row[0],\n            int(row[2])\n        ))\n    \n\"\"\"",
        "nb_lignes_script": 26,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "weight",
            "map"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/sv\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "TrimGalore": {
        "name_process": "TrimGalore",
        "string_process": "\nprocess TrimGalore {\n    container \"${container__trimgalore}\"\n    label 'io_limited'\n    errorStrategy 'ignore'\n\n    input:\n    tuple val(specimen), val(batch), file(R1), file(R2)\n\n    output:\n    tuple val(specimen), val(batch), file(\"${specimen}.R1.tg.fastq.gz\"), file(\"${specimen}.R2.tg.fastq.gz\")\n\n    \"\"\"\n    set -e\n\n    cp ${R1} R1.fastq.gz\n    cp ${R2} R2.fastq.gz\n\n    trim_galore \\\n    --gzip \\\n    --cores ${task.cpus} \\\n    --paired \\\n    R1.fastq.gz R2.fastq.gz\n\n    rm R1.fastq.gz\n    rm R2.fastq.gz\n    mv R1_val_1.fq.gz \"${specimen}.R1.tg.fastq.gz\"\n    mv R2_val_2.fq.gz \"${specimen}.R2.tg.fastq.gz\"\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "\"\"\"\n    set -e\n\n    cp ${R1} R1.fastq.gz\n    cp ${R2} R2.fastq.gz\n\n    trim_galore \\\n    --gzip \\\n    --cores ${task.cpus} \\\n    --paired \\\n    R1.fastq.gz R2.fastq.gz\n\n    rm R1.fastq.gz\n    rm R2.fastq.gz\n    mv R1_val_1.fq.gz \"${specimen}.R1.tg.fastq.gz\"\n    mv R2_val_2.fq.gz \"${specimen}.R2.tg.fastq.gz\"\n    \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1",
            "R2"
        ],
        "nb_inputs": 4,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__trimgalore}\"",
            "label 'io_limited'",
            "errorStrategy 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "barcodecop": {
        "name_process": "barcodecop",
        "string_process": "\nprocess barcodecop {\n    container \"${container__barcodecop}\"\n    label 'io_limited'\n    errorStrategy \"ignore\"\n\n    input:\n    tuple val(specimen), val(batch), file(R1), file(R2), file(I1), file(I2)\n    \n    output:\n    tuple val(specimen), val(batch), file(\"${R1.getSimpleName()}.bcc.fq.gz\"), file(\"${R2.getSimpleName()}.bcc.fq.gz\")\n\n    \"\"\"\n    set -e\n\n    barcodecop \\\n    ${I1} ${I2} \\\n    --match-filter \\\n    -f ${R1} \\\n    -o ${R1.getSimpleName()}.bcc.fq.gz &&\n    barcodecop \\\n    ${I1} ${I2} \\\n    --match-filter \\\n    -f ${R2} \\\n    -o ${R2.getSimpleName()}.bcc.fq.gz\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n    set -e\n\n    barcodecop \\\n    ${I1} ${I2} \\\n    --match-filter \\\n    -f ${R1} \\\n    -o ${R1.getSimpleName()}.bcc.fq.gz &&\n    barcodecop \\\n    ${I1} ${I2} \\\n    --match-filter \\\n    -f ${R2} \\\n    -o ${R2.getSimpleName()}.bcc.fq.gz\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1",
            "R2",
            "I1",
            "I2"
        ],
        "nb_inputs": 6,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__barcodecop}\"",
            "label 'io_limited'",
            "errorStrategy \"ignore\""
        ],
        "when": "",
        "stub": ""
    },
    "TrimGaloreSE": {
        "name_process": "TrimGaloreSE",
        "string_process": "\nprocess TrimGaloreSE {\n    container \"${container__trimgalore}\"\n    label 'io_limited'\n    errorStrategy 'ignore'\n\n    input:\n    tuple val(specimen), val(batch), file(R1)\n\n    output:\n    tuple val(specimen), val(batch), file(\"${specimen}.R1.tg.fastq.gz\")\n\n    \"\"\"\n    set -e\n\n    cp ${R1} R1.fastq.gz\n\n    trim_galore \\\n    --gzip \\\n    --cores ${task.cpus} \\\n    R1.fastq.gz\n\n    rm R1.fastq.gz\n    mv R1_trimmed.fq.gz \"${specimen}.R1.tg.fastq.gz\"\n    \"\"\"\n}",
        "nb_lignes_process": 24,
        "string_script": "\"\"\"\n    set -e\n\n    cp ${R1} R1.fastq.gz\n\n    trim_galore \\\n    --gzip \\\n    --cores ${task.cpus} \\\n    R1.fastq.gz\n\n    rm R1.fastq.gz\n    mv R1_trimmed.fq.gz \"${specimen}.R1.tg.fastq.gz\"\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1"
        ],
        "nb_inputs": 3,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__trimgalore}\"",
            "label 'io_limited'",
            "errorStrategy 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "output_failed": {
        "name_process": "output_failed",
        "string_process": "\nprocess output_failed {\n    container \"${container__barcodecop}\"\n    label 'io_limited'\n    publishDir \"${params.output}/sv/\", mode: 'copy'\n    errorStrategy 'retry'\n\n    input:\n        tuple val(specimens), val(reasons)\n    output:\n        file (\"failed_specimens.csv\")\n\n    \"\"\"\n    #!/usr/bin/env python\n    import csv\n    import re\n    specimens = re.sub(r'\\\\[|\\\\]', \"\", \"${specimens}\").split(',')\n    reasons = re.sub(r'\\\\[|\\\\]', \"\", \"${reasons}\").split(',')\n\n    with open(\"failed_specimens.csv\", 'wt') as out_h:\n        w = csv.writer(out_h)\n        w.writerow([\n            'specimen',\n            'reason'\n        ])\n        for sp, reason in zip(specimens, reasons):\n            w.writerow([sp.strip(), reason.strip()])\n    \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "\"\"\"\n    #!/usr/bin/env python\n    import csv\n    import re\n    specimens = re.sub(r'\\\\[|\\\\]', \"\", \"${specimens}\").split(',')\n    reasons = re.sub(r'\\\\[|\\\\]', \"\", \"${reasons}\").split(',')\n\n    with open(\"failed_specimens.csv\", 'wt') as out_h:\n        w = csv.writer(out_h)\n        w.writerow([\n            'specimen',\n            'reason'\n        ])\n        for sp, reason in zip(specimens, reasons):\n            w.writerow([sp.strip(), reason.strip()])\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimens",
            "reasons"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__barcodecop}\"",
            "label 'io_limited'",
            "publishDir \"${params.output}/sv/\", mode: 'copy'",
            "errorStrategy 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "dada2_ft": {
        "name_process": "dada2_ft",
        "string_process": "\nprocess dada2_ft {\n    container \"${container__dada2}\"\n    label 'io_mem'\n                            \n    errorStrategy \"ignore\"\n\n    input:\n        tuple val(specimen), val(batch), file(R1), file(R2)\n    \n    output:\n        tuple val(specimen), val(batch), file(\"${R1.getSimpleName()}.R1.dada2.ft.fq.gz\"), file(\"${R2.getSimpleName()}.R2.dada2.ft.fq.gz\")\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2'); \n    filterAndTrim(\n        '${R1}', '${R1.getSimpleName()}.R1.dada2.ft.fq.gz',\n        '${R2}', '${R2.getSimpleName()}.R2.dada2.ft.fq.gz',\n        trimLeft = ${params.trimLeft},\n        maxN = ${params.maxN},\n        maxEE = ${params.maxEE},\n        truncLen = c(${params.truncLenF}, ${params.truncLenR}),\n        truncQ = ${params.truncQ},\n        compress = TRUE,\n        verbose = TRUE,\n        multithread = ${task.cpus}\n    )\n    \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2'); \n    filterAndTrim(\n        '${R1}', '${R1.getSimpleName()}.R1.dada2.ft.fq.gz',\n        '${R2}', '${R2.getSimpleName()}.R2.dada2.ft.fq.gz',\n        trimLeft = ${params.trimLeft},\n        maxN = ${params.maxN},\n        maxEE = ${params.maxEE},\n        truncLen = c(${params.truncLenF}, ${params.truncLenR}),\n        truncQ = ${params.truncQ},\n        compress = TRUE,\n        verbose = TRUE,\n        multithread = ${task.cpus}\n    )\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1",
            "R2"
        ],
        "nb_inputs": 4,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"ignore\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_ft_se": {
        "name_process": "dada2_ft_se",
        "string_process": "\nprocess dada2_ft_se {\n    container \"${container__dada2}\"\n    label 'io_mem'\n                            \n    errorStrategy \"ignore\"\n\n    input:\n        tuple val(specimen), val(batch), file(R1)\n    \n    output:\n        tuple val(specimen), val(batch), file(\"${R1.getSimpleName()}.dada2.ft.fq.gz\")\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2'); \n    filterAndTrim(\n        '${R1}', '${R1.getSimpleName()}.dada2.ft.fq.gz',\n        trimLeft = ${params.trimLeft},\n        maxN = ${params.maxN},\n        maxEE = ${params.maxEE},\n        truncLen = ${params.truncLenF_se},\n        truncQ = ${params.truncQ},\n        compress = TRUE,\n        verbose = TRUE,\n        multithread = ${task.cpus}\n    )\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2'); \n    filterAndTrim(\n        '${R1}', '${R1.getSimpleName()}.dada2.ft.fq.gz',\n        trimLeft = ${params.trimLeft},\n        maxN = ${params.maxN},\n        maxEE = ${params.maxEE},\n        truncLen = ${params.truncLenF_se},\n        truncQ = ${params.truncQ},\n        compress = TRUE,\n        verbose = TRUE,\n        multithread = ${task.cpus}\n    )\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1"
        ],
        "nb_inputs": 3,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"ignore\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_ft_pyro": {
        "name_process": "dada2_ft_pyro",
        "string_process": "\nprocess dada2_ft_pyro {\n    container \"${container__dada2}\"\n    label 'io_mem'\n                            \n    errorStrategy \"ignore\"\n\n    input:\n        tuple val(specimen), val(batch), file(R1)\n    \n    output:\n        tuple val(specimen), val(batch), file(\"${R1.getSimpleName()}.dada2.ft.fq.gz\")\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2'); \n    filterAndTrim(\n        '${R1}', '${R1.getSimpleName()}.dada2.ft.fq.gz',\n        truncLen = ${params.truncLenF_pyro},\n        maxLen = ${params.maxLenPyro},\n        maxN = ${params.maxN},\n        maxEE = ${params.maxEE},\n        truncQ = ${params.truncQ},\n        compress = TRUE,\n        verbose = TRUE,\n        multithread = ${task.cpus}\n    )\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2'); \n    filterAndTrim(\n        '${R1}', '${R1.getSimpleName()}.dada2.ft.fq.gz',\n        truncLen = ${params.truncLenF_pyro},\n        maxLen = ${params.maxLenPyro},\n        maxN = ${params.maxN},\n        maxEE = ${params.maxEE},\n        truncQ = ${params.truncQ},\n        compress = TRUE,\n        verbose = TRUE,\n        multithread = ${task.cpus}\n    )\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1"
        ],
        "nb_inputs": 3,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"ignore\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_derep": {
        "name_process": "dada2_derep",
        "string_process": "\nprocess dada2_derep {\n    container \"${container__dada2}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(specimen), val(batch), file(R1), file(R2)\n    \n    output:\n        tuple val(specimen), val(batch), file(\"${specimen}.R1.dada2.ft.derep.rds\"), file(\"${specimen}.R2.dada2.ft.derep.rds\")\n    \n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_1 <- derepFastq('${R1}');\n    saveRDS(derep_1, '${specimen}.R1.dada2.ft.derep.rds');\n    derep_2 <- derepFastq('${R2}');\n    saveRDS(derep_2, '${specimen}.R2.dada2.ft.derep.rds');\n    \"\"\" \n}",
        "nb_lignes_process": 19,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_1 <- derepFastq('${R1}');\n    saveRDS(derep_1, '${specimen}.R1.dada2.ft.derep.rds');\n    derep_2 <- derepFastq('${R2}');\n    saveRDS(derep_2, '${specimen}.R2.dada2.ft.derep.rds');\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1",
            "R2"
        ],
        "nb_inputs": 4,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_derep_se": {
        "name_process": "dada2_derep_se",
        "string_process": "\nprocess dada2_derep_se {\n    container \"${container__dada2}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(specimen), val(batch), file(R1)\n    \n    output:\n        tuple val(specimen), val(batch), file(\"${specimen}.dada2.ft.derep.rds\")\n    \n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_1 <- derepFastq('${R1}');\n    saveRDS(derep_1, '${specimen}.dada2.ft.derep.rds');\n    \"\"\" \n}",
        "nb_lignes_process": 17,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_1 <- derepFastq('${R1}');\n    saveRDS(derep_1, '${specimen}.dada2.ft.derep.rds');\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1"
        ],
        "nb_inputs": 3,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_derep_pyro": {
        "name_process": "dada2_derep_pyro",
        "string_process": "\nprocess dada2_derep_pyro {\n    container \"${container__dada2}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(specimen), val(batch), file(R1)\n    \n    output:\n        tuple val(specimen), val(batch), file(\"${specimen}.dada2.ft.derep.rds\")\n    \n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_1 <- derepFastq('${R1}');\n    saveRDS(derep_1, '${specimen}.dada2.ft.derep.rds');\n    \"\"\" \n}",
        "nb_lignes_process": 17,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_1 <- derepFastq('${R1}');\n    saveRDS(derep_1, '${specimen}.dada2.ft.derep.rds');\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1"
        ],
        "nb_inputs": 3,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_learn_error": {
        "name_process": "dada2_learn_error",
        "string_process": "\nprocess dada2_learn_error {\n    container \"${container__dada2}\"\n    label 'multithread'\n    errorStrategy \"finish\"\n    publishDir \"${params.output}/sv/errM/${batch}\", mode: 'copy'\n\n    input:\n        tuple val(batch_specimens), val(batch), file(reads), val(read_num)\n\n    output:\n        tuple val(batch), val(read_num), file(\"${batch}.${read_num}.errM.rds\"), file(\"${batch}.${read_num}.errM.csv\")\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    err <- learnErrors(\n        unlist(strsplit(\n            '${reads}',\n            ' ',\n            fixed=TRUE\n        )),\n        multithread=${task.cpus},\n        MAX_CONSIST=${params.errM_maxConsist},\n        randomize=${params.errM_randomize},\n        nbases=${params.errM_nbases},\n        verbose=TRUE\n    );\n    saveRDS(err, \"${batch}.${read_num}.errM.rds\"); \n    write.csv(err, \"${batch}.${read_num}.errM.csv\");\n    \"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    err <- learnErrors(\n        unlist(strsplit(\n            '${reads}',\n            ' ',\n            fixed=TRUE\n        )),\n        multithread=${task.cpus},\n        MAX_CONSIST=${params.errM_maxConsist},\n        randomize=${params.errM_randomize},\n        nbases=${params.errM_nbases},\n        verbose=TRUE\n    );\n    saveRDS(err, \"${batch}.${read_num}.errM.rds\"); \n    write.csv(err, \"${batch}.${read_num}.errM.csv\");\n    \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch_specimens",
            "batch",
            "read_num",
            "reads"
        ],
        "nb_inputs": 4,
        "outputs": [
            "read_num"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'multithread'",
            "errorStrategy \"finish\"",
            "publishDir \"${params.output}/sv/errM/${batch}\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "dada2_learn_error_pyro": {
        "name_process": "dada2_learn_error_pyro",
        "string_process": "\nprocess dada2_learn_error_pyro {\n    container \"${container__dada2}\"\n    label 'multithread'\n    errorStrategy \"finish\"\n    publishDir \"${params.output}/sv/errM/${batch}\", mode: 'copy'\n\n    input:\n        tuple val(batch_specimens), val(batch), file(reads), val(read_num)\n\n    output:\n        tuple val(batch), val(read_num), file(\"${batch}.${read_num}.errM.rds\"), file(\"${batch}.${read_num}.errM.csv\")\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    err <- learnErrors(\n        unlist(strsplit(\n            '${reads}',\n            ' ',\n            fixed=TRUE\n        )),\n        multithread=${task.cpus},\n        HOMOPOLYMER_GAP_PENALTY=-1, BAND_SIZE=32,\n        MAX_CONSIST=${params.errM_maxConsist},\n        randomize=${params.errM_randomize},\n        nbases=${params.errM_nbases},\n        verbose=TRUE\n    );\n    saveRDS(err, \"${batch}.${read_num}.errM.rds\"); \n    write.csv(err, \"${batch}.${read_num}.errM.csv\");\n    \"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    err <- learnErrors(\n        unlist(strsplit(\n            '${reads}',\n            ' ',\n            fixed=TRUE\n        )),\n        multithread=${task.cpus},\n        HOMOPOLYMER_GAP_PENALTY=-1, BAND_SIZE=32,\n        MAX_CONSIST=${params.errM_maxConsist},\n        randomize=${params.errM_randomize},\n        nbases=${params.errM_nbases},\n        verbose=TRUE\n    );\n    saveRDS(err, \"${batch}.${read_num}.errM.rds\"); \n    write.csv(err, \"${batch}.${read_num}.errM.csv\");\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch_specimens",
            "batch",
            "read_num",
            "reads"
        ],
        "nb_inputs": 4,
        "outputs": [
            "read_num"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'multithread'",
            "errorStrategy \"finish\"",
            "publishDir \"${params.output}/sv/errM/${batch}\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "dada2_derep_batches": {
        "name_process": "dada2_derep_batches",
        "string_process": "\nprocess dada2_derep_batches {\n    container \"${container__dada2}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(batch), val(read_num), val(specimens), file(dereps), val(errM), val(dada_fns)\n    \n    output:\n        tuple val(batch), val(read_num), val(specimens), file(\"${batch}_${read_num}_derep.rds\"), val(errM), val(dada_fns)\n    \n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_str <- trimws('${dereps}')\n    print(derep_str)\n    derep <- lapply(\n        unlist(strsplit(derep_str, ' ', fixed=TRUE)),\n        readRDS\n    );\n    saveRDS(derep, \"${batch}_${read_num}_derep.rds\")\n    \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_str <- trimws('${dereps}')\n    print(derep_str)\n    derep <- lapply(\n        unlist(strsplit(derep_str, ' ', fixed=TRUE)),\n        readRDS\n    );\n    saveRDS(derep, \"${batch}_${read_num}_derep.rds\")\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch",
            "read_num",
            "specimens",
            "errM",
            "dada_fns",
            "dereps"
        ],
        "nb_inputs": 6,
        "outputs": [
            "dada_fns"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_derep_batches_pyro": {
        "name_process": "dada2_derep_batches_pyro",
        "string_process": "\nprocess dada2_derep_batches_pyro {\n    container \"${container__dada2}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(batch), val(read_num), val(specimens), file(dereps), val(errM), val(dada_fns)\n    \n    output:\n        tuple val(batch), val(read_num), val(specimens), file(\"${batch}_${read_num}_derep.rds\"), val(errM), val(dada_fns)\n    \n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep <- lapply(\n        unlist(strsplit('${dereps}', ' ', fixed=TRUE)),\n        readRDS\n    );\n    saveRDS(derep, \"${batch}_${read_num}_derep.rds\")\n    \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep <- lapply(\n        unlist(strsplit('${dereps}', ' ', fixed=TRUE)),\n        readRDS\n    );\n    saveRDS(derep, \"${batch}_${read_num}_derep.rds\")\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch",
            "read_num",
            "specimens",
            "errM",
            "dada_fns",
            "dereps"
        ],
        "nb_inputs": 6,
        "outputs": [
            "dada_fns"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_dada": {
        "name_process": "dada2_dada",
        "string_process": "\nprocess dada2_dada {\n    container \"${container__dada2}\"\n    label 'multithread'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(batch), val(read_num), val(specimens), file(derep), file(errM), val(dada_fns)\n\n    output:\n        tuple val(batch), val(read_num), val(specimens), file(\"${batch}_${read_num}_dada.rds\"), val(dada_fns)\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    errM <- readRDS('${errM}');\n    derep <- readRDS('${derep}');\n    dadaResult <- dada(derep, err=errM, multithread=${task.cpus}, verbose=TRUE, pool=\"pseudo\");\n    saveRDS(dadaResult, \"${batch}_${read_num}_dada.rds\");\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    errM <- readRDS('${errM}');\n    derep <- readRDS('${derep}');\n    dadaResult <- dada(derep, err=errM, multithread=${task.cpus}, verbose=TRUE, pool=\"pseudo\");\n    saveRDS(dadaResult, \"${batch}_${read_num}_dada.rds\");\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch",
            "read_num",
            "specimens",
            "dada_fns",
            "derep",
            "errM"
        ],
        "nb_inputs": 6,
        "outputs": [
            "dada_fns"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'multithread'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_dada_pyro": {
        "name_process": "dada2_dada_pyro",
        "string_process": "\nprocess dada2_dada_pyro {\n    container \"${container__dada2}\"\n    label 'multithread'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(batch), val(read_num), val(specimens), file(derep), file(errM), val(dada_fns)\n\n    output:\n        tuple val(batch), val(read_num), val(specimens), file(\"${batch}_${read_num}_dada.rds\"), val(dada_fns)\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    errM <- readRDS('${errM}');\n    derep <- readRDS('${derep}');\n    dadaResult <- dada(derep, err=errM, multithread=${task.cpus}, verbose=TRUE, pool=\"pseudo\",  HOMOPOLYMER_GAP_PENALTY=-1, BAND_SIZE=32);\n    saveRDS(dadaResult, \"${batch}_${read_num}_dada.rds\");\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    errM <- readRDS('${errM}');\n    derep <- readRDS('${derep}');\n    dadaResult <- dada(derep, err=errM, multithread=${task.cpus}, verbose=TRUE, pool=\"pseudo\",  HOMOPOLYMER_GAP_PENALTY=-1, BAND_SIZE=32);\n    saveRDS(dadaResult, \"${batch}_${read_num}_dada.rds\");\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch",
            "read_num",
            "specimens",
            "dada_fns",
            "derep",
            "errM"
        ],
        "nb_inputs": 6,
        "outputs": [
            "dada_fns"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'multithread'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_demultiplex_dada": {
        "name_process": "dada2_demultiplex_dada",
        "string_process": "\nprocess dada2_demultiplex_dada {\n    container \"${container__dada2}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(batch), val(read_num), val(specimens), file(dada), val(dada_fns)\n    \n    output:\n        tuple val(batch), val(read_num), val(specimens), file(dada_fns)\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    dadaResult <- readRDS('${dada}');\n    dada_names <- unlist(strsplit(\n        gsub('(\\\\\\\\[|\\\\\\\\])', \"\", \"${dada_fns}\"),\n        \", \"));\n    print(dada_names);\n    sapply(1:length(dadaResult), function(i) {\n        saveRDS(dadaResult[i], dada_names[i]);\n    });\n    \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    dadaResult <- readRDS('${dada}');\n    dada_names <- unlist(strsplit(\n        gsub('(\\\\\\\\[|\\\\\\\\])', \"\", \"${dada_fns}\"),\n        \", \"));\n    print(dada_names);\n    sapply(1:length(dadaResult), function(i) {\n        saveRDS(dadaResult[i], dada_names[i]);\n    });\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch",
            "read_num",
            "specimens",
            "dada_fns",
            "dada"
        ],
        "nb_inputs": 5,
        "outputs": [
            "specimens"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_merge": {
        "name_process": "dada2_merge",
        "string_process": "\nprocess dada2_merge {\n    container \"${container__dada2}\"\n    label 'multithread'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(specimen), val(batch), file(\"R1.dada2.rds\"), file(\"R2.dada2.rds\"), file(\"R1.derep.rds\"), file(\"R2.derep.rds\")\n\n    output:\n        tuple val(batch), val(specimen), file(\"${specimen}.dada2.merged.rds\")\n    \n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    dada_1 <- readRDS('R1.dada2.rds');\n    derep_1 <- readRDS('R1.derep.rds');\n    dada_2 <- readRDS('R2.dada2.rds');\n    derep_2 <- readRDS('R2.derep.rds');        \n    merger <- mergePairs(\n        dada_1, derep_1,\n        dada_2, derep_2,\n        verbose=TRUE,\n        trimOverhang=TRUE,\n        maxMismatch=${params.maxMismatch},\n        minOverlap=${params.minOverlap}\n    );\n    saveRDS(merger, \"${specimen}.dada2.merged.rds\");\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    dada_1 <- readRDS('R1.dada2.rds');\n    derep_1 <- readRDS('R1.derep.rds');\n    dada_2 <- readRDS('R2.dada2.rds');\n    derep_2 <- readRDS('R2.derep.rds');        \n    merger <- mergePairs(\n        dada_1, derep_1,\n        dada_2, derep_2,\n        verbose=TRUE,\n        trimOverhang=TRUE,\n        maxMismatch=${params.maxMismatch},\n        minOverlap=${params.minOverlap}\n    );\n    saveRDS(merger, \"${specimen}.dada2.merged.rds\");\n    \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch"
        ],
        "nb_inputs": 2,
        "outputs": [
            "specimen"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'multithread'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_seqtab_sp": {
        "name_process": "dada2_seqtab_sp",
        "string_process": "\nprocess dada2_seqtab_sp {\n    container \"${container__dada2}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(batch), val(specimen), file(merged)\n\n    output:\n        tuple val(batch), val(specimen), file(\"${specimen}.dada2.seqtab.rds\")\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    merged <- readRDS('${merged}');\n    seqtab <- makeSequenceTable(merged);\n    rownames(seqtab) <- c('${specimen}');\n    saveRDS(seqtab, '${specimen}.dada2.seqtab.rds');\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    merged <- readRDS('${merged}');\n    seqtab <- makeSequenceTable(merged);\n    rownames(seqtab) <- c('${specimen}');\n    saveRDS(seqtab, '${specimen}.dada2.seqtab.rds');\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch",
            "specimen",
            "merged"
        ],
        "nb_inputs": 3,
        "outputs": [
            "specimen"
        ],
        "nb_outputs": 1,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_seqtab_combine_batch": {
        "name_process": "dada2_seqtab_combine_batch",
        "string_process": "\nprocess dada2_seqtab_combine_batch {\n    container \"${container__fastcombineseqtab}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(batch), file(sp_seqtabs_rds)\n\n    output:\n        file(\"${batch}.dada2.seqtabs.rds\")\n\n    \"\"\"\n    set -e\n\n    combine_seqtab \\\n    --rds ${batch}.dada2.seqtabs.rds \\\n    --seqtabs ${sp_seqtabs_rds}\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    set -e\n\n    combine_seqtab \\\n    --rds ${batch}.dada2.seqtabs.rds \\\n    --seqtabs ${sp_seqtabs_rds}\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch",
            "sp_seqtabs_rds"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__fastcombineseqtab}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_seqtab_combine_all": {
        "name_process": "dada2_seqtab_combine_all",
        "string_process": "\nprocess dada2_seqtab_combine_all {\n    container \"${container__fastcombineseqtab}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        file(seqtabs_rds)\n\n    output:\n        file(\"combined.dada2.seqtabs.rds\")\n\n    \"\"\"\n    set -e\n\n    combine_seqtab \\\n    --rds combined.dada2.seqtabs.rds \\\n    --seqtabs ${seqtabs_rds}\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    set -e\n\n    combine_seqtab \\\n    --rds combined.dada2.seqtabs.rds \\\n    --seqtabs ${seqtabs_rds}\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "seqtabs_rds"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__fastcombineseqtab}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_remove_bimera": {
        "name_process": "dada2_remove_bimera",
        "string_process": "\nprocess dada2_remove_bimera {\n    container \"${container__dada2}\"\n    label 'mem_veryhigh'\n    errorStrategy \"finish\"\n    publishDir \"${params.output}/sv/\", mode: 'copy'\n\n    input:\n        file(combined_seqtab)\n\n    output:\n        file(\"dada2.combined.seqtabs.nochimera.csv\")\n        file(\"dada2.combined.seqtabs.nochimera.rds\")\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    seqtab <- readRDS('${combined_seqtab}');\n    seqtab_nochim <- removeBimeraDenovo(\n        seqtab,\n        method = '${params.chimera_method}',\n        multithread = ${task.cpus}\n    );\n    saveRDS(seqtab_nochim, 'dada2.combined.seqtabs.nochimera.rds'); \n    write.csv(seqtab_nochim, 'dada2.combined.seqtabs.nochimera.csv', na='');\n    print((sum(seqtab) - sum(seqtab_nochim)) / sum(seqtab));\n    \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    seqtab <- readRDS('${combined_seqtab}');\n    seqtab_nochim <- removeBimeraDenovo(\n        seqtab,\n        method = '${params.chimera_method}',\n        multithread = ${task.cpus}\n    );\n    saveRDS(seqtab_nochim, 'dada2.combined.seqtabs.nochimera.rds'); \n    write.csv(seqtab_nochim, 'dada2.combined.seqtabs.nochimera.csv', na='');\n    print((sum(seqtab) - sum(seqtab_nochim)) / sum(seqtab));\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "combined_seqtab"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'mem_veryhigh'",
            "errorStrategy \"finish\"",
            "publishDir \"${params.output}/sv/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "goods_filter_seqtab": {
        "name_process": "goods_filter_seqtab",
        "string_process": "\nprocess goods_filter_seqtab {\n    container \"${container__goodsfilter}\"\n    label 'io_mem'\n    publishDir \"${params.output}/sv/\", mode: 'copy'\n    errorStrategy \"finish\"\n\n    input:\n        file(seqtab_csv)\n\n    output:\n        file \"${seqtab_csv.getSimpleName()}.goodsfiltered.csv\"\n        file \"${seqtab_csv.getSimpleName()}.goods_converged.csv\"\n        path \"curves/*_collector.csv\"\n\n\n    \"\"\"\n    set -e \n\n\n    goodsfilter \\\n    --seqtable ${seqtab_csv} \\\n    --seqtable_filtered ${seqtab_csv.getSimpleName()}.goodsfiltered.csv \\\n    --converged_file ${seqtab_csv.getSimpleName()}.goods_converged.csv \\\n    --iteration_cutoff ${params.goods_convergence} \\\n    --min_prev ${params.min_sv_prev} \\\n    --min_reads ${params.goods_min_reads} \\\n    --curves_path curves/\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "\"\"\"\n    set -e \n\n\n    goodsfilter \\\n    --seqtable ${seqtab_csv} \\\n    --seqtable_filtered ${seqtab_csv.getSimpleName()}.goodsfiltered.csv \\\n    --converged_file ${seqtab_csv.getSimpleName()}.goods_converged.csv \\\n    --iteration_cutoff ${params.goods_convergence} \\\n    --min_prev ${params.min_sv_prev} \\\n    --min_reads ${params.goods_min_reads} \\\n    --curves_path curves/\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "seqtab_csv"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__goodsfilter}\"",
            "label 'io_mem'",
            "publishDir \"${params.output}/sv/\", mode: 'copy'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "RefpkgSearchRepo": {
        "name_process": "RefpkgSearchRepo",
        "string_process": "\nprocess RefpkgSearchRepo {\n    container \"${container__vsearch}\"\n    label = 'multithread'\n\n    input:\n        path(sv_fasta_f)\n        path(repo_fasta)\n    \n    output:\n        path \"${repo_fasta}.repo.recruits.fasta\", emit: recruits\n        path \"${repo_fasta}.uc\", emit: uc\n        path \"${repo_fasta}.sv.nohit.fasta\", emit: nohits\n        path \"${repo_fasta}.vsearch.log\", emit: log\n        \n\n    \"\"\"\n    vsearch \\\n    --threads=${task.cpus} \\\n    --usearch_global ${sv_fasta_f} \\\n    --db ${repo_fasta} \\\n    --id=${params.repo_min_id} \\\n    --strand both \\\n    --uc=${repo_fasta}.uc --uc_allhits \\\n    --notmatched=${repo_fasta}.sv.nohit.fasta \\\n    --dbmatched=${repo_fasta}.repo.recruits.fasta \\\n    --maxaccepts=${params.repo_max_accepts} \\\n    | tee -a ${repo_fasta}.vsearch.log\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "\"\"\"\n    vsearch \\\n    --threads=${task.cpus} \\\n    --usearch_global ${sv_fasta_f} \\\n    --db ${repo_fasta} \\\n    --id=${params.repo_min_id} \\\n    --strand both \\\n    --uc=${repo_fasta}.uc --uc_allhits \\\n    --notmatched=${repo_fasta}.sv.nohit.fasta \\\n    --dbmatched=${repo_fasta}.repo.recruits.fasta \\\n    --maxaccepts=${params.repo_max_accepts} \\\n    | tee -a ${repo_fasta}.vsearch.log\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "VSEARCH"
        ],
        "tools_url": [
            "https://bio.tools/vsearch"
        ],
        "tools_dico": [
            {
                "name": "VSEARCH",
                "uri": "https://bio.tools/vsearch",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2520",
                                    "term": "DNA mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0450",
                                    "term": "Chimera detection"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0450",
                                    "term": "Chimeric sequence detection"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2977",
                                "term": "Nucleic acid sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_0865",
                                "term": "Sequence similarity score"
                            }
                        ]
                    }
                ],
                "description": "High-throughput search and clustering sequence analysis tool. It supports de novo and reference based chimera detection, clustering, full-length and prefix dereplication, reverse complementation, masking, all-vs-all pairwise global alignment, exact and global alignment searching, shuffling, subsampling and sorting. It also supports FASTQ file analysis, filtering and conversion.",
                "homepage": "https://github.com/torognes/vsearch"
            }
        ],
        "inputs": [
            "sv_fasta_f",
            "repo_fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container \"${container__vsearch}\"",
            "label = 'multithread'"
        ],
        "when": "",
        "stub": ""
    },
    "CombinedRefFilter": {
        "name_process": "CombinedRefFilter",
        "string_process": "\nprocess CombinedRefFilter {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n\n    input:\n        path(repo_recruit_f)\n        path(repo_recruit_uc)\n        path(taxdb)\n        path(seq_info)\n\n    output:\n        path \"references.fasta\", emit: recruit_seq\n        path \"references_seq_info.csv\", emit: recruit_si\n\n\n\"\"\"\n#!/usr/bin/env python\nimport fastalite\nimport csv\nimport sqlite3\nfrom collections import defaultdict\n\ndef get_lineage(tax_id, cursor):\n    cur_tax_id = tax_id\n    lineage = [cur_tax_id]\n    while cur_tax_id != '1':\n        cur_tax_id = cursor.execute(\"SELECT parent_id FROM nodes where tax_id=?\", (cur_tax_id,)).fetchone()[0]\n        lineage.append(cur_tax_id)\n    return lineage\n\n# Get the seq <-> ID linkage and ID <-> seq linkage\nseq_ids = defaultdict(set)\nid_seq = {}\nwith open('${repo_recruit_f}', 'rt') as recruit_h:\n    for sr in fastalite.fastalite(recruit_h):\n        seq_ids[sr.seq].add(sr.id)\n        id_seq[sr.id] = sr.seq\n\nall_refs = set(id_seq.keys())\n\n### Start with the UC of alignment SV <-> Repo\nwith open('${repo_recruit_uc}', 'rt') as in_uc:\n    uc_data = [\n        (\n            row[8], # SV\n            row[9], # Reference_id\n            float(row[3]) # Pct ID\n        )\n        for row in \n        csv.reader(in_uc, delimiter='\\\\t')\n        if row[3] != '*' and row[9] in all_refs\n    ]\nsv_max_pctid = defaultdict(float)\n# Figure out the best-pct-id for each SV\nfor sv, ref, pctid in uc_data:\n    sv_max_pctid[sv] = max([sv_max_pctid[sv], pctid])\n\n\n\n# Load in seq_info\nwith open('${seq_info}', 'rt') as sif:\n    si_r = csv.DictReader(sif)\n    seq_info = {\n        r['seqname']: r\n        for r in si_r\n    }\n\n# For each reference sequence, pick a *representitive* seq_id\ntax_db = sqlite3.connect('${taxdb}')\ntax_db_cur = tax_db.cursor()\n\nseq_rep_id = {}\nfor seq, ids in seq_ids.items():\n    if len(ids) == 1:\n        # If there is only one ID for a sequence it automatically passes!\n        seq_rep_id[seq] = list(ids)[0]\n        continue\n    tax_ids = {seq_info[i]['tax_id']: i for i in ids if i in seq_info}\n    if len(tax_ids) == 1:\n        # Only one tax id, pick a random one as our champion\n        seq_rep_id[seq] = list(ids)[0]\n        continue\n    # Implicit else multiple taxa...\n    # Get the lineages for these taxa to root\n    tax_lineages = {\n        tid: get_lineage(tid, tax_db_cur)\n        for tid in tax_ids\n    }\n    # And the depth of each lineage\n    lin_depth_tax = {\n        len(lineage): tid\n        for tid, lineage in\n        tax_lineages.items()\n    }\n    # Pick the seq from the deepest lineage to be the representitive\n    seq_rep_id[seq] = tax_ids[\n            lin_depth_tax[max(lin_depth_tax.keys())]\n        ]\n\n# Only keep ref seqs as good as the best hit for an SV\nbesthit_ref_seqs = [\n    (sv, id_seq[ref])\n    for sv, ref, pctid\n    in uc_data\n    if sv_max_pctid[sv] == pctid\n]\n\n# Refseq -> besthit SV\nrefseq_sv = defaultdict(set)\nfor sv, refseq in besthit_ref_seqs:\n    refseq_sv[refseq].add(sv)\n\n# How many SV does each ref cover?\nref_sv_cnt = {\n    k: len(v)\n    for k, v in refseq_sv.items()\n}\n\n# Get rid of ref sequences that only represent less than MIN_REF_SV\nMIN_REF_SV = 2\nfiltered_sv_ref = [\n    (sv, seq_rep_id.get(ref))\n    for ref, svs in refseq_sv.items()\n    for sv in svs\n    if len(svs) >= MIN_REF_SV\n]\n\n# For the sv post the shared ref filter who no longer have a best hit, see if there is any hit for them \n# e.g. some reference in the current set that there is some identity, even if not as good as the best hits\ncovered_sv = {sv for sv, ref in filtered_sv_ref}\nextant_refs = {ref for sv, ref in filtered_sv_ref}\naddbacksv_refs = defaultdict(list)\nfor sv, ref_id, pct_id in uc_data:\n    if sv not in covered_sv:\n        if ref_id in extant_refs:\n            addbacksv_refs[sv].append((\n                ref_id, # Ref_ID\n                pct_id, # Pct_ID\n                True # Already part of our reference?\n            ))\n        elif pct_id == sv_max_pctid[sv]:\n            addbacksv_refs[sv].append((\n                ref_id,\n                pct_id,\n                False\n            ))\n# Great, now use this dict to make a decision of which refs to add for each sv\nfor sv, svr in addbacksv_refs.items():\n    sv_seq_pctid = {\n        id_seq.get(r): pct_id\n        for r, pct_id, already_in in svr\n        if not already_in\n    }\n    # Was there totally no representation?\n    if len([r for r, pct_id, already_in in svr if already_in]) == 0:\n        # if not, add in everything\n        filtered_sv_ref += [\n            (sv, seq_rep_id.get(s))\n            for s in sv_seq_pctid.keys()\n        ]\n    else:\n        # It's not perfect, but given we have some representation, just pick the longest sequence\n        filtered_sv_ref.append(\n            (\n                sv,\n                seq_rep_id.get(sorted(sv_seq_pctid.keys(), key=lambda v: len(v))[-1])\n            )\n        )\n\nfiltered_ref = {\n    ref\n    for sv, ref\n    in filtered_sv_ref\n}\n\n# Use this to create out outputted final output\nwith open('references.fasta', 'wt') as ref_out:\n    for ref_id in filtered_ref:\n        ref_out.write(\">{}\\\\n{}\\\\n\".format(\n            ref_id,\n            id_seq.get(ref_id)\n        ))\n\nsi_columns = list(seq_info.values())[0].keys()\n\nwith open('references_seq_info.csv', 'wt') as si_out:\n    si_writer = csv.DictWriter(\n        si_out,\n        fieldnames=si_columns\n    )\n    si_writer.writeheader()\n    si_writer.writerows([\n        r for i, r in seq_info.items()\n        if i in filtered_ref\n    ])\n\"\"\"\n}",
        "nb_lignes_process": 196,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport fastalite\nimport csv\nimport sqlite3\nfrom collections import defaultdict\n\ndef get_lineage(tax_id, cursor):\n    cur_tax_id = tax_id\n    lineage = [cur_tax_id]\n    while cur_tax_id != '1':\n        cur_tax_id = cursor.execute(\"SELECT parent_id FROM nodes where tax_id=?\", (cur_tax_id,)).fetchone()[0]\n        lineage.append(cur_tax_id)\n    return lineage\n\n# Get the seq <-> ID linkage and ID <-> seq linkage\nseq_ids = defaultdict(set)\nid_seq = {}\nwith open('${repo_recruit_f}', 'rt') as recruit_h:\n    for sr in fastalite.fastalite(recruit_h):\n        seq_ids[sr.seq].add(sr.id)\n        id_seq[sr.id] = sr.seq\n\nall_refs = set(id_seq.keys())\n\n### Start with the UC of alignment SV <-> Repo\nwith open('${repo_recruit_uc}', 'rt') as in_uc:\n    uc_data = [\n        (\n            row[8], # SV\n            row[9], # Reference_id\n            float(row[3]) # Pct ID\n        )\n        for row in \n        csv.reader(in_uc, delimiter='\\\\t')\n        if row[3] != '*' and row[9] in all_refs\n    ]\nsv_max_pctid = defaultdict(float)\n# Figure out the best-pct-id for each SV\nfor sv, ref, pctid in uc_data:\n    sv_max_pctid[sv] = max([sv_max_pctid[sv], pctid])\n\n\n\n# Load in seq_info\nwith open('${seq_info}', 'rt') as sif:\n    si_r = csv.DictReader(sif)\n    seq_info = {\n        r['seqname']: r\n        for r in si_r\n    }\n\n# For each reference sequence, pick a *representitive* seq_id\ntax_db = sqlite3.connect('${taxdb}')\ntax_db_cur = tax_db.cursor()\n\nseq_rep_id = {}\nfor seq, ids in seq_ids.items():\n    if len(ids) == 1:\n        # If there is only one ID for a sequence it automatically passes!\n        seq_rep_id[seq] = list(ids)[0]\n        continue\n    tax_ids = {seq_info[i]['tax_id']: i for i in ids if i in seq_info}\n    if len(tax_ids) == 1:\n        # Only one tax id, pick a random one as our champion\n        seq_rep_id[seq] = list(ids)[0]\n        continue\n    # Implicit else multiple taxa...\n    # Get the lineages for these taxa to root\n    tax_lineages = {\n        tid: get_lineage(tid, tax_db_cur)\n        for tid in tax_ids\n    }\n    # And the depth of each lineage\n    lin_depth_tax = {\n        len(lineage): tid\n        for tid, lineage in\n        tax_lineages.items()\n    }\n    # Pick the seq from the deepest lineage to be the representitive\n    seq_rep_id[seq] = tax_ids[\n            lin_depth_tax[max(lin_depth_tax.keys())]\n        ]\n\n# Only keep ref seqs as good as the best hit for an SV\nbesthit_ref_seqs = [\n    (sv, id_seq[ref])\n    for sv, ref, pctid\n    in uc_data\n    if sv_max_pctid[sv] == pctid\n]\n\n# Refseq -> besthit SV\nrefseq_sv = defaultdict(set)\nfor sv, refseq in besthit_ref_seqs:\n    refseq_sv[refseq].add(sv)\n\n# How many SV does each ref cover?\nref_sv_cnt = {\n    k: len(v)\n    for k, v in refseq_sv.items()\n}\n\n# Get rid of ref sequences that only represent less than MIN_REF_SV\nMIN_REF_SV = 2\nfiltered_sv_ref = [\n    (sv, seq_rep_id.get(ref))\n    for ref, svs in refseq_sv.items()\n    for sv in svs\n    if len(svs) >= MIN_REF_SV\n]\n\n# For the sv post the shared ref filter who no longer have a best hit, see if there is any hit for them \n# e.g. some reference in the current set that there is some identity, even if not as good as the best hits\ncovered_sv = {sv for sv, ref in filtered_sv_ref}\nextant_refs = {ref for sv, ref in filtered_sv_ref}\naddbacksv_refs = defaultdict(list)\nfor sv, ref_id, pct_id in uc_data:\n    if sv not in covered_sv:\n        if ref_id in extant_refs:\n            addbacksv_refs[sv].append((\n                ref_id, # Ref_ID\n                pct_id, # Pct_ID\n                True # Already part of our reference?\n            ))\n        elif pct_id == sv_max_pctid[sv]:\n            addbacksv_refs[sv].append((\n                ref_id,\n                pct_id,\n                False\n            ))\n# Great, now use this dict to make a decision of which refs to add for each sv\nfor sv, svr in addbacksv_refs.items():\n    sv_seq_pctid = {\n        id_seq.get(r): pct_id\n        for r, pct_id, already_in in svr\n        if not already_in\n    }\n    # Was there totally no representation?\n    if len([r for r, pct_id, already_in in svr if already_in]) == 0:\n        # if not, add in everything\n        filtered_sv_ref += [\n            (sv, seq_rep_id.get(s))\n            for s in sv_seq_pctid.keys()\n        ]\n    else:\n        # It's not perfect, but given we have some representation, just pick the longest sequence\n        filtered_sv_ref.append(\n            (\n                sv,\n                seq_rep_id.get(sorted(sv_seq_pctid.keys(), key=lambda v: len(v))[-1])\n            )\n        )\n\nfiltered_ref = {\n    ref\n    for sv, ref\n    in filtered_sv_ref\n}\n\n# Use this to create out outputted final output\nwith open('references.fasta', 'wt') as ref_out:\n    for ref_id in filtered_ref:\n        ref_out.write(\">{}\\\\n{}\\\\n\".format(\n            ref_id,\n            id_seq.get(ref_id)\n        ))\n\nsi_columns = list(seq_info.values())[0].keys()\n\nwith open('references_seq_info.csv', 'wt') as si_out:\n    si_writer = csv.DictWriter(\n        si_out,\n        fieldnames=si_columns\n    )\n    si_writer.writeheader()\n    si_writer.writerows([\n        r for i, r in seq_info.items()\n        if i in filtered_ref\n    ])\n\"\"\"",
        "nb_lignes_script": 180,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "repo_recruit_f",
            "repo_recruit_uc",
            "taxdb",
            "seq_info"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'"
        ],
        "when": "",
        "stub": ""
    },
    "FilterSeqInfo": {
        "name_process": "FilterSeqInfo",
        "string_process": "\nprocess FilterSeqInfo {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n\n    input:\n        file (repo_recruits_f)\n        file (repo_si)\n    \n    output:\n        file('refpkg.seq_info.csv')\n\n    \"\"\"\n    #!/usr/bin/env python\n    import fastalite\n    import csv\n\n    with open('${repo_recruits_f}', 'rt') as fasta_in:\n        seq_ids = {sr.id for sr in fastalite.fastalite(fasta_in)}\n    with open('${repo_si}', 'rt') as si_in, open('refpkg.seq_info.csv', 'wt') as si_out:\n        si_reader = csv.DictReader(si_in)\n        si_writer = csv.DictWriter(si_out, si_reader.fieldnames)\n        si_writer.writeheader()\n        for r in si_reader:\n            if r['seqname'] in seq_ids:\n                si_writer.writerow(r)\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n    #!/usr/bin/env python\n    import fastalite\n    import csv\n\n    with open('${repo_recruits_f}', 'rt') as fasta_in:\n        seq_ids = {sr.id for sr in fastalite.fastalite(fasta_in)}\n    with open('${repo_si}', 'rt') as si_in, open('refpkg.seq_info.csv', 'wt') as si_out:\n        si_reader = csv.DictReader(si_in)\n        si_writer = csv.DictWriter(si_out, si_reader.fieldnames)\n        si_writer.writeheader()\n        for r in si_reader:\n            if r['seqname'] in seq_ids:\n                si_writer.writerow(r)\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "repo_recruits_f",
            "repo_si"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'"
        ],
        "when": "",
        "stub": ""
    },
    "RemoveDroppedRecruits": {
        "name_process": "RemoveDroppedRecruits",
        "string_process": "\nprocess RemoveDroppedRecruits{\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n\n    input:\n        path (recruits_fasta)\n        path (seq_info)\n    \n    output:\n        path('refpkg_recruits.fasta')\n    \n    \"\"\"\n    #!/usr/bin/env python\n    import fastalite\n    import csv\n\n    with open('${seq_info}') as si_in:\n        si_reader = csv.DictReader(si_in)\n        seqinfo_seqs = {\n            r['seqname'] for r in si_reader\n        }\n    with open('${recruits_fasta}', 'rt') as fasta_in, open('refpkg_recruits.fasta', 'wt') as fasta_out:\n        for sr in fastalite.fastalite(fasta_in):\n            if sr.id in seqinfo_seqs:\n                fasta_out.write(\">{} {}\\\\n{}\\\\n\".format(\n                    sr.id,\n                    sr.description,\n                    sr.seq\n                )) \n    \"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "\"\"\"\n    #!/usr/bin/env python\n    import fastalite\n    import csv\n\n    with open('${seq_info}') as si_in:\n        si_reader = csv.DictReader(si_in)\n        seqinfo_seqs = {\n            r['seqname'] for r in si_reader\n        }\n    with open('${recruits_fasta}', 'rt') as fasta_in, open('refpkg_recruits.fasta', 'wt') as fasta_out:\n        for sr in fastalite.fastalite(fasta_in):\n            if sr.id in seqinfo_seqs:\n                fasta_out.write(\">{} {}\\\\n{}\\\\n\".format(\n                    sr.id,\n                    sr.description,\n                    sr.seq\n                )) \n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "recruits_fasta",
            "seq_info"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'"
        ],
        "when": "",
        "stub": ""
    },
    "DlBuildTaxtasticDB": {
        "name_process": "DlBuildTaxtasticDB",
        "string_process": "\nprocess DlBuildTaxtasticDB {\n    container = \"${container__taxtastic}\"\n    label = 'io_net'\n    errorStrategy = 'finish'\n\n    output:\n        file \"taxonomy.db\"\n\n    afterScript \"rm -rf dl/\"\n\n\n    \"\"\"\n    set -e\n\n    mkdir -p dl/ && \\\n    taxit new_database taxonomy.db -u ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip -p dl/\n    \"\"\"\n\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    set -e\n\n    mkdir -p dl/ && \\\n    taxit new_database taxonomy.db -u ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip -p dl/\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "TaxIt"
        ],
        "tools_url": [
            "https://bio.tools/TaxIt"
        ],
        "tools_dico": [
            {
                "name": "TaxIt",
                "uri": "https://bio.tools/TaxIt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3520",
                            "term": "Proteomics experiment"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3649",
                                    "term": "Target-Decoy"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An iterative and automated computational pipeline for untargeted strain-level identification using MS/MS spectra from pathogenic samples.",
                "homepage": "https://gitlab.com/rki_bioinformatics/TaxIt"
            }
        ],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__taxtastic}\"",
            "label = 'io_net'",
            "errorStrategy = 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "BuildTaxtasticDB": {
        "name_process": "BuildTaxtasticDB",
        "string_process": "\nprocess BuildTaxtasticDB {\n    container = \"${container__taxtastic}\"\n    label = 'io_limited'\n    errorStrategy = 'finish'\n\n    input:\n        file taxdump_zip_f\n\n    output:\n        file \"taxonomy.db\"\n\n    \"\"\"\n    taxit new_database taxonomy.db -z ${taxdump_zip_f}\n    \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "\"\"\"\n    taxit new_database taxonomy.db -z ${taxdump_zip_f}\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "TaxIt"
        ],
        "tools_url": [
            "https://bio.tools/TaxIt"
        ],
        "tools_dico": [
            {
                "name": "TaxIt",
                "uri": "https://bio.tools/TaxIt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3520",
                            "term": "Proteomics experiment"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3649",
                                    "term": "Target-Decoy"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An iterative and automated computational pipeline for untargeted strain-level identification using MS/MS spectra from pathogenic samples.",
                "homepage": "https://gitlab.com/rki_bioinformatics/TaxIt"
            }
        ],
        "inputs": [
            "taxdump_zip_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__taxtastic}\"",
            "label = 'io_limited'",
            "errorStrategy = 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "ConfirmSI": {
        "name_process": "ConfirmSI",
        "string_process": "\nprocess ConfirmSI {\n    container = \"${container__taxtastic}\"\n    label = 'io_mem'\n\n    input:\n        file taxonomy_db_f\n        file refpkg_si_f\n    \n    output:\n        file \"${refpkg_si_f.baseName}.corr.csv\"\n    \n    \"\"\"\n    taxit update_taxids \\\n    ${refpkg_si_f} \\\n    ${taxonomy_db_f} \\\n    -o ${refpkg_si_f.baseName}.corr.csv \\\n    -a drop\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    taxit update_taxids \\\n    ${refpkg_si_f} \\\n    ${taxonomy_db_f} \\\n    -o ${refpkg_si_f.baseName}.corr.csv \\\n    -a drop\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "TaxIt"
        ],
        "tools_url": [
            "https://bio.tools/TaxIt"
        ],
        "tools_dico": [
            {
                "name": "TaxIt",
                "uri": "https://bio.tools/TaxIt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3520",
                            "term": "Proteomics experiment"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3649",
                                    "term": "Target-Decoy"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An iterative and automated computational pipeline for untargeted strain-level identification using MS/MS spectra from pathogenic samples.",
                "homepage": "https://gitlab.com/rki_bioinformatics/TaxIt"
            }
        ],
        "inputs": [
            "taxonomy_db_f",
            "refpkg_si_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__taxtastic}\"",
            "label = 'io_mem'"
        ],
        "when": "",
        "stub": ""
    },
    "AlignRepoRecruits": {
        "name_process": "AlignRepoRecruits",
        "string_process": "\nprocess AlignRepoRecruits {\n    container = \"${container__infernal}\"\n    label = 'mem_veryhigh'\n\n    input:\n        file repo_recruits_f\n        file cm\n    \n    output:\n        file \"recruits.aln.sto\"\n        file \"recruits.aln.scores\" \n    \n    \"\"\"\n    cmalign \\\n    --cpu ${task.cpus} --noprob --dnaout --mxsize ${params.cmalign_mxsize} \\\n    --sfile recruits.aln.scores -o recruits.aln.sto \\\n    ${cm} ${repo_recruits_f}\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    cmalign \\\n    --cpu ${task.cpus} --noprob --dnaout --mxsize ${params.cmalign_mxsize} \\\n    --sfile recruits.aln.scores -o recruits.aln.sto \\\n    ${cm} ${repo_recruits_f}\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "repo_recruits_f",
            "cm"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__infernal}\"",
            "label = 'mem_veryhigh'"
        ],
        "when": "",
        "stub": ""
    },
    "ConvertAlnToPhy": {
        "name_process": "ConvertAlnToPhy",
        "string_process": "\nprocess ConvertAlnToPhy {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    errorStrategy \"finish\"\n\n    input: \n        file recruits_aln_sto_f\n    \n    output:\n        file \"recruits.aln.phy\"\n    \n    \"\"\"\n    #!/usr/bin/env python\n    from Bio import AlignIO\n\n    with open('recruits.aln.phy', 'wt') as out_h:\n        AlignIO.write(\n            AlignIO.read(\n                open('${recruits_aln_sto_f}', 'rt'),\n                'stockholm'\n            ),\n            out_h,\n            'phylip-relaxed'\n        )\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n    #!/usr/bin/env python\n    from Bio import AlignIO\n\n    with open('recruits.aln.phy', 'wt') as out_h:\n        AlignIO.write(\n            AlignIO.read(\n                open('${recruits_aln_sto_f}', 'rt'),\n                'stockholm'\n            ),\n            out_h,\n            'phylip-relaxed'\n        )\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "recruits_aln_sto_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "RaxmlTreeNG": {
        "name_process": "RaxmlTreeNG",
        "string_process": "\nprocess RaxmlTreeNG {\n    container = \"${container__raxmlng}\"\n    label = 'mem_veryhigh'\n    errorStrategy = 'finish'\n\n    input:\n        path recruits_aln_fasta_f\n    \n    output:\n        path \"refpkg.raxml.bestTree\", emit: tree\n        path \"refpkg.raxml.log\", emit: log\n        path \"refpkg.raxml.bestModel\", emit: model\n    \n    \"\"\"\n    raxml-ng \\\n    --parse \\\n    --model ${params.raxmlng_model} \\\n    --msa ${recruits_aln_fasta_f} \\\n    --seed ${params.raxmlng_seed}\n\n    raxml-ng \\\n    --prefix refpkg \\\n    --model ${params.raxmlng_model} \\\n    --msa ${recruits_aln_fasta_f}.raxml.rba \\\n    --tree pars{${params.raxmlng_parsimony_trees}},rand{${params.raxmlng_random_trees}} \\\n    --bs-cutoff ${params.raxmlng_bootstrap_cutoff} \\\n    --seed ${params.raxmlng_seed} \\\n    --threads ${task.cpus}\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "\"\"\"\n    raxml-ng \\\n    --parse \\\n    --model ${params.raxmlng_model} \\\n    --msa ${recruits_aln_fasta_f} \\\n    --seed ${params.raxmlng_seed}\n\n    raxml-ng \\\n    --prefix refpkg \\\n    --model ${params.raxmlng_model} \\\n    --msa ${recruits_aln_fasta_f}.raxml.rba \\\n    --tree pars{${params.raxmlng_parsimony_trees}},rand{${params.raxmlng_random_trees}} \\\n    --bs-cutoff ${params.raxmlng_bootstrap_cutoff} \\\n    --seed ${params.raxmlng_seed} \\\n    --threads ${task.cpus}\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "RAxML-NG"
        ],
        "tools_url": [
            "https://bio.tools/RAxML-NG"
        ],
        "tools_dico": [
            {
                "name": "RAxML-NG",
                "uri": "https://bio.tools/RAxML-NG",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3810",
                            "term": "Agricultural science"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0194",
                            "term": "Phylogenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0547",
                                    "term": "Phylogenetic inference (maximum likelihood and Bayesian methods)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0552",
                                    "term": "Phylogenetic tree bootstrapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0547",
                                    "term": "Phylogenetic tree construction (maximum likelihood and Bayesian methods)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0547",
                                    "term": "Phylogenetic tree generation (maximum likelihood and Bayesian methods)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Phylogenetic tree inference tool which uses maximum-likelihood (ML) optimality criterion.",
                "homepage": "https://raxml-ng.vital-it.ch/"
            }
        ],
        "inputs": [
            "recruits_aln_fasta_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__raxmlng}\"",
            "label = 'mem_veryhigh'",
            "errorStrategy = 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "RaxmlTree": {
        "name_process": "RaxmlTree",
        "string_process": "\nprocess RaxmlTree {\n    container = \"${container__raxml}\"\n    label = 'mem_veryhigh'\n    errorStrategy = 'retry'\n\n    input:\n        file recruits_aln_fasta_f\n    \n    output:\n        file \"RAxML_bestTree.refpkg\"\n        file \"RAxML_info.refpkg\"\n    \n    \"\"\"\n    raxmlHPC-PTHREADS-AVX2 \\\n    -n refpkg \\\n    -m ${params.raxml_model} \\\n    -s ${recruits_aln_fasta_f} \\\n    -p ${params.raxml_parsiomony_seed} \\\n    -T ${task.cpus}\n    \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "\"\"\"\n    raxmlHPC-PTHREADS-AVX2 \\\n    -n refpkg \\\n    -m ${params.raxml_model} \\\n    -s ${recruits_aln_fasta_f} \\\n    -p ${params.raxml_parsiomony_seed} \\\n    -T ${task.cpus}\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "recruits_aln_fasta_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__raxml}\"",
            "label = 'mem_veryhigh'",
            "errorStrategy = 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "RaxmlTree_cleanupInfo": {
        "name_process": "RaxmlTree_cleanupInfo",
        "string_process": "\nprocess RaxmlTree_cleanupInfo {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    errorStrategy = 'retry'\n\n    input:\n        file \"RAxML_info.unclean.refpkg\"\n    \n    output:\n        file \"RAxML_info.refpkg\"\n\n\n\"\"\"\n#!/usr/bin/env python\nwith open(\"RAxML_info.refpkg\",'wt') as out_h:\n    with open(\"RAxML_info.unclean.refpkg\", 'rt') as in_h:\n        past_cruft = False\n        for l in in_h:\n            if \"This is RAxML version\" == l[0:21]:\n                past_cruft = True\n            if past_cruft:\n                out_h.write(l)\n\"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nwith open(\"RAxML_info.refpkg\",'wt') as out_h:\n    with open(\"RAxML_info.unclean.refpkg\", 'rt') as in_h:\n        past_cruft = False\n        for l in in_h:\n            if \"This is RAxML version\" == l[0:21]:\n                past_cruft = True\n            if past_cruft:\n                out_h.write(l)\n\"\"\"",
        "nb_lignes_script": 10,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "errorStrategy = 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "TaxtableForSI": {
        "name_process": "TaxtableForSI",
        "string_process": "\nprocess TaxtableForSI {\n    container = \"${container__taxtastic}\"\n    label = 'io_limited'\n    errorStrategy = 'finish'\n\n    input:\n        file taxonomy_db_f \n        file refpkg_si_corr_f\n    output:\n        file \"refpkg.taxtable.csv\"\n\n    \"\"\"\n    taxit taxtable ${taxonomy_db_f} \\\n    --seq-info ${refpkg_si_corr_f} \\\n    --outfile refpkg.taxtable.csv\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "\"\"\"\n    taxit taxtable ${taxonomy_db_f} \\\n    --seq-info ${refpkg_si_corr_f} \\\n    --outfile refpkg.taxtable.csv\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "TaxIt"
        ],
        "tools_url": [
            "https://bio.tools/TaxIt"
        ],
        "tools_dico": [
            {
                "name": "TaxIt",
                "uri": "https://bio.tools/TaxIt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3520",
                            "term": "Proteomics experiment"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3649",
                                    "term": "Target-Decoy"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An iterative and automated computational pipeline for untargeted strain-level identification using MS/MS spectra from pathogenic samples.",
                "homepage": "https://gitlab.com/rki_bioinformatics/TaxIt"
            }
        ],
        "inputs": [
            "taxonomy_db_f",
            "refpkg_si_corr_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__taxtastic}\"",
            "label = 'io_limited'",
            "errorStrategy = 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "ObtainCM": {
        "name_process": "ObtainCM",
        "string_process": "\nprocess ObtainCM {\n    container = \"${container__infernal}\"\n    label = 'io_net'\n\n    output:\n        file \"SSU_rRNA_bacteria.cm\"\n    \n    \"\"\"\n    wget http://rfam.xfam.org/family/RF00177/cm -O SSU_rRNA_bacteria.cm \n    \"\"\"\n}",
        "nb_lignes_process": 10,
        "string_script": "\"\"\"\n    wget http://rfam.xfam.org/family/RF00177/cm -O SSU_rRNA_bacteria.cm \n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__infernal}\"",
            "label = 'io_net'"
        ],
        "when": "",
        "stub": ""
    },
    "CombineRefpkg_ng": {
        "name_process": "CombineRefpkg_ng",
        "string_process": "\nprocess CombineRefpkg_ng {\n    container = \"${container__taxtastic}\"\n    label = 'io_mem'\n\n    afterScript(\"rm -rf refpkg/*\")\n    publishDir \"${params.output}/refpkg/\", mode: 'copy'\n\n    input:\n        path recruits_aln_fasta_f\n        path recruits_aln_sto_f\n        path refpkg_tree_f \n        path refpkg_tree_stats_clean_f \n        path refpkg_tt_f\n        path refpkg_si_corr_f\n        path refpkg_cm\n        path raxmlng_model\n    \n    output:\n        path \"refpkg.tar.gz\"\n    \n\"\"\"\ntaxit create --locus 16S \\\n--package-name refpkg \\\n--clobber \\\n--aln-fasta ${recruits_aln_fasta_f} \\\n--aln-sto ${recruits_aln_sto_f} \\\n--tree-file ${refpkg_tree_f} \\\n--tree-stats ${refpkg_tree_stats_clean_f} \\\n--taxonomy ${refpkg_tt_f} \\\n--seq-info ${refpkg_si_corr_f} \\\n--profile ${refpkg_cm}\n\ncp ${raxmlng_model} refpkg/raxmlng.model.raw\npython << ENDPYTHON\nimport json\nimport hashlib\n\nmodel_str = open('refpkg/raxmlng.model.raw', 'rt').read()\nmodel = model_str.split(',')[0]\nwith open('refpkg/raxmlng.model', 'wt') as out_h:\n    out_h.write(model)\n\nmodelhash = hashlib.md5(model.encode('utf-8')).hexdigest()\ncontents = json.load(\n    open('refpkg/CONTENTS.json', 'rt')\n)\ncontents['files']['raxml_ng_model'] = 'raxmlng.model'\ncontents['md5']['raxml_ng_model'] = modelhash\n\njson.dump(\n    contents,\n    open('refpkg/CONTENTS.json', 'wt')\n)\nENDPYTHON\ntar cvf refpkg.tar  -C refpkg/ .\ngzip refpkg.tar\n\"\"\"\n}",
        "nb_lignes_process": 57,
        "string_script": "\"\"\"\ntaxit create --locus 16S \\\n--package-name refpkg \\\n--clobber \\\n--aln-fasta ${recruits_aln_fasta_f} \\\n--aln-sto ${recruits_aln_sto_f} \\\n--tree-file ${refpkg_tree_f} \\\n--tree-stats ${refpkg_tree_stats_clean_f} \\\n--taxonomy ${refpkg_tt_f} \\\n--seq-info ${refpkg_si_corr_f} \\\n--profile ${refpkg_cm}\n\ncp ${raxmlng_model} refpkg/raxmlng.model.raw\npython << ENDPYTHON\nimport json\nimport hashlib\n\nmodel_str = open('refpkg/raxmlng.model.raw', 'rt').read()\nmodel = model_str.split(',')[0]\nwith open('refpkg/raxmlng.model', 'wt') as out_h:\n    out_h.write(model)\n\nmodelhash = hashlib.md5(model.encode('utf-8')).hexdigest()\ncontents = json.load(\n    open('refpkg/CONTENTS.json', 'rt')\n)\ncontents['files']['raxml_ng_model'] = 'raxmlng.model'\ncontents['md5']['raxml_ng_model'] = modelhash\n\njson.dump(\n    contents,\n    open('refpkg/CONTENTS.json', 'wt')\n)\nENDPYTHON\ntar cvf refpkg.tar  -C refpkg/ .\ngzip refpkg.tar\n\"\"\"",
        "nb_lignes_script": 36,
        "language_script": "bash",
        "tools": [
            "TaxIt",
            "tximport",
            "MoDEL"
        ],
        "tools_url": [
            "https://bio.tools/TaxIt",
            "https://bio.tools/tximport",
            "https://bio.tools/model"
        ],
        "tools_dico": [
            {
                "name": "TaxIt",
                "uri": "https://bio.tools/TaxIt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3520",
                            "term": "Proteomics experiment"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3649",
                                    "term": "Target-Decoy"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An iterative and automated computational pipeline for untargeted strain-level identification using MS/MS spectra from pathogenic samples.",
                "homepage": "https://gitlab.com/rki_bioinformatics/TaxIt"
            },
            {
                "name": "tximport",
                "uri": "https://bio.tools/tximport",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "Gene transcripts"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Workflows"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "mRNA features"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Pipelines"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3680",
                                    "term": "RNA-Seq analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2497",
                                    "term": "Pathway or network analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An R/Bioconductor package that imports transcript-level abundance, estimated counts and transcript lengths, and summarizes into matrices for use with downstream gene-level analysis packages.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/tximport.html"
            },
            {
                "name": "MoDEL",
                "uri": "https://bio.tools/model",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0176",
                            "term": "Molecular dynamics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2426",
                                    "term": "Modelling and simulation"
                                }
                            ],
                            []
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0842",
                                "term": "Identifier"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2080",
                                "term": "Database search results"
                            }
                        ]
                    }
                ],
                "description": "Database of protein Molecular Dynamics simulations, with 1800 trajectories representing different structural clusters of the PDB.",
                "homepage": "http://mmb.irbbarcelona.org/MoDEL"
            }
        ],
        "inputs": [
            "recruits_aln_fasta_f",
            "recruits_aln_sto_f",
            "refpkg_tree_f",
            "refpkg_tree_stats_clean_f",
            "refpkg_tt_f",
            "refpkg_si_corr_f",
            "refpkg_cm",
            "raxmlng_model"
        ],
        "nb_inputs": 8,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__taxtastic}\"",
            "label = 'io_mem'",
            "afterScript(\"rm -rf refpkg/*\")",
            "publishDir \"${params.output}/refpkg/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "CombineRefpkg_og": {
        "name_process": "CombineRefpkg_og",
        "string_process": "\nprocess CombineRefpkg_og {\n    container = \"${container__pplacer}\"\n    label = 'io_mem'\n\n    afterScript(\"rm -rf refpkg/*\")\n    publishDir \"${params.output}/refpkg/\", mode: 'copy'\n\n    input:\n        file recruits_aln_fasta_f\n        file recruits_aln_sto_f\n        file refpkg_tree_f \n        file refpkg_tree_stats_clean_f \n        file refpkg_tt_f\n        file refpkg_si_corr_f\n        file refpkg_cm\n    \n    output:\n        file \"refpkg.tar.gz\"\n    \n    \"\"\"\n    taxit create --locus 16S \\\n    --package-name refpkg \\\n    --clobber \\\n    --aln-fasta ${recruits_aln_fasta_f} \\\n    --aln-sto ${recruits_aln_sto_f} \\\n    --tree-file ${refpkg_tree_f} \\\n    --tree-stats ${refpkg_tree_stats_clean_f} \\\n    --taxonomy ${refpkg_tt_f} \\\n    --seq-info ${refpkg_si_corr_f} \\\n    --profile ${refpkg_cm} && \\\n    ls -l refpkg/ && \\\n    tar czvf refpkg.tar.gz  -C refpkg/ .\n    \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "\"\"\"\n    taxit create --locus 16S \\\n    --package-name refpkg \\\n    --clobber \\\n    --aln-fasta ${recruits_aln_fasta_f} \\\n    --aln-sto ${recruits_aln_sto_f} \\\n    --tree-file ${refpkg_tree_f} \\\n    --tree-stats ${refpkg_tree_stats_clean_f} \\\n    --taxonomy ${refpkg_tt_f} \\\n    --seq-info ${refpkg_si_corr_f} \\\n    --profile ${refpkg_cm} && \\\n    ls -l refpkg/ && \\\n    tar czvf refpkg.tar.gz  -C refpkg/ .\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "TaxIt"
        ],
        "tools_url": [
            "https://bio.tools/TaxIt"
        ],
        "tools_dico": [
            {
                "name": "TaxIt",
                "uri": "https://bio.tools/TaxIt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3520",
                            "term": "Proteomics experiment"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3649",
                                    "term": "Target-Decoy"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An iterative and automated computational pipeline for untargeted strain-level identification using MS/MS spectra from pathogenic samples.",
                "homepage": "https://gitlab.com/rki_bioinformatics/TaxIt"
            }
        ],
        "inputs": [
            "recruits_aln_fasta_f",
            "recruits_aln_sto_f",
            "refpkg_tree_f",
            "refpkg_tree_stats_clean_f",
            "refpkg_tt_f",
            "refpkg_si_corr_f",
            "refpkg_cm"
        ],
        "nb_inputs": 7,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_mem'",
            "afterScript(\"rm -rf refpkg/*\")",
            "publishDir \"${params.output}/refpkg/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "AddRAxMLModel": {
        "name_process": "AddRAxMLModel",
        "string_process": "\nprocess AddRAxMLModel {\n    container = \"${container__taxtastic}\"\n    label = 'io_limited'\n\n    input:\n        path \"refpkg.tgz\"\n        path raxml_ng_model\n    output:\n        path 'refpkg.tar.gz'\n\"\"\"\n#!/usr/bin/env python\n\nimport tarfile\nimport json\nimport os\n\ntar_h = tarfile.open('refpkg.tgz')\ntar_contents_dict = {os.path.basename(f.name): f for f in tar_h.getmembers()}\ncontents = json.loads(\n    tar_h.extractfile(\n        tar_contents_dict['CONTENTS.json']\n    ).read().decode('utf-8')\n)\n\n\"\"\"\n\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n#!/usr/bin/env python\n\nimport tarfile\nimport json\nimport os\n\ntar_h = tarfile.open('refpkg.tgz')\ntar_contents_dict = {os.path.basename(f.name): f for f in tar_h.getmembers()}\ncontents = json.loads(\n    tar_h.extractfile(\n        tar_contents_dict['CONTENTS.json']\n    ).read().decode('utf-8')\n)\n\n\"\"\"",
        "nb_lignes_script": 15,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "raxml_ng_model"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jgolob__maliampi",
        "directive": [
            "container = \"${container__taxtastic}\"",
            "label = 'io_limited'"
        ],
        "when": "",
        "stub": ""
    }
}