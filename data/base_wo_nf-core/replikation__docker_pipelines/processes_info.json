{
    "dev": {
        "name_process": "dev",
        "string_process": "process dev {\n      publishDir \"DEV_WORKFLOW_OUT/${repeater}\", mode: 'copy', pattern: \"*.cf\"\n      label 'dev'\n                              \n    input:\n      file(database)\n                     \n    output:\n      file(\"*.cf\")\n    shell:\n      \"\"\"\n      tar xzf ${database}\n      ls\n      mv */* .\n      ls\n      centrifuge-build -p 7 --seed 42 --conversion-table ex.conv --taxonomy-tree ex.tree --name-table ex.name ex.fa ex\n      \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "      \"\"\"\n      tar xzf ${database}\n      ls\n      mv */* .\n      ls\n      centrifuge-build -p 7 --seed 42 --conversion-table ex.conv --taxonomy-tree ex.tree --name-table ex.name ex.fa ex\n      \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "database"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"DEV_WORKFLOW_OUT/${repeater}\", mode: 'copy', pattern: \"*.cf\"",
            "label 'dev'"
        ],
        "when": "",
        "stub": ""
    },
    "metamaps": {
        "name_process": "metamaps",
        "string_process": "process metamaps {\n      label 'metamaps'\n      publishDir \"${params.output}/${name}/metamaps\", mode: 'copy', pattern: \"classification_results.EM.*\"\n    input:\n      tuple val(name), file(fastq) \n      file(database) \n    output:\n      tuple val(name), file (\"classification_results.EM.*\")\n    script:\n      if (workflow.profile == 'gcloud') {\n      \"\"\"\n      metamaps mapDirectly -t ${task.cpus} --all -m 1000 -r ${database}/DB.fa -q ${fastq} -o classification_results\n      metamaps classify -t ${task.cpus} --mappings classification_results --DB ${database}\n      \"\"\"\n      }\n      else {\n      \"\"\"\n      metamaps mapDirectly -t ${task.cpus} --all -m 1000 -r ${database}/DB.fa -q ${fastq} -o classification_results --maxmemory ${params.memory}\n      metamaps classify -t ${task.cpus} --mappings classification_results --DB ${database}\n      \"\"\"\n      }\n}",
        "nb_lignes_process": 20,
        "string_script": "      if (workflow.profile == 'gcloud') {\n      \"\"\"\n      metamaps mapDirectly -t ${task.cpus} --all -m 1000 -r ${database}/DB.fa -q ${fastq} -o classification_results\n      metamaps classify -t ${task.cpus} --mappings classification_results --DB ${database}\n      \"\"\"\n      }\n      else {\n      \"\"\"\n      metamaps mapDirectly -t ${task.cpus} --all -m 1000 -r ${database}/DB.fa -q ${fastq} -o classification_results --maxmemory ${params.memory}\n      metamaps classify -t ${task.cpus} --mappings classification_results --DB ${database}\n      \"\"\"\n      }",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "fastq",
            "database"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'metamaps'",
            "publishDir \"${params.output}/${name}/metamaps\", mode: 'copy', pattern: \"classification_results.EM.*\""
        ],
        "when": "",
        "stub": ""
    },
    "plasflow": {
        "name_process": "plasflow",
        "string_process": "process plasflow {\n      publishDir \"${params.output}/${name}/chromosome\", mode: 'copy', pattern: \"${name}_chromosomes.fasta\"\n      publishDir \"${params.output}/${name}/plasmids\", mode: 'copy', pattern: \"${name}_plasmids.fasta\"\n      publishDir \"${params.output}/${name}/unclassified\", mode: 'copy', pattern: \"${name}_unclassified.fasta\"\n      label 'plasflow'\n    input:\n      tuple val(name), file(fasta) \n    output:\n      tuple val(name), file(\"${name}_chromosomes.fasta\"), file(\"${name}_plasmids.fasta\"), file(\"${name}_unclassified.fasta\")\n    script:\n      \"\"\"\n      PlasFlow.py --input ${fasta} --output ${name} --threshold 0.7\n      \"\"\"\n}",
        "nb_lignes_process": 12,
        "string_script": "      \"\"\"\n      PlasFlow.py --input ${fasta} --output ${name} --threshold 0.7\n      \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}/chromosome\", mode: 'copy', pattern: \"${name}_chromosomes.fasta\"",
            "publishDir \"${params.output}/${name}/plasmids\", mode: 'copy', pattern: \"${name}_plasmids.fasta\"",
            "publishDir \"${params.output}/${name}/unclassified\", mode: 'copy', pattern: \"${name}_unclassified.fasta\"",
            "label 'plasflow'"
        ],
        "when": "",
        "stub": ""
    },
    "plasflow_compare": {
        "name_process": "plasflow_compare",
        "string_process": "\nprocess plasflow_compare {\n      publishDir \"${params.output}/plasflow/${name}/chromosome\", mode: 'copy', pattern: \"${name}_${splitname}_chromosomes.fasta\"\n      publishDir \"${params.output}/plasflow/${name}/plasmids\", mode: 'copy', pattern: \"${name}_${splitname}_plasmids.fasta\"\n      publishDir \"${params.output}/plasflow/${name}/unclassified\", mode: 'copy', pattern: \"${name}_${splitname}_unclassified.fasta\"\n      label 'plasflow'\n                  \n    input:\n      tuple val(name), val(splitname), path(fasta) \n    output:\n      tuple val(name), val(splitname), val(\"chromosome\"), path(\"${name}_${splitname}_chromosomes.fasta\"), emit: genome, optional: true\n      tuple val(name), val(splitname), val(\"plasmid\"), path(\"${name}_${splitname}_plasmids.fasta\"), emit: plasmids, optional: true\n      tuple val(name), val(splitname), val(\"unclassified\"), path(\"${name}_${splitname}_unclassified.fasta\"), emit: unclassified, optional: true\n    script:\n      \"\"\"\n      PlasFlow.py --input ${fasta} --output ${name}_${splitname} --threshold 0.7\n      \n      for fastafile in ${name}_*.fasta; do\n        lines=\\$(cat \\${fastafile} | wc -l)\n        if [ \\${lines} -lt 2 ]; then rm -f \\${fastafile}; fi\n      done\n      \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "      \"\"\"\n      PlasFlow.py --input ${fasta} --output ${name}_${splitname} --threshold 0.7\n      \n      for fastafile in ${name}_*.fasta; do\n        lines=\\$(cat \\${fastafile} | wc -l)\n        if [ \\${lines} -lt 2 ]; then rm -f \\${fastafile}; fi\n      done\n      \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "splitname",
            "fasta"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/plasflow/${name}/chromosome\", mode: 'copy', pattern: \"${name}_${splitname}_chromosomes.fasta\"",
            "publishDir \"${params.output}/plasflow/${name}/plasmids\", mode: 'copy', pattern: \"${name}_${splitname}_plasmids.fasta\"",
            "publishDir \"${params.output}/plasflow/${name}/unclassified\", mode: 'copy', pattern: \"${name}_${splitname}_unclassified.fasta\"",
            "label 'plasflow'"
        ],
        "when": "",
        "stub": ""
    },
    "abricateParserFASTA": {
        "name_process": "abricateParserFASTA",
        "string_process": "process abricateParserFASTA {\n                                                                                         \n    label 'ubuntu'   \n  input:\n    tuple val(name), val(method), file(results) \n  output:\n\t  tuple val(name), val(method), file(\"*.csv\") \n  shell:\n    \"\"\"\n    printf \"amount;type;group\\\\n\" > !{name}_!{method}.csv\n  \t# beta-lactamase\n      blaData=\\$(tail -n+2 !{results} | grep -E \"bla[A-Z]|lactamase\" | cut -f6 | sort | uniq -c |\\\n        sed -e 's/^[ \\\\t]*//' | tr \" \" \";\" | sed -e 's/\\$/;beta-lactamase/') \n      printf \"\\${blaData}\\\\n\" >> !{name}_!{method}.csv\n    # tetracycline\n      tetData=\\$(tail -n+2 !{results} | grep \"tetracycline\" | cut -f6 | grep -v \"bla\" | sort |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;tetracycline-resistance/') \n      printf \"\\${tetData}\\\\n\" >> !{name}_!{method}.csv\n    # aminoglycoside\n      aminoData=\\$(tail -n+2 !{results} | grep -v \"efflux\" | grep \"aminoglycoside\" | cut -f6 | grep -v \"bla\" | sort |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;aminoglycoside-resistance/') \n      printf \"\\${aminoData}\\\\n\" >> !{name}_!{method}.csv\n    # efflux\n      effluxData=\\$(tail -n+2 !{results} | grep -v \"tetracycline\" | grep \"efflux\" | cut -f6 | grep -v \"bla\" | sort |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;efflux-system/') \n      printf \"\\${effluxData}\\\\n\" >> !{name}_!{method}.csv\n    # quinolones\n      quinoData=\\$(tail -n+2 !{results} |  grep -v \"efflux\" | grep \"quinolone\" | cut -f6 | grep -v \"bla\" | sort |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;quinolone-resistance/') \n      printf \"\\${quinoData}\\\\n\" >> !{name}_!{method}.csv\n    # other\n      otherData=\\$(tail -n+2 !{results} |  grep -v \"efflux\" | grep -v \"tetracycline\" | grep -v \"aminoglycoside\" | grep -v \"quinolone\" |\\\n        cut -f6 | grep -vE \"bla[A-Z]|lactamase\" |  sort |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;other-resistance-genes/') \n      printf \"\\${otherData}\\\\n\" >> !{name}_!{method}.csv\n    \"\"\"\n}",
        "nb_lignes_process": 35,
        "string_script": "    \"\"\"\n    printf \"amount;type;group\\\\n\" > !{name}_!{method}.csv\n  \t# beta-lactamase\n      blaData=\\$(tail -n+2 !{results} | grep -E \"bla[A-Z]|lactamase\" | cut -f6 | sort | uniq -c |\\\n        sed -e 's/^[ \\\\t]*//' | tr \" \" \";\" | sed -e 's/\\$/;beta-lactamase/') \n      printf \"\\${blaData}\\\\n\" >> !{name}_!{method}.csv\n    # tetracycline\n      tetData=\\$(tail -n+2 !{results} | grep \"tetracycline\" | cut -f6 | grep -v \"bla\" | sort |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;tetracycline-resistance/') \n      printf \"\\${tetData}\\\\n\" >> !{name}_!{method}.csv\n    # aminoglycoside\n      aminoData=\\$(tail -n+2 !{results} | grep -v \"efflux\" | grep \"aminoglycoside\" | cut -f6 | grep -v \"bla\" | sort |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;aminoglycoside-resistance/') \n      printf \"\\${aminoData}\\\\n\" >> !{name}_!{method}.csv\n    # efflux\n      effluxData=\\$(tail -n+2 !{results} | grep -v \"tetracycline\" | grep \"efflux\" | cut -f6 | grep -v \"bla\" | sort |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;efflux-system/') \n      printf \"\\${effluxData}\\\\n\" >> !{name}_!{method}.csv\n    # quinolones\n      quinoData=\\$(tail -n+2 !{results} |  grep -v \"efflux\" | grep \"quinolone\" | cut -f6 | grep -v \"bla\" | sort |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;quinolone-resistance/') \n      printf \"\\${quinoData}\\\\n\" >> !{name}_!{method}.csv\n    # other\n      otherData=\\$(tail -n+2 !{results} |  grep -v \"efflux\" | grep -v \"tetracycline\" | grep -v \"aminoglycoside\" | grep -v \"quinolone\" |\\\n        cut -f6 | grep -vE \"bla[A-Z]|lactamase\" |  sort |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;other-resistance-genes/') \n      printf \"\\${otherData}\\\\n\" >> !{name}_!{method}.csv\n    \"\"\"",
        "nb_lignes_script": 27,
        "language_script": "bash",
        "tools": [
            "ScType"
        ],
        "tools_url": [
            "https://bio.tools/ScType"
        ],
        "tools_dico": [
            {
                "name": "ScType",
                "uri": "https://bio.tools/ScType",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2229",
                            "term": "Cell biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3489",
                            "term": "Database management"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3489",
                            "term": "Database administration"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3432",
                                    "term": "Clustering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3435",
                                    "term": "Standardisation and normalisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3680",
                                    "term": "RNA-Seq analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "ScType is a tool for fully-automated cell type identification from single-cell RNA-seq data. ScType provides a complete pipeline for single-cell RNA-seq data analysis (including data processing, normalization and clustering) and cell-type annotation.",
                "homepage": "http://session.asuscomm.com:8080/"
            }
        ],
        "inputs": [
            "name",
            "method",
            "results"
        ],
        "nb_inputs": 3,
        "outputs": [
            "method"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'ubuntu'"
        ],
        "when": "",
        "stub": ""
    },
    "fasttree": {
        "name_process": "fasttree",
        "string_process": "process fasttree {\n    label \"fasttree\"\n    input:\n        tuple val(name), path(alignment)\n    output:\n        tuple val(name), path(\"tree.nwk\")\n    script:\n        \"\"\"\n        export OMP_NUM_THREADS=${task.cpus}\n        FastTree -gtr  ${alignment} > tree.nwk\n        \"\"\"\n}",
        "nb_lignes_process": 10,
        "string_script": "        \"\"\"\n        export OMP_NUM_THREADS=${task.cpus}\n        FastTree -gtr  ${alignment} > tree.nwk\n        \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "FastTree"
        ],
        "tools_url": [
            "https://bio.tools/fasttree"
        ],
        "tools_dico": [
            {
                "name": "FastTree",
                "uri": "https://bio.tools/fasttree",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0547",
                                    "term": "Phylogenetic inference (maximum likelihood and Bayesian methods)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0540",
                                    "term": "Phylogenetic inference (from molecular sequences)"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0547",
                                    "term": "Phylogenetic tree construction (maximum likelihood and Bayesian methods)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0547",
                                    "term": "Phylogenetic tree generation (maximum likelihood and Bayesian methods)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0540",
                                    "term": "Phylogenetic tree construction (from molecular sequences)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0540",
                                    "term": "Phylogenetic tree generation (from molecular sequences)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Infers approximately-maximum-likelihood phylogenetic trees from alignments of nucleotide or protein sequences.",
                "homepage": "http://www.microbesonline.org/fasttree/"
            }
        ],
        "inputs": [
            "name",
            "alignment"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label \"fasttree\""
        ],
        "when": "",
        "stub": ""
    },
    "makeblastdatabase": {
        "name_process": "makeblastdatabase",
        "string_process": "process makeblastdatabase {\n    label 'blast'   \n  input:\n    tuple val(name), path(fasta) \n  output:\n\t  tuple val(name), path(\"blast_database\", type: 'dir') \n  script:\n    \"\"\"\n  \tmakeblastdb -in ${fasta} -dbtype nucl -parse_seqids -out blast_database -title ${name}_db\n    \"\"\"\n}",
        "nb_lignes_process": 9,
        "string_script": "    \"\"\"\n  \tmakeblastdb -in ${fasta} -dbtype nucl -parse_seqids -out blast_database -title ${name}_db\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'blast'"
        ],
        "when": "",
        "stub": ""
    },
    "blastn": {
        "name_process": "blastn",
        "string_process": "\nprocess blastn {\n    label 'blast'   \n  input:\n    tuple val(name), path(fasta), path(database)\n    each method\n  output:\n\t  tuple val(name), path(\"*.tab\") \n  script:\n    \"\"\"\n  \tblastn -query ${fasta} -db ${database} -out ${name}.blast -outfmt \"6 sseqid qlen length sstart send\" -num_threads ${task.cpus} -evalue 10E-80\n    \"\"\"\n}",
        "nb_lignes_process": 11,
        "string_script": "    \"\"\"\n  \tblastn -query ${fasta} -db ${database} -out ${name}.blast -outfmt \"6 sseqid qlen length sstart send\" -num_threads ${task.cpus} -evalue 10E-80\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "G-BLASTN"
        ],
        "tools_url": [
            "https://bio.tools/g-blastn"
        ],
        "tools_dico": [
            {
                "name": "G-BLASTN",
                "uri": "https://bio.tools/g-blastn",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acids"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0346",
                                    "term": "Sequence similarity search"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2976",
                                "term": "Protein sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0857",
                                "term": "Sequence search results"
                            }
                        ]
                    }
                ],
                "description": "GPU-accelerated nucleotide alignment tool based on the widely used NCBI-BLAST.",
                "homepage": "http://www.comp.hkbu.edu.hk/~chxw/software/G-BLASTN.html"
            }
        ],
        "inputs": [
            "name",
            "fasta",
            "database",
            "method"
        ],
        "nb_inputs": 4,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'blast'"
        ],
        "when": "",
        "stub": ""
    },
    "flye": {
        "name_process": "flye",
        "string_process": "process flye {\n    label 'flye'  \n    errorStrategy 'ignore'\n  input:\n    tuple val(name), path(read)\n  output:\n    tuple val(name), path(read), path(\"${name}.fasta\")\n  script:\n    \"\"\"\n    flye --plasmids --meta -t ${task.cpus} --nano-raw ${read} -o assembly \n    mv assembly/assembly.fasta ${name}.fasta \n    \"\"\"\n  }",
        "nb_lignes_process": 11,
        "string_script": "    \"\"\"\n    flye --plasmids --meta -t ${task.cpus} --nano-raw ${read} -o assembly \n    mv assembly/assembly.fasta ${name}.fasta \n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "Flye"
        ],
        "tools_url": [
            "https://bio.tools/Flye"
        ],
        "tools_dico": [
            {
                "name": "Flye",
                "uri": "https://bio.tools/Flye",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Whole genome sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Genome sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "WGS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0523",
                                    "term": "Mapping assembly"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0524",
                                    "term": "De-novo assembly"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0523",
                                    "term": "Sequence assembly (mapping assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0524",
                                    "term": "De Bruijn graph"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0524",
                                    "term": "Sequence assembly (de-novo assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Flye is a de novo assembler for single molecule sequencing reads, such as those produced by PacBio and Oxford Nanopore Technologies. It is designed for a wide range of datasets, from small bacterial projects to large mammalian-scale assemblies. The package represents a complete pipeline: it takes raw PB / ONT reads as input and outputs polished contigs.",
                "homepage": "https://github.com/fenderglass/Flye"
            }
        ],
        "inputs": [
            "name",
            "read"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'flye'",
            "errorStrategy 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "gviz": {
        "name_process": "gviz",
        "string_process": "process gviz {\n    publishDir \"${params.output}/\", mode: 'copy'\n    label 'gviz' \n  input:\n    path(csv) \n  output:\n\t  path(\"overview.svg\") \n  script:\n    \"\"\"\n    gviz.R\n    \"\"\"\n\n}",
        "nb_lignes_process": 11,
        "string_script": "    \"\"\"\n    gviz.R\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "csv"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/\", mode: 'copy'",
            "label 'gviz'"
        ],
        "when": "",
        "stub": ""
    },
    "gtdbtk": {
        "name_process": "gtdbtk",
        "string_process": "\nprocess gtdbtk {\n      publishDir \"${params.output}/${name}/gtdbtk\", mode: 'copy', pattern: \"${name}-results\"\n      label 'gtdbtk'\n    input:\n      tuple val(name), path(dir) \n      file(database) \n    output:\n      tuple val(name), file(\"${name}-results\")\n    script:\n      \"\"\"\n      tar xzf ${database}\n      rm ${database}\n\n      DBNAME=\\$(echo release*/)\n      export GTDBTK_DATA_PATH=./\\${DBNAME}\n      mkdir -p scratch_dir\n\n      # file suffix get\n      SUFFIXNAME=\\$(ls ${dir} | head -1 | rev | cut -f1 -d \".\" | rev)\n\n      gtdbtk classify_wf  --genome_dir ${dir} --cpus 1 --out_dir ${name}-results -x \\${SUFFIXNAME} --scratch_dir scratch_dir\n      \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "      \"\"\"\n      tar xzf ${database}\n      rm ${database}\n\n      DBNAME=\\$(echo release*/)\n      export GTDBTK_DATA_PATH=./\\${DBNAME}\n      mkdir -p scratch_dir\n\n      # file suffix get\n      SUFFIXNAME=\\$(ls ${dir} | head -1 | rev | cut -f1 -d \".\" | rev)\n\n      gtdbtk classify_wf  --genome_dir ${dir} --cpus 1 --out_dir ${name}-results -x \\${SUFFIXNAME} --scratch_dir scratch_dir\n      \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "dir",
            "database"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}/gtdbtk\", mode: 'copy', pattern: \"${name}-results\"",
            "label 'gtdbtk'"
        ],
        "when": "",
        "stub": ""
    },
    "checkm": {
        "name_process": "checkm",
        "string_process": "process checkm {\n        label 'checkm'\n        publishDir \"${params.output}/${name}/quality_of_bins/\", mode: 'copy', pattern: \"summary.txt\"\n        publishDir \"${params.output}/${name}/quality_of_bins/\", mode: 'copy', pattern: \"taxonomy.txt\"\n        publishDir \"${params.output}/${name}/quality_of_bins/\", mode: 'copy', pattern: \"bin_qa_plot.png\"\n    input:\n        tuple val(name), path(bins)\n    output:\n        tuple val(name), path(\"summary.txt\")\n        tuple val(name), path(\"taxonomy.txt\"), file(\"bin_qa_plot.png\")\n    script:\n        \"\"\"\n        SUFFIXNAME=\\$(ls ${bins} | head -1 | rev | cut -f1 -d \".\" | rev)\n\n        mkdir temporary\n        mkdir ${name}_bin\n        cp ${bins}/*.fa* ${name}_bin/\n\n        checkm lineage_wf --tmpdir temporary --pplacer_threads 4 -t ${task.cpus} --reduced_tree -x \\$SUFFIXNAME ${name}_bin ${name}_checkm > summary.txt\n        checkm bin_qa_plot --image_type png -x \\$SUFFIXNAME ${name}_checkm ${name}_bin ${name}_checkm_plot\n        checkm tree_qa ${name}_checkm > taxonomy.txt\n\n        mv ${name}_checkm_plot/bin_qa_plot.png bin_qa_plot.png\n        \"\"\"\n    stub:\n    \"\"\"\n    touch summary.txt taxonomy.txt bin_qa_plot.png\n    \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "        \"\"\"\n        SUFFIXNAME=\\$(ls ${bins} | head -1 | rev | cut -f1 -d \".\" | rev)\n\n        mkdir temporary\n        mkdir ${name}_bin\n        cp ${bins}/*.fa* ${name}_bin/\n\n        checkm lineage_wf --tmpdir temporary --pplacer_threads 4 -t ${task.cpus} --reduced_tree -x \\$SUFFIXNAME ${name}_bin ${name}_checkm > summary.txt\n        checkm bin_qa_plot --image_type png -x \\$SUFFIXNAME ${name}_checkm ${name}_bin ${name}_checkm_plot\n        checkm tree_qa ${name}_checkm > taxonomy.txt\n\n        mv ${name}_checkm_plot/bin_qa_plot.png bin_qa_plot.png\n        \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "bins"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name",
            "name"
        ],
        "nb_outputs": 2,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'checkm'",
            "publishDir \"${params.output}/${name}/quality_of_bins/\", mode: 'copy', pattern: \"summary.txt\"",
            "publishDir \"${params.output}/${name}/quality_of_bins/\", mode: 'copy', pattern: \"taxonomy.txt\"",
            "publishDir \"${params.output}/${name}/quality_of_bins/\", mode: 'copy', pattern: \"bin_qa_plot.png\""
        ],
        "when": "",
        "stub": "\n    \"\"\"\n    touch summary.txt taxonomy.txt bin_qa_plot.png\n    \"\"\""
    },
    "centrifuge_download_db": {
        "name_process": "centrifuge_download_db",
        "string_process": "process centrifuge_download_db {\n        storeDir \"${params.databases}/centrifuge\"\n        label 'ubuntu'    \n      output:\n        file(\"gtdb_r89_54k_centrifuge.tar\")\n      script:\n        \"\"\"\n        wget https://monash.figshare.com/ndownloader/files/16378439 -O gtdb_r89_54k_centrifuge.tar\n        \"\"\"\n    }",
        "nb_lignes_process": 8,
        "string_script": "        \"\"\"\n        wget https://monash.figshare.com/ndownloader/files/16378439 -O gtdb_r89_54k_centrifuge.tar\n        \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "storeDir \"${params.databases}/centrifuge\"",
            "label 'ubuntu'"
        ],
        "when": "",
        "stub": ""
    },
    "bedtools": {
        "name_process": "bedtools",
        "string_process": "\nprocess bedtools {\n  label 'bedtools'\n    input:\n      tuple val(name) , file(bamfile)\n    output:\n      tuple val(name) , file(\"myseq.bedGraph\")\n    shell:\n    \"\"\"\n    bedtools genomecov -bg -ibam ${bamfile} > myseq.bedGraph\n    \"\"\"\n}",
        "nb_lignes_process": 10,
        "string_script": "    \"\"\"\n    bedtools genomecov -bg -ibam ${bamfile} > myseq.bedGraph\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "BEDTools"
        ],
        "tools_url": [
            "https://bio.tools/bedtools"
        ],
        "tools_dico": [
            {
                "name": "BEDTools",
                "uri": "https://bio.tools/bedtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Cartography"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "BEDTools is an extensive suite of utilities for comparing genomic features in BED format.",
                "homepage": "https://github.com/arq5x/bedtools2"
            }
        ],
        "inputs": [
            "name",
            "bamfile"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'bedtools'"
        ],
        "when": "",
        "stub": ""
    },
    "mafft": {
        "name_process": "mafft",
        "string_process": "process mafft {\n    label 'mafft'\n  input:\n    tuple val(name), path(dir) \n  output:\n\t  tuple val(name), path(\"protein_alignments.msa\")\n  script:\n    if (!params.filenames)\n    \"\"\"\n    cat ${dir}/* > all_proteins.aa\n  \tmafft --thread ${task.cpus} --auto all_proteins.aa > protein_alignments.msa\n    \"\"\"\n    else if (params.filenames)\n    \"\"\"\n    for file in ${dir}/* ; do\n      filename=\\$(basename \\${file})\n      cat \\${file} | sed \"1s/.*/>\\${filename%.*}/\"   >> all_proteins.aa\n    done\n\n  \tmafft --thread ${task.cpus} --auto all_proteins.aa > protein_alignments.msa\n    \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "    if (!params.filenames)\n    \"\"\"\n    cat ${dir}/* > all_proteins.aa\n  \tmafft --thread ${task.cpus} --auto all_proteins.aa > protein_alignments.msa\n    \"\"\"\n    else if (params.filenames)\n    \"\"\"\n    for file in ${dir}/* ; do\n      filename=\\$(basename \\${file})\n      cat \\${file} | sed \"1s/.*/>\\${filename%.*}/\"   >> all_proteins.aa\n    done\n\n  \tmafft --thread ${task.cpus} --auto all_proteins.aa > protein_alignments.msa\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "MAFFT"
        ],
        "tools_url": [
            "https://bio.tools/MAFFT"
        ],
        "tools_dico": [
            {
                "name": "MAFFT",
                "uri": "https://bio.tools/MAFFT",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0492",
                                    "term": "Multiple sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0492",
                                    "term": "Multiple alignment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            }
                        ]
                    }
                ],
                "description": "MAFFT (Multiple Alignment using Fast Fourier Transform) is a high speed multiple sequence alignment program.",
                "homepage": "http://mafft.cbrc.jp/alignment/server/index.html"
            }
        ],
        "inputs": [
            "name",
            "dir"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'mafft'"
        ],
        "when": "",
        "stub": ""
    },
    "bwaUnmapped": {
        "name_process": "bwaUnmapped",
        "string_process": "process bwaUnmapped {\n      publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}_non_mappers1_ids.lst\"\n      label 'bwa' \n    input:\n      tuple val(name), file(shortreads)\n      file(assembly) \n    output:\n      tuple val(name), file(\"${name}_non_mappers1_ids.lst\"), file(shortreads)\n    script:\n      \"\"\"\n      bwa index ${assembly}\n      bwa mem -t ${task.cpus} ${assembly} ${shortreads[0]} ${shortreads[1]} > mapped-genome.sam\n      samtools view -S -f4 mapped-genome.sam > sample.unmapped.sam\n      cut -f1 sample.unmapped.sam | sort | uniq > ${name}_non_mappers1_ids.lst\n      \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "      \"\"\"\n      bwa index ${assembly}\n      bwa mem -t ${task.cpus} ${assembly} ${shortreads[0]} ${shortreads[1]} > mapped-genome.sam\n      samtools view -S -f4 mapped-genome.sam > sample.unmapped.sam\n      cut -f1 sample.unmapped.sam | sort | uniq > ${name}_non_mappers1_ids.lst\n      \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "BWA",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/bwa",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "name",
            "shortreads",
            "assembly"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}_non_mappers1_ids.lst\"",
            "label 'bwa'"
        ],
        "when": "",
        "stub": ""
    },
    "fargene": {
        "name_process": "fargene",
        "string_process": "process fargene {\n    label 'fargene'\n    errorStrategy 'ignore'\n    validExitStatus 0,1\n  input:\n    tuple val(name), val(splitname), val(type), path(fasta) \n    each method\n  output:\n    path(\"${method}_${type}_${name}_*.fargene\") \n  script:\n    \"\"\"\n    filename=\\$(ls ${fasta})\n    touch \"${method}_${type}_${name}_\\${PWD##*/}.fargene\"\n  \tfargene -i ${fasta} --hmm-model ${method} -o ${name}_results \n    cp ${name}_results/results_summary.txt  ${method}_${type}_${name}_\\${PWD##*/}gencount.fargene\n    cat ${name}_results/hmmsearchresults/retrieved-genes-*-hmmsearched.out |  grep -v \"^#\" | sed -e s/\"\\${filename%.*}_\"//g > ${method}_${type}_${name}_\\${PWD##*/}coverage.fargene\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    filename=\\$(ls ${fasta})\n    touch \"${method}_${type}_${name}_\\${PWD##*/}.fargene\"\n  \tfargene -i ${fasta} --hmm-model ${method} -o ${name}_results \n    cp ${name}_results/results_summary.txt  ${method}_${type}_${name}_\\${PWD##*/}gencount.fargene\n    cat ${name}_results/hmmsearchresults/retrieved-genes-*-hmmsearched.out |  grep -v \"^#\" | sed -e s/\"\\${filename%.*}_\"//g > ${method}_${type}_${name}_\\${PWD##*/}coverage.fargene\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "splitname",
            "type",
            "fasta",
            "method"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'fargene'",
            "errorStrategy 'ignore'",
            "validExitStatus 0,1"
        ],
        "when": "",
        "stub": ""
    },
    "fargene_plasmid_screen": {
        "name_process": "fargene_plasmid_screen",
        "string_process": "\nprocess fargene_plasmid_screen {\n    label 'fargene'\n    validExitStatus 0,1\n                                                          \n  input:\n    tuple val(name), path(fasta) \n    each method\n  output:\n    tuple val(name), path(\"${method}_${name}_*.fargene\") optional true\n  script:\n    \"\"\"\n    filename=\\$(ls ${fasta})\n  \tfargene -i ${fasta} --hmm-model ${method} -o ${name}_results \n    cat ${name}_results/hmmsearchresults/retrieved-genes-*-hmmsearched.out |  grep -v \"^#\" | sed -e s/\"\\${filename%.*}_\"//g > ${method}_${name}_\\${PWD##*/}.fargene\n    \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "    \"\"\"\n    filename=\\$(ls ${fasta})\n  \tfargene -i ${fasta} --hmm-model ${method} -o ${name}_results \n    cat ${name}_results/hmmsearchresults/retrieved-genes-*-hmmsearched.out |  grep -v \"^#\" | sed -e s/\"\\${filename%.*}_\"//g > ${method}_${name}_\\${PWD##*/}.fargene\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "fasta",
            "method"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'fargene'",
            "validExitStatus 0,1"
        ],
        "when": "",
        "stub": ""
    },
    "abricatePlot": {
        "name_process": "abricatePlot",
        "string_process": "process abricatePlot {\n    publishDir \"${params.output}/${name}/ABR-Screening\", mode: 'copy', pattern: \"*.pdf\"\n    label 'ggplot2'\n    echo true\n    errorStrategy { task.exitStatus in 1..1 ? 'ignore' : 'terminate'}\n  input:\n    tuple val(name), file(results) \n  output:\n\t  tuple val(name), file(\"*.pdf\") optional true\n  script:\n    \"\"\"\n    #!/usr/bin/Rscript\n\n    library(ggplot2)\n    \n    inputdata <- read.table(\"${results}\", header = TRUE, sep = \";\")\n    \n    pdf(\"classification-overview.pdf\", height = 6, width = 10)\n      ggplot(data=inputdata, aes(x=type, y=amount, fill=group)) +\n      geom_bar(stat=\"identity\") +\n      theme(legend.position = \"none\") +\n      facet_wrap(~ group, scales = \"free_y\") + coord_flip()\n    dev.off()\n    \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "    \"\"\"\n    #!/usr/bin/Rscript\n\n    library(ggplot2)\n    \n    inputdata <- read.table(\"${results}\", header = TRUE, sep = \";\")\n    \n    pdf(\"classification-overview.pdf\", height = 6, width = 10)\n      ggplot(data=inputdata, aes(x=type, y=amount, fill=group)) +\n      geom_bar(stat=\"identity\") +\n      theme(legend.position = \"none\") +\n      facet_wrap(~ group, scales = \"free_y\") + coord_flip()\n    dev.off()\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "results"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}/ABR-Screening\", mode: 'copy', pattern: \"*.pdf\"",
            "label 'ggplot2'",
            "echo true",
            "errorStrategy { task.exitStatus in 1..1 ? 'ignore' : 'terminate'}"
        ],
        "when": "",
        "stub": ""
    },
    "sourmashclusterfasta": {
        "name_process": "sourmashclusterfasta",
        "string_process": "process sourmashclusterfasta {\n      publishDir \"${params.output}/${name}/cluster/\", mode: 'copy', pattern: \"*.pdf\"\n      label 'sourmash'\n    input:\n      tuple val(name), file(fasta) \n    output:\n      tuple val(name), file(\"*.pdf\")\n      tuple val(name), file(\"results.csv\")\n    script:\n      \"\"\"\n      sourmash sketch dna -p scaled=10000,k=31 ${fasta} -o signature.sig --singleton\n      sourmash compare signature.sig -o results_sig --csv results.csv\n      sourmash plot --pdf --subsample=250 --labels results_sig\n      \"\"\"\n}",
        "nb_lignes_process": 13,
        "string_script": "      \"\"\"\n      sourmash sketch dna -p scaled=10000,k=31 ${fasta} -o signature.sig --singleton\n      sourmash compare signature.sig -o results_sig --csv results.csv\n      sourmash plot --pdf --subsample=250 --labels results_sig\n      \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "sourmash"
        ],
        "tools_url": [
            "https://bio.tools/sourmash"
        ],
        "tools_dico": [
            {
                "name": "sourmash",
                "uri": "https://bio.tools/sourmash",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3307",
                            "term": "Computational biology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0346",
                                    "term": "Sequence similarity search"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix generation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Phylogenetic distance matrix generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2975",
                                "term": "Nucleic acid sequence (raw)"
                            }
                        ],
                        "output": []
                    }
                ],
                "description": "Compute and compare MinHash signatures for DNA data sets.",
                "homepage": "https://sourmash.readthedocs.io/en/latest/"
            }
        ],
        "inputs": [
            "name",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name",
            "name"
        ],
        "nb_outputs": 2,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}/cluster/\", mode: 'copy', pattern: \"*.pdf\"",
            "label 'sourmash'"
        ],
        "when": "",
        "stub": ""
    },
    "rmetaplot": {
        "name_process": "rmetaplot",
        "string_process": "process rmetaplot {\n      publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"Metagenomic-distribution.pdf\"\n      label 'rmetaplot'\n    input:\n      tuple val(name), file(sourmeta) \n    output:\n      tuple val(name), file(\"Metagenomic-distribution.pdf\")\n    script:\n      \"\"\"\n      shell-extract.sh\n      dot-diagram.R summary.results superkindoms.results \n      \"\"\"\n\n}",
        "nb_lignes_process": 12,
        "string_script": "      \"\"\"\n      shell-extract.sh\n      dot-diagram.R summary.results superkindoms.results \n      \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "sourmeta"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"Metagenomic-distribution.pdf\"",
            "label 'rmetaplot'"
        ],
        "when": "",
        "stub": ""
    },
    "abricateBatch": {
        "name_process": "abricateBatch",
        "string_process": "process abricateBatch {\n    label 'abricate'   \n  input:\n    tuple val(name), file(fasta) \n    each method\n  output:\n\t  tuple val(name), file(\"*.tab\") \n  script:\n    \"\"\"\n  \tabricate ${fasta} --nopath --quiet --mincov 80 --db ${method} > ${method}.tab\n    \"\"\"\n}",
        "nb_lignes_process": 10,
        "string_script": "    \"\"\"\n  \tabricate ${fasta} --nopath --quiet --mincov 80 --db ${method} > ${method}.tab\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "ABRicate"
        ],
        "tools_url": [
            "https://bio.tools/ABRicate"
        ],
        "tools_dico": [
            {
                "name": "ABRicate",
                "uri": "https://bio.tools/ABRicate",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3482",
                                    "term": "Antimicrobial resistance prediction"
                                }
                            ],
                            []
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_1234",
                                "term": "Sequence set (nucleic acid)"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0916",
                                "term": "Gene report"
                            }
                        ]
                    }
                ],
                "description": "Mass screening of contigs for antimicrobial resistance or virulence genes.",
                "homepage": "https://github.com/tseemann/abricate"
            }
        ],
        "inputs": [
            "name",
            "fasta",
            "method"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'abricate'"
        ],
        "when": "",
        "stub": ""
    },
    "prokka": {
        "name_process": "prokka",
        "string_process": "process prokka {\n      publishDir \"${params.output}/${name}/${params.assemblydir}\", mode: 'copy', pattern: \"${name}.gff\"\n      publishDir \"${params.output}/${name}/${params.assemblydir}\", mode: 'copy', pattern: \"${name}.gbk\"\n      label 'prokka'\n    input:\n      tuple val(name), file(assembly) \n    output:\n      tuple val(name), file(\"${name}.gff\"), file(\"${name}.gbk\")\n    script:\n      \"\"\"\n    \tprokka --cpus ${task.cpus} --outdir output --prefix annotation ${assembly}\n      mv output/annotation.gff ${name}.gff\n      mv output/annotation.gbk ${name}.gbk\n      \"\"\"\n}",
        "nb_lignes_process": 13,
        "string_script": "      \"\"\"\n    \tprokka --cpus ${task.cpus} --outdir output --prefix annotation ${assembly}\n      mv output/annotation.gff ${name}.gff\n      mv output/annotation.gbk ${name}.gbk\n      \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "Prokka"
        ],
        "tools_url": [
            "https://bio.tools/prokka"
        ],
        "tools_dico": [
            {
                "name": "Prokka",
                "uri": "https://bio.tools/prokka",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "Coding region prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0362",
                                    "term": "Genome annotation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "ORF prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "ORF finding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene finding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene calling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Software tool to annotate bacterial, archaeal and viral genomes quickly and produce standards-compliant output files.",
                "homepage": "https://github.com/tseemann/prokka"
            }
        ],
        "inputs": [
            "name",
            "assembly"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}/${params.assemblydir}\", mode: 'copy', pattern: \"${name}.gff\"",
            "publishDir \"${params.output}/${name}/${params.assemblydir}\", mode: 'copy', pattern: \"${name}.gbk\"",
            "label 'prokka'"
        ],
        "when": "",
        "stub": ""
    },
    "abricatePlotFASTA": {
        "name_process": "abricatePlotFASTA",
        "string_process": "process abricatePlotFASTA {\n    publishDir \"${params.output}/${name}/ABR-Screening\", mode: 'copy', pattern: \"*.pdf\"\n    label 'ggplot2'\n    errorStrategy { task.exitStatus in 1..1 ? 'ignore' : 'terminate'}\n  input:\n    tuple val(name), val(method), file(results) \n  output:\n\t  tuple val(name), val(method), file(\"*.pdf\") optional true\n  script:\n    \"\"\"\n    #!/usr/bin/Rscript\n\n    library(ggplot2)\n    \n    inputdata <- read.table(\"${results}\", header = TRUE, sep = \";\")\n    \n    pdf(\"classification-${method}.pdf\", height = 6, width = 10)\n      ggplot(data=inputdata, aes(x=type, y=amount, fill=group)) +\n      geom_bar(stat=\"identity\") +\n      theme(legend.position = \"none\") +\n      facet_wrap(~ group, scales = \"free_y\") + coord_flip()\n    dev.off()\n    \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "    \"\"\"\n    #!/usr/bin/Rscript\n\n    library(ggplot2)\n    \n    inputdata <- read.table(\"${results}\", header = TRUE, sep = \";\")\n    \n    pdf(\"classification-${method}.pdf\", height = 6, width = 10)\n      ggplot(data=inputdata, aes(x=type, y=amount, fill=group)) +\n      geom_bar(stat=\"identity\") +\n      theme(legend.position = \"none\") +\n      facet_wrap(~ group, scales = \"free_y\") + coord_flip()\n    dev.off()\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "method",
            "results"
        ],
        "nb_inputs": 3,
        "outputs": [
            "method"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}/ABR-Screening\", mode: 'copy', pattern: \"*.pdf\"",
            "label 'ggplot2'",
            "errorStrategy { task.exitStatus in 1..1 ? 'ignore' : 'terminate'}"
        ],
        "when": "",
        "stub": ""
    },
    "centrifuge": {
        "name_process": "centrifuge",
        "string_process": "process centrifuge {\n      publishDir \"${params.output}/${name}/centrifuge\", mode: 'copy', pattern: \"${name}_pavian_report_filtered.csv\"\n      publishDir \"${params.output}/${name}/centrifuge/raw\", mode: 'copy', pattern: \"${name}_centrifuge_filtered.out\"\n      publishDir \"${params.output}/${name}/centrifuge\", mode: 'copy', pattern: \"${name}_pavian_report_unfiltered.csv\"\n      publishDir \"${params.output}/${name}/centrifuge/raw\", mode: 'copy', pattern: \"${name}_centrifuge_unfiltered.out\"\n      label 'centrifuge'\n    input:\n      tuple val(name), file(fastq) \n      path(database) \n    output:\n      tuple val(name), file(\"${name}_centrifuge_filtered.out\"), file(\"${name}_pavian_report_filtered.csv\")\n      tuple val(name), file(\"${name}_centrifuge_unfiltered.out\"), file(\"${name}_pavian_report_unfiltered.csv\")\n    script:\n      \"\"\"\n      case \"${database}\" in\n      *.tar.gz)\n        tar xzf ${database}\n        ;;\n      *.gz | *.tgz ) \n        gzip -d ${database}\n        ;;\n      *.tar)\n        tar xf ${database}\n        ;;\n      esac\n      \n      DBname=\\$(ls *.[1-9].cf | head -1 | cut -f1 -d\".\")\n\n      # command\n        centrifuge -p ${task.cpus} -x \\${DBname} -k 5 --min-hitlen 16 \\\n        -U ${fastq} -S ${name}_centrifuge_unfiltered.out --report-file centrifuge_out.log\n\n\n      # filter\n        filter_centrifuge_hits.sh ${name}_centrifuge_unfiltered.out 150 250 ${name}_centrifuge_filtered.out\n\n      # reports\n        centrifuge-kreport -x \\${DBname} --min-score 300 --min-length 500 ${name}_centrifuge_filtered.out \\\n        > ${name}_pavian_report_filtered.csv\n\n        centrifuge-kreport -x \\${DBname} --min-score 300 --min-length 500 ${name}_centrifuge_unfiltered.out \\\n        > ${name}_pavian_report_unfiltered.csv\n      \"\"\"\n}",
        "nb_lignes_process": 42,
        "string_script": "      \"\"\"\n      case \"${database}\" in\n      *.tar.gz)\n        tar xzf ${database}\n        ;;\n      *.gz | *.tgz ) \n        gzip -d ${database}\n        ;;\n      *.tar)\n        tar xf ${database}\n        ;;\n      esac\n      \n      DBname=\\$(ls *.[1-9].cf | head -1 | cut -f1 -d\".\")\n\n      # command\n        centrifuge -p ${task.cpus} -x \\${DBname} -k 5 --min-hitlen 16 \\\n        -U ${fastq} -S ${name}_centrifuge_unfiltered.out --report-file centrifuge_out.log\n\n\n      # filter\n        filter_centrifuge_hits.sh ${name}_centrifuge_unfiltered.out 150 250 ${name}_centrifuge_filtered.out\n\n      # reports\n        centrifuge-kreport -x \\${DBname} --min-score 300 --min-length 500 ${name}_centrifuge_filtered.out \\\n        > ${name}_pavian_report_filtered.csv\n\n        centrifuge-kreport -x \\${DBname} --min-score 300 --min-length 500 ${name}_centrifuge_unfiltered.out \\\n        > ${name}_pavian_report_unfiltered.csv\n      \"\"\"",
        "nb_lignes_script": 29,
        "language_script": "bash",
        "tools": [
            "CASE",
            "Centrifuge"
        ],
        "tools_url": [
            "https://bio.tools/CASE",
            "https://bio.tools/centrifuge"
        ],
        "tools_dico": [
            {
                "name": "CASE",
                "uri": "https://bio.tools/CASE",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0749",
                            "term": "Transcription factors and regulatory sites"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0089",
                            "term": "Ontology and terminology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3436",
                                    "term": "Aggregation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3435",
                                    "term": "Standardisation and normalisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3559",
                                    "term": "Ontology visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3559",
                                    "term": "Ontology browsing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Advancing Coordinated Cyber-investigations and Tool Interoperability using a Community Developed Specification Language.\n\nSource files for the CASE website.\n\nAPI used for instantiating CASE objects (includes ontological verification and type checking).\n\nCyber-investigation Analysis Standard Expression (CASE).\n\nRead the CASE Wiki tab to learn everything you need to know about the Cyber-investigation Analysis Standard Expression (CASE) ontology. For learning about the Unified Cyber Ontology, CASE's parent, see UCO.\n\n\"@vocab\": \"http://case.example.org/core#\",.\n\nDET ER DINE PENGER DET DREIER SEG OM...\n\nVi er ikke st\ufffdrst, men garanterer effektiv behandling.\n\nLast ned v\ufffdr brosjyre i PDF format.\n\n||| COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/pymzml (GITHUB.COM).\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'UCO', 'cyber-investigation', 'cyber-investigations', 'plaso'",
                "homepage": "http://CASE.as"
            },
            {
                "name": "Centrifuge",
                "uri": "https://bio.tools/centrifuge",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Nucleic acid sequence analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Sequence analysis (nucleic acid)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A very rapid and memory-efficient system for the classification of DNA sequences from microbial samples. The system uses a novel indexing scheme based on the Burrows-Wheeler transform and the Ferragina-Manzini index, optimized specifically for the metagenomic classification problem. Together these advances enable timely and accurate analysis of large metagenomics data sets on conventional desktop computers.",
                "homepage": "https://ccb.jhu.edu/software/centrifuge/"
            }
        ],
        "inputs": [
            "name",
            "fastq",
            "database"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name",
            "name"
        ],
        "nb_outputs": 2,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}/centrifuge\", mode: 'copy', pattern: \"${name}_pavian_report_filtered.csv\"",
            "publishDir \"${params.output}/${name}/centrifuge/raw\", mode: 'copy', pattern: \"${name}_centrifuge_filtered.out\"",
            "publishDir \"${params.output}/${name}/centrifuge\", mode: 'copy', pattern: \"${name}_pavian_report_unfiltered.csv\"",
            "publishDir \"${params.output}/${name}/centrifuge/raw\", mode: 'copy', pattern: \"${name}_centrifuge_unfiltered.out\"",
            "label 'centrifuge'"
        ],
        "when": "",
        "stub": ""
    },
    "fastqTofasta": {
        "name_process": "fastqTofasta",
        "string_process": "process fastqTofasta {\n                                                                                             \n      label 'emboss'\n    input:\n      tuple val(name), file(fastq)\n    output:\n      tuple val(name), file(\"filtered_reads.fasta\")\n    script:\n      \"\"\"\n      seqret -sequence ${fastq} -outseq filtered_reads.fasta\n      \"\"\"\n}",
        "nb_lignes_process": 10,
        "string_script": "      \"\"\"\n      seqret -sequence ${fastq} -outseq filtered_reads.fasta\n      \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "seqret"
        ],
        "tools_url": [
            "https://bio.tools/seqret"
        ],
        "tools_dico": [
            {
                "name": "seqret",
                "uri": "https://bio.tools/seqret",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Biological databases"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Data management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Information systems"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Databases and information systems"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data extraction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Retrieval"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0849",
                                "term": "Sequence record"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0849",
                                "term": "Sequence record"
                            }
                        ]
                    }
                ],
                "description": "Read and write (return) sequences.",
                "homepage": "http://emboss.open-bio.org/rel/rel6/apps/seqret.html"
            }
        ],
        "inputs": [
            "name",
            "fastq"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'emboss'"
        ],
        "when": "",
        "stub": ""
    },
    "krona": {
        "name_process": "krona",
        "string_process": "process krona {\n      publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"${name}.html\"\n      label 'krona'\n    input:\n      tuple val(name), file (krona)\n    output:\n      tuple val(name), file(\"${name}.html\")\n    script:\n      \"\"\"\n      ktImportTaxonomy classification_results.EM.reads2Taxon.krona\n      mv taxonomy.krona.html ${name}.html\n      \"\"\"\n}",
        "nb_lignes_process": 11,
        "string_script": "      \"\"\"\n      ktImportTaxonomy classification_results.EM.reads2Taxon.krona\n      mv taxonomy.krona.html ${name}.html\n      \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "krona"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"${name}.html\"",
            "label 'krona'"
        ],
        "when": "",
        "stub": ""
    },
    "sourmashmeta": {
        "name_process": "sourmashmeta",
        "string_process": "process sourmashmeta {\n      publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"metagenomic-composition.txt\"\n      label 'sourmash'\n    input:\n      tuple val(name), file(fasta) \n      file(database) \n    output:\n      tuple val(name), file(\"metagenomic-composition.txt\")\n    script:\n      if (params.fasta)\n      \"\"\"\n      sourmash sketch dna -p scaled=10000,k=31 ${fasta} -o ${name}.sig \n      sourmash gather  ${name}.sig ${database} --ignore-abundance -o metagenomic-composition.txt\n      \"\"\"\n      else if (params.fastq)\n      \"\"\"\n      sourmash sketch dna -p scaled=10000,k=31 --track-abundance ${fasta} -o ${name}.sig \n      sourmash gather  ${name}.sig ${database} -o metagenomic-composition.txt\n      \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "      if (params.fasta)\n      \"\"\"\n      sourmash sketch dna -p scaled=10000,k=31 ${fasta} -o ${name}.sig \n      sourmash gather  ${name}.sig ${database} --ignore-abundance -o metagenomic-composition.txt\n      \"\"\"\n      else if (params.fastq)\n      \"\"\"\n      sourmash sketch dna -p scaled=10000,k=31 --track-abundance ${fasta} -o ${name}.sig \n      sourmash gather  ${name}.sig ${database} -o metagenomic-composition.txt\n      \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [
            "sourmash"
        ],
        "tools_url": [
            "https://bio.tools/sourmash"
        ],
        "tools_dico": [
            {
                "name": "sourmash",
                "uri": "https://bio.tools/sourmash",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3307",
                            "term": "Computational biology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0346",
                                    "term": "Sequence similarity search"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix generation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Phylogenetic distance matrix generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2975",
                                "term": "Nucleic acid sequence (raw)"
                            }
                        ],
                        "output": []
                    }
                ],
                "description": "Compute and compare MinHash signatures for DNA data sets.",
                "homepage": "https://sourmash.readthedocs.io/en/latest/"
            }
        ],
        "inputs": [
            "name",
            "fasta",
            "database"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"metagenomic-composition.txt\"",
            "label 'sourmash'"
        ],
        "when": "",
        "stub": ""
    },
    "sourclusterPlot": {
        "name_process": "sourclusterPlot",
        "string_process": "process sourclusterPlot {\n    publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"cluster_results.html\"\n    label 'bokeh'\n    echo true\n  input:\n    tuple val(name), file(results) \n  output:\n\t  tuple val(name), file(\"cluster_results.html\")\n  script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    from collections import defaultdict\n\n    import numpy as np\n    import scipy.cluster.hierarchy as sch\n\n    from bokeh.plotting import figure, show, save, output_file\n    from bokeh.sampledata.les_mis import data\n\n    with open(\"${results}\", 'r') as file: \n        header = next(file).strip().split(',')\n        D = []  # distance matrix\n        for source in header:\n            values = [float(x) for x in next(file).strip().split(',')]\n            D.append(values)\n\n    D_ = np.array(D)\n    Y = sch.linkage(D_, method='single')\n    Z = sch.dendrogram(Y, orientation='right')\n    ix = Z['leaves']\n\n    D_ = D_[ix, :]\n    D_ = D_[:, ix]\n    header_sorted = [header[i] for i in ix]\n\n    l = []\n    for i_, i in enumerate(D_):\n        for j_, j in enumerate(i):\n            l.append([header_sorted[i_], header_sorted[j_], j, '#225ea8'])   # l.append([header_sorted[i_], header_sorted[j_], j, '#ff7f00'])\n\n    xname, yname, alpha, color = zip(*l)\n\n    data=dict(\n        xname=xname,\n        yname=yname,\n        colors=color,\n        alphas=alpha,\n        )\n\n    p = figure(title='Clustered Jaccard distance (sourmash)',\n              x_axis_location='above', tools='hover,save',\n              x_range=list(reversed(header_sorted)), y_range=header_sorted,\n              tooltips = [('names', '@yname, @xname'), ('distance', '@alphas')]\n              )\n\n    p.plot_width = ${params.size}\n    p.plot_height = ${params.size}\n    p.grid.grid_line_color = None\n    p.axis.axis_line_color = None\n    p.axis.major_tick_line_color = None\n    p.axis.major_label_text_font_size = '5pt'\n    p.axis.major_label_standoff = 0\n    p.xaxis.major_label_orientation = np.pi/3\n\n    p.rect('xname', 'yname', 0.9, 0.9, source=data,\n          color='colors', alpha='alphas', line_color=None,\n          hover_line_color='black', hover_color='colors')\n\n\n    output_file(\"cluster_results.html\", title='Clustered Jaccard distance (sourmash)')\n    save(p)\n    \"\"\"\n}",
        "nb_lignes_process": 72,
        "string_script": "    \"\"\"\n    #!/usr/bin/env python\n\n    from collections import defaultdict\n\n    import numpy as np\n    import scipy.cluster.hierarchy as sch\n\n    from bokeh.plotting import figure, show, save, output_file\n    from bokeh.sampledata.les_mis import data\n\n    with open(\"${results}\", 'r') as file: \n        header = next(file).strip().split(',')\n        D = []  # distance matrix\n        for source in header:\n            values = [float(x) for x in next(file).strip().split(',')]\n            D.append(values)\n\n    D_ = np.array(D)\n    Y = sch.linkage(D_, method='single')\n    Z = sch.dendrogram(Y, orientation='right')\n    ix = Z['leaves']\n\n    D_ = D_[ix, :]\n    D_ = D_[:, ix]\n    header_sorted = [header[i] for i in ix]\n\n    l = []\n    for i_, i in enumerate(D_):\n        for j_, j in enumerate(i):\n            l.append([header_sorted[i_], header_sorted[j_], j, '#225ea8'])   # l.append([header_sorted[i_], header_sorted[j_], j, '#ff7f00'])\n\n    xname, yname, alpha, color = zip(*l)\n\n    data=dict(\n        xname=xname,\n        yname=yname,\n        colors=color,\n        alphas=alpha,\n        )\n\n    p = figure(title='Clustered Jaccard distance (sourmash)',\n              x_axis_location='above', tools='hover,save',\n              x_range=list(reversed(header_sorted)), y_range=header_sorted,\n              tooltips = [('names', '@yname, @xname'), ('distance', '@alphas')]\n              )\n\n    p.plot_width = ${params.size}\n    p.plot_height = ${params.size}\n    p.grid.grid_line_color = None\n    p.axis.axis_line_color = None\n    p.axis.major_tick_line_color = None\n    p.axis.major_label_text_font_size = '5pt'\n    p.axis.major_label_standoff = 0\n    p.xaxis.major_label_orientation = np.pi/3\n\n    p.rect('xname', 'yname', 0.9, 0.9, source=data,\n          color='colors', alpha='alphas', line_color=None,\n          hover_line_color='black', hover_color='colors')\n\n\n    output_file(\"cluster_results.html\", title='Clustered Jaccard distance (sourmash)')\n    save(p)\n    \"\"\"",
        "nb_lignes_script": 63,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "results"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"cluster_results.html\"",
            "label 'bokeh'",
            "echo true"
        ],
        "when": "",
        "stub": ""
    },
    "nanoplot": {
        "name_process": "nanoplot",
        "string_process": "process nanoplot {\n    label 'nanoplot'\n      publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"*.html\"\n      publishDir \"${params.output}/${name}/readquality/\", mode: 'copy', pattern: \"*_read_quality.txt\"\n      publishDir \"${params.output}/${name}/readquality/\", mode: 'copy', pattern: \"*.png\"\n      publishDir \"${params.output}/${name}/readquality/\", mode: 'copy', pattern: \"*.pdf\"\n    input:\n      tuple val(name), path(reads)\n    output:\n      tuple val(name), file(\"*.html\"), file(\"*.png\"), file(\"*.pdf\"), file(\"${name}_read_quality.txt\") \n    script:\n      \"\"\"\n      NanoPlot -t ${task.cpus} --fastq ${reads} --title ${name} --color darkslategrey --N50 --plots hex --loglength -f png --store\n      NanoPlot -t ${task.cpus} --pickle NanoPlot-data.pickle --title ${name} --color darkslategrey --N50 --plots hex --loglength -f pdf\n      mv *.html ${name}_read_quality_report.html\n      mv NanoStats.txt ${name}_read_quality.txt\n      \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "      \"\"\"\n      NanoPlot -t ${task.cpus} --fastq ${reads} --title ${name} --color darkslategrey --N50 --plots hex --loglength -f png --store\n      NanoPlot -t ${task.cpus} --pickle NanoPlot-data.pickle --title ${name} --color darkslategrey --N50 --plots hex --loglength -f pdf\n      mv *.html ${name}_read_quality_report.html\n      mv NanoStats.txt ${name}_read_quality.txt\n      \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'nanoplot'",
            "publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"*.html\"",
            "publishDir \"${params.output}/${name}/readquality/\", mode: 'copy', pattern: \"*_read_quality.txt\"",
            "publishDir \"${params.output}/${name}/readquality/\", mode: 'copy', pattern: \"*.png\"",
            "publishDir \"${params.output}/${name}/readquality/\", mode: 'copy', pattern: \"*.pdf\""
        ],
        "when": "",
        "stub": ""
    },
    "samtools": {
        "name_process": "samtools",
        "string_process": "process samtools {\n  label 'samtools'\n  publishDir \"${params.output}/\", mode: 'copy'\n    input:\n      tuple val(name) , file(bamfile)\n    output:\n      tuple val(name), file(\"results_*.txt\")\n    shell:\n    \"\"\"\n    samtools index ${bamfile}\n    samtools idxstats ${bamfile} > results_\\${PWD##*/}.txt\n    \"\"\"\n}",
        "nb_lignes_process": 11,
        "string_script": "    \"\"\"\n    samtools index ${bamfile}\n    samtools idxstats ${bamfile} > results_\\${PWD##*/}.txt\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "name",
            "bamfile"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'samtools'",
            "publishDir \"${params.output}/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "mafft_supp": {
        "name_process": "mafft_supp",
        "string_process": "process mafft_supp {\n    label 'mafft'\n  input:\n    tuple val(name), path(dir)\n    path(supplement_file)\n  output:\n\t  tuple val(name), path(\"protein_alignments.msa\")\n  script:\n    if (!params.filenames)\n    \"\"\"\n    cat ${dir}/* > all_proteins.aa\n    cat ${supplement_file} >> all_proteins.aa\n  \tmafft --thread ${task.cpus} --auto all_proteins.aa > protein_alignments.msa\n    \"\"\"\n    else if (params.filenames)\n    \"\"\"\n    for file in ${dir}/* ; do\n      filename=\\$(basename \\${file})\n      cat \\${file} | sed \"1s/.*/>\\${filename%.*}/\"   >> all_proteins.aa\n    done\n    cat ${supplement_file} >> all_proteins.aa\n    \n  \tmafft --thread ${task.cpus} --auto all_proteins.aa > protein_alignments.msa\n    \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "    if (!params.filenames)\n    \"\"\"\n    cat ${dir}/* > all_proteins.aa\n    cat ${supplement_file} >> all_proteins.aa\n  \tmafft --thread ${task.cpus} --auto all_proteins.aa > protein_alignments.msa\n    \"\"\"\n    else if (params.filenames)\n    \"\"\"\n    for file in ${dir}/* ; do\n      filename=\\$(basename \\${file})\n      cat \\${file} | sed \"1s/.*/>\\${filename%.*}/\"   >> all_proteins.aa\n    done\n    cat ${supplement_file} >> all_proteins.aa\n    \n  \tmafft --thread ${task.cpus} --auto all_proteins.aa > protein_alignments.msa\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "MAFFT"
        ],
        "tools_url": [
            "https://bio.tools/MAFFT"
        ],
        "tools_dico": [
            {
                "name": "MAFFT",
                "uri": "https://bio.tools/MAFFT",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0492",
                                    "term": "Multiple sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0492",
                                    "term": "Multiple alignment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            }
                        ]
                    }
                ],
                "description": "MAFFT (Multiple Alignment using Fast Fourier Transform) is a high speed multiple sequence alignment program.",
                "homepage": "http://mafft.cbrc.jp/alignment/server/index.html"
            }
        ],
        "inputs": [
            "name",
            "dir",
            "supplement_file"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'mafft'"
        ],
        "when": "",
        "stub": ""
    },
    "abricateParser": {
        "name_process": "abricateParser",
        "string_process": "process abricateParser {\n                                                                                         \n    label 'ubuntu'   \n  input:\n    tuple val(name), file(results) \n  output:\n\t  tuple val(name), file(\"*.csv\") \n  shell:\n    \"\"\"\n    printf \"amount;type;group\\\\n\" > !{name}_ncbi.csv\n  \t# beta-lactamase\n      blaData=\\$(grep -w \"ncbi\" !{results} | cut -f6 | grep \"bla\" | cut -f1 -d \"-\"| sort | uniq -c |\\\n        sed -e 's/^[ \\\\t]*//' | tr \" \" \";\" | sed -e 's/\\$/;beta-lactamase/') \n      printf \"\\${blaData}\\\\n\" >> !{name}_ncbi.csv\n    # tetracycline\n      tetData=\\$(grep -w \"ncbi\" !{results} | grep \"tetracycline\" | cut -f6 | grep -v \"bla\" | sort |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;tetracycline-resistance/') \n      printf \"\\${tetData}\\\\n\" >> !{name}_ncbi.csv\n    # aminoglycoside\n      aminoData=\\$(grep -w \"ncbi\" !{results} | grep -v \"efflux\" | grep \"aminoglycoside\" | cut -f6 | grep -v \"bla\" | sort | awk '{print substr(\\$0,0,4)}' |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;aminoglycoside-resistance/') \n      printf \"\\${aminoData}\\\\n\" >> !{name}_ncbi.csv\n    # efflux\n      effluxData=\\$(grep -w \"ncbi\" !{results} | grep -v \"tetracycline\" | grep \"efflux\" | cut -f6 | grep -v \"bla\" | sort | awk '{print substr(\\$0,0,5)}' |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;efflux-system/') \n      printf \"\\${effluxData}\\\\n\" >> !{name}_ncbi.csv\n    # quinolones\n      quinoData=\\$(grep -w \"ncbi\" !{results} |  grep -v \"efflux\" | grep \"quinolone\" | cut -f6 | grep -v \"bla\" | sort | awk '{print substr(\\$0,0,4)}' |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;quinolone-resistance/') \n      printf \"\\${quinoData}\\\\n\" >> !{name}_ncbi.csv\n    # other\n      otherData=\\$(grep -w \"ncbi\" !{results} |  grep -v \"efflux\" | grep -v \"tetracycline\" | grep -v \"aminoglycoside\" | grep -v \"quinolone\" |\\\n        cut -f6 | grep -v \"bla\" |  sort |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;other-resistance-genes/') \n      printf \"\\${otherData}\\\\n\" >> !{name}_ncbi.csv\n    \"\"\"\n}",
        "nb_lignes_process": 35,
        "string_script": "    \"\"\"\n    printf \"amount;type;group\\\\n\" > !{name}_ncbi.csv\n  \t# beta-lactamase\n      blaData=\\$(grep -w \"ncbi\" !{results} | cut -f6 | grep \"bla\" | cut -f1 -d \"-\"| sort | uniq -c |\\\n        sed -e 's/^[ \\\\t]*//' | tr \" \" \";\" | sed -e 's/\\$/;beta-lactamase/') \n      printf \"\\${blaData}\\\\n\" >> !{name}_ncbi.csv\n    # tetracycline\n      tetData=\\$(grep -w \"ncbi\" !{results} | grep \"tetracycline\" | cut -f6 | grep -v \"bla\" | sort |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;tetracycline-resistance/') \n      printf \"\\${tetData}\\\\n\" >> !{name}_ncbi.csv\n    # aminoglycoside\n      aminoData=\\$(grep -w \"ncbi\" !{results} | grep -v \"efflux\" | grep \"aminoglycoside\" | cut -f6 | grep -v \"bla\" | sort | awk '{print substr(\\$0,0,4)}' |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;aminoglycoside-resistance/') \n      printf \"\\${aminoData}\\\\n\" >> !{name}_ncbi.csv\n    # efflux\n      effluxData=\\$(grep -w \"ncbi\" !{results} | grep -v \"tetracycline\" | grep \"efflux\" | cut -f6 | grep -v \"bla\" | sort | awk '{print substr(\\$0,0,5)}' |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;efflux-system/') \n      printf \"\\${effluxData}\\\\n\" >> !{name}_ncbi.csv\n    # quinolones\n      quinoData=\\$(grep -w \"ncbi\" !{results} |  grep -v \"efflux\" | grep \"quinolone\" | cut -f6 | grep -v \"bla\" | sort | awk '{print substr(\\$0,0,4)}' |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;quinolone-resistance/') \n      printf \"\\${quinoData}\\\\n\" >> !{name}_ncbi.csv\n    # other\n      otherData=\\$(grep -w \"ncbi\" !{results} |  grep -v \"efflux\" | grep -v \"tetracycline\" | grep -v \"aminoglycoside\" | grep -v \"quinolone\" |\\\n        cut -f6 | grep -v \"bla\" |  sort |\\\n        uniq -c | sed -e 's/^[ \\\\t]*//'| tr -d \"'\" | tr \" \" \";\" | sed -e 's/\\$/;other-resistance-genes/') \n      printf \"\\${otherData}\\\\n\" >> !{name}_ncbi.csv\n    \"\"\"",
        "nb_lignes_script": 27,
        "language_script": "bash",
        "tools": [
            "ScType"
        ],
        "tools_url": [
            "https://bio.tools/ScType"
        ],
        "tools_dico": [
            {
                "name": "ScType",
                "uri": "https://bio.tools/ScType",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2229",
                            "term": "Cell biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3489",
                            "term": "Database management"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3489",
                            "term": "Database administration"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3432",
                                    "term": "Clustering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3435",
                                    "term": "Standardisation and normalisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3680",
                                    "term": "RNA-Seq analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "ScType is a tool for fully-automated cell type identification from single-cell RNA-seq data. ScType provides a complete pipeline for single-cell RNA-seq data analysis (including data processing, normalization and clustering) and cell-type annotation.",
                "homepage": "http://session.asuscomm.com:8080/"
            }
        ],
        "inputs": [
            "name",
            "results"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'ubuntu'"
        ],
        "when": "",
        "stub": ""
    },
    "parse_prokka": {
        "name_process": "parse_prokka",
        "string_process": "process parse_prokka {\n    publishDir \"${params.output}/\", mode: 'copy'\n    label 'ubuntu'   \n  input:\n    tuple val(name), path(results_prokka)\n  output:\n\t  tuple val(name), file(\"${name}_annotation_file.txt\") \n  script:\n    \"\"\"\n    # parse results into annotation_file\n    grep -w \"CDS\" ${results_prokka} | grep -v \"hypothetical protein\" | cut -f2,6,3,4 | sed \\$'s/\\$/\\\\\\tresistance_genes/' > annotation.tmp\n\n\n\n\n    # format to tab delimited and add uniq numbers behind each gene\n    awk '{print \\$4,\\$1,\\$2,\\$3,\\$5}' OFS='\\\\t' annotation.tmp |\\\n      awk '{printf \"%d\\\\t%s\\\\n\", NR, \\$0}' |\\\n      awk '{print \\$2\"_\"\\$1\"\\\\t\"\\$3\"\\\\t\"\\$4\"\\\\t\"\\$5\"\\\\t\"\\$6}' |\\\n      tr -d \"'\" \\\n      > ${name}_annotation_file.txt\n\n    # add fargene\n\n    cat ${results_fargene} | awk '{print \\$4\"\\\\t\"\\$1\"\\\\t\"\\$16\"\\\\t\"\\$17\"\\\\tfargene\"}' | sed 's|_seq._.||g' |\\\n      awk '{printf \"%d\\\\t%s\\\\n\", NR, \\$0}' |\\\n      awk '{print \\$2\"_\"\\$1\"\\\\t\"\\$3\"\\\\t\"\\$4\"\\\\t\"\\$5\"\\\\t\"\\$6}' |\\\n      tr -d \"'\" \\\n      >> ${name}_annotation_file.txt\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "    \"\"\"\n    # parse results into annotation_file\n    grep -w \"CDS\" ${results_prokka} | grep -v \"hypothetical protein\" | cut -f2,6,3,4 | sed \\$'s/\\$/\\\\\\tresistance_genes/' > annotation.tmp\n\n\n\n\n    # format to tab delimited and add uniq numbers behind each gene\n    awk '{print \\$4,\\$1,\\$2,\\$3,\\$5}' OFS='\\\\t' annotation.tmp |\\\n      awk '{printf \"%d\\\\t%s\\\\n\", NR, \\$0}' |\\\n      awk '{print \\$2\"_\"\\$1\"\\\\t\"\\$3\"\\\\t\"\\$4\"\\\\t\"\\$5\"\\\\t\"\\$6}' |\\\n      tr -d \"'\" \\\n      > ${name}_annotation_file.txt\n\n    # add fargene\n\n    cat ${results_fargene} | awk '{print \\$4\"\\\\t\"\\$1\"\\\\t\"\\$16\"\\\\t\"\\$17\"\\\\tfargene\"}' | sed 's|_seq._.||g' |\\\n      awk '{printf \"%d\\\\t%s\\\\n\", NR, \\$0}' |\\\n      awk '{print \\$2\"_\"\\$1\"\\\\t\"\\$3\"\\\\t\"\\$4\"\\\\t\"\\$5\"\\\\t\"\\$6}' |\\\n      tr -d \"'\" \\\n      >> ${name}_annotation_file.txt\n    \"\"\"",
        "nb_lignes_script": 21,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "results_prokka"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/\", mode: 'copy'",
            "label 'ubuntu'"
        ],
        "when": "",
        "stub": ""
    },
    "parse_plasmidinfo": {
        "name_process": "parse_plasmidinfo",
        "string_process": "process parse_plasmidinfo {\n    publishDir \"${params.output}/\", mode: 'copy'\n    label 'ubuntu'   \n  input:\n    tuple val(name), path(results_abricate), path(results_IS), path(results_fargene)\n  output:\n\t  tuple val(name), file(\"${name}_annotation_file.txt\") \n  script:\n    \"\"\"\n    # parse results into annotation_file\n    grep -v \"#FILE\" ${results_abricate} | grep -w \"ncbi\" | cut -f2,6,3,4 | sed \\$'s/\\$/\\\\\\tresistance_genes/' > annotation.tmp\n    grep -v \"#FILE\" ${results_abricate} | grep -w \"plasmidfinder\" | cut -f2,6,3,4 | sed \\$'s/\\$/\\\\\\tplasmid_genes/' >> annotation.tmp\n    grep -v \"#FILE\" ${results_IS} | cut -f2,6,3,4 | sed \\$'s/\\$/\\\\\\tInsertion_sequences/' >> annotation.tmp\n\n\n    # format to tab delimited and add uniq numbers behind each gene\n    awk '{print \\$4,\\$1,\\$2,\\$3,\\$5}' OFS='\\\\t' annotation.tmp |\\\n      awk '{printf \"%d\\\\t%s\\\\n\", NR, \\$0}' |\\\n      awk '{print \\$2\"_\"\\$1\"\\\\t\"\\$3\"\\\\t\"\\$4\"\\\\t\"\\$5\"\\\\t\"\\$6}' |\\\n      tr -d \"'\" \\\n      > ${name}_annotation_file.txt\n\n    # add fargene\n\n    cat ${results_fargene} | awk '{print \\$4\"\\\\t\"\\$1\"\\\\t\"\\$16\"\\\\t\"\\$17\"\\\\tfargene\"}' | sed 's|_seq._.||g' |\\\n      awk '{printf \"%d\\\\t%s\\\\n\", NR, \\$0}' |\\\n      awk '{print \\$2\"_\"\\$1\"\\\\t\"\\$3\"\\\\t\"\\$4\"\\\\t\"\\$5\"\\\\t\"\\$6}' |\\\n      tr -d \"'\" \\\n      >> ${name}_annotation_file.txt\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "    \"\"\"\n    # parse results into annotation_file\n    grep -v \"#FILE\" ${results_abricate} | grep -w \"ncbi\" | cut -f2,6,3,4 | sed \\$'s/\\$/\\\\\\tresistance_genes/' > annotation.tmp\n    grep -v \"#FILE\" ${results_abricate} | grep -w \"plasmidfinder\" | cut -f2,6,3,4 | sed \\$'s/\\$/\\\\\\tplasmid_genes/' >> annotation.tmp\n    grep -v \"#FILE\" ${results_IS} | cut -f2,6,3,4 | sed \\$'s/\\$/\\\\\\tInsertion_sequences/' >> annotation.tmp\n\n\n    # format to tab delimited and add uniq numbers behind each gene\n    awk '{print \\$4,\\$1,\\$2,\\$3,\\$5}' OFS='\\\\t' annotation.tmp |\\\n      awk '{printf \"%d\\\\t%s\\\\n\", NR, \\$0}' |\\\n      awk '{print \\$2\"_\"\\$1\"\\\\t\"\\$3\"\\\\t\"\\$4\"\\\\t\"\\$5\"\\\\t\"\\$6}' |\\\n      tr -d \"'\" \\\n      > ${name}_annotation_file.txt\n\n    # add fargene\n\n    cat ${results_fargene} | awk '{print \\$4\"\\\\t\"\\$1\"\\\\t\"\\$16\"\\\\t\"\\$17\"\\\\tfargene\"}' | sed 's|_seq._.||g' |\\\n      awk '{printf \"%d\\\\t%s\\\\n\", NR, \\$0}' |\\\n      awk '{print \\$2\"_\"\\$1\"\\\\t\"\\$3\"\\\\t\"\\$4\"\\\\t\"\\$5\"\\\\t\"\\$6}' |\\\n      tr -d \"'\" \\\n      >> ${name}_annotation_file.txt\n    \"\"\"",
        "nb_lignes_script": 21,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "results_abricate",
            "results_IS",
            "results_fargene"
        ],
        "nb_inputs": 4,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/\", mode: 'copy'",
            "label 'ubuntu'"
        ],
        "when": "",
        "stub": ""
    },
    "parse_samtools": {
        "name_process": "parse_samtools",
        "string_process": "process parse_samtools {\n    publishDir \"${params.output}/\", mode: 'copy'\n    label 'samtools'   \n  input:\n    tuple val(name), file(annotation), file(fasta)\n  output:\n\t  tuple val(name), file(\"${name}_chromosome_file.txt\"), file(annotation)\n  shell:\n    \"\"\"\n    contiglist=\\$(cat ${annotation} | cut -f 2 | sort | uniq )\n\n\n    # parse contig stats into chromosome_file\n    samtools faidx ${fasta}\n    \n    ## only using annotated contigs\n    while IFS= read -r contig ; do\n      grep \"\\${contig}\" ${fasta}.fai >> index.list\n    done < <(echo \"\\${contiglist}\")\n\n    cut -f1,2 index.list |  sed \\$'s/\\\\\\\\t/\\\\\\\\t1\\\\\\\\t/' | sort -h -r -k 3  > ${name}_chromosome_file.txt\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "    \"\"\"\n    contiglist=\\$(cat ${annotation} | cut -f 2 | sort | uniq )\n\n\n    # parse contig stats into chromosome_file\n    samtools faidx ${fasta}\n    \n    ## only using annotated contigs\n    while IFS= read -r contig ; do\n      grep \"\\${contig}\" ${fasta}.fai >> index.list\n    done < <(echo \"\\${contiglist}\")\n\n    cut -f1,2 index.list |  sed \\$'s/\\\\\\\\t/\\\\\\\\t1\\\\\\\\t/' | sort -h -r -k 3  > ${name}_chromosome_file.txt\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "name",
            "annotation",
            "fasta"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/\", mode: 'copy'",
            "label 'samtools'"
        ],
        "when": "",
        "stub": ""
    },
    "bakta": {
        "name_process": "bakta",
        "string_process": "process bakta {\n      publishDir \"${params.output}/${name}/bakta\", mode: 'copy', pattern: \"${name}-results\"\n      label 'bakta'\n    input:\n      tuple val(name), path(fasta) \n      file(database) \n    output:\n      tuple val(name), file(\"${name}-results\")\n    script:\n      \"\"\"\n      tar xzf ${database}\n      rm ${database}\n      export BAKTA_DB=./db\n\n      # try to update abr but skip if not possible\n      amrfinder_update --force_update --database db/amrfinderplus-db/ || true\n\n      bakta --output ${name}-results --threads ${task.cpus} ${fasta}\n\n      # reduce fingerprint on local systems\n      rm -rf db\n      \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "      \"\"\"\n      tar xzf ${database}\n      rm ${database}\n      export BAKTA_DB=./db\n\n      # try to update abr but skip if not possible\n      amrfinder_update --force_update --database db/amrfinderplus-db/ || true\n\n      bakta --output ${name}-results --threads ${task.cpus} ${fasta}\n\n      # reduce fingerprint on local systems\n      rm -rf db\n      \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "fasta",
            "database"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}/bakta\", mode: 'copy', pattern: \"${name}-results\"",
            "label 'bakta'"
        ],
        "when": "",
        "stub": ""
    },
    "downloadHuman": {
        "name_process": "downloadHuman",
        "string_process": "process downloadHuman {\n    label 'ubuntu'\n    storeDir 'stored-human-genome/'\n  output:\n    file(\"*.fna\")\n  shell:\n    \"\"\"\n    wget ftp://ftp.ncbi.nlm.nih.gov/genomes/refseq/vertebrate_mammalian/Homo_sapiens/reference/GCF_000001405.39_GRCh38.p13/GCF_000001405.39_GRCh38.p13_genomic.fna.gz\n    gzip -d GCF_000001405.39_GRCh38.p13_genomic.fna.gz\n    \"\"\"\n}",
        "nb_lignes_process": 9,
        "string_script": "    \"\"\"\n    wget ftp://ftp.ncbi.nlm.nih.gov/genomes/refseq/vertebrate_mammalian/Homo_sapiens/reference/GCF_000001405.39_GRCh38.p13/GCF_000001405.39_GRCh38.p13_genomic.fna.gz\n    gzip -d GCF_000001405.39_GRCh38.p13_genomic.fna.gz\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'ubuntu'",
            "storeDir 'stored-human-genome/'"
        ],
        "when": "",
        "stub": ""
    },
    "filter_fastq_by_length": {
        "name_process": "filter_fastq_by_length",
        "string_process": "process filter_fastq_by_length {\n    label 'ubuntu'\n  input:\n    tuple val(name), path(reads) \n  output:\n\t  tuple val(name), path(\"${name}_filtered.fastq.gz\") \n  script:\n    \"\"\"\n    case \"${reads}\" in\n      *.fastq.gz ) \n        zcat ${reads} | paste - - - - | awk -F\"\\\\t\" 'length(\\$2)  >= 400' | sed 's/\\\\t/\\\\n/g' |\\\n          awk -F\"\\\\t\" 'length(\\$2)  <= 700' | sed 's/\\\\t/\\\\n/g' | gzip > \"${name}_filtered.fastq.gz\"\n        ;;\n      *.fastq)\n        cat ${reads} | paste - - - - | awk -F\"\\\\t\" 'length(\\$2)  >= 400' | sed 's/\\\\t/\\\\n/g' |\\\n          awk -F\"\\\\t\" 'length(\\$2)  <= 700' | sed 's/\\\\t/\\\\n/g' | gzip > \"${name}_filtered.fastq.gz\"\n        ;;\n    esac   \n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "    \"\"\"\n    case \"${reads}\" in\n      *.fastq.gz ) \n        zcat ${reads} | paste - - - - | awk -F\"\\\\t\" 'length(\\$2)  >= 400' | sed 's/\\\\t/\\\\n/g' |\\\n          awk -F\"\\\\t\" 'length(\\$2)  <= 700' | sed 's/\\\\t/\\\\n/g' | gzip > \"${name}_filtered.fastq.gz\"\n        ;;\n      *.fastq)\n        cat ${reads} | paste - - - - | awk -F\"\\\\t\" 'length(\\$2)  >= 400' | sed 's/\\\\t/\\\\n/g' |\\\n          awk -F\"\\\\t\" 'length(\\$2)  <= 700' | sed 's/\\\\t/\\\\n/g' | gzip > \"${name}_filtered.fastq.gz\"\n        ;;\n    esac   \n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "CASE"
        ],
        "tools_url": [
            "https://bio.tools/CASE"
        ],
        "tools_dico": [
            {
                "name": "CASE",
                "uri": "https://bio.tools/CASE",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0749",
                            "term": "Transcription factors and regulatory sites"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0089",
                            "term": "Ontology and terminology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3436",
                                    "term": "Aggregation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3435",
                                    "term": "Standardisation and normalisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3559",
                                    "term": "Ontology visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3559",
                                    "term": "Ontology browsing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Advancing Coordinated Cyber-investigations and Tool Interoperability using a Community Developed Specification Language.\n\nSource files for the CASE website.\n\nAPI used for instantiating CASE objects (includes ontological verification and type checking).\n\nCyber-investigation Analysis Standard Expression (CASE).\n\nRead the CASE Wiki tab to learn everything you need to know about the Cyber-investigation Analysis Standard Expression (CASE) ontology. For learning about the Unified Cyber Ontology, CASE's parent, see UCO.\n\n\"@vocab\": \"http://case.example.org/core#\",.\n\nDET ER DINE PENGER DET DREIER SEG OM...\n\nVi er ikke st\ufffdrst, men garanterer effektiv behandling.\n\nLast ned v\ufffdr brosjyre i PDF format.\n\n||| COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/pymzml (GITHUB.COM).\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'UCO', 'cyber-investigation', 'cyber-investigations', 'plaso'",
                "homepage": "http://CASE.as"
            }
        ],
        "inputs": [
            "name",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'ubuntu'"
        ],
        "when": "",
        "stub": ""
    },
    "removeViaMapping": {
        "name_process": "removeViaMapping",
        "string_process": "process removeViaMapping {\n        publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"${name}_*_non_mappers.fastq.gz\"\n        label 'seqtk'\n    input:\n        tuple val(name), file(readlist), file(shortreads)\n    output:\n        tuple val(name), file(\"${name}_forward_non_mappers.fastq.gz\"), file(\"${name}_reverse_non_mappers.fastq.gz\")\n    script:\n        \"\"\"    \n        seqtk subseq ${shortreads[0]} ${readlist} | gzip > ${name}_forward_non_mappers.fastq.gz\n        seqtk subseq ${shortreads[1]} ${readlist} | gzip > ${name}_reverse_non_mappers.fastq.gz\n        \"\"\"\n}",
        "nb_lignes_process": 11,
        "string_script": "        \"\"\"    \n        seqtk subseq ${shortreads[0]} ${readlist} | gzip > ${name}_forward_non_mappers.fastq.gz\n        seqtk subseq ${shortreads[1]} ${readlist} | gzip > ${name}_reverse_non_mappers.fastq.gz\n        \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "seqtk"
        ],
        "tools_url": [
            "https://bio.tools/seqtk"
        ],
        "tools_dico": [
            {
                "name": "seqtk",
                "uri": "https://bio.tools/seqtk",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Biological databases"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Data management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Information systems"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Databases and information systems"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Data handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2121",
                                    "term": "Sequence file editing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Utility operation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File processing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Report handling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A tool for processing sequences in the FASTA or FASTQ format. It parses both FASTA and FASTQ files which can also be optionally compressed by gzip.",
                "homepage": "https://github.com/lh3/seqtk"
            }
        ],
        "inputs": [
            "name",
            "readlist",
            "shortreads"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"${name}_*_non_mappers.fastq.gz\"",
            "label 'seqtk'"
        ],
        "when": "",
        "stub": ""
    },
    "chromomap": {
        "name_process": "chromomap",
        "string_process": "process chromomap {\n      publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"sample_overview.html\"\n      label 'chromomap'\n      errorStrategy 'retry'\n      maxRetries 4\n    input:\n      tuple val(name), file(chromosome), file(annotation)\n    output:\n      tuple val(name), file(\"sample_overview.html\")\n    script:\n    if (task.attempt.toString() == '1')\n      \"\"\"\n      #!/usr/bin/env Rscript\n\n      library(chromoMap)\n      library(ggplot2)\n      library(plotly)\n\n      input <- read.delim(\"${chromosome}\", sep = '\\t', header = FALSE)\n      sizeh <- ( nrow(input) * 80 )\n\n      p <-  chromoMap(\"${chromosome}\",\"${annotation}\",\n            data_based_color_map = T,\n            data_type = \"categorical\",\n            segment_annotation = T,\n            data_colors = list(c(\"red\", \"lightblue\", \"orange\", \"green\")),\n            labels=T,\n            left_margin = 340, canvas_width = 1500, canvas_height = sizeh, chr_length = 12, ch_gap = 6)\n\n      htmlwidgets::saveWidget(as_widget(p), \"sample_overview.html\")\n      \"\"\"\n\n    else if (task.attempt.toString() == '2')\n      \"\"\"\n      #!/usr/bin/env Rscript\n\n      library(chromoMap)\n      library(ggplot2)\n      library(plotly)\n\n      input <- read.delim(\"${chromosome}\", sep = '\\t', header = FALSE)\n      sizeh <- ( nrow(input) * 80 )\n\n      p <-  chromoMap(\"${chromosome}\",\"${annotation}\",\n            data_based_color_map = T,\n            data_type = \"categorical\",\n            segment_annotation = T,\n            data_colors = list(c(\"lightblue\", \"orange\", \"red\")),\n            labels=T,\n            left_margin = 340, canvas_width = 1500, canvas_height = sizeh, chr_length = 12, ch_gap = 6)\n\n      htmlwidgets::saveWidget(as_widget(p), \"sample_overview.html\")\n      \"\"\"\n    else if (task.attempt.toString() == '3')\n      \"\"\"\n      #!/usr/bin/env Rscript\n\n      library(chromoMap)\n      library(ggplot2)\n      library(plotly)\n\n      input <- read.delim(\"${chromosome}\", sep = '\\t', header = FALSE)\n      sizeh <- ( nrow(input) * 80 )\n\n      p <-  chromoMap(\"${chromosome}\",\"${annotation}\",\n            data_based_color_map = T,\n            data_type = \"categorical\",\n            segment_annotation = T, \n            data_colors = list(c(\"lightblue\", \"orange\")),\n            labels=T,\n            left_margin = 340, canvas_width = 1500, canvas_height = sizeh, chr_length = 12, ch_gap = 6)\n\n      htmlwidgets::saveWidget(as_widget(p), \"sample_overview.html\")\n      \"\"\"\n    else if (task.attempt.toString() == '4')\n      \"\"\"\n      #!/usr/bin/env Rscript\n\n      library(chromoMap)\n      library(ggplot2)\n      library(plotly)\n\n      input <- read.delim(\"${chromosome}\", sep = '\\t', header = FALSE)\n      sizeh <- ( nrow(input) * 80 )\n\n      p <-  chromoMap(\"${chromosome}\",\"${annotation}\",\n            data_type = \"categorical\",\n            segment_annotation = T,\n            labels=T,\n            anno_col = c(\"lightblue\"),\n            left_margin = 340, canvas_width = 1400, canvas_height = sizeh, chr_length = 8, ch_gap = 6)\n\n      htmlwidgets::saveWidget(as_widget(p), \"sample_overview.html\")\n      \"\"\"\n    else if (task.attempt.toString() == '5')\n      \"\"\"\n      echo \"nothing found\" > sample_overview.html\n      \"\"\"\n}",
        "nb_lignes_process": 97,
        "string_script": "    if (task.attempt.toString() == '1')\n      \"\"\"\n      #!/usr/bin/env Rscript\n\n      library(chromoMap)\n      library(ggplot2)\n      library(plotly)\n\n      input <- read.delim(\"${chromosome}\", sep = '\\t', header = FALSE)\n      sizeh <- ( nrow(input) * 80 )\n\n      p <-  chromoMap(\"${chromosome}\",\"${annotation}\",\n            data_based_color_map = T,\n            data_type = \"categorical\",\n            segment_annotation = T,\n            data_colors = list(c(\"red\", \"lightblue\", \"orange\", \"green\")),\n            labels=T,\n            left_margin = 340, canvas_width = 1500, canvas_height = sizeh, chr_length = 12, ch_gap = 6)\n\n      htmlwidgets::saveWidget(as_widget(p), \"sample_overview.html\")\n      \"\"\"\n\n    else if (task.attempt.toString() == '2')\n      \"\"\"\n      #!/usr/bin/env Rscript\n\n      library(chromoMap)\n      library(ggplot2)\n      library(plotly)\n\n      input <- read.delim(\"${chromosome}\", sep = '\\t', header = FALSE)\n      sizeh <- ( nrow(input) * 80 )\n\n      p <-  chromoMap(\"${chromosome}\",\"${annotation}\",\n            data_based_color_map = T,\n            data_type = \"categorical\",\n            segment_annotation = T,\n            data_colors = list(c(\"lightblue\", \"orange\", \"red\")),\n            labels=T,\n            left_margin = 340, canvas_width = 1500, canvas_height = sizeh, chr_length = 12, ch_gap = 6)\n\n      htmlwidgets::saveWidget(as_widget(p), \"sample_overview.html\")\n      \"\"\"\n    else if (task.attempt.toString() == '3')\n      \"\"\"\n      #!/usr/bin/env Rscript\n\n      library(chromoMap)\n      library(ggplot2)\n      library(plotly)\n\n      input <- read.delim(\"${chromosome}\", sep = '\\t', header = FALSE)\n      sizeh <- ( nrow(input) * 80 )\n\n      p <-  chromoMap(\"${chromosome}\",\"${annotation}\",\n            data_based_color_map = T,\n            data_type = \"categorical\",\n            segment_annotation = T, \n            data_colors = list(c(\"lightblue\", \"orange\")),\n            labels=T,\n            left_margin = 340, canvas_width = 1500, canvas_height = sizeh, chr_length = 12, ch_gap = 6)\n\n      htmlwidgets::saveWidget(as_widget(p), \"sample_overview.html\")\n      \"\"\"\n    else if (task.attempt.toString() == '4')\n      \"\"\"\n      #!/usr/bin/env Rscript\n\n      library(chromoMap)\n      library(ggplot2)\n      library(plotly)\n\n      input <- read.delim(\"${chromosome}\", sep = '\\t', header = FALSE)\n      sizeh <- ( nrow(input) * 80 )\n\n      p <-  chromoMap(\"${chromosome}\",\"${annotation}\",\n            data_type = \"categorical\",\n            segment_annotation = T,\n            labels=T,\n            anno_col = c(\"lightblue\"),\n            left_margin = 340, canvas_width = 1400, canvas_height = sizeh, chr_length = 8, ch_gap = 6)\n\n      htmlwidgets::saveWidget(as_widget(p), \"sample_overview.html\")\n      \"\"\"\n    else if (task.attempt.toString() == '5')\n      \"\"\"\n      echo \"nothing found\" > sample_overview.html\n      \"\"\"",
        "nb_lignes_script": 87,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "chromosome",
            "annotation"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"sample_overview.html\"",
            "label 'chromomap'",
            "errorStrategy 'retry'",
            "maxRetries 4"
        ],
        "when": "",
        "stub": ""
    },
    "abricate": {
        "name_process": "abricate",
        "string_process": "process abricate {\n    publishDir \"${params.output}/${name}/ABR-Screening\", mode: 'copy', pattern: \"*.tab\"\n    label 'abricate'\n                                                                           \n  input:\n    tuple val(name), path(fasta) \n    each method\n  output:\n\t  tuple val(name), val(\"${method}\"), path(\"*.tab\")\n  script:\n    if (!params.update)\n    \"\"\"\n  \tabricate ${fasta} --nopath --quiet --mincov 85 --db ${method} > ${method}.tab\n    \"\"\"\n    else if (params.update)\n    \"\"\"\n    abricate-get_db --db ${method} --force\n  \tabricate ${fasta} --nopath --quiet  --db ${method} > ${method}.tab\n    \"\"\"\n\n}",
        "nb_lignes_process": 19,
        "string_script": "    if (!params.update)\n    \"\"\"\n  \tabricate ${fasta} --nopath --quiet --mincov 85 --db ${method} > ${method}.tab\n    \"\"\"\n    else if (params.update)\n    \"\"\"\n    abricate-get_db --db ${method} --force\n  \tabricate ${fasta} --nopath --quiet  --db ${method} > ${method}.tab\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "ABRicate"
        ],
        "tools_url": [
            "https://bio.tools/ABRicate"
        ],
        "tools_dico": [
            {
                "name": "ABRicate",
                "uri": "https://bio.tools/ABRicate",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3482",
                                    "term": "Antimicrobial resistance prediction"
                                }
                            ],
                            []
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_1234",
                                "term": "Sequence set (nucleic acid)"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0916",
                                "term": "Gene report"
                            }
                        ]
                    }
                ],
                "description": "Mass screening of contigs for antimicrobial resistance or virulence genes.",
                "homepage": "https://github.com/tseemann/abricate"
            }
        ],
        "inputs": [
            "name",
            "fasta",
            "method"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}/ABR-Screening\", mode: 'copy', pattern: \"*.tab\"",
            "label 'abricate'"
        ],
        "when": "",
        "stub": ""
    },
    "abricate_compare": {
        "name_process": "abricate_compare",
        "string_process": "\nprocess abricate_compare {\n    label 'abricate'\n                                                                              \n  input:\n    tuple val(name), val(splitname), val(type), path(fasta) \n    each method\n  output:\n\t  tuple val(name), path(\"${method}_${type}_${name}_*.abricate\")\n  script:\n    \"\"\"\n  \tabricate ${fasta} --nopath --quiet --mincov 85 --db ${method} > ${method}_${type}_${name}_\\${PWD##*/}.abricate\n    \"\"\"\n}",
        "nb_lignes_process": 12,
        "string_script": "    \"\"\"\n  \tabricate ${fasta} --nopath --quiet --mincov 85 --db ${method} > ${method}_${type}_${name}_\\${PWD##*/}.abricate\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "ABRicate"
        ],
        "tools_url": [
            "https://bio.tools/ABRicate"
        ],
        "tools_dico": [
            {
                "name": "ABRicate",
                "uri": "https://bio.tools/ABRicate",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3482",
                                    "term": "Antimicrobial resistance prediction"
                                }
                            ],
                            []
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_1234",
                                "term": "Sequence set (nucleic acid)"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0916",
                                "term": "Gene report"
                            }
                        ]
                    }
                ],
                "description": "Mass screening of contigs for antimicrobial resistance or virulence genes.",
                "homepage": "https://github.com/tseemann/abricate"
            }
        ],
        "inputs": [
            "name",
            "splitname",
            "type",
            "fasta",
            "method"
        ],
        "nb_inputs": 5,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'abricate'"
        ],
        "when": "",
        "stub": ""
    },
    "abricate_transposon": {
        "name_process": "abricate_transposon",
        "string_process": "\nprocess abricate_transposon {\n    label 'abricate'\n    publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"*.tab\"\n  input:\n    tuple val(name), path(fasta), path(database)\n  output:\n\t  tuple val(name), path(\"*.tab\")\n  script:\n    \"\"\"\n    mkdir -p db/mobile_elements\n    cp ${database} db/mobile_elements/sequences\n    abricate --setupdb --datadir \\$PWD/db\n\n  \tabricate ${fasta} --datadir \\$PWD/db --nopath --quiet --mincov 85 --db mobile_elements > mobile_elements_\\${PWD##*/}.tab\n    \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "    \"\"\"\n    mkdir -p db/mobile_elements\n    cp ${database} db/mobile_elements/sequences\n    abricate --setupdb --datadir \\$PWD/db\n\n  \tabricate ${fasta} --datadir \\$PWD/db --nopath --quiet --mincov 85 --db mobile_elements > mobile_elements_\\${PWD##*/}.tab\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "ABRicate"
        ],
        "tools_url": [
            "https://bio.tools/ABRicate"
        ],
        "tools_dico": [
            {
                "name": "ABRicate",
                "uri": "https://bio.tools/ABRicate",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3482",
                                    "term": "Antimicrobial resistance prediction"
                                }
                            ],
                            []
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_1234",
                                "term": "Sequence set (nucleic acid)"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0916",
                                "term": "Gene report"
                            }
                        ]
                    }
                ],
                "description": "Mass screening of contigs for antimicrobial resistance or virulence genes.",
                "homepage": "https://github.com/tseemann/abricate"
            }
        ],
        "inputs": [
            "name",
            "fasta",
            "database"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'abricate'",
            "publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"*.tab\""
        ],
        "when": "",
        "stub": ""
    },
    "cluster": {
        "name_process": "cluster",
        "string_process": "process cluster {\n    label 'cd_hit'   \n    publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"${name}_representatives.fasta\"\n  input:\n    tuple val(name), file(fasta)\n  output:\n    tuple val(name), file(\"${name}_representatives.fasta\")\n  script:\n    \"\"\"\n    psi-cd-hit.pl -i ${fasta} -o output -aL 0.6 -prog blastn -para ${task.cpus} -c 0.6 -g 1 \\\n    -s \"-evalue 10E-100 -max_target_seqs 100000 -qcov_hsp_perc 10 -max_hsps 10\"\n    grep \">\" output > representatives.txt\n    mv output ${name}_representatives.fasta\n    \"\"\"\n}",
        "nb_lignes_process": 13,
        "string_script": "    \"\"\"\n    psi-cd-hit.pl -i ${fasta} -o output -aL 0.6 -prog blastn -para ${task.cpus} -c 0.6 -g 1 \\\n    -s \"-evalue 10E-100 -max_target_seqs 100000 -qcov_hsp_perc 10 -max_hsps 10\"\n    grep \">\" output > representatives.txt\n    mv output ${name}_representatives.fasta\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'cd_hit'",
            "publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"${name}_representatives.fasta\""
        ],
        "when": "",
        "stub": ""
    },
    "toytree": {
        "name_process": "toytree",
        "string_process": "process toytree {\n    publishDir \"${params.output}/${name}/tree/\", mode: 'copy'\n    label 'toytree'\n  input:\n    tuple val(name), path(tree)\n  output:\n\t  tuple path(\"tree.svg\"), path(\"tree.pdf\"), path(\"tree_radial.svg\"), path(\"tree_radial.pdf\")\n  script:\n    \"\"\"\n    cp ${tree} tree_INPUT.newick\n    vis_tree_features.py\n    vis_tree_features_radial.py\n    \"\"\"\n}",
        "nb_lignes_process": 12,
        "string_script": "    \"\"\"\n    cp ${tree} tree_INPUT.newick\n    vis_tree_features.py\n    vis_tree_features_radial.py\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "tree"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}/tree/\", mode: 'copy'",
            "label 'toytree'"
        ],
        "when": "",
        "stub": ""
    },
    "sourmash_download_db": {
        "name_process": "sourmash_download_db",
        "string_process": "process sourmash_download_db {\n        storeDir \"${params.databases}/sourmash\"\n        label 'sourmash' \n      output:\n        path(\"gtdb-release202-k31.lca.json.gz\")\n      script:\n        \"\"\"\n        wget https://osf.io/9xdg2/download -O gtdb-release202-k31.lca.json.gz\n        \"\"\"\n    }",
        "nb_lignes_process": 8,
        "string_script": "        \"\"\"\n        wget https://osf.io/9xdg2/download -O gtdb-release202-k31.lca.json.gz\n        \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "storeDir \"${params.databases}/sourmash\"",
            "label 'sourmash'"
        ],
        "when": "",
        "stub": ""
    },
    "gtdbtk_download_db": {
        "name_process": "gtdbtk_download_db",
        "string_process": "process gtdbtk_download_db {\n        storeDir \"${params.databases}/gtdbtk\" \n        label 'ubuntu'    \n      output:\n        path(\"gtdbtk_data.tar.gz\")\n      script:\n        \"\"\"\n        wget --no-check-certificate https://data.gtdb.ecogenomic.org/releases/latest/auxillary_files/gtdbtk_data.tar.gz\n        \"\"\"\n    }",
        "nb_lignes_process": 8,
        "string_script": "        \"\"\"\n        wget --no-check-certificate https://data.gtdb.ecogenomic.org/releases/latest/auxillary_files/gtdbtk_data.tar.gz\n        \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "storeDir \"${params.databases}/gtdbtk\"",
            "label 'ubuntu'"
        ],
        "when": "",
        "stub": ""
    },
    "racon": {
        "name_process": "racon",
        "string_process": "process racon {\n      label 'racon'\n   input:\n      tuple val(name), path(read), path(assembly), path(mapping) \n   output:\n   \ttuple val(name), file(read), file(\"${name}_consensus.fasta\") \n   shell:\n      \"\"\"\n      racon -t ${task.cpus} ${read} ${mapping} ${assembly} > ${name}_consensus.fasta\n      \"\"\"\n  }",
        "nb_lignes_process": 9,
        "string_script": "      \"\"\"\n      racon -t ${task.cpus} ${read} ${mapping} ${assembly} > ${name}_consensus.fasta\n      \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "Racon"
        ],
        "tools_url": [
            "https://bio.tools/Racon"
        ],
        "tools_dico": [
            {
                "name": "Racon",
                "uri": "https://bio.tools/Racon",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plant biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Whole genome sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plant science"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plants"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Botany"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plant"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Genome sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "WGS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0523",
                                    "term": "Mapping assembly"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Sequence trimming"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0523",
                                    "term": "Sequence assembly (mapping assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Trimming"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The Possibility to Use Oxford Nanopore Technology | Ultrafast consensus module for raw de novo genome assembly of long uncorrected reads. http://genome.cshlp.org/content/early/2017/01/18/gr.214270.116 Note: This was the original repository which will no longer be officially maintained. Please use the new official repository here: | Racon is intended as a standalone consensus module to correct raw contigs generated by rapid assembly methods which do not include a consensus step | Consensus module for raw de novo DNA assembly of long uncorrected reads",
                "homepage": "https://github.com/isovic/racon"
            }
        ],
        "inputs": [
            "name",
            "read",
            "assembly",
            "mapping"
        ],
        "nb_inputs": 4,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'racon'"
        ],
        "when": "",
        "stub": ""
    },
    "minimap2": {
        "name_process": "minimap2",
        "string_process": "\nprocess minimap2 {\n  label 'minimap2'\n    input:\n      tuple val(name), path(reads), path(fastas)\n    output:\n      tuple val(name), path(\"ont_sorted.bam\")\n    shell:\n    \"\"\"\n    minimap2 -ax map-ont ${fastas} ${reads} | samtools view -bS - | samtools sort -@ ${task.cpus} - > ont_sorted.bam\n    \"\"\"\n}",
        "nb_lignes_process": 10,
        "string_script": "    \"\"\"\n    minimap2 -ax map-ont ${fastas} ${reads} | samtools view -bS - | samtools sort -@ ${task.cpus} - > ont_sorted.bam\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "Minimap2",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/minimap2",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "Minimap2",
                "uri": "https://bio.tools/minimap2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0491",
                                    "term": "Pairwise sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0491",
                                    "term": "Pairwise alignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Pairwise aligner for genomic and spliced nucleotide sequences",
                "homepage": "https://github.com/lh3/minimap2"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "name",
            "reads",
            "fastas"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'minimap2'"
        ],
        "when": "",
        "stub": ""
    },
    "minimap2_polish": {
        "name_process": "minimap2_polish",
        "string_process": "\nprocess minimap2_polish {\n  label 'minimap2'\n      input:\n  \t    tuple val(name), file(read), file(assembly) \n      output:\n        tuple val(name), file(read), file(assembly), file(\"${name}.paf\") \n      script:\n        \"\"\"\n      \tminimap2 -x map-ont -t ${task.cpus} ${assembly} ${read} > ${name}.paf\n        \"\"\"\n}",
        "nb_lignes_process": 10,
        "string_script": "        \"\"\"\n      \tminimap2 -x map-ont -t ${task.cpus} ${assembly} ${read} > ${name}.paf\n        \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "Minimap2"
        ],
        "tools_url": [
            "https://bio.tools/minimap2"
        ],
        "tools_dico": [
            {
                "name": "Minimap2",
                "uri": "https://bio.tools/minimap2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0491",
                                    "term": "Pairwise sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0491",
                                    "term": "Pairwise alignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Pairwise aligner for genomic and spliced nucleotide sequences",
                "homepage": "https://github.com/lh3/minimap2"
            }
        ],
        "inputs": [
            "name",
            "read",
            "assembly"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'minimap2'"
        ],
        "when": "",
        "stub": ""
    },
    "medaka": {
        "name_process": "medaka",
        "string_process": "process medaka {\n  label 'medaka'\n        publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}_polished.fasta\"\n      input:\n        tuple val(name), file(read), file(consensus) \n      output:\n  \t    tuple val(name), file(\"${name}_polished.fasta\") \n      script:\n        \"\"\"\n      \tmedaka_consensus -i ${read} -d ${consensus} -o polished -t ${task.cpus} -m ${params.model}\n        mv polished/consensus.fasta ${name}_polished.fasta\n      \t\"\"\"\n}",
        "nb_lignes_process": 11,
        "string_script": "        \"\"\"\n      \tmedaka_consensus -i ${read} -d ${consensus} -o polished -t ${task.cpus} -m ${params.model}\n        mv polished/consensus.fasta ${name}_polished.fasta\n      \t\"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "read",
            "consensus"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'medaka'",
            "publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}_polished.fasta\""
        ],
        "when": "",
        "stub": ""
    },
    "baloonplot": {
        "name_process": "baloonplot",
        "string_process": "process baloonplot {\n    publishDir \"${params.output}/\", mode: 'copy'\n    label 'baloonplot' \n  input:\n    path(csv) \n  output:\n\t  path(\"overview.svg\")\n    path(\"overview.pdf\") \n  script:\n  if (params.coverage)\n    \"\"\"\n    baloonrplot_coverage_count.R\n    \"\"\"\n  else if (!params.coverage)\n    \"\"\"\n    baloonrplot_gene_count.R\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "  if (params.coverage)\n    \"\"\"\n    baloonrplot_coverage_count.R\n    \"\"\"\n  else if (!params.coverage)\n    \"\"\"\n    baloonrplot_gene_count.R\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "csv"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/\", mode: 'copy'",
            "label 'baloonplot'"
        ],
        "when": "",
        "stub": ""
    },
    "filtlong": {
        "name_process": "filtlong",
        "string_process": "process filtlong {\n                                                                                         \n    label 'filtlong'\n  input:\n    tuple val(name), file(reads) \n  output:\n\t  tuple val(name), file(\"${name}_filtered.fastq\") \n  script:\n    \"\"\"\n  \tfiltlong --min_length 2000 ${reads} > ${name}_filtered.fastq\n    \"\"\"\n}",
        "nb_lignes_process": 10,
        "string_script": "    \"\"\"\n  \tfiltlong --min_length 2000 ${reads} > ${name}_filtered.fastq\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "Filtlong"
        ],
        "tools_url": [
            "https://bio.tools/Filtlong"
        ],
        "tools_dico": [
            {
                "name": "Filtlong",
                "uri": "https://bio.tools/Filtlong",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0749",
                            "term": "Transcription factors and regulatory sites"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0798",
                            "term": "Mobile genetic elements"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Whole genome sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Genome sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "WGS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0310",
                                    "term": "Sequence assembly"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Sequence trimming"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3482",
                                    "term": "Antimicrobial resistance prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3472",
                                    "term": "k-mer counting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3359",
                                    "term": "Splitting"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Trimming"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3359",
                                    "term": "File splitting"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Complete hybrid genome assembly of clinical multidrug-resistant Bacteroides fragilis isolates enables comprehensive identification of antimicrobial-resistance genes and plasmids.\n\nquality filtering tool for long reads.\n\nFiltlong is a tool for filtering long reads by quality. It can take a set of long reads and produce a smaller, better subset. It uses both read length (longer is better) and read identity (higher is better) when choosing which reads pass the filter.\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'Unicycler' (bio.tools/unicycler), 'Canu-corrected ONT', 'AMR', 'fragilis'",
                "homepage": "https://github.com/rrwick/Filtlong"
            }
        ],
        "inputs": [
            "name",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'filtlong'"
        ],
        "when": "",
        "stub": ""
    },
    "guppy_gpu": {
        "name_process": "guppy_gpu",
        "string_process": "process guppy_gpu {\n      echo true\n      maxForks 1\n      container = 'nanozoo/guppy_gpu:4.4.1-1--a3fcea3'\n      containerOptions '--gpus all'\n      publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"fastq_${params.output}\"\n    input:\n      tuple val(name), file(dir)\n    output:\n      tuple val(name), file(\"fastq_${params.output}\")\n    script:\n      if (params.config)\n      \"\"\"\n      guppy_basecaller -r -i ${dir} -s fastq_${params.output} \\\n      -c ${params.configtype} --device auto --trim_strategy dna -q 0\n      \"\"\"\n      else if(!params.barcode)\n      \"\"\"\n      guppy_basecaller -r -i ${dir} -s fastq_${params.output} \\\n      --flowcell ${params.flowcell} --kit ${params.kit} --device auto --trim_strategy dna -q 0\n      \"\"\"\n      else\n      \"\"\"\n      guppy_basecaller -r -i ${dir} -s fastq_${params.output} \\\n      --flowcell ${params.flowcell} --kit ${params.kit} --barcode_kits ${params.barcode} --device auto --trim_strategy dna -q 0 \\\n      --trim_barcodes\n      \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "      if (params.config)\n      \"\"\"\n      guppy_basecaller -r -i ${dir} -s fastq_${params.output} \\\n      -c ${params.configtype} --device auto --trim_strategy dna -q 0\n      \"\"\"\n      else if(!params.barcode)\n      \"\"\"\n      guppy_basecaller -r -i ${dir} -s fastq_${params.output} \\\n      --flowcell ${params.flowcell} --kit ${params.kit} --device auto --trim_strategy dna -q 0\n      \"\"\"\n      else\n      \"\"\"\n      guppy_basecaller -r -i ${dir} -s fastq_${params.output} \\\n      --flowcell ${params.flowcell} --kit ${params.kit} --barcode_kits ${params.barcode} --device auto --trim_strategy dna -q 0 \\\n      --trim_barcodes\n      \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "dir"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "echo true",
            "maxForks 1",
            "container = 'nanozoo/guppy_gpu:4.4.1-1--a3fcea3'",
            "containerOptions '--gpus all'",
            "publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"fastq_${params.output}\""
        ],
        "when": "",
        "stub": ""
    },
    "live_guppy_gpu": {
        "name_process": "live_guppy_gpu",
        "string_process": "\nprocess live_guppy_gpu {\n      maxForks 1\n      container = 'nanozoo/guppy_gpu:4.4.1-1--a3fcea3'\n      containerOptions '--gpus all'\n      publishDir \"${params.output}/${name}/fastq/\", mode: 'copy', pattern: \"fastq_${params.output}/*.fastq\"\n    input:\n      tuple val(name), file(fast5)\n    output:\n      tuple val(name), file(\"fastq_${params.output}/*.fastq\")\n    script:\n      \"\"\"\n      mkdir -p fast5_dir\n      mv ${fast5} fast5_dir\n      guppy_basecaller -r -i fast5_dir -s fastq_${params.output} \\\n        -c ${params.configtype} --device auto --trim_strategy dna -q 0\n      \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "      \"\"\"\n      mkdir -p fast5_dir\n      mv ${fast5} fast5_dir\n      guppy_basecaller -r -i fast5_dir -s fastq_${params.output} \\\n        -c ${params.configtype} --device auto --trim_strategy dna -q 0\n      \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "fast5"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "maxForks 1",
            "container = 'nanozoo/guppy_gpu:4.4.1-1--a3fcea3'",
            "containerOptions '--gpus all'",
            "publishDir \"${params.output}/${name}/fastq/\", mode: 'copy', pattern: \"fastq_${params.output}/*.fastq\""
        ],
        "when": "",
        "stub": ""
    },
    "bakta_database": {
        "name_process": "bakta_database",
        "string_process": "process bakta_database {\n        storeDir \"${params.databases}/bakta\" \n        label 'ubuntu'    \n      output:\n        path(\"db.tar.gz\")\n      script:\n        \"\"\"\n        wget --no-check-certificate https://zenodo.org/record/5215743/files/db.tar.gz\n        \"\"\"\n    }",
        "nb_lignes_process": 8,
        "string_script": "        \"\"\"\n        wget --no-check-certificate https://zenodo.org/record/5215743/files/db.tar.gz\n        \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "storeDir \"${params.databases}/bakta\"",
            "label 'ubuntu'"
        ],
        "when": "",
        "stub": ""
    },
    "centrifuge_illumina": {
        "name_process": "centrifuge_illumina",
        "string_process": "process centrifuge_illumina {\n      publishDir \"${params.output}/${name}/centrifuge\", mode: 'copy', pattern: \"${name}_pavian_report_filtered.csv\"\n      publishDir \"${params.output}/${name}/centrifuge\", mode: 'copy', pattern: \"${name}.out\"\n      label 'centrifuge'\n\n                                                                             \n                                                                  \n                                  \n                                       \n                      \n\n    input:\n      tuple val(name), file(fastq) \n      path(database) \n    output:\n      tuple val(name), file(\"${name}.out\"), file(\"${name}_pavian_report_filtered.csv\")\n    shell:\n      \"\"\"\n      case \"!{database}\" in\n      *.tar.gz)\n        tar xzf !{database}\n        ;;\n      *.gz | *.tgz ) \n        gzip -d !{database}\n        ;;\n      *.tar)\n        tar xf !{database}\n        ;;\n      esac\n      \n      DBname=\\$(ls *.[1-9].cf | head -1 | cut -f1 -d\".\")\n\n      centrifuge -p ${task.cpus} -x \\${DBname} -k 5 \\\n        -1 ${fastq[0]} -2 ${fastq[0]} -S centrifuge_results.out --report-file centrifuge_out.log\n\n      # filter based on score\n      < centrifuge_results.out awk '{if(NR < 2 || \\$4 >= 250) {print}}' | awk '{if(NR < 2 || \\$6 >= 150) {print}}' > centrifuge_filtered.out\n\n      centrifuge-kreport -x \\${DBname} centrifuge_filtered.out > ${name}_pavian_report_filtered.csv\n\n      mv centrifuge_filtered.out ${name}.out\n\n      \"\"\"\n}",
        "nb_lignes_process": 42,
        "string_script": "      \"\"\"\n      case \"!{database}\" in\n      *.tar.gz)\n        tar xzf !{database}\n        ;;\n      *.gz | *.tgz ) \n        gzip -d !{database}\n        ;;\n      *.tar)\n        tar xf !{database}\n        ;;\n      esac\n      \n      DBname=\\$(ls *.[1-9].cf | head -1 | cut -f1 -d\".\")\n\n      centrifuge -p ${task.cpus} -x \\${DBname} -k 5 \\\n        -1 ${fastq[0]} -2 ${fastq[0]} -S centrifuge_results.out --report-file centrifuge_out.log\n\n      # filter based on score\n      < centrifuge_results.out awk '{if(NR < 2 || \\$4 >= 250) {print}}' | awk '{if(NR < 2 || \\$6 >= 150) {print}}' > centrifuge_filtered.out\n\n      centrifuge-kreport -x \\${DBname} centrifuge_filtered.out > ${name}_pavian_report_filtered.csv\n\n      mv centrifuge_filtered.out ${name}.out\n\n      \"\"\"",
        "nb_lignes_script": 25,
        "language_script": "bash",
        "tools": [
            "CASE",
            "Centrifuge"
        ],
        "tools_url": [
            "https://bio.tools/CASE",
            "https://bio.tools/centrifuge"
        ],
        "tools_dico": [
            {
                "name": "CASE",
                "uri": "https://bio.tools/CASE",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0749",
                            "term": "Transcription factors and regulatory sites"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0089",
                            "term": "Ontology and terminology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3436",
                                    "term": "Aggregation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3435",
                                    "term": "Standardisation and normalisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3559",
                                    "term": "Ontology visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3559",
                                    "term": "Ontology browsing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Advancing Coordinated Cyber-investigations and Tool Interoperability using a Community Developed Specification Language.\n\nSource files for the CASE website.\n\nAPI used for instantiating CASE objects (includes ontological verification and type checking).\n\nCyber-investigation Analysis Standard Expression (CASE).\n\nRead the CASE Wiki tab to learn everything you need to know about the Cyber-investigation Analysis Standard Expression (CASE) ontology. For learning about the Unified Cyber Ontology, CASE's parent, see UCO.\n\n\"@vocab\": \"http://case.example.org/core#\",.\n\nDET ER DINE PENGER DET DREIER SEG OM...\n\nVi er ikke st\ufffdrst, men garanterer effektiv behandling.\n\nLast ned v\ufffdr brosjyre i PDF format.\n\n||| COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/pymzml (GITHUB.COM).\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'UCO', 'cyber-investigation', 'cyber-investigations', 'plaso'",
                "homepage": "http://CASE.as"
            },
            {
                "name": "Centrifuge",
                "uri": "https://bio.tools/centrifuge",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Nucleic acid sequence analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Sequence analysis (nucleic acid)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A very rapid and memory-efficient system for the classification of DNA sequences from microbial samples. The system uses a novel indexing scheme based on the Burrows-Wheeler transform and the Ferragina-Manzini index, optimized specifically for the metagenomic classification problem. Together these advances enable timely and accurate analysis of large metagenomics data sets on conventional desktop computers.",
                "homepage": "https://ccb.jhu.edu/software/centrifuge/"
            }
        ],
        "inputs": [
            "name",
            "fastq",
            "database"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}/centrifuge\", mode: 'copy', pattern: \"${name}_pavian_report_filtered.csv\"",
            "publishDir \"${params.output}/${name}/centrifuge\", mode: 'copy', pattern: \"${name}.out\"",
            "label 'centrifuge'"
        ],
        "when": "",
        "stub": ""
    },
    "overview_parser": {
        "name_process": "overview_parser",
        "string_process": "process overview_parser {\n    publishDir \"${params.output}/\", mode: 'copy'\n    label 'ubuntu'   \n  input:\n    path(results)\n    val(sampleIDs)\n  output:\n\t  file(\"input*.csv\") \n  shell:\n  if (params.coverage)\n    \"\"\"\n    all_sample_IDs=\\$(echo \"${sampleIDs}\" | tr -d \" []\" | tr \",\" \"\\\\n\")\n\n    header=\\$(echo \"${sampleIDs}\" | tr -d \" []\")\n    printf \"ID,type,type.1,method,\\${header}\\\\n\" > input.csv\n    printf \"ID,type,type.1,method,\\${header}\\\\n\" > input_coverage.csv\n\n    ABRIMETHODS=\\$(cat *.abricate | grep -v \"^#FILE\" | cut -f15 | sort | uniq )\n    FARMETHODS=\\$(head -n 2 -q *gencount.fargene | grep \"The used HMM-model was:\" | awk '{print \\$5}' | sort | uniq | sort )\n\n    ##############\n    # ABRICATE\n    ##############\n\n    while IFS= read -r method ; do\n      # chromosome\n      printf \"\\${method}-genome,\\${method},Genome,Antibiotic resistance\" >> input.csv\n      printf \"\\${method}-genome,\\${method},Genome,Antibiotic resistance\" >> input_coverage.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(cat *_chromosome_\\${sampleID}_*.abricate *_unclassified_\\${sampleID}_*.abricate | grep \"\\${method}\" | wc -l)\n        coverage=\\$(cat *_chromosome_\\${sampleID}_*.abricate *_unclassified_\\${sampleID}_*.abricate | grep \"\\${method}\" | cut -f2 | rev | cut -f1 -d \"_\" | rev | awk '{s+=\\$1} END {print s}')\n\n        if test \\$amount -gt 0 \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n          printf \",\\$coverage\" >> input_coverage.csv \n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n      printf \"\\\\n\" >> input_coverage.csv\n\n      # plasmid\n      printf \"\\${method}-plasmid,\\${method},Plasmid,Antibiotic resistance\" >> input.csv\n      printf \"\\${method}-plasmid,\\${method},Plasmid,Antibiotic resistance\" >> input_coverage.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(grep \"\\${method}\" *_plasmid_\\${sampleID}_*.abricate | wc -l)\n        coverage=\\$(cat *_plasmid_\\${sampleID}_*.abricate | grep \"\\${method}\" | cut -f2 | rev | cut -f1 -d \"_\" | rev | awk '{s+=\\$1} END {print s}')\n  \n        if test \\$amount -gt 0  \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n          printf \",\\$coverage\" >> input_coverage.csv \n\n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n      printf \"\\\\n\" >> input_coverage.csv\n\n    done < <(echo \"\\${ABRIMETHODS}\")  \n\n    ##############\n    # FARGENE \n    ##############\n\n    while IFS= read -r method ; do\n      # chromosome\n      printf \"\\${method%.hmm}-genome,\\${method%.hmm},Genome,beta-lactamase class\" >> input.csv\n      printf \"\\${method%.hmm}-genome,\\${method%.hmm},Genome,beta-lactamase class\" >> input_coverage.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(cat *_chromosome_\\${sampleID}_*.fargene *_unclassified_\\${sampleID}_*.fargene | grep -A 5 \"\\${method}\" |\\\n                grep \"Number of predicted genes\" |\\\n                awk '{printf \"%s\\\\n\",\\$5}' | awk '{s+=\\$1} END {print s}')\n\n        coverage=\\$(ls *_chromosome_\\${sampleID}_*coverage.fargene *_unclassified_\\${sampleID}_*coverage.fargene |\\\n            grep -i \"\\${method%.hmm}\" | xargs cat | cut -f1 -d \" \" | sed 's|_seq._.||g' | rev | cut -f1 -d\"_\" | rev | awk '{s+=\\$1} END {print s}')\n\n        if test \\$amount -gt 0 \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n        printf \",\\$coverage\" >> input_coverage.csv \n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n      printf \"\\\\n\" >> input_coverage.csv\n\n      # plasmid\n      printf \"\\${method%.hmm}-plasmid,\\${method%.hmm},Plasmid,beta-lactamase class\" >> input.csv\n      printf \"\\${method%.hmm}-plasmid,\\${method%.hmm},Plasmid,beta-lactamase class\" >> input_coverage.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(cat *_plasmid_\\${sampleID}_*.fargene | grep -A 5 \"\\${method}\" |\\\n                grep \"Number of predicted genes\" |\\\n                awk '{printf \"%s\\\\n\",\\$5}' | awk '{s+=\\$1} END {print s}')\n  \n      coverage=\\$(ls *_plasmid_\\${sampleID}_*coverage.fargene |\\\n          grep -i \"\\${method%.hmm}\" | xargs cat | cut -f1 -d \" \" | sed 's|_seq._.||g' | rev | cut -f1 -d\"_\" | rev | awk '{s+=\\$1} END {print s}')\n\n       if test \\$amount -gt 0  \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n        printf \",\\$coverage\" >> input_coverage.csv \n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n      printf \"\\\\n\" >> input_coverage.csv\n\n    done < <(echo \"\\${FARMETHODS}\")  \n    \"\"\"\n\nelse if (!params.coverage)\n    \"\"\"\n    all_sample_IDs=\\$(echo \"${sampleIDs}\" | tr -d \" []\" | tr \",\" \"\\\\n\")\n\n    header=\\$(echo \"${sampleIDs}\" | tr -d \" []\")\n    printf \"ID,type,type.1,method,\\${header}\\\\n\" > input.csv\n\n    ABRIMETHODS=\\$(cat *.abricate | grep -v \"^#FILE\" | cut -f15 | sort | uniq )\n    FARMETHODS=\\$(head -n 2 -q *.fargene | grep \"The used HMM-model was:\" | awk '{print \\$5}' | sort | uniq | sort )\n\n    ##############\n    # ABRICATE\n    ##############\n\n    while IFS= read -r method ; do\n      # chromosome\n      printf \"\\${method}-genome,\\${method},Genome,Antibiotic resistance\" >> input.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(cat *_chromosome_\\${sampleID}_*.abricate *_unclassified_\\${sampleID}_*.abricate | grep \"\\${method}\" | wc -l)\n  \n        if test \\$amount -gt 0 \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n\n      # plasmid\n      printf \"\\${method}-plasmid,\\${method},Plasmid,Antibiotic resistance\" >> input.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(grep \"\\${method}\" *_plasmid_\\${sampleID}_*.abricate | wc -l)\n  \n        if test \\$amount -gt 0  \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n\n    done < <(echo \"\\${ABRIMETHODS}\")  \n\n    ##############\n    # FARGENE \n    ##############\n\n    while IFS= read -r method ; do\n      # chromosome\n      printf \"\\${method%.hmm}-genome,\\${method%.hmm},Genome,beta-lactamase class\" >> input.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(cat *_chromosome_\\${sampleID}_*.fargene *_unclassified_\\${sampleID}_*.fargene | grep -A 5 \"\\${method}\" |\\\n                grep \"Number of predicted genes\" |\\\n                awk '{printf \"%s\\\\n\",\\$5}' | awk '{s+=\\$1} END {print s}')\n\n        if test \\$amount -gt 0 \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n\n      # plasmid\n      printf \"\\${method%.hmm}-plasmid,\\${method%.hmm},Plasmid,beta-lactamase class\" >> input.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(cat *_plasmid_\\${sampleID}_*.fargene | grep -A 5 \"\\${method}\" |\\\n                grep \"Number of predicted genes\" |\\\n                awk '{printf \"%s\\\\n\",\\$5}' | awk '{s+=\\$1} END {print s}')\n  \n        if test \\$amount -gt 0  \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n\n    done < <(echo \"\\${FARMETHODS}\")  \n    \"\"\"\n}",
        "nb_lignes_process": 227,
        "string_script": "  if (params.coverage)\n    \"\"\"\n    all_sample_IDs=\\$(echo \"${sampleIDs}\" | tr -d \" []\" | tr \",\" \"\\\\n\")\n\n    header=\\$(echo \"${sampleIDs}\" | tr -d \" []\")\n    printf \"ID,type,type.1,method,\\${header}\\\\n\" > input.csv\n    printf \"ID,type,type.1,method,\\${header}\\\\n\" > input_coverage.csv\n\n    ABRIMETHODS=\\$(cat *.abricate | grep -v \"^#FILE\" | cut -f15 | sort | uniq )\n    FARMETHODS=\\$(head -n 2 -q *gencount.fargene | grep \"The used HMM-model was:\" | awk '{print \\$5}' | sort | uniq | sort )\n\n    ##############\n    # ABRICATE\n    ##############\n\n    while IFS= read -r method ; do\n      # chromosome\n      printf \"\\${method}-genome,\\${method},Genome,Antibiotic resistance\" >> input.csv\n      printf \"\\${method}-genome,\\${method},Genome,Antibiotic resistance\" >> input_coverage.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(cat *_chromosome_\\${sampleID}_*.abricate *_unclassified_\\${sampleID}_*.abricate | grep \"\\${method}\" | wc -l)\n        coverage=\\$(cat *_chromosome_\\${sampleID}_*.abricate *_unclassified_\\${sampleID}_*.abricate | grep \"\\${method}\" | cut -f2 | rev | cut -f1 -d \"_\" | rev | awk '{s+=\\$1} END {print s}')\n\n        if test \\$amount -gt 0 \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n          printf \",\\$coverage\" >> input_coverage.csv \n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n      printf \"\\\\n\" >> input_coverage.csv\n\n      # plasmid\n      printf \"\\${method}-plasmid,\\${method},Plasmid,Antibiotic resistance\" >> input.csv\n      printf \"\\${method}-plasmid,\\${method},Plasmid,Antibiotic resistance\" >> input_coverage.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(grep \"\\${method}\" *_plasmid_\\${sampleID}_*.abricate | wc -l)\n        coverage=\\$(cat *_plasmid_\\${sampleID}_*.abricate | grep \"\\${method}\" | cut -f2 | rev | cut -f1 -d \"_\" | rev | awk '{s+=\\$1} END {print s}')\n  \n        if test \\$amount -gt 0  \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n          printf \",\\$coverage\" >> input_coverage.csv \n\n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n      printf \"\\\\n\" >> input_coverage.csv\n\n    done < <(echo \"\\${ABRIMETHODS}\")  \n\n    ##############\n    # FARGENE \n    ##############\n\n    while IFS= read -r method ; do\n      # chromosome\n      printf \"\\${method%.hmm}-genome,\\${method%.hmm},Genome,beta-lactamase class\" >> input.csv\n      printf \"\\${method%.hmm}-genome,\\${method%.hmm},Genome,beta-lactamase class\" >> input_coverage.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(cat *_chromosome_\\${sampleID}_*.fargene *_unclassified_\\${sampleID}_*.fargene | grep -A 5 \"\\${method}\" |\\\n                grep \"Number of predicted genes\" |\\\n                awk '{printf \"%s\\\\n\",\\$5}' | awk '{s+=\\$1} END {print s}')\n\n        coverage=\\$(ls *_chromosome_\\${sampleID}_*coverage.fargene *_unclassified_\\${sampleID}_*coverage.fargene |\\\n            grep -i \"\\${method%.hmm}\" | xargs cat | cut -f1 -d \" \" | sed 's|_seq._.||g' | rev | cut -f1 -d\"_\" | rev | awk '{s+=\\$1} END {print s}')\n\n        if test \\$amount -gt 0 \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n        printf \",\\$coverage\" >> input_coverage.csv \n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n      printf \"\\\\n\" >> input_coverage.csv\n\n      # plasmid\n      printf \"\\${method%.hmm}-plasmid,\\${method%.hmm},Plasmid,beta-lactamase class\" >> input.csv\n      printf \"\\${method%.hmm}-plasmid,\\${method%.hmm},Plasmid,beta-lactamase class\" >> input_coverage.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(cat *_plasmid_\\${sampleID}_*.fargene | grep -A 5 \"\\${method}\" |\\\n                grep \"Number of predicted genes\" |\\\n                awk '{printf \"%s\\\\n\",\\$5}' | awk '{s+=\\$1} END {print s}')\n  \n      coverage=\\$(ls *_plasmid_\\${sampleID}_*coverage.fargene |\\\n          grep -i \"\\${method%.hmm}\" | xargs cat | cut -f1 -d \" \" | sed 's|_seq._.||g' | rev | cut -f1 -d\"_\" | rev | awk '{s+=\\$1} END {print s}')\n\n       if test \\$amount -gt 0  \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n        printf \",\\$coverage\" >> input_coverage.csv \n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n      printf \"\\\\n\" >> input_coverage.csv\n\n    done < <(echo \"\\${FARMETHODS}\")  \n    \"\"\"\n\nelse if (!params.coverage)\n    \"\"\"\n    all_sample_IDs=\\$(echo \"${sampleIDs}\" | tr -d \" []\" | tr \",\" \"\\\\n\")\n\n    header=\\$(echo \"${sampleIDs}\" | tr -d \" []\")\n    printf \"ID,type,type.1,method,\\${header}\\\\n\" > input.csv\n\n    ABRIMETHODS=\\$(cat *.abricate | grep -v \"^#FILE\" | cut -f15 | sort | uniq )\n    FARMETHODS=\\$(head -n 2 -q *.fargene | grep \"The used HMM-model was:\" | awk '{print \\$5}' | sort | uniq | sort )\n\n    ##############\n    # ABRICATE\n    ##############\n\n    while IFS= read -r method ; do\n      # chromosome\n      printf \"\\${method}-genome,\\${method},Genome,Antibiotic resistance\" >> input.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(cat *_chromosome_\\${sampleID}_*.abricate *_unclassified_\\${sampleID}_*.abricate | grep \"\\${method}\" | wc -l)\n  \n        if test \\$amount -gt 0 \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n\n      # plasmid\n      printf \"\\${method}-plasmid,\\${method},Plasmid,Antibiotic resistance\" >> input.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(grep \"\\${method}\" *_plasmid_\\${sampleID}_*.abricate | wc -l)\n  \n        if test \\$amount -gt 0  \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n\n    done < <(echo \"\\${ABRIMETHODS}\")  \n\n    ##############\n    # FARGENE \n    ##############\n\n    while IFS= read -r method ; do\n      # chromosome\n      printf \"\\${method%.hmm}-genome,\\${method%.hmm},Genome,beta-lactamase class\" >> input.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(cat *_chromosome_\\${sampleID}_*.fargene *_unclassified_\\${sampleID}_*.fargene | grep -A 5 \"\\${method}\" |\\\n                grep \"Number of predicted genes\" |\\\n                awk '{printf \"%s\\\\n\",\\$5}' | awk '{s+=\\$1} END {print s}')\n\n        if test \\$amount -gt 0 \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n\n      # plasmid\n      printf \"\\${method%.hmm}-plasmid,\\${method%.hmm},Plasmid,beta-lactamase class\" >> input.csv\n\n      while IFS= read -r sampleID ; do\n        amount=\\$(cat *_plasmid_\\${sampleID}_*.fargene | grep -A 5 \"\\${method}\" |\\\n                grep \"Number of predicted genes\" |\\\n                awk '{printf \"%s\\\\n\",\\$5}' | awk '{s+=\\$1} END {print s}')\n  \n        if test \\$amount -gt 0  \n        then \n          printf \",\\$amount\" >> input.csv \n        else  \n          printf \",NA\" >> input.csv \n        fi\n\n      done < <(echo \"\\${all_sample_IDs}\")\n\n      printf \"\\\\n\" >> input.csv\n\n    done < <(echo \"\\${FARMETHODS}\")  \n    \"\"\"",
        "nb_lignes_script": 218,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "results",
            "sampleIDs"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/\", mode: 'copy'",
            "label 'ubuntu'"
        ],
        "when": "",
        "stub": ""
    },
    "sourmashclassification": {
        "name_process": "sourmashclassification",
        "string_process": "process sourmashclassification {\n      publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"taxonomic-classification.txt\"\n      label 'sourmash'\n    input:\n      tuple val(name), file(fasta) \n      file(database) \n    output:\n      tuple val(name), file(\"taxonomic-classification.txt\")\n    script:\n      \"\"\"\n      sourmash sketch dna -p scaled=10000,k=31 ${fasta} -o ${name}.sig\n      sourmash lca classify --db ${database} --query ${name}.sig -o taxonomic-classification.txt\n      \"\"\"\n}",
        "nb_lignes_process": 12,
        "string_script": "      \"\"\"\n      sourmash sketch dna -p scaled=10000,k=31 ${fasta} -o ${name}.sig\n      sourmash lca classify --db ${database} --query ${name}.sig -o taxonomic-classification.txt\n      \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "sourmash"
        ],
        "tools_url": [
            "https://bio.tools/sourmash"
        ],
        "tools_dico": [
            {
                "name": "sourmash",
                "uri": "https://bio.tools/sourmash",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3307",
                            "term": "Computational biology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0346",
                                    "term": "Sequence similarity search"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix generation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Phylogenetic distance matrix generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2975",
                                "term": "Nucleic acid sequence (raw)"
                            }
                        ],
                        "output": []
                    }
                ],
                "description": "Compute and compare MinHash signatures for DNA data sets.",
                "homepage": "https://sourmash.readthedocs.io/en/latest/"
            }
        ],
        "inputs": [
            "name",
            "fasta",
            "database"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}\", mode: 'copy', pattern: \"taxonomic-classification.txt\"",
            "label 'sourmash'"
        ],
        "when": "",
        "stub": ""
    },
    "sourmashclusterdir": {
        "name_process": "sourmashclusterdir",
        "string_process": "process sourmashclusterdir {\n      publishDir \"${params.output}/${name}/cluster/\", mode: 'copy', pattern: \"*.pdf\"\n      publishDir \"${params.output}/${name}/cluster/\", mode: 'copy', pattern: \"results.csv\"\n      label 'sourmash'\n    input:\n      tuple val(name), file(dir) \n    output:\n      tuple val(name), file(\"*.pdf\")\n      tuple val(name), file(\"results.csv\")\n    script:\n      \"\"\"\n      cp ${dir}/*.* .\n      sourmash sketch dna -p scaled=10000,k=31 *.fa*\n      sourmash compare *.sig -o results_sig --csv results.csv\n      sourmash plot --pdf --subsample=250 --labels results_sig\n      \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "      \"\"\"\n      cp ${dir}/*.* .\n      sourmash sketch dna -p scaled=10000,k=31 *.fa*\n      sourmash compare *.sig -o results_sig --csv results.csv\n      sourmash plot --pdf --subsample=250 --labels results_sig\n      \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "sourmash"
        ],
        "tools_url": [
            "https://bio.tools/sourmash"
        ],
        "tools_dico": [
            {
                "name": "sourmash",
                "uri": "https://bio.tools/sourmash",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3307",
                            "term": "Computational biology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0346",
                                    "term": "Sequence similarity search"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix generation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Phylogenetic distance matrix generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2975",
                                "term": "Nucleic acid sequence (raw)"
                            }
                        ],
                        "output": []
                    }
                ],
                "description": "Compute and compare MinHash signatures for DNA data sets.",
                "homepage": "https://sourmash.readthedocs.io/en/latest/"
            }
        ],
        "inputs": [
            "name",
            "dir"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name",
            "name"
        ],
        "nb_outputs": 2,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "publishDir \"${params.output}/${name}/cluster/\", mode: 'copy', pattern: \"*.pdf\"",
            "publishDir \"${params.output}/${name}/cluster/\", mode: 'copy', pattern: \"results.csv\"",
            "label 'sourmash'"
        ],
        "when": "",
        "stub": ""
    },
    "filter_fasta_by_length": {
        "name_process": "filter_fasta_by_length",
        "string_process": "process filter_fasta_by_length {\n    label 'ubuntu'\n  input:\n    tuple val(name), path(fasta)\n  output:\n\t  tuple val(name), path(\"${name}_filtered.fasta\")\n  script:\n    \"\"\"\n  \t# make fasta files to one liner\n    sed ':a;N;/^>/M!s/\\n//;ta;P;D' ${fasta} |\\\n    awk '/^>/ { getline seq } length(seq) > 1500 { print \\$0 \"\\\\n\" seq }' |\\\n    awk '/^>/ { getline seq } length(seq) < 1000000 { print \\$0 \"\\\\n\" seq }' > ${name}_filtered.fasta \n    \"\"\"\n}",
        "nb_lignes_process": 12,
        "string_script": "    \"\"\"\n  \t# make fasta files to one liner\n    sed ':a;N;/^>/M!s/\\n//;ta;P;D' ${fasta} |\\\n    awk '/^>/ { getline seq } length(seq) > 1500 { print \\$0 \"\\\\n\" seq }' |\\\n    awk '/^>/ { getline seq } length(seq) < 1000000 { print \\$0 \"\\\\n\" seq }' > ${name}_filtered.fasta \n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "TAP"
        ],
        "tools_url": [
            "https://bio.tools/tap"
        ],
        "tools_dico": [
            {
                "name": "TAP",
                "uri": "https://bio.tools/tap",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2814",
                            "term": "Protein structure analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_2814",
                            "term": "Protein structure"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2423",
                                    "term": "Prediction and recognition"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Validate the local torsion angles of a protein structure.",
                "homepage": "http://protein.bio.unipd.it/tap/"
            }
        ],
        "inputs": [
            "name",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__docker_pipelines",
        "directive": [
            "label 'ubuntu'"
        ],
        "when": "",
        "stub": ""
    }
}