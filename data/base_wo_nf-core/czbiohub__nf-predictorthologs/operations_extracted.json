{
    "OPERATION_1": {
        "string": "ch_multiqc_custom_config = Channel.fromPath(params.multiqc_config, checkIfExists: true)",
        "origin": [
            [
                "params.multiqc_config, checkIfExists: true",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_multiqc_custom_config",
                "P"
            ]
        ]
    },
    "OPERATION_2": {
        "string": "ch_multiqc_custom_config = Channel.empty()",
        "origin": [],
        "gives": [
            [
                "ch_multiqc_custom_config",
                "P"
            ]
        ]
    },
    "OPERATION_3": {
        "string": "Channel.fromPath(params.hashes)\n      .ifEmpty { exit 1, \"params.hashes was empty - no input files supplied\" }\n      .splitText()\n      .map{ row -> tuple(\"hash\", row.replaceAll(\"\\\\s+\", \"\") )}\n      .transpose()\n      .dump( tag: 'ch_hash_to_group' )\n      .into { ch_hash_to_group_for_joining; ch_hash_to_group_for_hash2kmer }",
        "origin": [
            [
                "params.hashes",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_hash_to_group_for_joining",
                "P"
            ],
            [
                "ch_hash_to_group_for_hash2kmer",
                "P"
            ]
        ]
    },
    "OPERATION_4": {
        "string": "Channel.fromPath(params.bai)\n        .ifEmpty { exit 1, \"params.bai was empty - no input files supplied\" }\n        .set { ch_bai }",
        "origin": [
            [
                "params.bai",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_bai",
                "P"
            ]
        ]
    },
    "OPERATION_5": {
        "string": "Channel.fromPath(params.bam)\n        .ifEmpty { exit 1, \"params.bam was empty - no input files supplied\" }\n        .combine(ch_bai)\n        .set { ch_bam_bai }",
        "origin": [
            [
                "ch_bai",
                "P"
            ],
            [
                "params.bam",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_bam_bai",
                "P"
            ]
        ]
    },
    "OPERATION_6": {
        "string": "Channel.fromPath(params.bed)\n        .ifEmpty { exit 1, \"params.bed was empty - no input files supplied\" }\n        .splitText()\n        .map {row -> row.split()}\n        .map { row -> [ row[3], row[0], row[1], row[2] ] }                                           \n        .combine(ch_bam_bai)\n        .dump ( tag: 'ch_bam_bai' )\n        .set {ch_bed_bam_bai}",
        "origin": [
            [
                "ch_bam_bai",
                "P"
            ],
            [
                "params.bed",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_bed_bam_bai",
                "P"
            ]
        ]
    },
    "OPERATION_7": {
        "string": "Channel.fromPath(params.bam)\n        .ifEmpty { exit 1, \"params.bam was empty, no input file supplied\" }\n        .into { ch_bam_for_dedup }",
        "origin": [
            [
                "params.bam",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_bam_for_dedup",
                "P"
            ]
        ]
    },
    "OPERATION_8": {
        "string": "Channel.fromPath(params.protein_fastas)\n        .ifEmpty { exit 1, \"params.protein_fastas was empty - no input files supplied\" }\n        .dump ( tag: 'ch_protein_fastas' )\n        .set { ch_protein_fastas }",
        "origin": [
            [
                "params.protein_fastas",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_protein_fastas",
                "P"
            ]
        ]
    },
    "OPERATION_9": {
        "string": "Channel\n      .fromPath(params.csv)\n      .splitCsv(header:true)\n      .map{ row -> tuple(row.sample_id, tuple(file(row.fasta)))}\n      .ifEmpty { exit 1, \"params.csv (${params.csv}) was empty - no input files supplied\" }\n      .dump( tag: 'ch_protein_fastas__from_csv' )\n      .set { ch_protein_fastas }",
        "origin": [
            [
                "params.csv",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_protein_fastas",
                "P"
            ]
        ]
    },
    "OPERATION_10": {
        "string": "Channel\n      .from(params.protein_fasta_paths)\n      .map { row -> file(row[1][0], checkIfExists: true) }\n      .ifEmpty { exit 1, \"params.protein_fasta_paths was empty - no input files supplied\" }\n      .dump(tag: \"protein_fasta_paths\")\n      .set { ch_protein_fastas }",
        "origin": [
            [
                "params.protein_fasta_paths",
                "V"
            ]
        ],
        "gives": [
            [
                "ch_protein_fastas",
                "P"
            ]
        ]
    },
    "OPERATION_11": {
        "string": "Channel\n        .fromPath(params.csv)\n        .splitCsv(header:true)\n        .map{ row -> tuple(row.sample_id, tuple(file(row.read1)))}\n        .ifEmpty { exit 1, \"params.csv (${params.csv}) was empty - no input files supplied\" }\n        .dump(tag: \"reads_single_end\")\n        .into { ch_read_files_fastqc; ch_read_files_trimming; ch_read_files_translate }",
        "origin": [
            [
                "params.csv",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_read_files_fastqc",
                "P"
            ],
            [
                "ch_read_files_trimming",
                "P"
            ],
            [
                "ch_read_files_translate",
                "P"
            ]
        ]
    },
    "OPERATION_12": {
        "string": "Channel\n        .fromPath(params.csv)\n        .splitCsv(header:true)\n        .map{ row -> tuple(row.sample_id, tuple(file(row.read1), file(row.read2)))}\n        .ifEmpty { exit 1, \"params.csv (${params.csv}) was empty - no input files supplied\" }\n        .dump(tag: \"reads_paired_end\")\n        .into { ch_read_files_fastqc; ch_read_files_trimming; ch_read_files_translate }",
        "origin": [
            [
                "params.csv",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_read_files_fastqc",
                "P"
            ],
            [
                "ch_read_files_trimming",
                "P"
            ],
            [
                "ch_read_files_translate",
                "P"
            ]
        ]
    },
    "OPERATION_13": {
        "string": "Channel\n        .from(params.readPaths)\n        .map { row -> [ row[0], [ file(row[1][0], checkIfExists: true) ] ] }\n        .ifEmpty { exit 1, \"params.readPaths was empty - no input files supplied\" }\n        .dump(tag: \"reads_single_end\")\n        .into { ch_read_files_fastqc; ch_read_files_trimming; ch_read_files_translate }",
        "origin": [
            [
                "params.readPaths",
                "V"
            ]
        ],
        "gives": [
            [
                "ch_read_files_fastqc",
                "P"
            ],
            [
                "ch_read_files_trimming",
                "P"
            ],
            [
                "ch_read_files_translate",
                "P"
            ]
        ]
    },
    "OPERATION_14": {
        "string": "Channel\n        .from(params.readPaths)\n        .map { row -> [ row[0], [ file(row[1][0], checkIfExists: true), file(row[1][1], checkIfExists: true) ] ] }\n        .ifEmpty { exit 1, \"params.readPaths was empty - no input files supplied\" }\n        .dump(tag: \"reads_paired_end\")\n        .into { ch_read_files_fastqc; ch_read_files_trimming; ch_read_files_translate }",
        "origin": [
            [
                "params.readPaths",
                "V"
            ]
        ],
        "gives": [
            [
                "ch_read_files_fastqc",
                "P"
            ],
            [
                "ch_read_files_trimming",
                "P"
            ],
            [
                "ch_read_files_translate",
                "P"
            ]
        ]
    },
    "OPERATION_15": {
        "string": "Channel\n      .fromFilePairs(params.reads, size: params.single_end ? 1 : 2)\n      .ifEmpty { exit 1, \"Cannot find any reads matching: ${params.reads}\\nNB: Path needs to be enclosed in quotes!\\nIf this is single-end data, please specify --single_end on the command line.\" }\n      .dump(tag: \"read_paths\")\n      .into { ch_read_files_fastqc; ch_read_files_trimming }",
        "origin": [
            [
                "params.reads, size: params.single_end ? 1 : 2",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_read_files_fastqc",
                "P"
            ],
            [
                "ch_read_files_trimming",
                "P"
            ]
        ]
    },
    "OPERATION_16": {
        "string": "Channel.fromPath(params.hashes)\n      .ifEmpty { exit 1, \"params.hashes was empty - no input files supplied\" }\n      .splitText()\n      .map{ row -> tuple(row.replaceAll(\"\\\\s+\", \"\"), \"hash\" )}\n      .transpose()\n      .into { ch_hash_to_group_for_joining_after_hash2kmer;\n        ch_hash_to_group_for_joining_after_hash2sig;\n        ch_hash_to_group_for_hash2kmer;\n        ch_hash_to_group_for_hash2sig\n       }",
        "origin": [
            [
                "params.hashes",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_hash_to_group_for_joining_after_hash2kmer",
                "P"
            ],
            [
                "ch_hash_to_group_for_joining_after_hash2sig",
                "P"
            ],
            [
                "ch_hash_to_group_for_hash2kmer",
                "P"
            ],
            [
                "ch_hash_to_group_for_hash2sig",
                "P"
            ]
        ]
    },
    "OPERATION_17": {
        "string": "Channel\n      .fromPath ( params.csv )\n      .splitCsv ( header:true )\n      .branch { row ->\n        aligned: row.is_aligned == \"aligned\"\n        unaligned: row.is_aligned == \"unaligned\"\n      }\n      .set { ch_csv_is_aligned }",
        "origin": [
            [
                "params.csv",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_csv_is_aligned",
                "P"
            ]
        ]
    },
    "OPERATION_18": {
        "string": "Channel\n      .fromPath(params.csv)\n      .splitCsv(header:true)\n      .filter{ row -> row.is_aligned == 'unaligned' }\n      .ifEmpty { exit 1, \"is_aligned column can contain only aligned/unaligned values\"}\n      .dump( tag: 'csv_unaligned' )\n      .map{ row -> tuple(row.group, file(row.sig, checkIfExists: true)) }\n      .ifEmpty { exit 1, \"params.csv (${params.csv}) 'group' or 'sig' column was empty\" }\n      .groupTuple()\n      .dump( tag: 'ch_per_group_unaligned_sig' )\n      .set{ ch_per_group_unaligned_sig }",
        "origin": [
            [
                "params.csv",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_per_group_unaligned_sig",
                "P"
            ]
        ]
    },
    "OPERATION_19": {
        "string": "ch_csv = Channel.fromPath(params.csv)",
        "origin": [
            [
                "params.csv",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_csv",
                "P"
            ]
        ]
    },
    "OPERATION_20": {
        "string": "Channel\n      .fromPath(params.csv)\n      .splitCsv(header:true)\n      .map{ row -> file(row.sig, checkIfExists: true) }\n      .ifEmpty { exit 1, \"params.csv (${params.csv}) 'sig' column was empty\" }\n      .collect()\n      .map{ it -> [it] }                                                                                \n                                                          \n                                                                                                                                \n                                                                                                                                \n                                                                                                                         \n                                                                                                                                \n                                                                                                                                    \n                                                                                                                        \n                                                                                                                                         \n                                                                                                                                 \n                                                                                                                        \n                                                                                                                         \n      .dump( tag: \"ch_all_signatures_flat_list_for_diff_hash\" )\n      .into{ ch_all_signatures_flat_list_for_diff_hash }",
        "origin": [
            [
                "params.csv",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_all_signatures_flat_list_for_diff_hash",
                "P"
            ]
        ]
    },
    "OPERATION_21": {
        "string": "Channel\n      .fromPath(params.csv)\n      .splitCsv(header:true)\n      .map{ row -> file(row.sig, checkIfExists: true) }\n      .ifEmpty { exit 1, \"params.csv (${params.csv}) 'sig' column was empty\" }\n      .collect()\n      .into{ ch_all_signatures_flattened_for_finding_matches }",
        "origin": [
            [
                "params.csv",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_all_signatures_flattened_for_finding_matches",
                "P"
            ]
        ]
    },
    "OPERATION_22": {
        "string": "Channel\n      .fromPath(params.csv)\n      .splitCsv(header:true)\n      .map{ row -> tuple(row.group, file(row.fasta, checkIfExists: true)) }\n      .ifEmpty { exit 1, \"params.csv (${params.csv}) 'fasta' column was empty\" }\n      .groupTuple()\n      .dump( tag: 'ch_group_to_fasta' )\n      .set{ ch_group_to_fasta }",
        "origin": [
            [
                "params.csv",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_group_to_fasta",
                "P"
            ]
        ]
    },
    "OPERATION_23": {
        "string": "Channel\n      .fromPath(params.csv)\n      .splitCsv(header:true)\n      .map{ row -> tuple(row.group) }\n      .unique()\n      .ifEmpty { exit 1, \"params.csv (${params.csv}) 'group' column was empty\" }\n      .dump(tag: 'csv_unique_groups')\n                                                              \n                                                      \n      .combine( ch_all_signatures_flat_list_for_diff_hash )\n      .dump(tag: 'ch_groups_with_all_signatures_for_diff_hash')\n                                                            \n                                       \n                                                                                                                                 \n                                                                                                                                 \n                                                                                                                          \n                                                                                                                                 \n                                                                                                                                     \n                                                                                                                         \n                                                                                                                                          \n                                                                                                                                  \n                                                                                                                         \n                                                                                                                          \n                                                            \n                             \n                                                                                                                               \n                                                                                                                               \n                                                                                                                        \n                                                                                                                               \n                                                                                                                                   \n                                                                                                                       \n                                                                                                                                        \n                                                                                                                                \n                                                                                                                       \n                                                                                                                        \n      .into { ch_groups_with_all_signatures_for_diff_hash }",
        "origin": [
            [
                "ch_all_signatures_flat_list_for_diff_hash",
                "P"
            ],
            [
                "params.csv",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_groups_with_all_signatures_for_diff_hash",
                "P"
            ]
        ]
    },
    "OPERATION_24": {
        "string": "Channel.fromPath(params.proteome_translate_fasta, checkIfExists: true)\n       .ifEmpty { exit 1, \"Peptide fasta file not found: ${params.proteome_translate_fasta}\" }\n       .set{ ch_proteome_translate_fasta }",
        "origin": [
            [
                "params.proteome_translate_fasta, checkIfExists: true",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_proteome_translate_fasta",
                "P"
            ]
        ]
    },
    "OPERATION_25": {
        "string": "Channel.fromPath(params.proteome_search_fasta, checkIfExists: true)\n     .ifEmpty { exit 1, \"Reference proteome fasta file not found: ${params.proteome_search_fasta}\" }\n     .into{ ch_diamond_reference_fasta; ch_sourmash_reference_fasta }",
        "origin": [
            [
                "params.proteome_search_fasta, checkIfExists: true",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_diamond_reference_fasta",
                "P"
            ],
            [
                "ch_sourmash_reference_fasta",
                "P"
            ]
        ]
    },
    "OPERATION_26": {
        "string": "Channel.fromPath(params.taxonmap_gz, checkIfExists: true)\n     .ifEmpty { exit 1, \"Diamond Taxon map file not found: ${params.taxonmap_gz}\" }\n     .set{ ch_diamond_taxonmap_gz }",
        "origin": [
            [
                "params.taxonmap_gz, checkIfExists: true",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_diamond_taxonmap_gz",
                "P"
            ]
        ]
    },
    "OPERATION_27": {
        "string": "Channel.fromPath(params.taxdmp_zip, checkIfExists: true)\n     .ifEmpty { exit 1, \"Diamond taxon dump file not found: ${params.taxdmp_zip}\" }\n     .set{ ch_diamond_taxdmp_zip }",
        "origin": [
            [
                "params.taxdmp_zip, checkIfExists: true",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_diamond_taxdmp_zip",
                "P"
            ]
        ]
    },
    "OPERATION_28": {
        "string": "Channel.fromPath(params.diamond_database, checkIfExists: true)\n       .ifEmpty { exit 1, \"Diamond database file not found: ${params.diamond_database}\" }\n       .set{ ch_diamond_db }",
        "origin": [
            [
                "params.diamond_database, checkIfExists: true",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_diamond_db",
                "P"
            ]
        ]
    },
    "OPERATION_29": {
        "string": "Channel.fromPath(params.sourmash_index, checkIfExists: true)\n       .ifEmpty { exit 1, \"Sourmash SBT Index file not found: ${params.sourmash_index}\" }\n       .set{ ch_sourmash_index }",
        "origin": [
            [
                "params.sourmash_index, checkIfExists: true",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_sourmash_index",
                "P"
            ]
        ]
    },
    "OPERATION_30": {
        "string": "Channel.fromPath(params.infernal_db, checkIfExists: true)\n         .ifEmpty { exit 1, \"Infernal database file not found: ${params.infernal_db}\" }\n         .set{ ch_infernal_db_gz }",
        "origin": [
            [
                "params.infernal_db, checkIfExists: true",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_infernal_db_gz",
                "P"
            ]
        ]
    },
    "OPERATION_31": {
        "string": "Channel.fromPath(params.infernal_db, checkIfExists: true)\n         .ifEmpty { exit 1, \"Infernal database file not found: ${params.infernal_db}\" }\n         .set{ ch_infernal_db }",
        "origin": [
            [
                "params.infernal_db, checkIfExists: true",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_infernal_db",
                "P"
            ]
        ]
    },
    "OPERATION_32": {
        "string": "Channel.fromPath(params.rfam_clan_info, checkIfExists: true)\n       .ifEmpty { exit 1, \"Rfam Clan Information file not found: ${params.rfam_clan_info}\" }\n       .set{ ch_rfam_clan_info }",
        "origin": [
            [
                "params.rfam_clan_info, checkIfExists: true",
                "A"
            ]
        ],
        "gives": [
            [
                "ch_rfam_clan_info",
                "P"
            ]
        ]
    },
    "OPERATION_33": {
        "string": "Channel.from(params.translate_peptide_ksize) { \nch_peptide_ksize  = \n } else { \nch_peptide_ksize  = \n }",
        "origin": [
            [
                "params.translate_peptide_ksize",
                "V"
            ]
        ],
        "gives": []
    },
    "OPERATION_34": {
        "string": "Channel.from(params.translate_peptide_molecule) { \nch_peptide_molecule  = \n } else { \nch_peptide_molecule  = \n }",
        "origin": [
            [
                "params.translate_peptide_molecule",
                "V"
            ]
        ],
        "gives": []
    },
    "OPERATION_35": {
        "string": "ch_fastqc_results = Channel.empty()",
        "origin": [],
        "gives": [
            [
                "ch_fastqc_results",
                "P"
            ]
        ]
    },
    "OPERATION_36": {
        "string": "ch_fastp_results = Channel.empty()",
        "origin": [],
        "gives": [
            [
                "ch_fastp_results",
                "P"
            ]
        ]
    },
    "OPERATION_37": {
        "string": "ch_fastp_results = Channel.empty()",
        "origin": [],
        "gives": [
            [
                "ch_fastp_results",
                "P"
            ]
        ]
    },
    "OPERATION_38": {
        "string": "ch_hash_to_group_for_hash2kmer\n    .map{ it -> it[1] }\n    .into{ ch_hashes_for_hash2kmer; ch_hashes_for_hash2sig }",
        "origin": [
            [
                "ch_hash_to_group_for_hash2kmer",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_hashes_for_hash2kmer",
                "P"
            ],
            [
                "ch_hashes_for_hash2sig",
                "P"
            ]
        ]
    },
    "OPERATION_39": {
        "string": "ch_protein_fastas\n                                  \n      .map { it -> tuple(false, \n                         file(it, checkIfExists: true).getBaseName(), \n                         file(it, checkIfExists: true))\n            }\n                                         \n      .filter { it -> it[2].size() > 0 }\n      .dump ( tag: 'ch_protein_fastas__ch_protein_seq_for_diamond' )\n      .set { ch_protein_seq_for_diamond }",
        "origin": [
            [
                "ch_protein_fastas",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_protein_seq_for_diamond",
                "P"
            ]
        ]
    },
    "OPERATION_40": {
        "string": "ch_hash_to_group_for_hash2kmer\n    .map{ it -> it[0] }\n    .set{ ch_hashes_for_hash2kmer }",
        "origin": [
            [
                "ch_hash_to_group_for_hash2kmer",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_hashes_for_hash2kmer",
                "P"
            ]
        ]
    },
    "OPERATION_41": {
        "string": "ch_csv_is_aligned.unaligned().dump( tag: 'ch_csv_is_aligned.unaligned' )\n      .map{ row -> tuple(row.group, row.sample_id, row.sig, row.fasta) }\n      .dump( tag: 'ch_unaligned_sig_fasta' )\n      .into { ch_unaligned_sig_fasta }",
        "origin": [
            [
                "ch_csv_is_aligned",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_unaligned_sig_fasta",
                "P"
            ]
        ]
    },
    "OPERATION_42": {
        "string": "ch_peptide_molecule\n  .combine(ch_peptide_ksize)\n  .dump ( tag: 'ch_translate_molecule_ksize' )\n  .set { ch_translate_molecule_ksize }",
        "origin": [
            [
                "ch_peptide_molecule",
                "P"
            ],
            [
                "ch_peptide_ksize",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_translate_molecule_ksize",
                "P"
            ]
        ]
    },
    "OPERATION_43": {
        "string": "summary.collect { k,v -> \"${k.padRight(25)}: $v\" }.join(\"\\n\")",
        "origin": [
            [
                "summary",
                "P"
            ]
        ],
        "gives": []
    },
    "OPERATION_44": {
        "string": "ch_intersected\n                                              \n      .filter{ it[1].size() > 20 }\n      .into { ch_read_files_fastqc; ch_read_files_trimming }",
        "origin": [
            [
                "ch_intersected",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_read_files_fastqc",
                "P"
            ],
            [
                "ch_read_files_trimming",
                "P"
            ]
        ]
    },
    "OPERATION_45": {
        "string": "ch_intersected\n                                            \n    .filter{ it[1].size() > 20 }\n    .into { ch_read_files_fastqc; ch_read_files_trimming }",
        "origin": [
            [
                "ch_intersected",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_read_files_fastqc",
                "P"
            ],
            [
                "ch_read_files_trimming",
                "P"
            ]
        ]
    },
    "OPERATION_46": {
        "string": "ch_sencha_bloom_filters\n      .groupTuple(by: [0, 1, 2])\n      .combine(ch_reads_trimmed)\n      .dump( tag: 'ch_sencha_bloom_filters_grouptuple' )\n                                                   \n                                       \n                       \n                   \n                                                                                                            \n                                 \n                                                   \n    .set{ ch_sencha_bloom_filters_grouptuple }",
        "origin": [
            [
                "ch_sencha_bloom_filters",
                "P"
            ],
            [
                "ch_reads_trimmed",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_sencha_bloom_filters_grouptuple",
                "P"
            ]
        ]
    },
    "OPERATION_47": {
        "string": "ch_translated_proteins_potentially_empty\n    .filter{ it[2].size() > 0 }\n    .dump(tag: \"ch_translated_proteins_potentially_empty\")\n                                                       \n                                                   \n                             \n                                                                                                 \n    .set{ ch_protein_seq_for_diamond }",
        "origin": [
            [
                "ch_translated_proteins_potentially_empty",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_protein_seq_for_diamond",
                "P"
            ]
        ]
    },
    "OPERATION_48": {
        "string": "ch_noncoding_nucleotides_potentially_empty\n    .filter { it[1].size() > 0 }\n    .set { ch_noncoding_nucleotides }",
        "origin": [
            [
                "ch_noncoding_nucleotides_potentially_empty",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_noncoding_nucleotides",
                "P"
            ]
        ]
    },
    "OPERATION_49": {
        "string": "ch_informative_hashes_files\n      .dump(tag: 'ch_informative_hashes_files')\n                                \n      .map{ it -> tuple(it[1].splitText(), it[0])}\n                                             \n      .dump(tag: 'ch_informative_hashes_files_split')\n      .transpose()\n                         \n                         \n                         \n      .dump(tag: 'ch_hash_to_group')\n      .into {\n        ch_hash_to_group_for_finding_matches\n        ch_hash_to_group_for_joining_after_hash2kmer;\n        ch_hash_to_group_for_joining_after_hash2sig;\n        ch_hash_to_group_for_hash2kmer;\n        ch_hash_to_group_for_hash2sig }",
        "origin": [
            [
                "ch_informative_hashes_files",
                "P"
            ]
        ],
        "gives": []
    },
    "OPERATION_50": {
        "string": "ch_hash_to_group_for_finding_matches\n    .map{ it -> it[0] }\n    .unique()\n    .into{ ch_informative_hashes_flattened }",
        "origin": [
            [
                "ch_hash_to_group_for_finding_matches",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_informative_hashes_flattened",
                "P"
            ]
        ]
    },
    "OPERATION_51": {
        "string": "ch_protein_fastas\n    .map{ it -> it[1] }                                         \n    .collect()                                     \n    .map{ it -> [it] }                                                          \n    .set{ ch_protein_fastas_flat_list }",
        "origin": [
            [
                "ch_protein_fastas",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_protein_fastas_flat_list",
                "P"
            ]
        ]
    },
    "OPERATION_52": {
        "string": "ch_hashes_for_hash2kmer\n      .combine( ch_protein_fastas_flat_list )\n      .set { ch_hashes_with_fastas_for_hash2kmer }",
        "origin": [
            [
                "ch_hashes_for_hash2kmer",
                "P"
            ],
            [
                "ch_protein_fastas_flat_list",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_hashes_with_fastas_for_hash2kmer",
                "P"
            ]
        ]
    },
    "OPERATION_53": {
        "string": "ch_hash_to_group_for_hash2kmer\n    .join( ch_group_to_fasta )\n    .dump( tag: 'group_to_hashes_for_hash2kmer__combine__ch_group_to_fasta' )\n    .into{ ch_hashes_with_fastas_for_hash2kmer }",
        "origin": [
            [
                "ch_hash_to_group_for_hash2kmer",
                "P"
            ],
            [
                "ch_group_to_fasta",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_hashes_with_fastas_for_hash2kmer",
                "P"
            ]
        ]
    },
    "OPERATION_54": {
        "string": "ch_hash_to_group_for_hash2sig\n    .map{ it -> it[0] }\n    .into{ ch_hashes_for_hash2sig }",
        "origin": [
            [
                "ch_hash_to_group_for_hash2sig",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_hashes_for_hash2sig",
                "P"
            ]
        ]
    },
    "OPERATION_55": {
        "string": "ch_seqs_from_hash2kmer_to_print.dump(tag: 'ch_seqs_from_hash2kmer_to_print')",
        "origin": [
            [
                "ch_seqs_from_hash2kmer_to_print",
                "P"
            ]
        ],
        "gives": []
    },
    "OPERATION_56": {
        "string": "ch_hash_to_group_for_joining_after_hash2kmer\n    .join(ch_seqs_from_hash2kmer)\n    .dump(tag: 'ch_hash_to_group_for_joining__ch_protein_seq_from_hash2kmer')\n    .set{ ch_protein_seq_for_diamond }",
        "origin": [
            [
                "ch_hash_to_group_for_joining_after_hash2kmer",
                "P"
            ],
            [
                "ch_seqs_from_hash2kmer",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_protein_seq_for_diamond",
                "P"
            ]
        ]
    },
    "OPERATION_57": {
        "string": "ch_hash_sigs_from_hash2sig_to_print.dump(tag: 'ch_hash_sigs_from_hash2sig_to_print')",
        "origin": [
            [
                "ch_hash_sigs_from_hash2sig_to_print",
                "P"
            ]
        ],
        "gives": []
    },
    "OPERATION_58": {
        "string": "ch_hash_to_group_for_joining_after_hash2sig\n      .join( ch_hash_sigs_from_hash2sig_to_join )\n                                                                                                \n                                                                                                      \n      .dump( tag: 'ch_hash_to_group_for_joining_after_hash2sig__ch_hash_sigs_from_hash2sig_to_join' )\n      .map{ it -> tuple(it[1], it[0], it[2], it[3]) }\n      .dump( tag: 'ch_group_to_hash_sig' )\n                                                                                                      \n      .set{ ch_group_to_hash_sig }",
        "origin": [
            [
                "ch_hash_to_group_for_joining_after_hash2sig",
                "P"
            ],
            [
                "ch_hash_sigs_from_hash2sig_to_join",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_group_to_hash_sig",
                "P"
            ]
        ]
    },
    "OPERATION_59": {
        "string": "ch_per_group_unaligned_sig\n      .join( ch_group_to_hash_sig )\n                                     \n                     \n                                                                                                                      \n                                                                                                                      \n                                                                                                                      \n                                                                                                                      \n                                                                                                                                \n                                 \n                                   \n                                       \n      .dump( tag: 'ch_group_to_hash_sig_with_group_unaligned_sigs' )\n      .into{ ch_group_to_hash_sig_with_group_unaligned_sigs }",
        "origin": [
            [
                "ch_per_group_unaligned_sig",
                "P"
            ],
            [
                "ch_group_to_hash_sig",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_group_to_hash_sig_with_group_unaligned_sigs",
                "P"
            ]
        ]
    },
    "OPERATION_60": {
        "string": "ch_hash_sigs_in_unaligned\n      .dump( tag: 'ch_hash_sigs_in_unaligned' )\n                                        \n      .branch{\n        aligned: it[4].size() == 0\n        unaligned: it[4].size() > 0\n      }\n      .set{ ch_hashes_sigs_branched }",
        "origin": [
            [
                "ch_hash_sigs_in_unaligned",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_hashes_sigs_branched",
                "P"
            ]
        ]
    },
    "OPERATION_61": {
        "string": "ch_hashes_sigs_branched\n        .unaligned().map { it -> tuple(it[0], it[1], it[2], it[3]) }\n        .dump ( tag: 'ch_hashes_in_group_unaligned_sigs' )\n        .set { ch_group_hash_sigs_to_query }",
        "origin": [
            [
                "ch_hashes_sigs_branched",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_group_hash_sigs_to_query",
                "P"
            ]
        ]
    },
    "OPERATION_62": {
        "string": "ch_hashes_sigs_branched\n        .aligned().map { it -> tuple(it[0], it[1], it[2]) }\n        .dump ( tag: 'ch_hashes_in_group_aligned' )\n        .set { ch_hashes_in_group_aligned }",
        "origin": [
            [
                "ch_hashes_sigs_branched",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_hashes_in_group_aligned",
                "P"
            ]
        ]
    },
    "OPERATION_63": {
        "string": "ch_reads_trimmed = ch_read_files_trimming",
        "origin": [
            [
                "ch_read_files_trimming",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_reads_trimmed",
                "P"
            ]
        ]
    },
    "OPERATION_64": {
        "string": "ch_group_hash_sigs_to_query = ch_group_to_hash_sig",
        "origin": [
            [
                "ch_group_to_hash_sig",
                "P"
            ]
        ],
        "gives": [
            [
                "ch_group_hash_sigs_to_query",
                "P"
            ]
        ]
    }
}