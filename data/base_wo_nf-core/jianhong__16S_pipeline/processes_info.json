{
    "SAMPLESHEET_CHECK": {
        "name_process": "SAMPLESHEET_CHECK",
        "string_process": "process SAMPLESHEET_CHECK {\n    tag \"$samplesheet\"\n\n    conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/python:3.8.3' :\n        'quay.io/biocontainers/python:3.8.3' }\"\n\n    input:\n    path samplesheet\n\n    output:\n    path '*.csv'       , emit: csv\n    path \"versions.yml\", emit: versions\n\n    script:                                                                        \n    \"\"\"\n    check_samplesheet.py \\\\\n        $samplesheet \\\\\n        samplesheet.valid.csv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        python: \\$(python --version | sed 's/Python //g')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "    \"\"\"\n    check_samplesheet.py \\\\\n        $samplesheet \\\\\n        samplesheet.valid.csv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        python: \\$(python --version | sed 's/Python //g')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "samplesheet"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__16S_pipeline",
        "directive": [
            "tag \"$samplesheet\"",
            "conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/python:3.8.3' : 'quay.io/biocontainers/python:3.8.3' }\""
        ],
        "when": "",
        "stub": ""
    },
    "FASTQC": {
        "name_process": "FASTQC",
        "string_process": "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}",
        "nb_lignes_process": 42,
        "string_script": "    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }",
        "nb_lignes_script": 24,
        "language_script": "bash",
        "tools": [
            "FastQC"
        ],
        "tools_url": [
            "https://bio.tools/fastqc"
        ],
        "tools_dico": [
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            }
        ],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__16S_pipeline",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' : 'quay.io/biocontainers/fastqc:0.11.9--0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "FILTERING": {
        "name_process": "FILTERING",
        "string_process": "process FILTERING {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-dada2:1.22.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    val single_end\n\n    output:\n    tuple val(meta), path(\"$prefix\"), path(\"*.rds\") , emit: reads\n    path '*.png'                                    , emit: qc\n    path \"versions.yml\"                             , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    # Applying dada2 pipeline to bioreactor time-series\n    ## Following tutorial http://benjjneb.github.io/dada2_pipeline_MV/tutorial.html\n    ## and here http://benjjneb.github.io/dada2_pipeline_MV/bigdata.html\n\n    pkgs <- c(\"dada2\", \"ShortRead\", \"ggplot2\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    DEMUXD_READS = \"$reads\"\n    TRIM = FALSE\n    OUTFOLDER = \"$prefix\"\n    FILTER_STATS = \"filter_stats.rds\"\n    NCORE <- ifelse(.Platform[[\"OS.type\"]]!=\"windows\", as.numeric(\"$task.cpus\"), FALSE)\n    PAIRED_END <- \"$single_end\" != \"true\"\n\n    args <- strsplit(\"${args}\", \"\\\\\\\\s+\")[[1]]\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    grepl(\"trimming_reads\", \"$args\")\n    option_list <- list(\"trim\"=c(\"--trimming_reads\", \"-t\", \"logical\"),\n                        \"trimLeft\"=c(\"--trim_left\", \"-a\", \"integer\"),\n                        \"trimRight\"=c(\"--trim_right\", \"-b\", \"integer\"),\n                        \"truncLenLeft\"=c(\"--trunc_length_left\", \"-m\", \"integer\"),\n                        \"truncLenRight\"=c(\"--trunc_length_right\", \"-n\", \"integer\"))\n    opt <- parse_args(option_list, args)\n    if(!is.null(opt[[\"trim\"]])){\n        TRIM <- TRUE\n    }\n    if(!is.null(opt[[\"trimLeft\"]])){\n        trimLeft <- trimLeft1 <- opt[[\"trimLeft\"]]\n    }else{\n        trimLeft1 <- 0\n        trimLeft <- NULL\n    }\n    if(!is.null(opt[[\"trimRight\"]])){\n        trimLeft <- c(trimLeft1, opt[[\"trimRight\"]])\n    }\n    if(!is.null(opt[[\"truncLenLeft\"]])){\n        truncLen <- truncLen1 <- opt[[\"truncLenLeft\"]]\n    }else{\n        truncLen1 <- 0\n        truncLen <- NULL\n    }\n    if(!is.null(opt[[\"truncLenRight\"]])){\n        truncLen <- c(truncLen1, opt[[\"truncLenRight\"]])\n    }\n\n    # Filtering and Trimming --------------------------------------------------\n    #note s1 - is run number\n    #note r1 - is forward, r2 - is reverse\n    # Forward and Reverse Filenames\n    if(dir.exists(DEMUXD_READS)){\n        files <- list.files(DEMUXD_READS)\n    }else{\n        files <- strsplit(DEMUXD_READS, \"\\\\\\\\s+\")[[1]]\n    }\n\n    fnFs.s1 <- files[grepl(\"_R1[_.]\", files)]\n    fnRs.s1 <- files[grepl(\"_R2[_.]\", files)]\n\n    # Sort to ensure filenames are in the same order\n    fnFs.s1 <- sort(fnFs.s1)\n    fnRs.s1 <- sort(fnRs.s1)\n\n    sample.names.1 <- sapply(strsplit(fnFs.s1, \"_R1[_.].*?(fastq|fq)\", fixed=FALSE), `[`, 1)\n    if(PAIRED_END){\n        ## match pairs\n        sample.names.2 <- sapply(strsplit(fnRs.s1,\"_R2[_.].*?(fastq|fq)\", fixed=FALSE), `[`, 1)\n        sample.names.shared <- intersect(sample.names.1, sample.names.2)\n        fnFs.s1 <- fnFs.s1[match(sample.names.shared, sample.names.1)]\n        fnRs.s1 <- fnRs.s1[match(sample.names.shared, sample.names.2)]\n    }\n\n    # Fully Specify the path for the fnFs and fnRs\n    if(dir.exists(DEMUXD_READS)){\n        fnFs.s1 <- file.path(DEMUXD_READS, fnFs.s1)\n        fnRs.s1 <- file.path(DEMUXD_READS, fnRs.s1)\n    }\n\n    # Examine qulaity profiles of the forward and reverse reads\n    p <- plotQualityProfile(fnFs.s1[[1]])\n    ggsave('Forward_quality_profile_s1.png', plot=p)\n    p_F <- plotQualityProfile(fnFs.s1, aggregate=TRUE)\n    ggsave('Forward_quality_profile_aggregate.png', plot=p_F)\n    if(PAIRED_END){\n        p <- plotQualityProfile(fnRs.s1[[1]])\n        ggsave('Reverse_quality_profile_s1.png', plot=p)\n        p_R <- plotQualityProfile(fnRs.s1, aggregate=TRUE)\n        ggsave('Reverse_quality_profile_aggregate.png', plot=p_R)\n    }\n    if(TRIM){ # Reads look really good quality don't filter here\n        getTrimRange <- function(x){\n            l <- lapply(x[[\"layers\"]], function(.ele) .ele[[\"data\"]])\n            m <- lapply(x[[\"layers\"]], function(.ele) .ele[[\"mapping\"]])\n            id <- lapply(m, function(.ele) any(grepl(\"Mean\", as.character(.ele[[\"y\"]]))))\n            id <- which(unlist(id))\n            if(length(id)>0){\n                d <- l[[id[1]]]\n                pos <- which(d[, \"Mean\"] < 30)\n                if(length(pos)>0){\n                    pos <- which(d[, \"Mean\"] >= 30)\n                    ## start pos\n                    pos_l <- pos[1]\n                    ## end pos\n                    pos_r <- pos[length(pos)]\n                    c(pos_l, pos_r-pos_l+1)\n                }else{\n                    c(0, 0)\n                }\n            }else{\n                c(0, 0)\n            }\n        }\n        if(is.null(trimLeft[1]) || is.null(truncLen[1])){\n            trim_range_L <- getTrimRange(p_F)\n            if(PAIRED_END){\n                trim_range_R <- getTrimRange(p_R)\n                trimLeft = c(trim_range_L[1], trim_range_R[1])\n                truncLen = c(trim_range_L[2], trim_range_R[2])\n            }else{\n                trimLeft = c(trim_range_L[1])\n                truncLen = c(trim_range_L[2])\n            }\n        }\n    }else{\n        if(PAIRED_END){\n            trimLeft = c(0, 0)\n            truncLen = c(0, 0)\n        }else{\n            trimLeft = c(0)\n            truncLen = c(0)\n        }\n    }\n\n    # Perform filtering and trimming\n\n    #update trimLeft based on plot output\n    # For the first sequencing run\n    dir.create(OUTFOLDER)\n    filtFs.s1 <- file.path(OUTFOLDER, paste0(sample.names.1,\"_F_filt.fastq.gz\"))\n    filtRs.s1 <- file.path(OUTFOLDER, paste0(sample.names.1,\"_R_filt.fastq.gz\"))\n    if(PAIRED_END){\n        out <- filterAndTrim(fwd=fnFs.s1, filt=filtFs.s1,\n                            rev=fnRs.s1, filt.rev=filtRs.s1,\n                            trimLeft=trimLeft, truncLen=truncLen,\n                            maxN=0, maxEE=2, truncQ=2,\n                            compress=TRUE, verbose=TRUE,\n                            rm.phix=TRUE, multithread=NCORE)\n    }else{\n        out <- filterAndTrim(fwd=fnFs.s1, filt=filtFs.s1,\n                            trimLeft=trimLeft, truncLen=truncLen,\n                            maxN=0, maxEE=2, truncQ=2,\n                            compress=TRUE, verbose=TRUE,\n                            rm.phix=TRUE, multithread=NCORE)\n    }\n    saveRDS(out, FILTER_STATS)\n    \"\"\"\n}",
        "nb_lignes_process": 198,
        "string_script": "    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    # Applying dada2 pipeline to bioreactor time-series\n    ## Following tutorial http://benjjneb.github.io/dada2_pipeline_MV/tutorial.html\n    ## and here http://benjjneb.github.io/dada2_pipeline_MV/bigdata.html\n\n    pkgs <- c(\"dada2\", \"ShortRead\", \"ggplot2\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    DEMUXD_READS = \"$reads\"\n    TRIM = FALSE\n    OUTFOLDER = \"$prefix\"\n    FILTER_STATS = \"filter_stats.rds\"\n    NCORE <- ifelse(.Platform[[\"OS.type\"]]!=\"windows\", as.numeric(\"$task.cpus\"), FALSE)\n    PAIRED_END <- \"$single_end\" != \"true\"\n\n    args <- strsplit(\"${args}\", \"\\\\\\\\s+\")[[1]]\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    grepl(\"trimming_reads\", \"$args\")\n    option_list <- list(\"trim\"=c(\"--trimming_reads\", \"-t\", \"logical\"),\n                        \"trimLeft\"=c(\"--trim_left\", \"-a\", \"integer\"),\n                        \"trimRight\"=c(\"--trim_right\", \"-b\", \"integer\"),\n                        \"truncLenLeft\"=c(\"--trunc_length_left\", \"-m\", \"integer\"),\n                        \"truncLenRight\"=c(\"--trunc_length_right\", \"-n\", \"integer\"))\n    opt <- parse_args(option_list, args)\n    if(!is.null(opt[[\"trim\"]])){\n        TRIM <- TRUE\n    }\n    if(!is.null(opt[[\"trimLeft\"]])){\n        trimLeft <- trimLeft1 <- opt[[\"trimLeft\"]]\n    }else{\n        trimLeft1 <- 0\n        trimLeft <- NULL\n    }\n    if(!is.null(opt[[\"trimRight\"]])){\n        trimLeft <- c(trimLeft1, opt[[\"trimRight\"]])\n    }\n    if(!is.null(opt[[\"truncLenLeft\"]])){\n        truncLen <- truncLen1 <- opt[[\"truncLenLeft\"]]\n    }else{\n        truncLen1 <- 0\n        truncLen <- NULL\n    }\n    if(!is.null(opt[[\"truncLenRight\"]])){\n        truncLen <- c(truncLen1, opt[[\"truncLenRight\"]])\n    }\n\n    # Filtering and Trimming --------------------------------------------------\n    #note s1 - is run number\n    #note r1 - is forward, r2 - is reverse\n    # Forward and Reverse Filenames\n    if(dir.exists(DEMUXD_READS)){\n        files <- list.files(DEMUXD_READS)\n    }else{\n        files <- strsplit(DEMUXD_READS, \"\\\\\\\\s+\")[[1]]\n    }\n\n    fnFs.s1 <- files[grepl(\"_R1[_.]\", files)]\n    fnRs.s1 <- files[grepl(\"_R2[_.]\", files)]\n\n    # Sort to ensure filenames are in the same order\n    fnFs.s1 <- sort(fnFs.s1)\n    fnRs.s1 <- sort(fnRs.s1)\n\n    sample.names.1 <- sapply(strsplit(fnFs.s1, \"_R1[_.].*?(fastq|fq)\", fixed=FALSE), `[`, 1)\n    if(PAIRED_END){\n        ## match pairs\n        sample.names.2 <- sapply(strsplit(fnRs.s1,\"_R2[_.].*?(fastq|fq)\", fixed=FALSE), `[`, 1)\n        sample.names.shared <- intersect(sample.names.1, sample.names.2)\n        fnFs.s1 <- fnFs.s1[match(sample.names.shared, sample.names.1)]\n        fnRs.s1 <- fnRs.s1[match(sample.names.shared, sample.names.2)]\n    }\n\n    # Fully Specify the path for the fnFs and fnRs\n    if(dir.exists(DEMUXD_READS)){\n        fnFs.s1 <- file.path(DEMUXD_READS, fnFs.s1)\n        fnRs.s1 <- file.path(DEMUXD_READS, fnRs.s1)\n    }\n\n    # Examine qulaity profiles of the forward and reverse reads\n    p <- plotQualityProfile(fnFs.s1[[1]])\n    ggsave('Forward_quality_profile_s1.png', plot=p)\n    p_F <- plotQualityProfile(fnFs.s1, aggregate=TRUE)\n    ggsave('Forward_quality_profile_aggregate.png', plot=p_F)\n    if(PAIRED_END){\n        p <- plotQualityProfile(fnRs.s1[[1]])\n        ggsave('Reverse_quality_profile_s1.png', plot=p)\n        p_R <- plotQualityProfile(fnRs.s1, aggregate=TRUE)\n        ggsave('Reverse_quality_profile_aggregate.png', plot=p_R)\n    }\n    if(TRIM){ # Reads look really good quality don't filter here\n        getTrimRange <- function(x){\n            l <- lapply(x[[\"layers\"]], function(.ele) .ele[[\"data\"]])\n            m <- lapply(x[[\"layers\"]], function(.ele) .ele[[\"mapping\"]])\n            id <- lapply(m, function(.ele) any(grepl(\"Mean\", as.character(.ele[[\"y\"]]))))\n            id <- which(unlist(id))\n            if(length(id)>0){\n                d <- l[[id[1]]]\n                pos <- which(d[, \"Mean\"] < 30)\n                if(length(pos)>0){\n                    pos <- which(d[, \"Mean\"] >= 30)\n                    ## start pos\n                    pos_l <- pos[1]\n                    ## end pos\n                    pos_r <- pos[length(pos)]\n                    c(pos_l, pos_r-pos_l+1)\n                }else{\n                    c(0, 0)\n                }\n            }else{\n                c(0, 0)\n            }\n        }\n        if(is.null(trimLeft[1]) || is.null(truncLen[1])){\n            trim_range_L <- getTrimRange(p_F)\n            if(PAIRED_END){\n                trim_range_R <- getTrimRange(p_R)\n                trimLeft = c(trim_range_L[1], trim_range_R[1])\n                truncLen = c(trim_range_L[2], trim_range_R[2])\n            }else{\n                trimLeft = c(trim_range_L[1])\n                truncLen = c(trim_range_L[2])\n            }\n        }\n    }else{\n        if(PAIRED_END){\n            trimLeft = c(0, 0)\n            truncLen = c(0, 0)\n        }else{\n            trimLeft = c(0)\n            truncLen = c(0)\n        }\n    }\n\n    # Perform filtering and trimming\n\n    #update trimLeft based on plot output\n    # For the first sequencing run\n    dir.create(OUTFOLDER)\n    filtFs.s1 <- file.path(OUTFOLDER, paste0(sample.names.1,\"_F_filt.fastq.gz\"))\n    filtRs.s1 <- file.path(OUTFOLDER, paste0(sample.names.1,\"_R_filt.fastq.gz\"))\n    if(PAIRED_END){\n        out <- filterAndTrim(fwd=fnFs.s1, filt=filtFs.s1,\n                            rev=fnRs.s1, filt.rev=filtRs.s1,\n                            trimLeft=trimLeft, truncLen=truncLen,\n                            maxN=0, maxEE=2, truncQ=2,\n                            compress=TRUE, verbose=TRUE,\n                            rm.phix=TRUE, multithread=NCORE)\n    }else{\n        out <- filterAndTrim(fwd=fnFs.s1, filt=filtFs.s1,\n                            trimLeft=trimLeft, truncLen=truncLen,\n                            maxN=0, maxEE=2, truncQ=2,\n                            compress=TRUE, verbose=TRUE,\n                            rm.phix=TRUE, multithread=NCORE)\n    }\n    saveRDS(out, FILTER_STATS)\n    \"\"\"",
        "nb_lignes_script": 180,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads",
            "single_end"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__16S_pipeline",
        "directive": [
            "tag \"$meta.id\"",
            "conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-dada2:1.22.0--r41h399db7b_0' : 'quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "QIIME_IMPORT": {
        "name_process": "QIIME_IMPORT",
        "string_process": "process QIIME_IMPORT {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"qiime2::qiime2=2021.11 qiime2::q2cli=2021.11 qiime2::q2-demux=2021.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'library://martinjf/default/qiime2:2021.8' :\n        'quay.io/qiime2/core:2021.11' }\"\n\n    input:\n    tuple val(meta), path(reads1, stageAs: \"sync/forward.fastq.gz\"), path(reads2, stageAs: \"sync/reverse.fastq.gz\"), path(index, stageAs: \"sync/barcodes.fastq.gz\")\n    val single_end\n\n    output:\n    tuple val(meta), path(\"${prefix}_emp-sequences.qza\")       , emit: reads\n    path \"versions.yml\", emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix     = task.ext.prefix ?: \"${meta.id}\"\n    def type   = single_end ? 'EMPSingleEndSequences' : 'EMPPairedEndSequences'\n    \"\"\"\n    if [ \"${single_end}\" == \"true\" ]; then\n        mv sync/forward.fastq.gz sync/sequences.fastq.gz\n    fi\n    qiime tools import \\\\\n        --type $type \\\\\n        --input-path sync \\\\\n        --output-path ${prefix}_emp-sequences.qza \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n    \\$(qiime info | sed -n '/: [0-9.]/p' | sed 's/.*/    &/g')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def args   = task.ext.args ?: ''\n    prefix     = task.ext.prefix ?: \"${meta.id}\"\n    def type   = single_end ? 'EMPSingleEndSequences' : 'EMPPairedEndSequences'\n    \"\"\"\n    if [ \"${single_end}\" == \"true\" ]; then\n        mv sync/forward.fastq.gz sync/sequences.fastq.gz\n    fi\n    qiime tools import \\\\\n        --type $type \\\\\n        --input-path sync \\\\\n        --output-path ${prefix}_emp-sequences.qza \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n    \\$(qiime info | sed -n '/: [0-9.]/p' | sed 's/.*/    &/g')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [
            "QIIME"
        ],
        "tools_url": [
            "https://bio.tools/qiime"
        ],
        "tools_dico": [
            {
                "name": "QIIME",
                "uri": "https://bio.tools/qiime",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3070",
                            "term": "Biology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3070",
                            "term": "Biological science"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2945",
                                    "term": "Analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Open-source bioinformatics pipeline for performing microbiome analysis from raw DNA sequencing data. The pipeline is designed to take users from raw sequencing data generated on the Illumina or other platforms through publication quality graphics and statistics. This includes demultiplexing and quality filtering, OTU picking, taxonomic assignment, and phylogenetic reconstruction, and diversity analyses and visualizations.",
                "homepage": "http://qiime.org/"
            }
        ],
        "inputs": [
            "meta",
            "reads1",
            "reads2",
            "index",
            "single_end"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__16S_pipeline",
        "directive": [
            "tag \"$meta.id\"",
            "conda (params.enable_conda ? \"qiime2::qiime2=2021.11 qiime2::q2cli=2021.11 qiime2::q2-demux=2021.11\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'library://martinjf/default/qiime2:2021.8' : 'quay.io/qiime2/core:2021.11' }\""
        ],
        "when": "",
        "stub": ""
    },
    "DADA2": {
        "name_process": "DADA2",
        "string_process": "process DADA2 {\n    tag \"$meta.id\"\n    tag \"process_high\"\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-dada2:1.22.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(reads), path(stats)\n    path train_set\n    path species_assignment\n\n    output:\n    tuple val(meta), path(\"*.rds\")       , emit: robj\n    path '*.{png,csv}'                   , emit: qc\n    path 'dada2.out.txt'                 , emit: log\n    path \"versions.yml\"                  , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    # Applying dada2 pipeline to bioreactor time-series\n    ## Following tutorial http://benjjneb.github.io/dada2_pipeline_MV/tutorial.html\n    ## and here http://benjjneb.github.io/dada2_pipeline_MV/tutorial.html\n\n    pkgs <- c(\"dada2\", \"ggplot2\", \"Biostrings\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    set.seed(4)\n\n    FILTPATH <- \"$reads\"\n    NCORE <- ifelse(.Platform[[\"OS.type\"]]!=\"windows\", as.numeric(\"$task.cpus\"), FALSE)\n    TRAIN_SET <- \"$train_set\"\n    SPECIES_ASSIGNMENT <- \"$species_assignment\"\n    STATS <- \"$stats\"\n    SEQL1 <- 0\n    SEQL2 <- 0\n    TRYRC <- FALSE\n    SAMPLENAMES <- \"samplenames.1.rds\"\n    SEQTAB_S1 <- \"seqtab.s1.rds\"\n    SEQTAB <- \"seqtab.nochim.rds\"\n    TAXTAB <- \"taxtab.rds\"\n\n    args <- strsplit(\"${args}\", \"\\\\\\\\s+\")[[1]]\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"seqlen1\"=c(\"--seq1\", \"-a\", \"integer\"),\n                        \"seqlen2\"=c(\"--seq2\", \"-b\", \"integer\"),\n                        \"tryRC\"=c(\"--tryRC\", \"-r\", \"logical\"))\n    opt <- parse_args(option_list, args)\n    if(!is.null(opt[[\"seqlen1\"]])){\n        SEQL1 <- opt[[\"seqlen1\"]]\n    }\n    if(!is.null(opt[[\"seqlen2\"]])){\n        SEQL2 <- opt[[\"seqlen2\"]]\n    }\n    if(!is.null(opt[[\"tryRC\"]])){\n        TRYRC <- opt[[\"tryRC\"]]\n    }\n\n    # stats\n    getN <- function(x) sum(getUniques(x))\n    track <- readRDS(STATS)\n\n    # Find filenames ----------------------------------------------------------\n\n    # Forward and reverse filenames\n    filts.s1 <- list.files(FILTPATH, full.names=TRUE)\n\n    # Sort to ensure fileneames are in the same order\n    filts.s1 <- sort(filts.s1)\n    sample.names.1 <- sapply(strsplit(basename(filts.s1),\"_\"), `[`, 1)\n    names(filts.s1) <- sample.names.1\n\n\n    # Separate forward and reverse samples\n    filtFs.s1 <- filts.s1[grepl(\"_F_filt\",filts.s1)]\n    filtRs.s1 <- filts.s1[grepl(\"_R_filt\",filts.s1)]\n\n    PAIRED_END <- length(filtRs.s1) == length(filtFs.s1)\n\n    sample.names.1 <- sapply(strsplit(basename(filtFs.s1), \"_\"), `[`, 1)\n    saveRDS(sample.names.1, SAMPLENAMES)\n\n    # Dereplication -----------------------------------------------------------\n\n    # Learn Error Rates\n    ## aim to learn from about 1M total reads - so just need subset of samples\n    ## source: http://benjjneb.github.io/dada2_pipeline_MV/bigdata.html\n    if(length(sample.names.1)>36){\n        filts.learn.s1 <- sample(sample.names.1, 36)\n    }else{\n        filts.learn.s1 <- sample.names.1\n    }\n\n    derepFs.s1.learn <- derepFastq(filtFs.s1[filts.learn.s1], verbose=TRUE)\n    if(PAIRED_END) derepRs.s1.learn <- derepFastq(filtRs.s1[filts.learn.s1], verbose=TRUE)\n\n    # Sample Inference --------------------------------------------------------\n\n    dadaFs.s1.learn <- dada(derepFs.s1.learn, err=NULL, selfConsist=TRUE, multithread=NCORE)\n    if(PAIRED_END) dadaRs.s1.learn <- dada(derepRs.s1.learn, err=NULL, selfConsist=TRUE, multithread=NCORE)\n    rm(derepFs.s1.learn)\n    if(PAIRED_END) rm(derepRs.s1.learn)\n\n    # # Visualize estimated error rates\n    p<- plotErrors(dadaFs.s1.learn[[1]], nominalQ=TRUE)\n    ggsave(\"dada_errors_F_s1.png\", plot=p)\n    if(PAIRED_END) {\n        p<- plotErrors(dadaRs.s1.learn[[1]], nominalQ=TRUE)\n        ggsave(\"dada_errors_R_s1.png\", plot=p)\n    }\n\n    # Just keep the error profiles\n    errFs.s1 <- dadaFs.s1.learn[[1]][[\"err_out\"]]\n    if(PAIRED_END) errRs.s1 <- dadaRs.s1.learn[[1]][[\"err_out\"]]\n    rm(dadaFs.s1.learn)\n    if(PAIRED_END) rm(dadaRs.s1.learn)\n\n    # Now sample inference for entire dataset\n    # Run 1\n    derepFs.s1 <- vector(\"list\", length(sample.names.1))\n    dadaFs.s1 <- vector(\"list\", length(sample.names.1))\n    names(dadaFs.s1) <- sample.names.1\n    names(derepFs.s1) <- sample.names.1\n    if(PAIRED_END){\n        derepRs.s1 <- vector(\"list\", length(sample.names.1))\n        dadaRs.s1 <- vector(\"list\", length(sample.names.1))\n        names(dadaRs.s1) <- sample.names.1\n        names(derepRs.s1) <- sample.names.1\n    }\n\n    for (sam in sample.names.1){\n        message(\"Processing:\", sam, \"\\n\")\n        derepFs.s1[[sam]] <- derepFastq(filtFs.s1[[sam]])\n        dadaFs.s1[[sam]] <- dada(derepFs.s1[[sam]], err=errFs.s1, multithread=NCORE)\n        if(PAIRED_END){\n            derepRs.s1[[sam]] <- derepFastq(filtRs.s1[[sam]])\n            dadaRs.s1[[sam]] <- dada(derepRs.s1[[sam]], err=errRs.s1, multithread=NCORE)\n        }\n    }\n\n    # Run 1: Merge Paired Reads\n    if(PAIRED_END){\n        mergers.s1 <- mergePairs(dadaFs.s1, derepFs.s1, dadaRs.s1, derepRs.s1, verbose=TRUE)\n        head(mergers.s1)\n        track <- cbind(track, sapply(dadaFs.s1, getN), sapply(dadaRs.s1, getN),\n                        sapply(mergers.s1, getN))\n        colnames(track) <- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\", \"merged\")\n        # Run 1: Clear up space\n        rm(derepFs.s1, derepRs.s1, dadaFs.s1, dadaRs.s1)\n    }else{\n        mergers.s1 <- dadaFs.s1\n        track <- cbind(track, sapply(dadaFs.s1, getN))\n        colnames(track) <- c(\"input\", \"filtered\", \"denoisedF\")\n        rm(derepFs.s1, dadaFs.s1)\n    }\n\n    # Construct Sequence Table ------------------------------------------------\n\n    #To use only the forward reads\n    #follow https://github.com/benjjneb/dada2/issues/134\n\n    seqtab.s1 <- makeSequenceTable(mergers.s1)\n    if(TRYRC){ # merging the reverseComplement sequences\n        cnames <- DNAStringSet(colnames(seqtab.s1))\n        cnames_rc <- reverseComplement(cnames)\n        comb <- combn(seq_along(cnames), 2, simplify=TRUE)\n        cnames <- as.character(cnames)\n        cnames_rc <- as.character(cnames_rc)\n        adist <- cnames[comb[1, ]] == cnames_rc[comb[2, ]]\n        if(any(adist)){\n            tobemerged <- comb[, adist, drop=FALSE]\n            seqtab.s1[, cnames[tobemerged[1, ]]] <-\n                seqtab.s1[, cnames[tobemerged[1, ]]] +\n                seqtab.s1[, cnames[tobemerged[2, ]]]\n            seqtab.s1 <- seqtab.s1[, -tobemerged[2, ], drop=FALSE]\n        }\n    }\n    saveRDS(seqtab.s1, SEQTAB_S1)\n    dim(seqtab.s1)\n    # Inspect the distributioh of sequence lengths\n    seqlenTab <- table(nchar(colnames(seqtab.s1)))\n    write.csv(seqlenTab, \"seqlenTab.csv\", row.names=FALSE)\n    if(SEQL1==0 && SEQL2==0){## auto detect cutoff range\n        maxV <- which.max(seqlenTab)\n        maxV <- as.numeric(names(seqlenTab)[maxV])\n        if(length(maxV)>1){\n            if(any(abs(diff(maxV))>2)){\n                stop(\"can not determine cutoff SEQ1 and SEQ2\")\n            }\n            maxV <- maxV[median(seq_along(maxV))]\n        }\n        SEQL1 <- maxV - 2\n        SEQL2 <- maxV + 2\n    }\n    #Trim sequences of interest\n    seqtab.s1 <- seqtab.s1[,nchar(colnames(seqtab.s1)) %in% seq(SEQL1,SEQL2)]\n    # Inspect the distributioh of sequence lengths\n    seqlenTab <- table(nchar(colnames(seqtab.s1)))\n    write.csv(seqlenTab, \"seqlenTab.filt.csv\", row.names=FALSE)\n\n    # Remove Chimeras ---------------------------------------------------------\n\n    seqtab.s1.nochim <- removeBimeraDenovo(seqtab.s1, method='consensus', multithread=NCORE, verbose=TRUE)\n    freq_chimeric <- c(number_row=nrow(seqtab.s1.nochim), number_col=ncol(seqtab.s1.nochim), frequency=sum(seqtab.s1.nochim)/sum(seqtab.s1))\n    write.csv(t(freq_chimeric), \"freq_chimeric.csv\", row.names=FALSE)\n    saveRDS(seqtab.s1.nochim, \"seqtab.s1.nochim.rds\")\n\n    track <- cbind(track, \"nonchim\"=rowSums(seqtab.s1.nochim))\n    rownames(track) <- sample.names.1\n    write.csv(track, \"processing_tracking.csv\")\n\n    # Merge Sequence Tables Together ------------------------------------------\n\n    seqtab.nochim <- seqtab.s1.nochim\n    saveRDS(seqtab.nochim, SEQTAB)\n\n\n    # Simplify naming ---------------------------------------------------------\n\n    seqtab <- seqtab.nochim\n\n    # Assign Taxonomy ---------------------------------------------------------\n    # Following: http://benjjneb.github.io/dada2_pipeline_MV/species.html\n\n    # Assign using Naive Bayes RDP\n    taxtab <- assignTaxonomy(colnames(seqtab), TRAIN_SET, tryRC=TRYRC, multithread=NCORE)\n\n    # improve with exact genus-species matches\n    # this step is pretty slow, should improve in later releases\n    # - note: Not allowing multiple species matches in default setting\n    taxtab <- addSpecies(taxtab, SPECIES_ASSIGNMENT, tryRC=TRYRC, verbose=TRUE)\n    saveRDS(taxtab, TAXTAB)\n\n    # How many sequences are classified at different levels? (percent)\n    classify_levels <- colSums(!is.na(taxtab))/nrow(taxtab)\n    write.csv(t(classify_levels), \"classify_levels.csv\", row.names=FALSE)\n\n    # copy the log file\n    file.copy(\".command.log\", \"dada2.out.txt\")\n    \"\"\"\n}",
        "nb_lignes_process": 265,
        "string_script": "    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    # Applying dada2 pipeline to bioreactor time-series\n    ## Following tutorial http://benjjneb.github.io/dada2_pipeline_MV/tutorial.html\n    ## and here http://benjjneb.github.io/dada2_pipeline_MV/tutorial.html\n\n    pkgs <- c(\"dada2\", \"ggplot2\", \"Biostrings\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    set.seed(4)\n\n    FILTPATH <- \"$reads\"\n    NCORE <- ifelse(.Platform[[\"OS.type\"]]!=\"windows\", as.numeric(\"$task.cpus\"), FALSE)\n    TRAIN_SET <- \"$train_set\"\n    SPECIES_ASSIGNMENT <- \"$species_assignment\"\n    STATS <- \"$stats\"\n    SEQL1 <- 0\n    SEQL2 <- 0\n    TRYRC <- FALSE\n    SAMPLENAMES <- \"samplenames.1.rds\"\n    SEQTAB_S1 <- \"seqtab.s1.rds\"\n    SEQTAB <- \"seqtab.nochim.rds\"\n    TAXTAB <- \"taxtab.rds\"\n\n    args <- strsplit(\"${args}\", \"\\\\\\\\s+\")[[1]]\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"seqlen1\"=c(\"--seq1\", \"-a\", \"integer\"),\n                        \"seqlen2\"=c(\"--seq2\", \"-b\", \"integer\"),\n                        \"tryRC\"=c(\"--tryRC\", \"-r\", \"logical\"))\n    opt <- parse_args(option_list, args)\n    if(!is.null(opt[[\"seqlen1\"]])){\n        SEQL1 <- opt[[\"seqlen1\"]]\n    }\n    if(!is.null(opt[[\"seqlen2\"]])){\n        SEQL2 <- opt[[\"seqlen2\"]]\n    }\n    if(!is.null(opt[[\"tryRC\"]])){\n        TRYRC <- opt[[\"tryRC\"]]\n    }\n\n    # stats\n    getN <- function(x) sum(getUniques(x))\n    track <- readRDS(STATS)\n\n    # Find filenames ----------------------------------------------------------\n\n    # Forward and reverse filenames\n    filts.s1 <- list.files(FILTPATH, full.names=TRUE)\n\n    # Sort to ensure fileneames are in the same order\n    filts.s1 <- sort(filts.s1)\n    sample.names.1 <- sapply(strsplit(basename(filts.s1),\"_\"), `[`, 1)\n    names(filts.s1) <- sample.names.1\n\n\n    # Separate forward and reverse samples\n    filtFs.s1 <- filts.s1[grepl(\"_F_filt\",filts.s1)]\n    filtRs.s1 <- filts.s1[grepl(\"_R_filt\",filts.s1)]\n\n    PAIRED_END <- length(filtRs.s1) == length(filtFs.s1)\n\n    sample.names.1 <- sapply(strsplit(basename(filtFs.s1), \"_\"), `[`, 1)\n    saveRDS(sample.names.1, SAMPLENAMES)\n\n    # Dereplication -----------------------------------------------------------\n\n    # Learn Error Rates\n    ## aim to learn from about 1M total reads - so just need subset of samples\n    ## source: http://benjjneb.github.io/dada2_pipeline_MV/bigdata.html\n    if(length(sample.names.1)>36){\n        filts.learn.s1 <- sample(sample.names.1, 36)\n    }else{\n        filts.learn.s1 <- sample.names.1\n    }\n\n    derepFs.s1.learn <- derepFastq(filtFs.s1[filts.learn.s1], verbose=TRUE)\n    if(PAIRED_END) derepRs.s1.learn <- derepFastq(filtRs.s1[filts.learn.s1], verbose=TRUE)\n\n    # Sample Inference --------------------------------------------------------\n\n    dadaFs.s1.learn <- dada(derepFs.s1.learn, err=NULL, selfConsist=TRUE, multithread=NCORE)\n    if(PAIRED_END) dadaRs.s1.learn <- dada(derepRs.s1.learn, err=NULL, selfConsist=TRUE, multithread=NCORE)\n    rm(derepFs.s1.learn)\n    if(PAIRED_END) rm(derepRs.s1.learn)\n\n    # # Visualize estimated error rates\n    p<- plotErrors(dadaFs.s1.learn[[1]], nominalQ=TRUE)\n    ggsave(\"dada_errors_F_s1.png\", plot=p)\n    if(PAIRED_END) {\n        p<- plotErrors(dadaRs.s1.learn[[1]], nominalQ=TRUE)\n        ggsave(\"dada_errors_R_s1.png\", plot=p)\n    }\n\n    # Just keep the error profiles\n    errFs.s1 <- dadaFs.s1.learn[[1]][[\"err_out\"]]\n    if(PAIRED_END) errRs.s1 <- dadaRs.s1.learn[[1]][[\"err_out\"]]\n    rm(dadaFs.s1.learn)\n    if(PAIRED_END) rm(dadaRs.s1.learn)\n\n    # Now sample inference for entire dataset\n    # Run 1\n    derepFs.s1 <- vector(\"list\", length(sample.names.1))\n    dadaFs.s1 <- vector(\"list\", length(sample.names.1))\n    names(dadaFs.s1) <- sample.names.1\n    names(derepFs.s1) <- sample.names.1\n    if(PAIRED_END){\n        derepRs.s1 <- vector(\"list\", length(sample.names.1))\n        dadaRs.s1 <- vector(\"list\", length(sample.names.1))\n        names(dadaRs.s1) <- sample.names.1\n        names(derepRs.s1) <- sample.names.1\n    }\n\n    for (sam in sample.names.1){\n        message(\"Processing:\", sam, \"\\n\")\n        derepFs.s1[[sam]] <- derepFastq(filtFs.s1[[sam]])\n        dadaFs.s1[[sam]] <- dada(derepFs.s1[[sam]], err=errFs.s1, multithread=NCORE)\n        if(PAIRED_END){\n            derepRs.s1[[sam]] <- derepFastq(filtRs.s1[[sam]])\n            dadaRs.s1[[sam]] <- dada(derepRs.s1[[sam]], err=errRs.s1, multithread=NCORE)\n        }\n    }\n\n    # Run 1: Merge Paired Reads\n    if(PAIRED_END){\n        mergers.s1 <- mergePairs(dadaFs.s1, derepFs.s1, dadaRs.s1, derepRs.s1, verbose=TRUE)\n        head(mergers.s1)\n        track <- cbind(track, sapply(dadaFs.s1, getN), sapply(dadaRs.s1, getN),\n                        sapply(mergers.s1, getN))\n        colnames(track) <- c(\"input\", \"filtered\", \"denoisedF\", \"denoisedR\", \"merged\")\n        # Run 1: Clear up space\n        rm(derepFs.s1, derepRs.s1, dadaFs.s1, dadaRs.s1)\n    }else{\n        mergers.s1 <- dadaFs.s1\n        track <- cbind(track, sapply(dadaFs.s1, getN))\n        colnames(track) <- c(\"input\", \"filtered\", \"denoisedF\")\n        rm(derepFs.s1, dadaFs.s1)\n    }\n\n    # Construct Sequence Table ------------------------------------------------\n\n    #To use only the forward reads\n    #follow https://github.com/benjjneb/dada2/issues/134\n\n    seqtab.s1 <- makeSequenceTable(mergers.s1)\n    if(TRYRC){ # merging the reverseComplement sequences\n        cnames <- DNAStringSet(colnames(seqtab.s1))\n        cnames_rc <- reverseComplement(cnames)\n        comb <- combn(seq_along(cnames), 2, simplify=TRUE)\n        cnames <- as.character(cnames)\n        cnames_rc <- as.character(cnames_rc)\n        adist <- cnames[comb[1, ]] == cnames_rc[comb[2, ]]\n        if(any(adist)){\n            tobemerged <- comb[, adist, drop=FALSE]\n            seqtab.s1[, cnames[tobemerged[1, ]]] <-\n                seqtab.s1[, cnames[tobemerged[1, ]]] +\n                seqtab.s1[, cnames[tobemerged[2, ]]]\n            seqtab.s1 <- seqtab.s1[, -tobemerged[2, ], drop=FALSE]\n        }\n    }\n    saveRDS(seqtab.s1, SEQTAB_S1)\n    dim(seqtab.s1)\n    # Inspect the distributioh of sequence lengths\n    seqlenTab <- table(nchar(colnames(seqtab.s1)))\n    write.csv(seqlenTab, \"seqlenTab.csv\", row.names=FALSE)\n    if(SEQL1==0 && SEQL2==0){## auto detect cutoff range\n        maxV <- which.max(seqlenTab)\n        maxV <- as.numeric(names(seqlenTab)[maxV])\n        if(length(maxV)>1){\n            if(any(abs(diff(maxV))>2)){\n                stop(\"can not determine cutoff SEQ1 and SEQ2\")\n            }\n            maxV <- maxV[median(seq_along(maxV))]\n        }\n        SEQL1 <- maxV - 2\n        SEQL2 <- maxV + 2\n    }\n    #Trim sequences of interest\n    seqtab.s1 <- seqtab.s1[,nchar(colnames(seqtab.s1)) %in% seq(SEQL1,SEQL2)]\n    # Inspect the distributioh of sequence lengths\n    seqlenTab <- table(nchar(colnames(seqtab.s1)))\n    write.csv(seqlenTab, \"seqlenTab.filt.csv\", row.names=FALSE)\n\n    # Remove Chimeras ---------------------------------------------------------\n\n    seqtab.s1.nochim <- removeBimeraDenovo(seqtab.s1, method='consensus', multithread=NCORE, verbose=TRUE)\n    freq_chimeric <- c(number_row=nrow(seqtab.s1.nochim), number_col=ncol(seqtab.s1.nochim), frequency=sum(seqtab.s1.nochim)/sum(seqtab.s1))\n    write.csv(t(freq_chimeric), \"freq_chimeric.csv\", row.names=FALSE)\n    saveRDS(seqtab.s1.nochim, \"seqtab.s1.nochim.rds\")\n\n    track <- cbind(track, \"nonchim\"=rowSums(seqtab.s1.nochim))\n    rownames(track) <- sample.names.1\n    write.csv(track, \"processing_tracking.csv\")\n\n    # Merge Sequence Tables Together ------------------------------------------\n\n    seqtab.nochim <- seqtab.s1.nochim\n    saveRDS(seqtab.nochim, SEQTAB)\n\n\n    # Simplify naming ---------------------------------------------------------\n\n    seqtab <- seqtab.nochim\n\n    # Assign Taxonomy ---------------------------------------------------------\n    # Following: http://benjjneb.github.io/dada2_pipeline_MV/species.html\n\n    # Assign using Naive Bayes RDP\n    taxtab <- assignTaxonomy(colnames(seqtab), TRAIN_SET, tryRC=TRYRC, multithread=NCORE)\n\n    # improve with exact genus-species matches\n    # this step is pretty slow, should improve in later releases\n    # - note: Not allowing multiple species matches in default setting\n    taxtab <- addSpecies(taxtab, SPECIES_ASSIGNMENT, tryRC=TRYRC, verbose=TRUE)\n    saveRDS(taxtab, TAXTAB)\n\n    # How many sequences are classified at different levels? (percent)\n    classify_levels <- colSums(!is.na(taxtab))/nrow(taxtab)\n    write.csv(t(classify_levels), \"classify_levels.csv\", row.names=FALSE)\n\n    # copy the log file\n    file.copy(\".command.log\", \"dada2.out.txt\")\n    \"\"\"",
        "nb_lignes_script": 244,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads",
            "stats",
            "train_set",
            "species_assignment"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__16S_pipeline",
        "directive": [
            "tag \"$meta.id\"",
            "tag \"process_high\"",
            "conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-dada2:1.22.0--r41h399db7b_0' : 'quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "PHYLOSEQ": {
        "name_process": "PHYLOSEQ",
        "string_process": "process PHYLOSEQ {\n    tag \"$meta.id\"\n    tag \"process_high\"\n    tag \"error_ignore\"\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-phyloseq=1.38.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-phyloseq:1.38.0--r41hdfd78af_0' :\n        'quay.io/biocontainers/bioconductor-phyloseq:1.38.0--r41hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(rds)\n    path metadata\n\n    output:\n    tuple val(meta), path(\"$prefix\")         , emit: phyloseq\n    tuple val(meta), path(\"$krona_folder/*\") , emit: krona, optional: true\n    path 'phyloseq.out.txt'                  , emit: log\n    path \"versions.yml\"                      , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    krona_folder = \"4Krona\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    # Applying dada2 pipeline to bioreactor time-series\n    ## Following tutorial http://benjjneb.github.io/dada2_pipeline_MV/tutorial.html\n    ## and here http://benjjneb.github.io/dada2_pipeline_MV/tutorial.html\n\n    pkgs <- c(\"phyloseq\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    set.seed(4)\n\n    MAPPING <- \"$metadata\"\n    NCORE <- as.numeric(\"$task.cpus\")\n    SEQTAB_S1 <- \"seqtab.s1.rds\"\n    SEQTAB <- \"seqtab.nochim.rds\"\n    TAXTAB <- \"taxtab.rds\"\n    SAMPLENAMES <- \"samplenames.1.rds\"\n    OUTFOLDER <- \"$prefix\"\n    SAMPLEID_COL <- \"SampleID\"\n    KRONA_FOLDER <- \"$krona_folder\"\n\n    # Make phyloseq object ----------------------------------------------------\n    # import data from dada2 output\n    sample.names.1 <- readRDS(SAMPLENAMES)\n    seqtab.s1 <- readRDS(SEQTAB_S1)\n    seqtab <- readRDS(SEQTAB)\n    taxtab <- readRDS(TAXTAB)\n\n    # Import mapping\n    map1 <- read.csv(MAPPING, stringsAsFactors = FALSE)\n    map1 <- map1[map1[, SAMPLEID_COL] %in% sample.names.1,]\n    map <- as.data.frame(map1) # without this line get sam_data slot empty error from phyloseq\n    rownames(map) <- map[, SAMPLEID_COL]\n\n    # Make refseq object and extract sequences from tables\n    refseq <- colnames(seqtab)\n    names(refseq) <- paste0('seq_', seq_along(refseq))\n    colnames(seqtab) <- names(refseq[match(colnames(seqtab), refseq)])\n    rownames(taxtab) <- names(refseq[match(rownames(taxtab), refseq)])\n\n    # Write the taxtable, seqtable, and refseq to ascii ------------------------\n    dir.create(OUTFOLDER, recursive=TRUE)\n    write.table(seqtab, file=file.path(OUTFOLDER, 'seqtab.nochim.tsv'), quote=FALSE, sep='\\\\t')\n    write.table(taxtab, file=file.path(OUTFOLDER, 'taxtab.nochim.tsv'), quote=FALSE, sep='\\\\t')\n    write.table(refseq, file=file.path(OUTFOLDER, 'refseqs.nochim.tsv'), quote=FALSE, sep='\\\\t', col.names = FALSE)\n\n    # Combine into phyloseq object\n    ps <- phyloseq(otu_table(seqtab, taxa_are_rows = FALSE), sample_data(map), tax_table(taxtab))\n    saveRDS(ps, file.path(OUTFOLDER, 'phyloseq.rds'))\n\n    # copy the log file\n    file.copy(\".command.log\", \"phyloseq.out.txt\")\n\n    # output file for krona\n    df <- psmelt(ps)\n    cn <- c(\"Abundance\", SAMPLEID_COL, rank_names(ps))\n    cn <- intersect(cn, colnames(df))\n    if(SAMPLEID_COL %in% cn){\n        for(i in rank_names(ps)){\n            if(!i %in% cn){\n                df[, i] <- NA\n            }\n        }\n        df <- df[, c(\"Abundance\", SAMPLEID_COL, rank_names(ps)), drop=FALSE]\n        df[, SAMPLEID_COL] <- as.factor(gsub(\"[^0-9a-zA-Z]\", \"_\", df[, SAMPLEID_COL])) ## trim the sample names\n        dir.create(KRONA_FOLDER, recursive=TRUE)\n        for(lvl in levels(df[, SAMPLEID_COL])){\n            write.table(\n                df[which(df[, SAMPLEID_COL] == lvl & df[, \"Abundance\"] != 0), -2, drop=FALSE],\n                file = file.path(KRONA_FOLDER, paste0(lvl, \".txt\")),\n                sep = \"\\\\t\", row.names = FALSE, col.names = FALSE,\n                na = \"\", quote = FALSE)\n        }\n    }\n    \"\"\"\n}",
        "nb_lignes_process": 106,
        "string_script": "    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    krona_folder = \"4Krona\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    # Applying dada2 pipeline to bioreactor time-series\n    ## Following tutorial http://benjjneb.github.io/dada2_pipeline_MV/tutorial.html\n    ## and here http://benjjneb.github.io/dada2_pipeline_MV/tutorial.html\n\n    pkgs <- c(\"phyloseq\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    set.seed(4)\n\n    MAPPING <- \"$metadata\"\n    NCORE <- as.numeric(\"$task.cpus\")\n    SEQTAB_S1 <- \"seqtab.s1.rds\"\n    SEQTAB <- \"seqtab.nochim.rds\"\n    TAXTAB <- \"taxtab.rds\"\n    SAMPLENAMES <- \"samplenames.1.rds\"\n    OUTFOLDER <- \"$prefix\"\n    SAMPLEID_COL <- \"SampleID\"\n    KRONA_FOLDER <- \"$krona_folder\"\n\n    # Make phyloseq object ----------------------------------------------------\n    # import data from dada2 output\n    sample.names.1 <- readRDS(SAMPLENAMES)\n    seqtab.s1 <- readRDS(SEQTAB_S1)\n    seqtab <- readRDS(SEQTAB)\n    taxtab <- readRDS(TAXTAB)\n\n    # Import mapping\n    map1 <- read.csv(MAPPING, stringsAsFactors = FALSE)\n    map1 <- map1[map1[, SAMPLEID_COL] %in% sample.names.1,]\n    map <- as.data.frame(map1) # without this line get sam_data slot empty error from phyloseq\n    rownames(map) <- map[, SAMPLEID_COL]\n\n    # Make refseq object and extract sequences from tables\n    refseq <- colnames(seqtab)\n    names(refseq) <- paste0('seq_', seq_along(refseq))\n    colnames(seqtab) <- names(refseq[match(colnames(seqtab), refseq)])\n    rownames(taxtab) <- names(refseq[match(rownames(taxtab), refseq)])\n\n    # Write the taxtable, seqtable, and refseq to ascii ------------------------\n    dir.create(OUTFOLDER, recursive=TRUE)\n    write.table(seqtab, file=file.path(OUTFOLDER, 'seqtab.nochim.tsv'), quote=FALSE, sep='\\\\t')\n    write.table(taxtab, file=file.path(OUTFOLDER, 'taxtab.nochim.tsv'), quote=FALSE, sep='\\\\t')\n    write.table(refseq, file=file.path(OUTFOLDER, 'refseqs.nochim.tsv'), quote=FALSE, sep='\\\\t', col.names = FALSE)\n\n    # Combine into phyloseq object\n    ps <- phyloseq(otu_table(seqtab, taxa_are_rows = FALSE), sample_data(map), tax_table(taxtab))\n    saveRDS(ps, file.path(OUTFOLDER, 'phyloseq.rds'))\n\n    # copy the log file\n    file.copy(\".command.log\", \"phyloseq.out.txt\")\n\n    # output file for krona\n    df <- psmelt(ps)\n    cn <- c(\"Abundance\", SAMPLEID_COL, rank_names(ps))\n    cn <- intersect(cn, colnames(df))\n    if(SAMPLEID_COL %in% cn){\n        for(i in rank_names(ps)){\n            if(!i %in% cn){\n                df[, i] <- NA\n            }\n        }\n        df <- df[, c(\"Abundance\", SAMPLEID_COL, rank_names(ps)), drop=FALSE]\n        df[, SAMPLEID_COL] <- as.factor(gsub(\"[^0-9a-zA-Z]\", \"_\", df[, SAMPLEID_COL])) ## trim the sample names\n        dir.create(KRONA_FOLDER, recursive=TRUE)\n        for(lvl in levels(df[, SAMPLEID_COL])){\n            write.table(\n                df[which(df[, SAMPLEID_COL] == lvl & df[, \"Abundance\"] != 0), -2, drop=FALSE],\n                file = file.path(KRONA_FOLDER, paste0(lvl, \".txt\")),\n                sep = \"\\\\t\", row.names = FALSE, col.names = FALSE,\n                na = \"\", quote = FALSE)\n        }\n    }\n    \"\"\"",
        "nb_lignes_script": 85,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "rds",
            "metadata"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__16S_pipeline",
        "directive": [
            "tag \"$meta.id\"",
            "tag \"process_high\"",
            "tag \"error_ignore\"",
            "conda (params.enable_conda ? \"bioconda::bioconductor-phyloseq=1.38.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-phyloseq:1.38.0--r41hdfd78af_0' : 'quay.io/biocontainers/bioconductor-phyloseq:1.38.0--r41hdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "BCL2FASTQ": {
        "name_process": "BCL2FASTQ",
        "string_process": "process BCL2FASTQ {\n    tag \"$raw\"\n    tag 'process_high'\n\n    input:\n    path raw\n    path samplesheet\n    val bcl2fastq\n\n    output:\n    path 'fastq/Undetermined_*.fastq.gz' , emit: reads\n    path \"versions.yml\"                  , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    \"\"\"\n    mkdir -p fastq\n    ${bcl2fastq} \\\\\n            --runfolder-dir $raw \\\\\n            --output-dir fastq/ \\\\\n            --sample-sheet $samplesheet \\\\\n            --processing-threads $task.cpus \\\\\n            $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcl2fastq: \\$(echo \\$(bcl2fastq --version 2>&1) | sed 's/^.*bcl2fastq v//; s/Copyright.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "    def args   = task.ext.args ?: ''\n    \"\"\"\n    mkdir -p fastq\n    ${bcl2fastq} \\\\\n            --runfolder-dir $raw \\\\\n            --output-dir fastq/ \\\\\n            --sample-sheet $samplesheet \\\\\n            --processing-threads $task.cpus \\\\\n            $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bcl2fastq: \\$(echo \\$(bcl2fastq --version 2>&1) | sed 's/^.*bcl2fastq v//; s/Copyright.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "raw",
            "samplesheet",
            "bcl2fastq"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__16S_pipeline",
        "directive": [
            "tag \"$raw\"",
            "tag 'process_high'"
        ],
        "when": "",
        "stub": ""
    },
    "QIIME_DEMUX": {
        "name_process": "QIIME_DEMUX",
        "string_process": "process QIIME_DEMUX {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"qiime2::qiime2=2021.11 qiime2::q2cli=2021.11 qiime2::q2-demux=2021.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'library://martinjf/default/qiime2:2021.8' :\n        'quay.io/qiime2/core:2021.11' }\"\n\n    input:\n    tuple val(meta), path(qza)\n    path barcodes\n    val single_end\n\n    output:\n    tuple val(meta), path(\"${prefix}_demux-full.qza\")   , emit: reads\n    tuple val(meta), path(\"${prefix}_demux-details.qza\"), emit: details\n    tuple val(meta), path(\"${prefix}_demux-summary.qzv\"), emit: summary\n    path \"versions.yml\", emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    mkdir -p tmp\n    export TMPDIR=./tmp\n    export TMP=./tmp\n    export TEMP=./tmp\n\n    if [ \"${single_end}\" == \"true\" ]; then\n        qiime demux emp-single \\\\\n            --m-barcodes-file $barcodes \\\\\n            --i-seqs $qza \\\\\n            --o-per-sample-sequences ${prefix}_demux-full.qza \\\\\n            --o-error-correction-details ${prefix}_demux-details.qza \\\\\n            $args\n    else\n        qiime demux emp-paired \\\\\n            --m-barcodes-file $barcodes \\\\\n            --i-seqs $qza \\\\\n            --o-per-sample-sequences ${prefix}_demux-full.qza \\\\\n            --o-error-correction-details ${prefix}_demux-details.qza \\\\\n            $args\n    fi\n\n    qiime demux summarize \\\\\n        --i-data ${prefix}_demux-full.qza \\\\\n        --o-visualization ${prefix}_demux-summary.qzv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n    \\$(qiime info | sed -n '/: [0-9.]/p' | sed 's/.*/    &/g')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 52,
        "string_script": "    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    mkdir -p tmp\n    export TMPDIR=./tmp\n    export TMP=./tmp\n    export TEMP=./tmp\n\n    if [ \"${single_end}\" == \"true\" ]; then\n        qiime demux emp-single \\\\\n            --m-barcodes-file $barcodes \\\\\n            --i-seqs $qza \\\\\n            --o-per-sample-sequences ${prefix}_demux-full.qza \\\\\n            --o-error-correction-details ${prefix}_demux-details.qza \\\\\n            $args\n    else\n        qiime demux emp-paired \\\\\n            --m-barcodes-file $barcodes \\\\\n            --i-seqs $qza \\\\\n            --o-per-sample-sequences ${prefix}_demux-full.qza \\\\\n            --o-error-correction-details ${prefix}_demux-details.qza \\\\\n            $args\n    fi\n\n    qiime demux summarize \\\\\n        --i-data ${prefix}_demux-full.qza \\\\\n        --o-visualization ${prefix}_demux-summary.qzv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n    \\$(qiime info | sed -n '/: [0-9.]/p' | sed 's/.*/    &/g')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 32,
        "language_script": "bash",
        "tools": [
            "QIIME"
        ],
        "tools_url": [
            "https://bio.tools/qiime"
        ],
        "tools_dico": [
            {
                "name": "QIIME",
                "uri": "https://bio.tools/qiime",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3070",
                            "term": "Biology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3070",
                            "term": "Biological science"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2945",
                                    "term": "Analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Open-source bioinformatics pipeline for performing microbiome analysis from raw DNA sequencing data. The pipeline is designed to take users from raw sequencing data generated on the Illumina or other platforms through publication quality graphics and statistics. This includes demultiplexing and quality filtering, OTU picking, taxonomic assignment, and phylogenetic reconstruction, and diversity analyses and visualizations.",
                "homepage": "http://qiime.org/"
            }
        ],
        "inputs": [
            "meta",
            "qza",
            "barcodes",
            "single_end"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__16S_pipeline",
        "directive": [
            "tag \"$meta.id\"",
            "conda (params.enable_conda ? \"qiime2::qiime2=2021.11 qiime2::q2cli=2021.11 qiime2::q2-demux=2021.11\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'library://martinjf/default/qiime2:2021.8' : 'quay.io/qiime2/core:2021.11' }\""
        ],
        "when": "",
        "stub": ""
    },
    "SYNC_BARCODES": {
        "name_process": "SYNC_BARCODES",
        "string_process": "process SYNC_BARCODES {\n    tag \"$meta.id\"\n    tag \"process_high\"\n\n    conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/python:3.8.3' :\n        'quay.io/biocontainers/python:3.8.3' }\"\n\n    input:\n    tuple val(meta), path(barcodes), path(reads1, stageAs: 'R1.fastq.gz'), path(reads2)\n\n    output:\n    tuple val(meta), path(\"${prefix}_R1.paired.fastq.gz\"), path(reads2), path(\"${prefix}_I1.synced.fastq.gz\"), emit: reads\n    path \"versions.yml\", emit: versions\n\n    script:\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    sync_paired_end_reads.py \\\\\n        $barcodes $barcodes \\\\\n        $reads1 \\\\\n        ${prefix}_I1.synced.fastq.gz \\\\\n        ${prefix}_R1.paired.fastq.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        python: \\$(python --version | sed 's/Python //g')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    sync_paired_end_reads.py \\\\\n        $barcodes $barcodes \\\\\n        $reads1 \\\\\n        ${prefix}_I1.synced.fastq.gz \\\\\n        ${prefix}_R1.paired.fastq.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        python: \\$(python --version | sed 's/Python //g')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "barcodes",
            "reads1",
            "reads2"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__16S_pipeline",
        "directive": [
            "tag \"$meta.id\"",
            "tag \"process_high\"",
            "conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/python:3.8.3' : 'quay.io/biocontainers/python:3.8.3' }\""
        ],
        "when": "",
        "stub": ""
    },
    "CUSTOM_DUMPSOFTWAREVERSIONS": {
        "name_process": "CUSTOM_DUMPSOFTWAREVERSIONS",
        "string_process": "process CUSTOM_DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n\n                                                                                                  \n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\"    , emit: yml\n    path \"software_versions_mqc.yml\", emit: mqc_yml\n    path \"versions.yml\"             , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    template 'dumpsoftwareversions.py'\n}",
        "nb_lignes_process": 19,
        "string_script": "    def args = task.ext.args ?: ''\n    template 'dumpsoftwareversions.py'",
        "nb_lignes_script": 1,
        "language_script": "bash",
        "tools": [
            "docxtemplate"
        ],
        "tools_url": [
            "https://bio.tools/docxtemplate"
        ],
        "tools_dico": [
            {
                "name": "docxtemplate",
                "uri": "https://bio.tools/docxtemplate",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3314",
                            "term": "Chemistry"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0176",
                            "term": "Molecular dynamics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3489",
                            "term": "Database management"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3489",
                            "term": "Database administration"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0249",
                                    "term": "Protein geometry calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0322",
                                    "term": "Molecular model refinement"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Deposition"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data deposition"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> VERY_LOW CONFIDENCE! | > CORRECT NAME OF TOOL COULD ALSO BE 'Phenix', 'restraints', 'Amber', 'refinement' | Improved chemistry restraints for crystallographic refinement by integrating the Amber force field into Phenix | Word templates and tools for Windows | The IUCr Word templates utilize the content management features and document styles of Word to format your manuscript and to store essential details for submission of your manuscript",
                "homepage": "http://journals.iucr.org/services/docxtemplate/"
            }
        ],
        "inputs": [
            "versions"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__16S_pipeline",
        "directive": [
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' : 'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "REMOVE_PRIMERS": {
        "name_process": "REMOVE_PRIMERS",
        "string_process": "process REMOVE_PRIMERS {\n    tag \"$meta.id\"\n    tag 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::trimmomatic=0.39\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/trimmomatic:0.39--hdfd78af_2' :\n        'quay.io/biocontainers/trimmomatic:0.39--hdfd78af_2' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    val single_end\n\n    output:\n    tuple val(meta), path(\"${prefix}_R1.paired.fastq.gz\"), path(\"${prefix}_R2.paired.fastq.gz\"), emit: paired\n    tuple val(meta), path(\"${prefix}_R1.unpaired.fastq.gz\"), path(\"${prefix}_R2.unpaired.fastq.gz\"), optional:true, emit: unpaired\n    path '*.trim_out.log', emit: log\n    path \"versions.yml\", emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    if [ \"${single_end}\" == \"true\" ]; then\n        touch ${prefix}_R2.paired.fastq.gz\n        trimmomatic SE \\\\\n                    -threads $task.cpus \\\\\n                    ${reads[0]} \\\\\n                    ${prefix}_R1.paired.fastq.gz \\\\\n                    $args \\\\\n                    > ${prefix}.trim_out.log 2>&1\n    else\n        trimmomatic PE \\\\\n                    -threads $task.cpus \\\\\n                    ${reads[0]} ${reads[1]} \\\\\n                    ${prefix}_R1.paired.fastq.gz \\\\\n                    ${prefix}_R1.unpaired.fastq.gz \\\\\n                    ${prefix}_R2.paired.fastq.gz \\\\\n                    ${prefix}_R2.unpaired.fastq.gz \\\\\n                    $args \\\\\n                    > ${prefix}.trim_out.log 2>&1\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        trimmomatic: \\$(trimmomatic -version)\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 47,
        "string_script": "    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    if [ \"${single_end}\" == \"true\" ]; then\n        touch ${prefix}_R2.paired.fastq.gz\n        trimmomatic SE \\\\\n                    -threads $task.cpus \\\\\n                    ${reads[0]} \\\\\n                    ${prefix}_R1.paired.fastq.gz \\\\\n                    $args \\\\\n                    > ${prefix}.trim_out.log 2>&1\n    else\n        trimmomatic PE \\\\\n                    -threads $task.cpus \\\\\n                    ${reads[0]} ${reads[1]} \\\\\n                    ${prefix}_R1.paired.fastq.gz \\\\\n                    ${prefix}_R1.unpaired.fastq.gz \\\\\n                    ${prefix}_R2.paired.fastq.gz \\\\\n                    ${prefix}_R2.unpaired.fastq.gz \\\\\n                    $args \\\\\n                    > ${prefix}.trim_out.log 2>&1\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        trimmomatic: \\$(trimmomatic -version)\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 27,
        "language_script": "bash",
        "tools": [
            "Trimmomatic"
        ],
        "tools_url": [
            "https://bio.tools/trimmomatic"
        ],
        "tools_dico": [
            {
                "name": "Trimmomatic",
                "uri": "https://bio.tools/trimmomatic",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Sequence trimming"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Trimming"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0006",
                                "term": "Data"
                            },
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0006",
                                "term": "Data"
                            }
                        ]
                    }
                ],
                "description": "A flexible read trimming tool for Illumina NGS data",
                "homepage": "http://www.usadellab.org/cms/index.php?page=trimmomatic"
            }
        ],
        "inputs": [
            "meta",
            "reads",
            "single_end"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__16S_pipeline",
        "directive": [
            "tag \"$meta.id\"",
            "tag 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::trimmomatic=0.39\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/trimmomatic:0.39--hdfd78af_2' : 'quay.io/biocontainers/trimmomatic:0.39--hdfd78af_2' }\""
        ],
        "when": "",
        "stub": ""
    },
    "QIIME_EXPORT": {
        "name_process": "QIIME_EXPORT",
        "string_process": "process QIIME_EXPORT {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"qiime2::qiime2=2021.11 qiime2::q2cli=2021.11 qiime2::q2-demux=2021.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'library://martinjf/default/qiime2:2021.8' :\n        'quay.io/qiime2/core:2021.11' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"$prefix\")       , emit: reads\n    path \"versions.yml\", emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    qiime tools export \\\\\n        --input-path $reads \\\\\n        --output-path ${prefix} \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n    \\$(qiime info | sed -n '/: [0-9.]/p' | sed 's/.*/    &/g')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "    def args   = task.ext.args ?: ''\n    prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    qiime tools export \\\\\n        --input-path $reads \\\\\n        --output-path ${prefix} \\\\\n        $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n    \\$(qiime info | sed -n '/: [0-9.]/p' | sed 's/.*/    &/g')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "QIIME"
        ],
        "tools_url": [
            "https://bio.tools/qiime"
        ],
        "tools_dico": [
            {
                "name": "QIIME",
                "uri": "https://bio.tools/qiime",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3070",
                            "term": "Biology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3070",
                            "term": "Biological science"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2945",
                                    "term": "Analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Open-source bioinformatics pipeline for performing microbiome analysis from raw DNA sequencing data. The pipeline is designed to take users from raw sequencing data generated on the Illumina or other platforms through publication quality graphics and statistics. This includes demultiplexing and quality filtering, OTU picking, taxonomic assignment, and phylogenetic reconstruction, and diversity analyses and visualizations.",
                "homepage": "http://qiime.org/"
            }
        ],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__16S_pipeline",
        "directive": [
            "tag \"$meta.id\"",
            "conda (params.enable_conda ? \"qiime2::qiime2=2021.11 qiime2::q2cli=2021.11 qiime2::q2-demux=2021.11\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'library://martinjf/default/qiime2:2021.8' : 'quay.io/qiime2/core:2021.11' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MULTIQC": {
        "name_process": "MULTIQC",
        "string_process": "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "MultiQC"
        ],
        "tools_url": [
            "https://bio.tools/multiqc"
        ],
        "tools_dico": [
            {
                "name": "MultiQC",
                "uri": "https://bio.tools/multiqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0091",
                            "term": "Bioinformatics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2428",
                                    "term": "Validation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2048",
                                "term": "Report"
                            }
                        ]
                    }
                ],
                "description": "MultiQC aggregates results from multiple bioinformatics analyses across many samples into a single report. It searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.",
                "homepage": "http://multiqc.info/"
            }
        ],
        "inputs": [
            "multiqc_files"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__16S_pipeline",
        "directive": [
            "label 'process_medium'",
            "conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' : 'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    }
}