{
    "metaphlan2_fastq": {
        "name_process": "metaphlan2_fastq",
        "string_process": " process metaphlan2_fastq {\n    tag \"MetaPhlAn2 composition for paired end fastq reads\"\n    container \"${container__metaphlan2}\"\n    label = 'mem_medium'\n    errorStrategy 'finish'\n\n                                                                                        \n    publishDir path: \"${params.output_folder}MetaPhlAn2/\", mode: \"copy\"\n\n    input:\n    tuple specimen, file(R1), file(R2)\n    \n    output:\n    tuple val(specimen), file(\"${specimen}.metaphlan2.tsv\")\n\n\"\"\"\nset -e\n\nbowtie2 --threads ${task.cpus} -x /metaphlan/metaphlan_databases/mpa_v20_m200 -1 ${R1} -2 ${R2} | \\\nmetaphlan2.py --nproc ${task.cpus} --input_type sam -t rel_ab_w_read_stats -o ${specimen}.metaphlan2.tsv\n\n\"\"\"\n    }",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\nset -e\n\nbowtie2 --threads ${task.cpus} -x /metaphlan/metaphlan_databases/mpa_v20_m200 -1 ${R1} -2 ${R2} | \\\nmetaphlan2.py --nproc ${task.cpus} --input_type sam -t rel_ab_w_read_stats -o ${specimen}.metaphlan2.tsv\n\n\"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "Rbowtie2"
        ],
        "tools_url": [
            "https://bio.tools/rbowtie2"
        ],
        "tools_dico": [
            {
                "name": "Rbowtie2",
                "uri": "https://bio.tools/rbowtie2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence merging"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence splicing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package provides an R wrapper of the popular bowtie2 sequencing reads aligner and AdapterRemoval, a convenient tool for rapid adapter trimming, identification, and read merging.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/Rbowtie2.html"
            }
        ],
        "inputs": [
            "R1",
            "R2",
            "specimen"
        ],
        "nb_inputs": 3,
        "outputs": [
            "specimen"
        ],
        "nb_outputs": 1,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"MetaPhlAn2 composition for paired end fastq reads\"",
            "container \"${container__metaphlan2}\"",
            "label = 'mem_medium'",
            "errorStrategy 'finish'",
            "publishDir path: \"${params.output_folder}MetaPhlAn2/\", mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    },
    "metaphlan2_fasta": {
        "name_process": "metaphlan2_fasta",
        "string_process": " process metaphlan2_fasta {\n    tag \"MetaPhlAn2 composition for paired end fastq reads\"\n    container \"${container__metaphlan2}\"\n    label = 'mem_veryhigh'\n    errorStrategy 'finish'\n                   \n\n                                                                                        \n    publishDir path: \"${params.output_folder}MetaPhlAn2/\", mode: \"copy\"\n\n    input:\n    tuple specimen, file(R1)\n    \n    output:\n    tuple val(specimen), file(\"${specimen}.metaphlan2.tsv\")\n\n\"\"\"\nset -e\n\nbowtie2 --threads ${task.cpus} -x /metaphlan/metaphlan_databases/mpa_v20_m200 -f -U ${R1} | \\\nmetaphlan2.py --nproc ${task.cpus} --input_type sam -t rel_ab_w_read_stats -o ${specimen}.metaphlan2.tsv\n\n\"\"\"\n    }",
        "nb_lignes_process": 22,
        "string_script": "\"\"\"\nset -e\n\nbowtie2 --threads ${task.cpus} -x /metaphlan/metaphlan_databases/mpa_v20_m200 -f -U ${R1} | \\\nmetaphlan2.py --nproc ${task.cpus} --input_type sam -t rel_ab_w_read_stats -o ${specimen}.metaphlan2.tsv\n\n\"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "Rbowtie2"
        ],
        "tools_url": [
            "https://bio.tools/rbowtie2"
        ],
        "tools_dico": [
            {
                "name": "Rbowtie2",
                "uri": "https://bio.tools/rbowtie2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence merging"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence splicing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package provides an R wrapper of the popular bowtie2 sequencing reads aligner and AdapterRemoval, a convenient tool for rapid adapter trimming, identification, and read merging.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/Rbowtie2.html"
            }
        ],
        "inputs": [
            "R1",
            "specimen"
        ],
        "nb_inputs": 2,
        "outputs": [
            "specimen"
        ],
        "nb_outputs": 1,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"MetaPhlAn2 composition for paired end fastq reads\"",
            "container \"${container__metaphlan2}\"",
            "label = 'mem_veryhigh'",
            "errorStrategy 'finish'",
            "publishDir path: \"${params.output_folder}MetaPhlAn2/\", mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    },
    "join_metaphlan2": {
        "name_process": "join_metaphlan2",
        "string_process": " process join_metaphlan2 {\n    container \"${container__pandas}\"\n    label = 'mem_medium'\n    errorStrategy 'finish'\n\n    input:\n    path metaphlan_tsv_list\n    \n    output:\n    path \"metaphlan.results.csv.gz\"\n\n\"\"\"\n#!/usr/bin/env python3\n\nimport pandas as pd\n\n# Save all of the results to a dict\ndf = dict()\n\n# Iterate over each input file\nfor fp in \"${metaphlan_tsv_list}\".split(\" \"):\n\n    # Make sure that the file has the expected suffix\n    assert fp.endswith(\".metaphlan2.tsv\"), fp\n\n    # Get the specimen name from the file name\n    specimen_name = fp.replace(\".metaphlan2.tsv\", \"\")\n\n    df[specimen_name] = pd.read_csv(\n        fp,\n        skiprows=1,\n        sep=\"\\\\t\"\n    ).set_index(\n        \"#clade_name\"\n    )[\n        \"relative_abundance\"\n    ]\n\n# Format all results as a DataFrame\ndf = pd.DataFrame(df)\n\n# Fill in all missing values with 0\ndf = df.fillna(0)\n\n# Save to a single CSV\ndf.reset_index(\n).rename(\n    columns = dict([(\"index\", \"taxon\")])\n).to_csv(\"metaphlan.results.csv.gz\")\n    \n\n\"\"\"\n    }",
        "nb_lignes_process": 51,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport pandas as pd\n\n# Save all of the results to a dict\ndf = dict()\n\n# Iterate over each input file\nfor fp in \"${metaphlan_tsv_list}\".split(\" \"):\n\n    # Make sure that the file has the expected suffix\n    assert fp.endswith(\".metaphlan2.tsv\"), fp\n\n    # Get the specimen name from the file name\n    specimen_name = fp.replace(\".metaphlan2.tsv\", \"\")\n\n    df[specimen_name] = pd.read_csv(\n        fp,\n        skiprows=1,\n        sep=\"\\\\t\"\n    ).set_index(\n        \"#clade_name\"\n    )[\n        \"relative_abundance\"\n    ]\n\n# Format all results as a DataFrame\ndf = pd.DataFrame(df)\n\n# Fill in all missing values with 0\ndf = df.fillna(0)\n\n# Save to a single CSV\ndf.reset_index(\n).rename(\n    columns = dict([(\"index\", \"taxon\")])\n).to_csv(\"metaphlan.results.csv.gz\")\n    \n\n\"\"\"",
        "nb_lignes_script": 40,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "metaphlan_tsv_list"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"${container__pandas}\"",
            "label = 'mem_medium'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "Barcodecop": {
        "name_process": "Barcodecop",
        "string_process": "\nprocess Barcodecop {\n    tag \"Validate barcode demultiplexing for WGS reads\"\n    container \"${container__barcodecop}\"\n    label 'mem_medium'\n    errorStrategy 'finish'\n\n    input:\n        tuple specimen, file(R1), file(R2), file(I1), file(I2)\n\n    output:\n        tuple specimen, file(\"${R1}.bcc.fq.gz\"), file(\"${R2}.bcc.fq.gz\"), emit: bcc_to_cutadapt_ch\n        tuple specimen, file(\"${R1}.bcc.fq.gz\"), file(\"${R2}.bcc.fq.gz\"), emit: bcc_empty_ch\n\"\"\"\nset -e\n\necho \"Running barcodecop on ${R1}\"\nbarcodecop \\\n${I1} ${I2} \\\n--match-filter \\\n-f ${R1} \\\n-o OUTPUT_R1.fq.gz\necho \"Done\"\n\necho \"Renaming output file to ${R1}.bcc.fq.gz\"\nmv OUTPUT_R1.fq.gz ${R1}.bcc.fq.gz\necho \"Done\"\n\necho \"Running barcodecop on ${R2}\"\nbarcodecop \\\n${I1} ${I2} \\\n--match-filter \\\n-f ${R2} \\\n-o OUTPUT_R2.fq.gz\necho \"Done\"\n\necho \"Renaming output file to ${R2}.bcc.fq.gz\"\nmv OUTPUT_R2.fq.gz ${R2}.bcc.fq.gz\necho \"Done\"\n\"\"\"\n}",
        "nb_lignes_process": 39,
        "string_script": "\"\"\"\nset -e\n\necho \"Running barcodecop on ${R1}\"\nbarcodecop \\\n${I1} ${I2} \\\n--match-filter \\\n-f ${R1} \\\n-o OUTPUT_R1.fq.gz\necho \"Done\"\n\necho \"Renaming output file to ${R1}.bcc.fq.gz\"\nmv OUTPUT_R1.fq.gz ${R1}.bcc.fq.gz\necho \"Done\"\n\necho \"Running barcodecop on ${R2}\"\nbarcodecop \\\n${I1} ${I2} \\\n--match-filter \\\n-f ${R2} \\\n-o OUTPUT_R2.fq.gz\necho \"Done\"\n\necho \"Renaming output file to ${R2}.bcc.fq.gz\"\nmv OUTPUT_R2.fq.gz ${R2}.bcc.fq.gz\necho \"Done\"\n\"\"\"",
        "nb_lignes_script": 26,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "R1",
            "R2",
            "I1",
            "I2",
            "specimen"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Validate barcode demultiplexing for WGS reads\"",
            "container \"${container__barcodecop}\"",
            "label 'mem_medium'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "TrimGalore": {
        "name_process": "TrimGalore",
        "string_process": "\nprocess TrimGalore {\n    container \"${container__trimgalore}\"\n    label 'io_limited'\n    errorStrategy 'ignore'\n\n    input:\n    tuple val(specimen), file(R1), file(R2)\n\n    output:\n    tuple val(specimen), file(\"${R1.getSimpleName()}__R1.tg.fastq.gz\"), file(\"${R2.getSimpleName()}__R2.tg.fastq.gz\")\n\n    \"\"\"\n    set -e\n\n    ln -s ${R1} R1.fastq.gz\n    ln -s ${R2} R2.fastq.gz\n\n    trim_galore \\\n    --gzip \\\n    --cores ${task.cpus} \\\n    --paired \\\n    R1.fastq.gz R2.fastq.gz\n\n    mv R1_val_1.fq.gz \"${R1.getSimpleName()}__R1.tg.fastq.gz\"\n    mv R2_val_2.fq.gz \"${R2.getSimpleName()}__R2.tg.fastq.gz\"\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n    set -e\n\n    ln -s ${R1} R1.fastq.gz\n    ln -s ${R2} R2.fastq.gz\n\n    trim_galore \\\n    --gzip \\\n    --cores ${task.cpus} \\\n    --paired \\\n    R1.fastq.gz R2.fastq.gz\n\n    mv R1_val_1.fq.gz \"${R1.getSimpleName()}__R1.tg.fastq.gz\"\n    mv R2_val_2.fq.gz \"${R2.getSimpleName()}__R2.tg.fastq.gz\"\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "R1",
            "R2"
        ],
        "nb_inputs": 3,
        "outputs": [
            "specimen"
        ],
        "nb_outputs": 1,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"${container__trimgalore}\"",
            "label 'io_limited'",
            "errorStrategy 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "Download_hg_index": {
        "name_process": "Download_hg_index",
        "string_process": "\nprocess Download_hg_index {\n    tag \"Download human reference genome\"\n    container \"${container__bwa}\"\n    errorStrategy \"finish\"\n    label 'io_net'\n\n    output:\n    file 'hg_index.tar.gz'\n    \n\"\"\"\nset -e\n\nwget --quiet ${params.hg_index_url} -O hg_index.tar.gz\n\"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "\"\"\"\nset -e\n\nwget --quiet ${params.hg_index_url} -O hg_index.tar.gz\n\"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Download human reference genome\"",
            "container \"${container__bwa}\"",
            "errorStrategy \"finish\"",
            "label 'io_net'"
        ],
        "when": "",
        "stub": ""
    },
    "BWA_remove_human": {
        "name_process": "BWA_remove_human",
        "string_process": "\nprocess BWA_remove_human {\n    tag \"Remove human reads\"\n    container \"${container__bwa}\"\n    errorStrategy 'finish'\n    label 'mem_veryhigh'\n\n\n    input:\n        file hg_index_tgz\n        tuple sample_name, file(R1), file(R2)\n\n    output:\n        tuple sample_name, file(\"${R1.getSimpleName()}.noadapt.nohuman.fq.gz\"), file(\"${R2.getSimpleName()}.noadapt.nohuman.fq.gz\")\n\n\n\"\"\"\nset - e\n\n\nbwa_index_fn=\\$(tar -ztvf ${hg_index_tgz} | head -1 | sed \\'s/.* //\\')\nbwa_index_prefix=\\${bwa_index_fn%.*}\necho BWA index prefix is \\${bwa_index_prefix} | tee -a ${R1.getSimpleName()}.nohuman.log\necho Extracting BWA index | tee -a ${R1.getSimpleName()}.nohuman.log\nmkdir -p hg_index/ \ntar -I pigz -xf ${hg_index_tgz} -C hg_index/ | tee -a ${R1.getSimpleName()}.nohuman.log\necho Files in index directory: | tee -a ${R1.getSimpleName()}.nohuman.log\nls -l -h hg_index | tee -a ${R1.getSimpleName()}.nohuman.log\necho Running BWA | tee -a ${R1.getSimpleName()}.nohuman.log\nbwa mem -t ${task.cpus} \\\n-T ${params.min_hg_align_score} \\\n-o alignment.sam \\\nhg_index/\\$bwa_index_prefix \\\n${R1} ${R2} \\\n| tee -a ${R1.getSimpleName()}.nohuman.log\necho Checking if alignment is empty  | tee -a ${R1.getSimpleName()}.nohuman.log\n[[ -s alignment.sam ]]\necho Extracting Unaligned Pairs | tee -a ${R1.getSimpleName()}.nohuman.log\nsamtools fastq alignment.sam \\\n--threads ${task.cpus} -f 12 \\\n-1 ${R1.getSimpleName()}.noadapt.nohuman.fq.gz -2 ${R2.getSimpleName()}.noadapt.nohuman.fq.gz \\\n| tee -a ${R1.getSimpleName()}.nohuman.log\necho Done | tee -a ${R1.getSimpleName()}.nohuman.log\n\nrm -rf hg_index/*\necho Cleanup Done\n\"\"\"\n}",
        "nb_lignes_process": 46,
        "string_script": "\"\"\"\nset - e\n\n\nbwa_index_fn=\\$(tar -ztvf ${hg_index_tgz} | head -1 | sed \\'s/.* //\\')\nbwa_index_prefix=\\${bwa_index_fn%.*}\necho BWA index prefix is \\${bwa_index_prefix} | tee -a ${R1.getSimpleName()}.nohuman.log\necho Extracting BWA index | tee -a ${R1.getSimpleName()}.nohuman.log\nmkdir -p hg_index/ \ntar -I pigz -xf ${hg_index_tgz} -C hg_index/ | tee -a ${R1.getSimpleName()}.nohuman.log\necho Files in index directory: | tee -a ${R1.getSimpleName()}.nohuman.log\nls -l -h hg_index | tee -a ${R1.getSimpleName()}.nohuman.log\necho Running BWA | tee -a ${R1.getSimpleName()}.nohuman.log\nbwa mem -t ${task.cpus} \\\n-T ${params.min_hg_align_score} \\\n-o alignment.sam \\\nhg_index/\\$bwa_index_prefix \\\n${R1} ${R2} \\\n| tee -a ${R1.getSimpleName()}.nohuman.log\necho Checking if alignment is empty  | tee -a ${R1.getSimpleName()}.nohuman.log\n[[ -s alignment.sam ]]\necho Extracting Unaligned Pairs | tee -a ${R1.getSimpleName()}.nohuman.log\nsamtools fastq alignment.sam \\\n--threads ${task.cpus} -f 12 \\\n-1 ${R1.getSimpleName()}.noadapt.nohuman.fq.gz -2 ${R2.getSimpleName()}.noadapt.nohuman.fq.gz \\\n| tee -a ${R1.getSimpleName()}.nohuman.log\necho Done | tee -a ${R1.getSimpleName()}.nohuman.log\n\nrm -rf hg_index/*\necho Cleanup Done\n\"\"\"",
        "nb_lignes_script": 30,
        "language_script": "bash",
        "tools": [
            "BWA",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/bwa",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "hg_index_tgz",
            "R1",
            "R2",
            "sample_name"
        ],
        "nb_inputs": 4,
        "outputs": [
            "sample_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Remove human reads\"",
            "container \"${container__bwa}\"",
            "errorStrategy 'finish'",
            "label 'mem_veryhigh'"
        ],
        "when": "",
        "stub": ""
    },
    "JoinFASTQ": {
        "name_process": "JoinFASTQ",
        "string_process": "\nprocess JoinFASTQ {\n    tag \"Join FASTQ files per-specimen\"\n    container \"${container__fastatools}\"\n    label = 'mem_medium'\n    errorStrategy 'finish'\n\n                                                                                        \n    publishDir path: \"${params.output}qc/\", enabled: params.savereads, mode: \"copy\"\n\n    input:\n    tuple val(sample), file(\"R1.*.fastq.gz\"), file(\"R2.*.fastq.gz\")\n    \n    output:\n    tuple val(sample), file(\"${sample}.R1.fastq.gz\"), file(\"${sample}.R2.fastq.gz\")\n\n\"\"\"\nset -e\n\nls -lah *\n\ncombine_fastq_pairs.py \\\n-1 R1*fastq.gz \\\n-2 R2*fastq.gz \\\n--normalize-ids \\\n-o1 \"${sample}.R1.fastq.gz\" \\\n-o2 \"${sample}.R2.fastq.gz\"\n\n(( \\$(gunzip -c \"${sample}.R1.fastq.gz\" | head | wc -l) > 1 ))\n(( \\$(gunzip -c \"${sample}.R2.fastq.gz\" | head | wc -l) > 1 ))\n\n\"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "\"\"\"\nset -e\n\nls -lah *\n\ncombine_fastq_pairs.py \\\n-1 R1*fastq.gz \\\n-2 R2*fastq.gz \\\n--normalize-ids \\\n-o1 \"${sample}.R1.fastq.gz\" \\\n-o2 \"${sample}.R2.fastq.gz\"\n\n(( \\$(gunzip -c \"${sample}.R1.fastq.gz\" | head | wc -l) > 1 ))\n(( \\$(gunzip -c \"${sample}.R2.fastq.gz\" | head | wc -l) > 1 ))\n\n\"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample"
        ],
        "nb_inputs": 1,
        "outputs": [
            "sample"
        ],
        "nb_outputs": 1,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Join FASTQ files per-specimen\"",
            "container \"${container__fastatools}\"",
            "label = 'mem_medium'",
            "errorStrategy 'finish'",
            "publishDir path: \"${params.output}qc/\", enabled: params.savereads, mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    },
    "OutputManifest": {
        "name_process": "OutputManifest",
        "string_process": "\nprocess OutputManifest {\n    container \"${container__ubuntu}\"\n\n    publishDir path: \"${params.output}qc/\", enabled: params.savereads, mode: \"copy\"\n\n    input:\n        path R1s\n        path R2s\n        val manifestStr\n    \n    output:\n        path 'manifest.qc.csv'\n        path R1s\n        path R2s\n\n    \"\"\"\n        echo \"${manifestStr}\" > manifest.qc.csv\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n        echo \"${manifestStr}\" > manifest.qc.csv\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "R1s",
            "R2s",
            "manifestStr"
        ],
        "nb_inputs": 3,
        "outputs": [
            "R1s",
            "R2s"
        ],
        "nb_outputs": 2,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"${container__ubuntu}\"",
            "publishDir path: \"${params.output}qc/\", enabled: params.savereads, mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    },
    "get_taxonomic_db": {
        "name_process": "get_taxonomic_db",
        "string_process": "\nprocess get_taxonomic_db {\n    container \"quay.io/fhcrc-microbiome/famli@sha256:25c34c73964f06653234dd7804c3cf5d9cf520bc063723e856dae8b16ba74b0c\"\n    label 'io_net'\n    output:\n    path \"ref.faa.gz\" into ref_faa_gz\n    path \"prot.accession2taxid.gz\" into prot_accession2taxid_gz\n    path \"nodes.dmp\" into nodes_dmp\n\n\"\"\"\nset -e\n\necho \"Fetching ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip\"\nwget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip\n\necho \"Unzipping taxdmp.zip\"\nunzip taxdmp.zip\n\necho \"Making sure that nodes.dmp was found in taxdmp.zip\"\nls -lahtr\n[[ -s nodes.dmp ]]\n\necho \"Fetching ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.gz\"\nwget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.gz\n\necho \"Fetching the NCBI RefSeq non-redundant proteins\"\nwget -r -l1 -np \"ftp://ftp.ncbi.nlm.nih.gov/refseq/release/bacteria/\" -P ./ -A \"bacteria.nonredundant_protein.*.protein.faa.gz\"\n\necho \"Combining downloaded files\"\nfor fp in ./ftp.ncbi.nlm.nih.gov/refseq/release/bacteria/bacteria.nonredundant_protein.*.protein.faa.gz; do\n    \n    cat \\$fp >> ref.faa.gz\n    \n    rm \\$fp\n\ndone\n\n\n\"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "\"\"\"\nset -e\n\necho \"Fetching ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip\"\nwget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip\n\necho \"Unzipping taxdmp.zip\"\nunzip taxdmp.zip\n\necho \"Making sure that nodes.dmp was found in taxdmp.zip\"\nls -lahtr\n[[ -s nodes.dmp ]]\n\necho \"Fetching ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.gz\"\nwget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.gz\n\necho \"Fetching the NCBI RefSeq non-redundant proteins\"\nwget -r -l1 -np \"ftp://ftp.ncbi.nlm.nih.gov/refseq/release/bacteria/\" -P ./ -A \"bacteria.nonredundant_protein.*.protein.faa.gz\"\n\necho \"Combining downloaded files\"\nfor fp in ./ftp.ncbi.nlm.nih.gov/refseq/release/bacteria/bacteria.nonredundant_protein.*.protein.faa.gz; do\n    \n    cat \\$fp >> ref.faa.gz\n    \n    rm \\$fp\n\ndone\n\n\n\"\"\"",
        "nb_lignes_script": 29,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [
            "ref_faa_gz",
            "prot_accession2taxid_gz",
            "nodes_dmp"
        ],
        "nb_outputs": 3,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/famli@sha256:25c34c73964f06653234dd7804c3cf5d9cf520bc063723e856dae8b16ba74b0c\"",
            "label 'io_net'"
        ],
        "when": "",
        "stub": ""
    },
    "index_taxonomic_db": {
        "name_process": "index_taxonomic_db",
        "string_process": "\nprocess index_taxonomic_db {\n    container \"quay.io/fhcrc-microbiome/famli@sha256:25c34c73964f06653234dd7804c3cf5d9cf520bc063723e856dae8b16ba74b0c\"\n    label 'mem_veryhigh'\n    publishDir params.output_folder\n\n    input:\n    file ref_faa_gz\n    file prot_accession2taxid_gz\n    file nodes_dmp\n\n    output:\n    path \"${params.output_prefix}.refseq.tax.dmnd\"\n\"\"\"\necho \"Indexing with DIAMOND\"\ndiamond \\\n    makedb \\\n    --in ${ref_faa_gz} \\\n    --db ${params.output_prefix}.refseq.tax.dmnd \\\n    --threads ${task.cpus} \\\n    --taxonmap ${prot_accession2taxid_gz} \\\n    --taxonnodes ${nodes_dmp}\n\"\"\"    \n\n}",
        "nb_lignes_process": 23,
        "string_script": "\"\"\"\necho \"Indexing with DIAMOND\"\ndiamond \\\n    makedb \\\n    --in ${ref_faa_gz} \\\n    --db ${params.output_prefix}.refseq.tax.dmnd \\\n    --threads ${task.cpus} \\\n    --taxonmap ${prot_accession2taxid_gz} \\\n    --taxonnodes ${nodes_dmp}\n\"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [
            "Diamond"
        ],
        "tools_url": [
            "https://bio.tools/diamond"
        ],
        "tools_dico": [
            {
                "name": "Diamond",
                "uri": "https://bio.tools/diamond",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0258",
                                    "term": "Sequence alignment analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Sequence aligner for protein and translated DNA searches and functions as a drop-in replacement for the NCBI BLAST software tools. It is suitable for protein-protein search as well as DNA-protein search on short reads and longer sequences including contigs and assemblies, providing a speedup of BLAST ranging up to x20,000.",
                "homepage": "https://github.com/bbuchfink/diamond"
            }
        ],
        "inputs": [
            "ref_faa_gz",
            "prot_accession2taxid_gz",
            "nodes_dmp"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/famli@sha256:25c34c73964f06653234dd7804c3cf5d9cf520bc063723e856dae8b16ba74b0c\"",
            "label 'mem_veryhigh'",
            "publishDir params.output_folder"
        ],
        "when": "",
        "stub": ""
    },
    "get_eggnog_db": {
        "name_process": "get_eggnog_db",
        "string_process": "\nprocess get_eggnog_db {\n    container \"quay.io/biocontainers/eggnog-mapper:2.0.1--py_1\"\n    label 'io_net'\n    publishDir params.output_folder\n\n    output:\n    path \"${params.output_prefix}.eggnog.db\"\n    path \"${params.output_prefix}.eggnog_proteins.dmnd\"\n\n\"\"\"\nset -e\n\ndownload_eggnog_data.py -y --data_dir ./\n\nmv eggnog.db ${params.output_prefix}.eggnog.db\nmv eggnog_proteins.dmnd ${params.output_prefix}.eggnog_proteins.dmnd\n\n\n\"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "\"\"\"\nset -e\n\ndownload_eggnog_data.py -y --data_dir ./\n\nmv eggnog.db ${params.output_prefix}.eggnog.db\nmv eggnog_proteins.dmnd ${params.output_prefix}.eggnog_proteins.dmnd\n\n\n\"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"quay.io/biocontainers/eggnog-mapper:2.0.1--py_1\"",
            "label 'io_net'",
            "publishDir params.output_folder"
        ],
        "when": "",
        "stub": ""
    },
    "diamondDB": {
        "name_process": "diamondDB",
        "string_process": "\nprocess diamondDB {\n    tag \"Make a DIAMOND database\"\n    container \"quay.io/fhcrc-microbiome/famli:v1.5\"\n    label 'mem_veryhigh'\n    errorStrategy 'finish'\n    publishDir \"${params.output_folder}/ref/\", mode: \"copy\"\n    \n    input:\n    file fasta\n\n    output:\n    file \"genes.dmnd\"\n\n    \"\"\"\n    set -e\n    diamond \\\n      makedb \\\n      --in ${fasta} \\\n      --db genes.dmnd \\\n      --threads ${task.cpus}\n    \"\"\"\n\n}",
        "nb_lignes_process": 22,
        "string_script": "\"\"\"\n    set -e\n    diamond \\\n      makedb \\\n      --in ${fasta} \\\n      --db genes.dmnd \\\n      --threads ${task.cpus}\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [
            "Diamond"
        ],
        "tools_url": [
            "https://bio.tools/diamond"
        ],
        "tools_dico": [
            {
                "name": "Diamond",
                "uri": "https://bio.tools/diamond",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0258",
                                    "term": "Sequence alignment analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Sequence aligner for protein and translated DNA searches and functions as a drop-in replacement for the NCBI BLAST software tools. It is suitable for protein-protein search as well as DNA-protein search on short reads and longer sequences including contigs and assemblies, providing a speedup of BLAST ranging up to x20,000.",
                "homepage": "https://github.com/bbuchfink/diamond"
            }
        ],
        "inputs": [
            "fasta"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Make a DIAMOND database\"",
            "container \"quay.io/fhcrc-microbiome/famli:v1.5\"",
            "label 'mem_veryhigh'",
            "errorStrategy 'finish'",
            "publishDir \"${params.output_folder}/ref/\", mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    },
    "diamond": {
        "name_process": "diamond",
        "string_process": "\nprocess diamond {\n    tag \"Align to the gene catalog\"\n    container \"quay.io/fhcrc-microbiome/famli:v1.5\"\n    label 'mem_veryhigh'\n    errorStrategy 'finish'\n    \n    input:\n    tuple val(sample_name), file(R1), file(R2)\n    file refdb\n    \n    output:\n    tuple sample_name, file(\"${sample_name}.aln.gz\")\n\n    \"\"\"\n    set -e\n\n    for fp in ${R1} ${R2}; do\n        cat \\$fp | \\\n        gunzip -c | \\\n        awk \"{if(NR % 4 == 1){print \\\\\"@\\$fp\\\\\" NR }else{if(NR % 4 == 3){print \\\\\"+\\\\\"}else{print}}}\" | \\\n        gzip -c \\\n        > query.fastq.gz\n\n        diamond \\\n        blastx \\\n        --query query.fastq.gz \\\n        --out \\$fp.aln.gz \\\n        --threads ${task.cpus} \\\n        --db ${refdb} \\\n        --outfmt 6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qlen slen \\\n        --min-score ${params.dmnd_min_score} \\\n        --query-cover ${params.dmnd_min_coverage} \\\n        --id ${params.dmnd_min_identity} \\\n        --top ${params.dmnd_top_pct} \\\n        --block-size ${task.memory.toMega() / (1024 * 6 )} \\\n        --query-gencode ${params.gencode} \\\n        --compress 1 \\\n        --unal 0\n    done\n\n    cat *aln.gz > ${sample_name}.aln.gz\n    \"\"\"\n\n}",
        "nb_lignes_process": 43,
        "string_script": "\"\"\"\n    set -e\n\n    for fp in ${R1} ${R2}; do\n        cat \\$fp | \\\n        gunzip -c | \\\n        awk \"{if(NR % 4 == 1){print \\\\\"@\\$fp\\\\\" NR }else{if(NR % 4 == 3){print \\\\\"+\\\\\"}else{print}}}\" | \\\n        gzip -c \\\n        > query.fastq.gz\n\n        diamond \\\n        blastx \\\n        --query query.fastq.gz \\\n        --out \\$fp.aln.gz \\\n        --threads ${task.cpus} \\\n        --db ${refdb} \\\n        --outfmt 6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qlen slen \\\n        --min-score ${params.dmnd_min_score} \\\n        --query-cover ${params.dmnd_min_coverage} \\\n        --id ${params.dmnd_min_identity} \\\n        --top ${params.dmnd_top_pct} \\\n        --block-size ${task.memory.toMega() / (1024 * 6 )} \\\n        --query-gencode ${params.gencode} \\\n        --compress 1 \\\n        --unal 0\n    done\n\n    cat *aln.gz > ${sample_name}.aln.gz\n    \"\"\"",
        "nb_lignes_script": 28,
        "language_script": "bash",
        "tools": [
            "Diamond"
        ],
        "tools_url": [
            "https://bio.tools/diamond"
        ],
        "tools_dico": [
            {
                "name": "Diamond",
                "uri": "https://bio.tools/diamond",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0258",
                                    "term": "Sequence alignment analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Sequence aligner for protein and translated DNA searches and functions as a drop-in replacement for the NCBI BLAST software tools. It is suitable for protein-protein search as well as DNA-protein search on short reads and longer sequences including contigs and assemblies, providing a speedup of BLAST ranging up to x20,000.",
                "homepage": "https://github.com/bbuchfink/diamond"
            }
        ],
        "inputs": [
            "sample_name",
            "R1",
            "R2",
            "refdb"
        ],
        "nb_inputs": 4,
        "outputs": [
            "sample_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Align to the gene catalog\"",
            "container \"quay.io/fhcrc-microbiome/famli:v1.5\"",
            "label 'mem_veryhigh'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "famli": {
        "name_process": "famli",
        "string_process": "\nprocess famli {\n    tag \"Deduplicate multi-mapping reads\"\n    container \"quay.io/fhcrc-microbiome/famli:v1.5\"\n    label 'mem_veryhigh'\n    publishDir \"${params.output_folder}/abund/details/\", mode: \"copy\"\n    errorStrategy 'finish'\n    \n    input:\n    tuple sample_name, file(input_aln)\n    \n    output:\n    path \"${sample_name}.json.gz\"\n\n    \"\"\"\n    set -e\n    famli \\\n      filter \\\n      --input ${input_aln} \\\n      --output ${sample_name}.json \\\n      --threads ${task.cpus} \\\n      --batchsize ${params.famli_batchsize} \\\n      --sd-mean-cutoff ${params.sd_mean_cutoff}\n    gzip ${sample_name}.json\n    \"\"\"\n\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n    set -e\n    famli \\\n      filter \\\n      --input ${input_aln} \\\n      --output ${sample_name}.json \\\n      --threads ${task.cpus} \\\n      --batchsize ${params.famli_batchsize} \\\n      --sd-mean-cutoff ${params.sd_mean_cutoff}\n    gzip ${sample_name}.json\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [
            "Filter"
        ],
        "tools_url": [
            "https://bio.tools/filter"
        ],
        "tools_dico": [
            {
                "name": "Filter",
                "uri": "https://bio.tools/filter",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Workflows"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Pipelines"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3187",
                                    "term": "Sequence contamination filtering"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Produce a filtered version of an sRNA dataset, controlled by several user-defined criteria, including sequence length, abundance, complexity, transfer and ribosomal RNA removal.",
                "homepage": "http://srna-workbench.cmp.uea.ac.uk/filter-2/"
            }
        ],
        "inputs": [
            "input_aln",
            "sample_name"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Deduplicate multi-mapping reads\"",
            "container \"quay.io/fhcrc-microbiome/famli:v1.5\"",
            "label 'mem_veryhigh'",
            "publishDir \"${params.output_folder}/abund/details/\", mode: \"copy\"",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "assembleAbundances": {
        "name_process": "assembleAbundances",
        "string_process": "\nprocess assembleAbundances {\n    tag \"Make gene ~ sample abundance matrix\"\n    container \"quay.io/fhcrc-microbiome/experiment-collection:v0.2\"\n    label \"mem_veryhigh\"\n    errorStrategy 'finish'\n\n    input:\n    file sample_jsons\n    val cag_batchsize\n    val output_prefix\n\n    output:\n    file \"gene_list.*.csv.gz\"\n    file \"specimen_gene_count.csv.gz\"\n    file \"${output_prefix}.details.hdf5\"\n    path \"gene_length.csv.gz\"\n    path \"specimen_reads_aligned.csv.gz\"\n    path \"gene_abundance.zarr.tar\"\n\n\n    \"\"\"\n#!/usr/bin/env python3\n\nimport logging\nimport numpy as np\nimport os\nimport pandas as pd\nimport tarfile\nimport gzip\nimport json\nimport pickle\nimport zarr\npickle.HIGHEST_PROTOCOL = 4\n\n# Set up logging\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [assembleAbundances] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write logs to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\ndef read_json(fp):\n    return pd.DataFrame(\n        json.load(\n            gzip.open(fp, \"rt\")\n        )\n    )\n\n# Parse the file list\nsample_jsons = \"${sample_jsons}\".split(\" \")\nlogging.info(\"Getting ready to read in data for %d samples\" % len(sample_jsons))\n\n# Keep track of the complete set of sample names observed in these samples\nall_sample_names = set([])\n\n# Also keep track of the complete set of gene names observed in these samples\nall_gene_names = set([])\n\n# All abundance tables will be written out to HDF5\nstore = pd.HDFStore(\"${output_prefix}.details.hdf5\", \"w\")\n\n# Keep track of the length of each gene\ngene_length_dict = dict()\n\n# Keep track of the number of reads aligned per sample\nspecimen_reads_aligned = dict()\n\n# Keep track of the number of genes detected per sample\nspecimen_genes_detected = dict()\n\n# Iterate over the list of files\nfor fp in sample_jsons:\n    # Get the sample name from the file name\n    # This is only possible because we control the file naming scheme in the `famli` process\n    assert fp.endswith(\".json.gz\")\n    sample_name = fp[:-len(\".json.gz\")]\n\n    logging.info(\"Reading in %s from %s\" % (sample_name, fp))\n    df = read_json(fp)\n\n    logging.info(\"Saving to HDF5\")\n    df.to_hdf(store, \"/abund/gene/long/%s\" % sample_name)\n    \n    # Add the sample name to the total list\n    all_sample_names.add(sample_name)\n\n    # Add the gene names to the total list\n    for gene_name in df[\"id\"].values:\n        all_gene_names.add(gene_name)\n\n    # Add the gene lengths\n    for _, r in df.iterrows():\n        gene_length_dict[r[\"id\"]] = r[\"length\"]\n\n    # Add in the number of reads aligned\n    specimen_reads_aligned[sample_name] = df[\"nreads\"].sum()\n\n    # Add in the number of genes detected\n    specimen_genes_detected[sample_name] = df.shape[0]\n\nstore.close()\n\n# Serialize sample and gene names\nsample_names = list(all_sample_names)\nall_gene_names = list(all_gene_names)\n\n# Write out the number of genes detected per sample\npd.DataFrame(\n    dict(\n        n_genes_aligned = specimen_genes_detected\n    )\n).reset_index(\n).rename(\n    columns = dict(\n        index = \"specimen\"\n    )\n).to_csv(\n    \"specimen_gene_count.csv.gz\",\n    index=None,\n    compression = \"gzip\"\n)\n\n# Write out the number of reads aligned per sample\npd.DataFrame(\n    dict(\n        n_reads_aligned = specimen_reads_aligned\n    )\n).reset_index(\n).rename(\n    columns = dict(\n        index = \"specimen\"\n    )\n).to_csv(\n    \"specimen_reads_aligned.csv.gz\",\n    index=None,\n    compression = \"gzip\"\n)\n\n# Write out the CSV table with the length of each gene\npd.DataFrame([\n    dict(\n        gene = gene, \n        length = length\n    )\n    for gene, length in gene_length_dict.items()\n]).to_csv(\n    \"gene_length.csv.gz\",\n    index = None,\n    compression = \"gzip\"\n)\n\n# Write out the gene names in batches of ${cag_batchsize}\nfor ix, gene_list in enumerate([\n    all_gene_names[ix: (ix + ${cag_batchsize})]\n    for ix in range(0, len(all_gene_names), ${cag_batchsize})\n]):\n    print(\"Writing out %d genes in batch %d\" % (len(gene_list), ix))\n    with gzip.open(\"gene_list.%d.csv.gz\" % ix, \"wt\") as handle:\n        handle.write(\"\\\\n\".join(gene_list))\n\n# Write out the sequencing depth of each gene in each specimen in zarr format\nz = zarr.open(\n    \"gene_abundance.zarr\",\n    mode=\"w\",\n    shape=(len(all_gene_names), len(sample_names)), \n    chunks=True,\n    dtype='f4'\n)\n\n# Iterate over the list of files\nfor fp in sample_jsons:\n    # Get the sample name from the file name\n    assert fp.endswith(\".json.gz\")\n    sample_name = fp[:-len(\".json.gz\")]\n\n    logging.info(\"Reading in %s from %s\" % (sample_name, fp))\n    df = read_json(fp)\n\n    # Calculate the proportional depth of sequencing per gene\n    gene_depth = df.set_index(\n        \"id\"\n    ).reindex(\n        index=all_gene_names\n    )[\n        \"depth\"\n    ].fillna(\n        0\n    ) / df[\n        \"depth\"\n    ].sum()\n\n    logging.info(\"Saving %s to gene_abundance.zarr\" % sample_name)\n    # Save the sequencing depth to zarr\n    z[\n        :,\n        sample_names.index(sample_name)\n    ] = gene_depth.values\n\n# Write out the sample names and gene names\nlogging.info(\"Writing out sample_names.json.gz\")\nwith gzip.open(\"sample_names.json.gz\", \"wt\") as fo:\n    json.dump(sample_names, fo)\n\nlogging.info(\"Writing out gene_names.json.gz\")\nwith gzip.open(\"gene_names.json.gz\", \"wt\") as fo:\n    json.dump(all_gene_names, fo)\n\nlogging.info(\"Creating gene_abundance.zarr.tar\")\nwith tarfile.open(\"gene_abundance.zarr.tar\", \"w\") as tar:\n    for name in [\n        \"gene_names.json.gz\",\n        \"sample_names.json.gz\",\n        \"gene_abundance.zarr\",\n    ]:\n        logging.info(\"Adding %s to gene_abundance.zarr.tar\" % name)\n        tar.add(name)\n\nlogging.info(\"Done\")\n\n    \"\"\"\n\n}",
        "nb_lignes_process": 226,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport logging\nimport numpy as np\nimport os\nimport pandas as pd\nimport tarfile\nimport gzip\nimport json\nimport pickle\nimport zarr\npickle.HIGHEST_PROTOCOL = 4\n\n# Set up logging\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [assembleAbundances] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write logs to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\ndef read_json(fp):\n    return pd.DataFrame(\n        json.load(\n            gzip.open(fp, \"rt\")\n        )\n    )\n\n# Parse the file list\nsample_jsons = \"${sample_jsons}\".split(\" \")\nlogging.info(\"Getting ready to read in data for %d samples\" % len(sample_jsons))\n\n# Keep track of the complete set of sample names observed in these samples\nall_sample_names = set([])\n\n# Also keep track of the complete set of gene names observed in these samples\nall_gene_names = set([])\n\n# All abundance tables will be written out to HDF5\nstore = pd.HDFStore(\"${output_prefix}.details.hdf5\", \"w\")\n\n# Keep track of the length of each gene\ngene_length_dict = dict()\n\n# Keep track of the number of reads aligned per sample\nspecimen_reads_aligned = dict()\n\n# Keep track of the number of genes detected per sample\nspecimen_genes_detected = dict()\n\n# Iterate over the list of files\nfor fp in sample_jsons:\n    # Get the sample name from the file name\n    # This is only possible because we control the file naming scheme in the `famli` process\n    assert fp.endswith(\".json.gz\")\n    sample_name = fp[:-len(\".json.gz\")]\n\n    logging.info(\"Reading in %s from %s\" % (sample_name, fp))\n    df = read_json(fp)\n\n    logging.info(\"Saving to HDF5\")\n    df.to_hdf(store, \"/abund/gene/long/%s\" % sample_name)\n    \n    # Add the sample name to the total list\n    all_sample_names.add(sample_name)\n\n    # Add the gene names to the total list\n    for gene_name in df[\"id\"].values:\n        all_gene_names.add(gene_name)\n\n    # Add the gene lengths\n    for _, r in df.iterrows():\n        gene_length_dict[r[\"id\"]] = r[\"length\"]\n\n    # Add in the number of reads aligned\n    specimen_reads_aligned[sample_name] = df[\"nreads\"].sum()\n\n    # Add in the number of genes detected\n    specimen_genes_detected[sample_name] = df.shape[0]\n\nstore.close()\n\n# Serialize sample and gene names\nsample_names = list(all_sample_names)\nall_gene_names = list(all_gene_names)\n\n# Write out the number of genes detected per sample\npd.DataFrame(\n    dict(\n        n_genes_aligned = specimen_genes_detected\n    )\n).reset_index(\n).rename(\n    columns = dict(\n        index = \"specimen\"\n    )\n).to_csv(\n    \"specimen_gene_count.csv.gz\",\n    index=None,\n    compression = \"gzip\"\n)\n\n# Write out the number of reads aligned per sample\npd.DataFrame(\n    dict(\n        n_reads_aligned = specimen_reads_aligned\n    )\n).reset_index(\n).rename(\n    columns = dict(\n        index = \"specimen\"\n    )\n).to_csv(\n    \"specimen_reads_aligned.csv.gz\",\n    index=None,\n    compression = \"gzip\"\n)\n\n# Write out the CSV table with the length of each gene\npd.DataFrame([\n    dict(\n        gene = gene, \n        length = length\n    )\n    for gene, length in gene_length_dict.items()\n]).to_csv(\n    \"gene_length.csv.gz\",\n    index = None,\n    compression = \"gzip\"\n)\n\n# Write out the gene names in batches of ${cag_batchsize}\nfor ix, gene_list in enumerate([\n    all_gene_names[ix: (ix + ${cag_batchsize})]\n    for ix in range(0, len(all_gene_names), ${cag_batchsize})\n]):\n    print(\"Writing out %d genes in batch %d\" % (len(gene_list), ix))\n    with gzip.open(\"gene_list.%d.csv.gz\" % ix, \"wt\") as handle:\n        handle.write(\"\\\\n\".join(gene_list))\n\n# Write out the sequencing depth of each gene in each specimen in zarr format\nz = zarr.open(\n    \"gene_abundance.zarr\",\n    mode=\"w\",\n    shape=(len(all_gene_names), len(sample_names)), \n    chunks=True,\n    dtype='f4'\n)\n\n# Iterate over the list of files\nfor fp in sample_jsons:\n    # Get the sample name from the file name\n    assert fp.endswith(\".json.gz\")\n    sample_name = fp[:-len(\".json.gz\")]\n\n    logging.info(\"Reading in %s from %s\" % (sample_name, fp))\n    df = read_json(fp)\n\n    # Calculate the proportional depth of sequencing per gene\n    gene_depth = df.set_index(\n        \"id\"\n    ).reindex(\n        index=all_gene_names\n    )[\n        \"depth\"\n    ].fillna(\n        0\n    ) / df[\n        \"depth\"\n    ].sum()\n\n    logging.info(\"Saving %s to gene_abundance.zarr\" % sample_name)\n    # Save the sequencing depth to zarr\n    z[\n        :,\n        sample_names.index(sample_name)\n    ] = gene_depth.values\n\n# Write out the sample names and gene names\nlogging.info(\"Writing out sample_names.json.gz\")\nwith gzip.open(\"sample_names.json.gz\", \"wt\") as fo:\n    json.dump(sample_names, fo)\n\nlogging.info(\"Writing out gene_names.json.gz\")\nwith gzip.open(\"gene_names.json.gz\", \"wt\") as fo:\n    json.dump(all_gene_names, fo)\n\nlogging.info(\"Creating gene_abundance.zarr.tar\")\nwith tarfile.open(\"gene_abundance.zarr.tar\", \"w\") as tar:\n    for name in [\n        \"gene_names.json.gz\",\n        \"sample_names.json.gz\",\n        \"gene_abundance.zarr\",\n    ]:\n        logging.info(\"Adding %s to gene_abundance.zarr.tar\" % name)\n        tar.add(name)\n\nlogging.info(\"Done\")\n\n    \"\"\"",
        "nb_lignes_script": 204,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_jsons",
            "cag_batchsize",
            "output_prefix"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Make gene ~ sample abundance matrix\"",
            "container \"quay.io/fhcrc-microbiome/experiment-collection:v0.2\"",
            "label \"mem_veryhigh\"",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "calcCAGabund": {
        "name_process": "calcCAGabund",
        "string_process": "\nprocess calcCAGabund {\n    tag \"Make CAG ~ sample abundance matrix\"\n    container \"quay.io/fhcrc-microbiome/experiment-collection:v0.2\"\n    label \"mem_veryhigh\"\n    errorStrategy 'finish'\n    publishDir \"${params.output_folder}/abund/\", mode: \"copy\"\n\n    input:\n    path cag_csv_gz\n\n    output:\n    file \"CAG.abund.feather\"\n\n    \"\"\"\n#!/usr/bin/env python3\n\nfrom collections import defaultdict\nimport gzip\nimport json\nimport numpy as np\nimport os\nimport pandas as pd\nimport logging\nimport tarfile\nimport zarr\n\n# Set up logging\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [calculateCAGabundance] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write logs to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\n# Read in the dictionary linking genes and CAGs\ncags = {\n    cag_id: cag_df[\"gene\"].tolist()\n    for cag_id, cag_df in pd.read_csv(\n        \"${cag_csv_gz}\",\n        compression=\"gzip\"\n    ).groupby(\n        \"CAG\"\n    )\n}\n\n# Extract everything from the gene_abundance.zarr.tar\nlogging.info(\"Extracting ${gene_abundances_zarr_tar}\")\nwith tarfile.open(\"${gene_abundances_zarr_tar}\") as tar:\n    tar.extractall()\n\n# Make sure that the expected contents are present\nfor fp in [\n    \"gene_names.json.gz\",\n    \"sample_names.json.gz\",\n    \"gene_abundance.zarr\",\n]:\n    logging.info(\"Checking that %s is present\" % fp)\n    assert os.path.exists(fp)\n\n# Read in the gene names and sample names as indexed in the zarr\nlogging.info(\"Reading in gene_names.json.gz\")\nwith gzip.open(\"gene_names.json.gz\", \"rt\") as handle:\n    gene_names = json.load(handle)\n\nlogging.info(\"Reading in sample_names.json.gz\")\nwith gzip.open(\"sample_names.json.gz\", \"rt\") as handle:\n    sample_names = json.load(handle)\n\n# Open the zarr store\nlogging.info(\"Reading in gene abundances from gene_abundance.zarr\")\nz = zarr.open(\"gene_abundance.zarr\", mode=\"r\")\n\n# Set up an array for CAG abundances\ndf = pd.DataFrame(\n    data = np.zeros(\n        (len(cags), len(sample_names)),\n        dtype = np.float32,\n    ),\n    dtype = np.float32,\n    columns = sample_names,\n    index = list(range(len(cags))),\n)\n\n# Iterate over each sample\nfor sample_ix, sample_name in enumerate(sample_names):\n\n    # Read the depth of sequencing for each gene\n    logging.info(\"Reading gene abundances for %s\" % sample_name)\n    sample_gene_depth = pd.Series(z[:, sample_ix], index=gene_names)\n\n    # Sum up the depth by CAG\n    df[sample_name] = pd.Series({\n        cag_ix: sample_gene_depth.reindex(index=cag_gene_list).sum()\n        for cag_ix, cag_gene_list in cags.items()\n    })\n\n\n# Now write out to a feather file\nlogging.info(\"Building a single DataFrame of CAG abundances\")\ndf.reset_index(\n).to_feather(\n    \"CAG.abund.feather\"\n)\n\nlogging.info(\"Done\")\n    \"\"\"\n}",
        "nb_lignes_process": 110,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nfrom collections import defaultdict\nimport gzip\nimport json\nimport numpy as np\nimport os\nimport pandas as pd\nimport logging\nimport tarfile\nimport zarr\n\n# Set up logging\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [calculateCAGabundance] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write logs to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\n# Read in the dictionary linking genes and CAGs\ncags = {\n    cag_id: cag_df[\"gene\"].tolist()\n    for cag_id, cag_df in pd.read_csv(\n        \"${cag_csv_gz}\",\n        compression=\"gzip\"\n    ).groupby(\n        \"CAG\"\n    )\n}\n\n# Extract everything from the gene_abundance.zarr.tar\nlogging.info(\"Extracting ${gene_abundances_zarr_tar}\")\nwith tarfile.open(\"${gene_abundances_zarr_tar}\") as tar:\n    tar.extractall()\n\n# Make sure that the expected contents are present\nfor fp in [\n    \"gene_names.json.gz\",\n    \"sample_names.json.gz\",\n    \"gene_abundance.zarr\",\n]:\n    logging.info(\"Checking that %s is present\" % fp)\n    assert os.path.exists(fp)\n\n# Read in the gene names and sample names as indexed in the zarr\nlogging.info(\"Reading in gene_names.json.gz\")\nwith gzip.open(\"gene_names.json.gz\", \"rt\") as handle:\n    gene_names = json.load(handle)\n\nlogging.info(\"Reading in sample_names.json.gz\")\nwith gzip.open(\"sample_names.json.gz\", \"rt\") as handle:\n    sample_names = json.load(handle)\n\n# Open the zarr store\nlogging.info(\"Reading in gene abundances from gene_abundance.zarr\")\nz = zarr.open(\"gene_abundance.zarr\", mode=\"r\")\n\n# Set up an array for CAG abundances\ndf = pd.DataFrame(\n    data = np.zeros(\n        (len(cags), len(sample_names)),\n        dtype = np.float32,\n    ),\n    dtype = np.float32,\n    columns = sample_names,\n    index = list(range(len(cags))),\n)\n\n# Iterate over each sample\nfor sample_ix, sample_name in enumerate(sample_names):\n\n    # Read the depth of sequencing for each gene\n    logging.info(\"Reading gene abundances for %s\" % sample_name)\n    sample_gene_depth = pd.Series(z[:, sample_ix], index=gene_names)\n\n    # Sum up the depth by CAG\n    df[sample_name] = pd.Series({\n        cag_ix: sample_gene_depth.reindex(index=cag_gene_list).sum()\n        for cag_ix, cag_gene_list in cags.items()\n    })\n\n\n# Now write out to a feather file\nlogging.info(\"Building a single DataFrame of CAG abundances\")\ndf.reset_index(\n).to_feather(\n    \"CAG.abund.feather\"\n)\n\nlogging.info(\"Done\")\n    \"\"\"",
        "nb_lignes_script": 96,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "cag_csv_gz"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Make CAG ~ sample abundance matrix\"",
            "container \"quay.io/fhcrc-microbiome/experiment-collection:v0.2\"",
            "label \"mem_veryhigh\"",
            "errorStrategy 'finish'",
            "publishDir \"${params.output_folder}/abund/\", mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    },
    "renameHDF": {
        "name_process": "renameHDF",
        "string_process": "\nprocess renameHDF{\n    container \"ubuntu:20.04\"\n    label 'io_limited'\n\n    input:\n        path \"INPUT.RESULTS.HDF\"\n        val new_file_name\n\n    output:\n        path \"${new_file_name}\"\n\n\"\"\"#!/bin/bash\n\nmv INPUT.RESULTS.HDF ${new_file_name}\n\"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "\"\"\"#!/bin/bash\n\nmv INPUT.RESULTS.HDF ${new_file_name}\n\"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "new_file_name"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"ubuntu:20.04\"",
            "label 'io_limited'"
        ],
        "when": "",
        "stub": ""
    },
    "makeInitialCAGs": {
        "name_process": "makeInitialCAGs",
        "string_process": "\nprocess makeInitialCAGs {\n    tag \"Group gene subsets by co-abundance\"\n    container \"${container__find_cags}\"\n    label \"mem_medium\"\n    errorStrategy 'finish'\n\n    input:\n    path gene_abundances_zarr_tar\n    path gene_list_csv\n\n    output:\n    file \"CAGs.csv.gz\"\n    file \"CAGs.abund.feather\"\n\n    \"\"\"\n#!/usr/bin/env python3\n\nimport gzip\nimport json\nimport logging\nfrom multiprocessing import Pool\nimport nmslib\nimport numpy as np\nimport os\nimport pandas as pd\nimport tarfile\nimport zarr\nimport shutil\nfrom ann_linkage_clustering.lib import make_cags_with_ann\nfrom ann_linkage_clustering.lib import iteratively_refine_cags\nfrom ann_linkage_clustering.lib import make_nmslib_index\n\n# Set up logging\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [makeInitialCAGs] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write logs to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\n# Set up the multiprocessing pool\nthreads = int(\"${task.cpus}\")\npool = Pool(threads)\n\n# Extract everything from the gene_abundance.zarr.tar\nlogging.info(\"Extracting ${gene_abundances_zarr_tar}\")\nwith tarfile.open(\"${gene_abundances_zarr_tar}\") as tar:\n    tar.extractall()\n\n# Make sure that the expected contents are present\nfor fp in [\n    \"gene_names.json.gz\",\n    \"sample_names.json.gz\",\n    \"gene_abundance.zarr\",\n]:\n    logging.info(\"Checking that %s is present\" % fp)\n    assert os.path.exists(fp)\n\n# Read in the gene names and sample names as indexed in the zarr\nlogging.info(\"Reading in gene_names.json.gz\")\nwith gzip.open(\"gene_names.json.gz\", \"rt\") as handle:\n    gene_names = json.load(handle)\n\nlogging.info(\"Reading in sample_names.json.gz\")\nwith gzip.open(\"sample_names.json.gz\", \"rt\") as handle:\n    sample_names = json.load(handle)\n\n# Set the file path to the genes for this subset\ngene_list_csv = \"${gene_list_csv}\"\n\n# Make sure the files exist\nassert os.path.exists(gene_list_csv), gene_list_csv\n\nlogging.info(\"Reading in the list of genes for this shard from %s\" % (gene_list_csv))\ngene_list = [\n    line.rstrip(\"\\\\n\")\n    for line in gzip.open(gene_list_csv, \"rt\")\n]\nlogging.info(\"This shard contains %d genes\" % (len(gene_list)))\n\n# Open the zarr store\nlogging.info(\"Reading in gene abundances from gene_abundance.zarr\")\nz = zarr.open(\"gene_abundance.zarr\", mode=\"r\")\n\n# Set up an array for gene abundances\ndf = pd.DataFrame(\n    data = np.zeros(\n        (len(gene_list), len(sample_names)),\n        dtype = np.float32,\n    ),\n    dtype = np.float32,\n    index = gene_list,\n    columns = sample_names\n)\n\n# Iterate over each sample\nfor sample_ix, sample_name in enumerate(sample_names):\n\n    # Read in the abundances for this sample\n    logging.info(\"Reading in gene abundances for %s\" % sample_name)\n    df[sample_name] = pd.Series(\n        z[:, sample_ix],\n        index=gene_names\n    ).reindex(\n        gene_list\n    ).apply(\n        np.float32\n    )\n\nmax_dist = float(\"${params.distance_threshold}\")\nlogging.info(\"Maximum cosine distance: %s\" % max_dist)\n\n# Make the nmslib index\nlogging.info(\"Making the HNSW index\")\nindex = nmslib.init(method='hnsw', space='cosinesimil')\nlogging.info(\"Adding %d genes to the nmslib index\" % (df.shape[0]))\nindex.addDataPointBatch(df.values)\nlogging.info(\"Making the index\")\nindex.createIndex({'post': 2, \"M\": 100}, print_progress=True)\n\n\n# Make the CAGs\nlogging.info(\"Making first-round CAGs\")\ncags = make_cags_with_ann(\n    index,\n    max_dist,\n    df.copy(),\n    pool,\n    threads=threads,\n    distance_metric=\"${params.distance_metric}\",\n    linkage_type=\"${params.linkage_type}\"\n)\n\nlogging.info(\"Closing the process pool\")\npool.close()\n\nlogging.info(\"Clearing the previous index from memory\")\ndel index\n\nlogging.info(\"Refining CAGS\")\niteratively_refine_cags(\n    cags,\n    df.copy(),\n    max_dist,\n    threads=threads,\n    distance_metric=\"${params.distance_metric}\",\n    linkage_type=\"${params.linkage_type}\",\n    max_iters = 5\n)\n\nlogging.info(\"Sorting CAGs by size\")\ncags = {\n    ix: list_of_genes\n    for ix, list_of_genes in enumerate(\n        sorted(\n            list(\n                cags.values()\n            ), \n            key=len, \n            reverse=True\n        )\n    )\n}\n\nlogging.info(\"Computing relative abundance of CAGs\")\n# Rows are CAGs, columns are specimens\ncags_abund_df = pd.DataFrame({\n    cag_id: df.reindex(\n        index=list_of_genes\n    ).sum()\n    for cag_id, list_of_genes in cags.items()\n}).T.sort_index()\n\nfp_out = \"CAGs.abund.feather\"\nlogging.info(\"Saving CAG relative abundances to %s\" % fp_out)\ncags_abund_df.reset_index().to_feather(\n    fp_out\n)\n\nlogging.info(\"Formatting CAG membership as a DataFrame\")\ncags_df = pd.DataFrame(\n    [\n        [ix, gene_id]\n        for ix, list_of_genes in cags.items()\n        for gene_id in list_of_genes\n    ],\n    columns=[\"CAG\", \"gene\"]\n)\n\nlogging.info(\"Largest CAGs:\")\nfor cag_id, cag_size in cags_df[\"CAG\"].value_counts().head().items():\n    logging.info(\"CAG ID: %d, %d genes\" % (cag_id, cag_size))\n\nfp_out = \"CAGs.csv.gz\"\n\nlogging.info(\"Writing out CAG membership to %s\" % fp_out)\ncags_df.to_csv(fp_out, compression=\"gzip\", index=None)\n\nlogging.info(\"Deleting the temporary zarr\")\ndel z\nshutil.rmtree(\"gene_abundance.zarr\")\n\nlogging.info(\"Done\")\n    \"\"\"\n}",
        "nb_lignes_process": 208,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport gzip\nimport json\nimport logging\nfrom multiprocessing import Pool\nimport nmslib\nimport numpy as np\nimport os\nimport pandas as pd\nimport tarfile\nimport zarr\nimport shutil\nfrom ann_linkage_clustering.lib import make_cags_with_ann\nfrom ann_linkage_clustering.lib import iteratively_refine_cags\nfrom ann_linkage_clustering.lib import make_nmslib_index\n\n# Set up logging\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [makeInitialCAGs] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write logs to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\n# Set up the multiprocessing pool\nthreads = int(\"${task.cpus}\")\npool = Pool(threads)\n\n# Extract everything from the gene_abundance.zarr.tar\nlogging.info(\"Extracting ${gene_abundances_zarr_tar}\")\nwith tarfile.open(\"${gene_abundances_zarr_tar}\") as tar:\n    tar.extractall()\n\n# Make sure that the expected contents are present\nfor fp in [\n    \"gene_names.json.gz\",\n    \"sample_names.json.gz\",\n    \"gene_abundance.zarr\",\n]:\n    logging.info(\"Checking that %s is present\" % fp)\n    assert os.path.exists(fp)\n\n# Read in the gene names and sample names as indexed in the zarr\nlogging.info(\"Reading in gene_names.json.gz\")\nwith gzip.open(\"gene_names.json.gz\", \"rt\") as handle:\n    gene_names = json.load(handle)\n\nlogging.info(\"Reading in sample_names.json.gz\")\nwith gzip.open(\"sample_names.json.gz\", \"rt\") as handle:\n    sample_names = json.load(handle)\n\n# Set the file path to the genes for this subset\ngene_list_csv = \"${gene_list_csv}\"\n\n# Make sure the files exist\nassert os.path.exists(gene_list_csv), gene_list_csv\n\nlogging.info(\"Reading in the list of genes for this shard from %s\" % (gene_list_csv))\ngene_list = [\n    line.rstrip(\"\\\\n\")\n    for line in gzip.open(gene_list_csv, \"rt\")\n]\nlogging.info(\"This shard contains %d genes\" % (len(gene_list)))\n\n# Open the zarr store\nlogging.info(\"Reading in gene abundances from gene_abundance.zarr\")\nz = zarr.open(\"gene_abundance.zarr\", mode=\"r\")\n\n# Set up an array for gene abundances\ndf = pd.DataFrame(\n    data = np.zeros(\n        (len(gene_list), len(sample_names)),\n        dtype = np.float32,\n    ),\n    dtype = np.float32,\n    index = gene_list,\n    columns = sample_names\n)\n\n# Iterate over each sample\nfor sample_ix, sample_name in enumerate(sample_names):\n\n    # Read in the abundances for this sample\n    logging.info(\"Reading in gene abundances for %s\" % sample_name)\n    df[sample_name] = pd.Series(\n        z[:, sample_ix],\n        index=gene_names\n    ).reindex(\n        gene_list\n    ).apply(\n        np.float32\n    )\n\nmax_dist = float(\"${params.distance_threshold}\")\nlogging.info(\"Maximum cosine distance: %s\" % max_dist)\n\n# Make the nmslib index\nlogging.info(\"Making the HNSW index\")\nindex = nmslib.init(method='hnsw', space='cosinesimil')\nlogging.info(\"Adding %d genes to the nmslib index\" % (df.shape[0]))\nindex.addDataPointBatch(df.values)\nlogging.info(\"Making the index\")\nindex.createIndex({'post': 2, \"M\": 100}, print_progress=True)\n\n\n# Make the CAGs\nlogging.info(\"Making first-round CAGs\")\ncags = make_cags_with_ann(\n    index,\n    max_dist,\n    df.copy(),\n    pool,\n    threads=threads,\n    distance_metric=\"${params.distance_metric}\",\n    linkage_type=\"${params.linkage_type}\"\n)\n\nlogging.info(\"Closing the process pool\")\npool.close()\n\nlogging.info(\"Clearing the previous index from memory\")\ndel index\n\nlogging.info(\"Refining CAGS\")\niteratively_refine_cags(\n    cags,\n    df.copy(),\n    max_dist,\n    threads=threads,\n    distance_metric=\"${params.distance_metric}\",\n    linkage_type=\"${params.linkage_type}\",\n    max_iters = 5\n)\n\nlogging.info(\"Sorting CAGs by size\")\ncags = {\n    ix: list_of_genes\n    for ix, list_of_genes in enumerate(\n        sorted(\n            list(\n                cags.values()\n            ), \n            key=len, \n            reverse=True\n        )\n    )\n}\n\nlogging.info(\"Computing relative abundance of CAGs\")\n# Rows are CAGs, columns are specimens\ncags_abund_df = pd.DataFrame({\n    cag_id: df.reindex(\n        index=list_of_genes\n    ).sum()\n    for cag_id, list_of_genes in cags.items()\n}).T.sort_index()\n\nfp_out = \"CAGs.abund.feather\"\nlogging.info(\"Saving CAG relative abundances to %s\" % fp_out)\ncags_abund_df.reset_index().to_feather(\n    fp_out\n)\n\nlogging.info(\"Formatting CAG membership as a DataFrame\")\ncags_df = pd.DataFrame(\n    [\n        [ix, gene_id]\n        for ix, list_of_genes in cags.items()\n        for gene_id in list_of_genes\n    ],\n    columns=[\"CAG\", \"gene\"]\n)\n\nlogging.info(\"Largest CAGs:\")\nfor cag_id, cag_size in cags_df[\"CAG\"].value_counts().head().items():\n    logging.info(\"CAG ID: %d, %d genes\" % (cag_id, cag_size))\n\nfp_out = \"CAGs.csv.gz\"\n\nlogging.info(\"Writing out CAG membership to %s\" % fp_out)\ncags_df.to_csv(fp_out, compression=\"gzip\", index=None)\n\nlogging.info(\"Deleting the temporary zarr\")\ndel z\nshutil.rmtree(\"gene_abundance.zarr\")\n\nlogging.info(\"Done\")\n    \"\"\"",
        "nb_lignes_script": 193,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "gene_abundances_zarr_tar",
            "gene_list_csv"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Group gene subsets by co-abundance\"",
            "container \"${container__find_cags}\"",
            "label \"mem_medium\"",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "refineCAGs": {
        "name_process": "refineCAGs",
        "string_process": "\nprocess refineCAGs {\n    tag \"Group all genes by co-abundance\"\n    container \"${container__find_cags}\"\n    label \"mem_veryhigh\"\n    errorStrategy 'finish'\n\n    input:\n    file \"shard.CAG.*.csv.gz\"\n    file \"shard.CAG.*.feather\"\n\n    output:\n    file \"CAGs.csv.gz\"\n    file \"CAGs.abund.feather\"\n\n    \"\"\"\n#!/usr/bin/env python3\n\nfrom collections import defaultdict\nimport gzip\nimport json\nimport logging\nfrom multiprocessing import Pool\nimport nmslib\nimport numpy as np\nimport os\nimport pandas as pd\nimport tarfile\nimport zarr\nimport shutil\nfrom ann_linkage_clustering.lib import make_cags_with_ann\nfrom ann_linkage_clustering.lib import iteratively_refine_cags\nfrom ann_linkage_clustering.lib import make_nmslib_index\n\n# Set up logging\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [makeFinalCAGs] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write logs to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\n# Set the file path to the CAGs made for each subset\ncag_csv_list = [\n    fp\n    for fp in os.listdir(\".\")\n    if fp.startswith(\"shard.CAG.\") and \".csv.gz\" in fp\n]\ncag_feather_list = [\n    fp\n    for fp in os.listdir(\".\")\n    if fp.startswith(\"shard.CAG.\") and \".feather\" in fp\n]\n\n# Make sure all of the files have the complete ending\n# (incompletely staged files will have a random suffix appended)\nfor fp in cag_csv_list:\n    assert fp.endswith(\".csv.gz\"), \"Incomplete input file found: %s\" % (fp)\n\n    # Make sure that we have a feather file which matches the CAG membership\n    assert fp.replace(\".csv.gz\", \".feather\") in cag_feather_list, \"Names of CAG membership and feather files do not match\"\n\nassert len(cag_csv_list) > 0, \"Didn't find CAGs from any previous shard\"\nassert len(cag_csv_list) == len(cag_feather_list), \"Number of CSV and feather files does not match\"\n\n# Keep track of the abundances of the CAGs from the inputs\ncag_abund = {}\n\n# Keep track of the genes that the previous set of CAGs corresponded to\ncag_membership = {}\n\nlogging.info(\"Reading in CAGs from previous shard\")\nix = 0\nfor fp in cag_csv_list:\n\n    # Read in the gene membership from this shard in chunks\n    shard_cags_membership = {}\n    shard_cag_set = set([])\n    for chunk in pd.read_csv(fp, compression=\"gzip\", sep=\",\", iterator=True, chunksize=10000):\n        for _, r in chunk.iterrows():\n            shard_cags_membership[r[\"gene\"]] = r[\"CAG\"]\n            shard_cag_set.add(r[\"CAG\"])\n\n    logging.info(\"Read in %d genes in %d CAGs from %s\" % (\n        len(shard_cags_membership),\n        len(shard_cag_set),\n        fp\n    ))\n\n    # Read in the abundances of the CAGs in this shard\n    feather_fp = fp.replace(\".csv.gz\", \".feather\")\n    shard_cags_abundance = pd.read_feather(\n        feather_fp\n    ).set_index(\n        \"index\"\n    )\n    logging.info(\"Read in abundances for %d CAGs from %s\" % (\n        shard_cags_abundance.shape[0],\n        feather_fp\n    ))\n\n    # Make sure that the number of CAGs is the same in both files\n    assert shard_cags_abundance.shape[0] == len(shard_cag_set)\n\n    # Transform each of the CAG IDs from the shard into a new CAG ID for the combined set\n    logging.info(\"Mapping shard CAG IDs into complete set\")\n    cag_id_mapping = {}\n    for previous_cag_id in list(shard_cag_set):\n        cag_id_mapping[previous_cag_id] = ix\n        ix += 1\n\n    # Record which gene goes with which CAG (with the new IDs)\n    logging.info(\"Recording gene membership in complete set\")\n    for gene_id, shard_cag_id in shard_cags_membership.items():\n        cag_membership[gene_id] = cag_id_mapping[shard_cag_id]\n\n    # Also change the CAG IDs for the abundance table\n    logging.info(\"Mapping CAG abundances into complete set\")\n    for shard_cag_id, shard_cag_abund in shard_cags_abundance.iterrows():\n        # Add to the cag_abund dict using the updated CAG ID\n        cag_abund[\n            cag_id_mapping[shard_cag_id]\n        ] = shard_cag_abund\n\n    logging.info(\"Cleaning up temporary data objects\")\n    del shard_cags_abundance\n    del shard_cags_membership\n\n# Combine all of the tables\nlogging.info(\"Combining CAG abundance across all CAGs\")\ncag_abund = pd.DataFrame(cag_abund).T\nlogging.info(\"Combining CAG membership across all CAGs\")\ncag_membership = pd.Series(cag_membership)\n\n# Calculate the size of all of the input CAGs\ninput_cag_size = cag_membership.value_counts()\n\n# Make sure that things all add up\nlogging.info(\"Number of CAGs with abundances: %d\" % cag_abund.shape[0])\nlogging.info(\"Number of CAGs with members: %d\" % input_cag_size.shape[0])\nlogging.info(\"Largest CAG index: %d\" % max(input_cag_size.index.values))\nassert cag_abund.shape[0] == ix\nassert cag_abund.shape[0] == int(max(input_cag_size.index.values)) + 1\nassert cag_abund.shape[0] == input_cag_size.shape[0]\n\nlogging.info(\n    \"Read in %d CAGs from %d shards covering %d genes\" % (\n        cag_abund.shape[0], \n        len(cag_csv_list), \n        cag_membership.shape[0]\n    )\n)\n\n\nmax_dist = float(\"${params.distance_threshold}\")\nlogging.info(\"Maximum cosine distance: %s\" % max_dist)\n\n# In the `iteratively_refine_cags` step, CAGs will be combined\ngrouped_cags = {\n    cag_ix: [cag_ix]\n    for cag_ix in cag_abund.index.values\n}\n\n# Parse the number of threads available\nthreads = int(\"${task.cpus}\")\nlogging.info(\"Number of threads available: %d\" % threads)\n\nlogging.info(\"Refining CAGS\")\niteratively_refine_cags(\n    grouped_cags,\n    cag_abund.copy(),\n    max_dist,\n    threads=threads,\n    distance_metric=\"${params.distance_metric}\",\n    linkage_type=\"${params.linkage_type}\",\n    max_iters = 10\n)\n\nlogging.info(\"Sorting CAGs by size\")\noutput_cag_size = pd.Series({\n    new_cag_id: sum([\n        input_cag_size[old_cag_id]\n        for old_cag_id in old_cag_id_list\n    ])\n    for new_cag_id, old_cag_id_list in grouped_cags.items()\n}).sort_values(\n    ascending=False\n)\n\noutput_cag_ranking = pd.Series(\n    range(output_cag_size.shape[0]), \n    index=output_cag_size.index\n)\n\n# Name the CAGs based on the rank order of aggregate size (num. genes)\nlogging.info(\"Renaming genes with new CAG groupings\")\nnew_cag_mapping = {\n    old_cag_id: output_cag_ranking[new_cag_id]\n    for new_cag_id, old_cag_id_list in grouped_cags.items()\n    for old_cag_id in old_cag_id_list\n}\n\n# Update the CAG membership table and format as a DataFrame\nlogging.info(\"Updating the CAG membership table\")\ncag_membership = pd.DataFrame({\n    \"gene\": cag_membership.index.values,\n    \"CAG\": cag_membership.apply(new_cag_mapping.get)\n})\n\nlogging.info(\"Computing the abundance of new CAGs\")\ncag_abund = pd.DataFrame({\n    output_cag_ranking[new_cag_id]: cag_abund.reindex(index=old_cag_id_list).sum()\n    for new_cag_id, old_cag_id_list in grouped_cags.items()\n}).T.sort_index().reset_index()\n\nlogging.info(\"Largest CAGs:\")\nfor cag_id, cag_size in cag_membership[\"CAG\"].value_counts().head().items():\n    logging.info(\"CAG %d, %d genes\" % (cag_id, cag_size))\n\nfp_out = \"CAGs.csv.gz\"\nlogging.info(\"Writing out CAG membership to %s\" % fp_out)\ncag_membership.to_csv(fp_out, compression=\"gzip\", index=None)\n\nfp_out = \"CAGs.abund.feather\"\nlogging.info(\"Writing out CAG abundance to %s\" % fp_out)\ncag_abund.to_feather(fp_out)\n\nlogging.info(\"Done\")\nos._exit(0)\n    \"\"\"\n}",
        "nb_lignes_process": 233,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nfrom collections import defaultdict\nimport gzip\nimport json\nimport logging\nfrom multiprocessing import Pool\nimport nmslib\nimport numpy as np\nimport os\nimport pandas as pd\nimport tarfile\nimport zarr\nimport shutil\nfrom ann_linkage_clustering.lib import make_cags_with_ann\nfrom ann_linkage_clustering.lib import iteratively_refine_cags\nfrom ann_linkage_clustering.lib import make_nmslib_index\n\n# Set up logging\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [makeFinalCAGs] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write logs to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\n# Set the file path to the CAGs made for each subset\ncag_csv_list = [\n    fp\n    for fp in os.listdir(\".\")\n    if fp.startswith(\"shard.CAG.\") and \".csv.gz\" in fp\n]\ncag_feather_list = [\n    fp\n    for fp in os.listdir(\".\")\n    if fp.startswith(\"shard.CAG.\") and \".feather\" in fp\n]\n\n# Make sure all of the files have the complete ending\n# (incompletely staged files will have a random suffix appended)\nfor fp in cag_csv_list:\n    assert fp.endswith(\".csv.gz\"), \"Incomplete input file found: %s\" % (fp)\n\n    # Make sure that we have a feather file which matches the CAG membership\n    assert fp.replace(\".csv.gz\", \".feather\") in cag_feather_list, \"Names of CAG membership and feather files do not match\"\n\nassert len(cag_csv_list) > 0, \"Didn't find CAGs from any previous shard\"\nassert len(cag_csv_list) == len(cag_feather_list), \"Number of CSV and feather files does not match\"\n\n# Keep track of the abundances of the CAGs from the inputs\ncag_abund = {}\n\n# Keep track of the genes that the previous set of CAGs corresponded to\ncag_membership = {}\n\nlogging.info(\"Reading in CAGs from previous shard\")\nix = 0\nfor fp in cag_csv_list:\n\n    # Read in the gene membership from this shard in chunks\n    shard_cags_membership = {}\n    shard_cag_set = set([])\n    for chunk in pd.read_csv(fp, compression=\"gzip\", sep=\",\", iterator=True, chunksize=10000):\n        for _, r in chunk.iterrows():\n            shard_cags_membership[r[\"gene\"]] = r[\"CAG\"]\n            shard_cag_set.add(r[\"CAG\"])\n\n    logging.info(\"Read in %d genes in %d CAGs from %s\" % (\n        len(shard_cags_membership),\n        len(shard_cag_set),\n        fp\n    ))\n\n    # Read in the abundances of the CAGs in this shard\n    feather_fp = fp.replace(\".csv.gz\", \".feather\")\n    shard_cags_abundance = pd.read_feather(\n        feather_fp\n    ).set_index(\n        \"index\"\n    )\n    logging.info(\"Read in abundances for %d CAGs from %s\" % (\n        shard_cags_abundance.shape[0],\n        feather_fp\n    ))\n\n    # Make sure that the number of CAGs is the same in both files\n    assert shard_cags_abundance.shape[0] == len(shard_cag_set)\n\n    # Transform each of the CAG IDs from the shard into a new CAG ID for the combined set\n    logging.info(\"Mapping shard CAG IDs into complete set\")\n    cag_id_mapping = {}\n    for previous_cag_id in list(shard_cag_set):\n        cag_id_mapping[previous_cag_id] = ix\n        ix += 1\n\n    # Record which gene goes with which CAG (with the new IDs)\n    logging.info(\"Recording gene membership in complete set\")\n    for gene_id, shard_cag_id in shard_cags_membership.items():\n        cag_membership[gene_id] = cag_id_mapping[shard_cag_id]\n\n    # Also change the CAG IDs for the abundance table\n    logging.info(\"Mapping CAG abundances into complete set\")\n    for shard_cag_id, shard_cag_abund in shard_cags_abundance.iterrows():\n        # Add to the cag_abund dict using the updated CAG ID\n        cag_abund[\n            cag_id_mapping[shard_cag_id]\n        ] = shard_cag_abund\n\n    logging.info(\"Cleaning up temporary data objects\")\n    del shard_cags_abundance\n    del shard_cags_membership\n\n# Combine all of the tables\nlogging.info(\"Combining CAG abundance across all CAGs\")\ncag_abund = pd.DataFrame(cag_abund).T\nlogging.info(\"Combining CAG membership across all CAGs\")\ncag_membership = pd.Series(cag_membership)\n\n# Calculate the size of all of the input CAGs\ninput_cag_size = cag_membership.value_counts()\n\n# Make sure that things all add up\nlogging.info(\"Number of CAGs with abundances: %d\" % cag_abund.shape[0])\nlogging.info(\"Number of CAGs with members: %d\" % input_cag_size.shape[0])\nlogging.info(\"Largest CAG index: %d\" % max(input_cag_size.index.values))\nassert cag_abund.shape[0] == ix\nassert cag_abund.shape[0] == int(max(input_cag_size.index.values)) + 1\nassert cag_abund.shape[0] == input_cag_size.shape[0]\n\nlogging.info(\n    \"Read in %d CAGs from %d shards covering %d genes\" % (\n        cag_abund.shape[0], \n        len(cag_csv_list), \n        cag_membership.shape[0]\n    )\n)\n\n\nmax_dist = float(\"${params.distance_threshold}\")\nlogging.info(\"Maximum cosine distance: %s\" % max_dist)\n\n# In the `iteratively_refine_cags` step, CAGs will be combined\ngrouped_cags = {\n    cag_ix: [cag_ix]\n    for cag_ix in cag_abund.index.values\n}\n\n# Parse the number of threads available\nthreads = int(\"${task.cpus}\")\nlogging.info(\"Number of threads available: %d\" % threads)\n\nlogging.info(\"Refining CAGS\")\niteratively_refine_cags(\n    grouped_cags,\n    cag_abund.copy(),\n    max_dist,\n    threads=threads,\n    distance_metric=\"${params.distance_metric}\",\n    linkage_type=\"${params.linkage_type}\",\n    max_iters = 10\n)\n\nlogging.info(\"Sorting CAGs by size\")\noutput_cag_size = pd.Series({\n    new_cag_id: sum([\n        input_cag_size[old_cag_id]\n        for old_cag_id in old_cag_id_list\n    ])\n    for new_cag_id, old_cag_id_list in grouped_cags.items()\n}).sort_values(\n    ascending=False\n)\n\noutput_cag_ranking = pd.Series(\n    range(output_cag_size.shape[0]), \n    index=output_cag_size.index\n)\n\n# Name the CAGs based on the rank order of aggregate size (num. genes)\nlogging.info(\"Renaming genes with new CAG groupings\")\nnew_cag_mapping = {\n    old_cag_id: output_cag_ranking[new_cag_id]\n    for new_cag_id, old_cag_id_list in grouped_cags.items()\n    for old_cag_id in old_cag_id_list\n}\n\n# Update the CAG membership table and format as a DataFrame\nlogging.info(\"Updating the CAG membership table\")\ncag_membership = pd.DataFrame({\n    \"gene\": cag_membership.index.values,\n    \"CAG\": cag_membership.apply(new_cag_mapping.get)\n})\n\nlogging.info(\"Computing the abundance of new CAGs\")\ncag_abund = pd.DataFrame({\n    output_cag_ranking[new_cag_id]: cag_abund.reindex(index=old_cag_id_list).sum()\n    for new_cag_id, old_cag_id_list in grouped_cags.items()\n}).T.sort_index().reset_index()\n\nlogging.info(\"Largest CAGs:\")\nfor cag_id, cag_size in cag_membership[\"CAG\"].value_counts().head().items():\n    logging.info(\"CAG %d, %d genes\" % (cag_id, cag_size))\n\nfp_out = \"CAGs.csv.gz\"\nlogging.info(\"Writing out CAG membership to %s\" % fp_out)\ncag_membership.to_csv(fp_out, compression=\"gzip\", index=None)\n\nfp_out = \"CAGs.abund.feather\"\nlogging.info(\"Writing out CAG abundance to %s\" % fp_out)\ncag_abund.to_feather(fp_out)\n\nlogging.info(\"Done\")\nos._exit(0)\n    \"\"\"",
        "nb_lignes_script": 218,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Group all genes by co-abundance\"",
            "container \"${container__find_cags}\"",
            "label \"mem_veryhigh\"",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "extractAnnotations": {
        "name_process": "extractAnnotations",
        "string_process": "\nprocess extractAnnotations {\n\n    container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"\n    label \"mem_medium\"\n\n    input:\n    file results_hdf\n\n    output:\n    file \"${params.output_prefix}.genes.csv\"\n\n    publishDir \"${params.output_folder}\", mode: 'copy', overwrite: true\n\n\"\"\"#!/usr/bin/env python3\n\nimport os\nimport pandas as pd\n\n# Set the input path\nresults_hdf = '${results_hdf}'\n\n# Make sure that the file is present in the working folder\nassert os.path.exists(results_hdf)\n\n# Set up a function to filter a DataFrame by the query string\nquery_str = '${params.query}'\ndef filter_df(df):\n    return df.loc[\n        df['eggNOG_desc'].fillna('').apply(lambda s: query_str in s)\n    ]\n\n# Read in the table in chunks and filter as we go\ndf = pd.concat([\n    filter_df(chunk_df)\n    for chunk_df in pd.read_hdf(\n        results_hdf, \n        '/annot/gene/all', \n        iterator=True\n    )\n])\nprint(\"Number of genes containing the query '%s': %d\" % (query_str, df.shape[0]))\n\n# If there are any genes matching this string\nif df.shape[0] > 0:\n\n    # Write out the smaller table\n    df.to_csv(\"${params.output_prefix}.genes.csv\", index=None)\n\n    print(\"Done\")\n\nelse:\n\n    print(\"NO GENES FOUND MATCHING THE QUERY: %s\" % query_str)\n\n\"\"\"\n\n}",
        "nb_lignes_process": 56,
        "string_script": "\"\"\"#!/usr/bin/env python3\n\nimport os\nimport pandas as pd\n\n# Set the input path\nresults_hdf = '${results_hdf}'\n\n# Make sure that the file is present in the working folder\nassert os.path.exists(results_hdf)\n\n# Set up a function to filter a DataFrame by the query string\nquery_str = '${params.query}'\ndef filter_df(df):\n    return df.loc[\n        df['eggNOG_desc'].fillna('').apply(lambda s: query_str in s)\n    ]\n\n# Read in the table in chunks and filter as we go\ndf = pd.concat([\n    filter_df(chunk_df)\n    for chunk_df in pd.read_hdf(\n        results_hdf, \n        '/annot/gene/all', \n        iterator=True\n    )\n])\nprint(\"Number of genes containing the query '%s': %d\" % (query_str, df.shape[0]))\n\n# If there are any genes matching this string\nif df.shape[0] > 0:\n\n    # Write out the smaller table\n    df.to_csv(\"${params.output_prefix}.genes.csv\", index=None)\n\n    print(\"Done\")\n\nelse:\n\n    print(\"NO GENES FOUND MATCHING THE QUERY: %s\" % query_str)\n\n\"\"\"",
        "nb_lignes_script": 41,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "results_hdf"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"",
            "label \"mem_medium\""
        ],
        "when": "",
        "stub": ""
    },
    "extractManifest": {
        "name_process": "extractManifest",
        "string_process": "\nprocess extractManifest {\n\n    container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"\n    label \"mem_medium\"\n\n    input:\n    file results_hdf\n\n    output:\n    file \"${params.output_prefix}.manifest.csv\"\n\n    publishDir \"${params.output_folder}\", mode: 'copy', overwrite: true\n\n\"\"\"#!/usr/bin/env python3\n\nimport os\nimport pandas as pd\n\n# Set the input path\nresults_hdf = '${results_hdf}'\n\n# Make sure that the file is present in the working folder\nassert os.path.exists(results_hdf)\n\n# Read the manifest\nmanifest_df = pd.read_hdf(results_hdf, \"/manifest\")\n\n# Write out to a file\nmanifest_df.to_csv(\"${params.output_prefix}.manifest.csv\", index=None)\n\n\"\"\"\n\n}",
        "nb_lignes_process": 32,
        "string_script": "\"\"\"#!/usr/bin/env python3\n\nimport os\nimport pandas as pd\n\n# Set the input path\nresults_hdf = '${results_hdf}'\n\n# Make sure that the file is present in the working folder\nassert os.path.exists(results_hdf)\n\n# Read the manifest\nmanifest_df = pd.read_hdf(results_hdf, \"/manifest\")\n\n# Write out to a file\nmanifest_df.to_csv(\"${params.output_prefix}.manifest.csv\", index=None)\n\n\"\"\"",
        "nb_lignes_script": 17,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "results_hdf"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"",
            "label \"mem_medium\""
        ],
        "when": "",
        "stub": ""
    },
    "extractFASTA": {
        "name_process": "extractFASTA",
        "string_process": "\nprocess extractFASTA {\n\n    container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"\n    label \"io_limited\"\n\n    input:\n    file gene_csv\n    file gene_fasta_gz\n\n    output:\n    file \"${params.output_prefix}.genes.fasta.gz\"\n\n    publishDir \"${params.output_folder}\", mode: 'copy', overwrite: true\n\n\"\"\"#!/usr/bin/env python3\n\nfrom Bio.SeqIO.FastaIO import SimpleFastaParser\nimport gzip\nimport os\nimport pandas as pd\n\n# Set the input paths\n# Make sure that the files are present in the working folder\ngene_csv = '${gene_csv}'\nassert os.path.exists(gene_csv)\ngene_fasta_gz = '${gene_fasta_gz}'\nassert os.path.exists(gene_fasta_gz)\noutput_fasta_gz = '${params.output_prefix}.genes.fasta.gz'\nassert not os.path.exists(output_fasta_gz)\n\n# Read in the table\ndf = pd.read_csv(gene_csv)\nprint(\"Read in annotations for %d genes\" % df.shape[0])\n\n# Get the list of genes\ngene_names = set(df['gene'].tolist())\n\n# Keep a counter of how many genes we've found\nn_found = 0\n\n# Open the input and output files\nwith gzip.open(gene_fasta_gz, 'rt') as handle_in, gzip.open(output_fasta_gz, 'wt') as handle_out:\n\n    # Iterate over the inputs\n    for gene_name, gene_seq in SimpleFastaParser(handle_in):\n\n        # If this is one of the genes we are looking for\n        if gene_name in gene_names:\n\n            # Write it out\n            handle_out.write(\">%s\\\\n%s\\\\n\" % (gene_name, gene_seq))\n\n            # Increment the counter\n            n_found += 1\n\n# Report the number of genes which were found\nprint(\"Wrote out %d gene sequences\" % n_found)\nprint(\"DONE\")\n\n\"\"\"\n\n}",
        "nb_lignes_process": 61,
        "string_script": "\"\"\"#!/usr/bin/env python3\n\nfrom Bio.SeqIO.FastaIO import SimpleFastaParser\nimport gzip\nimport os\nimport pandas as pd\n\n# Set the input paths\n# Make sure that the files are present in the working folder\ngene_csv = '${gene_csv}'\nassert os.path.exists(gene_csv)\ngene_fasta_gz = '${gene_fasta_gz}'\nassert os.path.exists(gene_fasta_gz)\noutput_fasta_gz = '${params.output_prefix}.genes.fasta.gz'\nassert not os.path.exists(output_fasta_gz)\n\n# Read in the table\ndf = pd.read_csv(gene_csv)\nprint(\"Read in annotations for %d genes\" % df.shape[0])\n\n# Get the list of genes\ngene_names = set(df['gene'].tolist())\n\n# Keep a counter of how many genes we've found\nn_found = 0\n\n# Open the input and output files\nwith gzip.open(gene_fasta_gz, 'rt') as handle_in, gzip.open(output_fasta_gz, 'wt') as handle_out:\n\n    # Iterate over the inputs\n    for gene_name, gene_seq in SimpleFastaParser(handle_in):\n\n        # If this is one of the genes we are looking for\n        if gene_name in gene_names:\n\n            # Write it out\n            handle_out.write(\">%s\\\\n%s\\\\n\" % (gene_name, gene_seq))\n\n            # Increment the counter\n            n_found += 1\n\n# Report the number of genes which were found\nprint(\"Wrote out %d gene sequences\" % n_found)\nprint(\"DONE\")\n\n\"\"\"",
        "nb_lignes_script": 45,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "gene_csv",
            "gene_fasta_gz"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"",
            "label \"io_limited\""
        ],
        "when": "",
        "stub": ""
    },
    "extractAbund": {
        "name_process": "extractAbund",
        "string_process": "\nprocess extractAbund {\n\n    container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"\n    label \"mem_medium\"\n\n    input:\n    file gene_csv\n    file details_hdf\n    file manifest_csv\n\n    output:\n    file \"${params.output_prefix}.*.csv.gz\"\n\n    publishDir \"${params.output_folder}\", mode: 'copy', overwrite: true\n\n\"\"\"#!/usr/bin/env python3\n\nimport os\nimport pandas as pd\n\n# Set the input paths\n# Make sure that the files are present in the working folder\ngene_csv = '${gene_csv}'\nassert os.path.exists(gene_csv)\ndetails_hdf = '${details_hdf}'\nassert os.path.exists(details_hdf)\nmanifest_csv = '${manifest_csv}'\nassert os.path.exists(manifest_csv)\n\n# Read in the table of gene annotations\ndf = pd.read_csv(gene_csv)\nprint(\"Read in annotations for %d genes\" % df.shape[0])\n\n# Get the list of genes\ngene_names = set(df['gene'].tolist())\n\n# Read in the manifest\nmanifest_df = pd.read_csv(manifest_csv)\n\n# Keep the complete set of gene abundances in long format\noutput = []\n\n# Open a connection to the HDF store\nwith pd.HDFStore(details_hdf, 'r') as store:\n\n    # Set up an object to save the proportion of gene copies in each specimen\n    for specimen_name in manifest_df['specimen'].unique():\n\n        print(\"Reading in abundances for specimen '%s'\" % specimen_name)\n\n        # Read in the full table\n        specimen_df = pd.read_hdf(store, \"/abund/gene/long/%s\" % specimen_name)\n\n        # Get the total depth for all gene copies\n        tot = specimen_df['depth'].sum()\n\n        # Subset to the genes of interest, add the specimen, add the proportion of gene copies, and append to the output\n        output.append(\n            specimen_df.loc[\n                specimen_df['id'].isin(gene_names)\n            ].assign(\n                specimen = specimen_name,\n                prop = lambda d: d['depth'] / tot\n            )\n        )\n\n# Combine all of the output\nprint(\"Combining all outputs\")\noutput = pd.concat(\n    output\n).reset_index(\n    drop=True\n)\n\n# Add the eggNOG annotation\nprint(\"Adding eggNOG names\")\noutput = output.assign(\n    eggNOG_desc = output['id'].apply(\n        df.set_index('gene')['eggNOG_desc'].get\n    )\n)\n\n# Save the long output\nprint(\"Saving long output\")\noutput.to_csv(\n    \"${params.output_prefix}.long.csv.gz\",\n    index=None,\n    compression='gzip'\n)\n\n# Save the wide output\noutput.pivot_table(\n    index=\"specimen\",\n    columns=\"eggNOG_desc\",\n    values=\"prop\",\n    aggfunc=sum\n).fillna(\n    0\n).reset_index(\n).to_csv(\n    \"${params.output_prefix}.wide.csv.gz\",\n    index=None,\n    compression='gzip'\n)\n\nprint(\"DONE\")\n\n\"\"\"\n\n}",
        "nb_lignes_process": 109,
        "string_script": "\"\"\"#!/usr/bin/env python3\n\nimport os\nimport pandas as pd\n\n# Set the input paths\n# Make sure that the files are present in the working folder\ngene_csv = '${gene_csv}'\nassert os.path.exists(gene_csv)\ndetails_hdf = '${details_hdf}'\nassert os.path.exists(details_hdf)\nmanifest_csv = '${manifest_csv}'\nassert os.path.exists(manifest_csv)\n\n# Read in the table of gene annotations\ndf = pd.read_csv(gene_csv)\nprint(\"Read in annotations for %d genes\" % df.shape[0])\n\n# Get the list of genes\ngene_names = set(df['gene'].tolist())\n\n# Read in the manifest\nmanifest_df = pd.read_csv(manifest_csv)\n\n# Keep the complete set of gene abundances in long format\noutput = []\n\n# Open a connection to the HDF store\nwith pd.HDFStore(details_hdf, 'r') as store:\n\n    # Set up an object to save the proportion of gene copies in each specimen\n    for specimen_name in manifest_df['specimen'].unique():\n\n        print(\"Reading in abundances for specimen '%s'\" % specimen_name)\n\n        # Read in the full table\n        specimen_df = pd.read_hdf(store, \"/abund/gene/long/%s\" % specimen_name)\n\n        # Get the total depth for all gene copies\n        tot = specimen_df['depth'].sum()\n\n        # Subset to the genes of interest, add the specimen, add the proportion of gene copies, and append to the output\n        output.append(\n            specimen_df.loc[\n                specimen_df['id'].isin(gene_names)\n            ].assign(\n                specimen = specimen_name,\n                prop = lambda d: d['depth'] / tot\n            )\n        )\n\n# Combine all of the output\nprint(\"Combining all outputs\")\noutput = pd.concat(\n    output\n).reset_index(\n    drop=True\n)\n\n# Add the eggNOG annotation\nprint(\"Adding eggNOG names\")\noutput = output.assign(\n    eggNOG_desc = output['id'].apply(\n        df.set_index('gene')['eggNOG_desc'].get\n    )\n)\n\n# Save the long output\nprint(\"Saving long output\")\noutput.to_csv(\n    \"${params.output_prefix}.long.csv.gz\",\n    index=None,\n    compression='gzip'\n)\n\n# Save the wide output\noutput.pivot_table(\n    index=\"specimen\",\n    columns=\"eggNOG_desc\",\n    values=\"prop\",\n    aggfunc=sum\n).fillna(\n    0\n).reset_index(\n).to_csv(\n    \"${params.output_prefix}.wide.csv.gz\",\n    index=None,\n    compression='gzip'\n)\n\nprint(\"DONE\")\n\n\"\"\"",
        "nb_lignes_script": 92,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "gene_csv",
            "details_hdf",
            "manifest_csv"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"",
            "label \"mem_medium\""
        ],
        "when": "",
        "stub": ""
    },
    "updateFormula": {
        "name_process": "updateFormula",
        "string_process": "\nprocess updateFormula{\n    container \"${container__pandas}\"\n    label 'mem_medium'\n    errorStrategy 'finish'\n\n    input:\n        path results_hdf\n\n    output:\n        path \"*corncob.hdf5\"\n        path \"manifest.csv\"\n        path \"CAG.assignments.csv.gz\"\n\n\"\"\"\n#!/usr/bin/env python3\n\nimport pandas as pd\nimport shutil\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\n# Come up with a new name for the output file\nformula_name = \"${params.formula}\".replace(\n    \" \", \"_\"\n).replace(\n    \"+\", \"_\"\n).replace(\n    \",\", \"_\"\n).replace(\n    \"*\", \"_\"\n).replace(\n    \":\", \"_\"\n).replace(\n    \"__\", \"_\"\n).replace(\n    \"__\", \"_\"\n)\nnew_file_name = \"${params.output_prefix}.%s.corncob.hdf5\" % formula_name\n\n# Copy the input file to a new output file object\nprint(\"Copying %s to %s\" % (\"${results_hdf}\", new_file_name))\nshutil.copyfile(\"${results_hdf}\", new_file_name)\n\n# Open a connection to the store\nwith pd.HDFStore(new_file_name, \"a\") as store:\n    \n    # Read in the existing summary table\n    df = pd.read_hdf(store, \"/summary/experiment\")\n\n    print(\"Existing summary:\")\n    print(df)\n\n    # Add the formula\n    df = pd.concat([\n        df.query(\"variable != 'formula'\"),\n        pd.DataFrame([dict(variable='formula', value='${params.formula}')])\n    ])\n\n    print(\"\")\n    print(\"New summary:\")\n    print(df)\n\n    # Write back to the store\n    df.to_hdf(store, \"/summary/experiment\")\n\n    # Extract the manifest\n    print(\"Extracting the manifest\")\n    pd.read_hdf(store, \"/manifest\").to_csv(\"manifest.csv\")\n\n    # Extract the table of genes making up each CAG\n    print(\"Extracting the gene~CAG table\")\n    pd.read_hdf(\n        store, \n        \"/annot/gene/all\",\n        columns=[\"gene\", \"CAG\"]\n    ).to_csv(\n        \"CAG.assignments.csv.gz\",\n        index=None\n    )\n\nprint(\"Done\")\n\"\"\"\n\n}",
        "nb_lignes_process": 83,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport pandas as pd\nimport shutil\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\n# Come up with a new name for the output file\nformula_name = \"${params.formula}\".replace(\n    \" \", \"_\"\n).replace(\n    \"+\", \"_\"\n).replace(\n    \",\", \"_\"\n).replace(\n    \"*\", \"_\"\n).replace(\n    \":\", \"_\"\n).replace(\n    \"__\", \"_\"\n).replace(\n    \"__\", \"_\"\n)\nnew_file_name = \"${params.output_prefix}.%s.corncob.hdf5\" % formula_name\n\n# Copy the input file to a new output file object\nprint(\"Copying %s to %s\" % (\"${results_hdf}\", new_file_name))\nshutil.copyfile(\"${results_hdf}\", new_file_name)\n\n# Open a connection to the store\nwith pd.HDFStore(new_file_name, \"a\") as store:\n    \n    # Read in the existing summary table\n    df = pd.read_hdf(store, \"/summary/experiment\")\n\n    print(\"Existing summary:\")\n    print(df)\n\n    # Add the formula\n    df = pd.concat([\n        df.query(\"variable != 'formula'\"),\n        pd.DataFrame([dict(variable='formula', value='${params.formula}')])\n    ])\n\n    print(\"\")\n    print(\"New summary:\")\n    print(df)\n\n    # Write back to the store\n    df.to_hdf(store, \"/summary/experiment\")\n\n    # Extract the manifest\n    print(\"Extracting the manifest\")\n    pd.read_hdf(store, \"/manifest\").to_csv(\"manifest.csv\")\n\n    # Extract the table of genes making up each CAG\n    print(\"Extracting the gene~CAG table\")\n    pd.read_hdf(\n        store, \n        \"/annot/gene/all\",\n        columns=[\"gene\", \"CAG\"]\n    ).to_csv(\n        \"CAG.assignments.csv.gz\",\n        index=None\n    )\n\nprint(\"Done\")\n\"\"\"",
        "nb_lignes_script": 68,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "results_hdf"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"${container__pandas}\"",
            "label 'mem_medium'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "linclust": {
        "name_process": "linclust",
        "string_process": "\nprocess linclust {\n    tag \"Cluster genes with similar sequences\"\n    container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"\n    label 'mem_medium'\n    errorStrategy 'finish'\n    \n    input:\n    file \"input.genes.*.fasta.gz\"\n    \n    output:\n    file \"output.genes.fasta.gz\"\n    \n\"\"\"\n#!/bin/bash\n\nset -e\n\n# Combine input files\necho \"Combining input files\"\ncat input.genes.* > input.genes.fasta.gz\n\n# Don't run linclust if the file is extremely small\n# In this case, the subsequent DIAMOND deduplication\n# will be sufficient.\n# Also, linclust seems to break on very small input files\n\nif (( \\$(gunzip -c input.genes.fasta.gz | grep -v '>' | wc -c ) < 1000000 )); then\n\n    echo \"Input files are < 1M characters -- skipping linclust\"\n    mv input.genes.fasta.gz output.genes.fasta.gz\n\nelse\n\n    # Make the MMSeqs2 database\n    echo \"Running linclust\"\n    mmseqs createdb input.genes.fasta.gz db\n\n    # Cluster the protein sequences\n    mmseqs linclust db cluster_db ./ \\\n        --min-seq-id ${params.min_identity / 100} \\\n        --max-seqs 100000 \\\n        -c ${params.min_coverage / 100}\n\n    # Get the representative sequences\n    mmseqs result2repseq db cluster_db genes\n    mmseqs result2flat db db genes output.genes.fasta --use-fasta-header\n    echo \"Compressing\"\n    gzip output.genes.fasta\n\n    echo \"Done\"\n\nfi\n\n\"\"\"\n}",
        "nb_lignes_process": 54,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\n# Combine input files\necho \"Combining input files\"\ncat input.genes.* > input.genes.fasta.gz\n\n# Don't run linclust if the file is extremely small\n# In this case, the subsequent DIAMOND deduplication\n# will be sufficient.\n# Also, linclust seems to break on very small input files\n\nif (( \\$(gunzip -c input.genes.fasta.gz | grep -v '>' | wc -c ) < 1000000 )); then\n\n    echo \"Input files are < 1M characters -- skipping linclust\"\n    mv input.genes.fasta.gz output.genes.fasta.gz\n\nelse\n\n    # Make the MMSeqs2 database\n    echo \"Running linclust\"\n    mmseqs createdb input.genes.fasta.gz db\n\n    # Cluster the protein sequences\n    mmseqs linclust db cluster_db ./ \\\n        --min-seq-id ${params.min_identity / 100} \\\n        --max-seqs 100000 \\\n        -c ${params.min_coverage / 100}\n\n    # Get the representative sequences\n    mmseqs result2repseq db cluster_db genes\n    mmseqs result2flat db db genes output.genes.fasta --use-fasta-header\n    echo \"Compressing\"\n    gzip output.genes.fasta\n\n    echo \"Done\"\n\nfi\n\n\"\"\"",
        "nb_lignes_script": 41,
        "language_script": "bash",
        "tools": [
            "MMseqs"
        ],
        "tools_url": [
            "https://bio.tools/mmseqs"
        ],
        "tools_dico": [
            {
                "name": "MMseqs",
                "uri": "https://bio.tools/mmseqs",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0623",
                            "term": "Gene and protein families"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0623",
                            "term": "Genes, gene family or system"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0291",
                                    "term": "Sequence clustering"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0291",
                                    "term": "Sequence cluster construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0291",
                                    "term": "Sequence cluster generation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Software suite for very fast protein sequence searches and clustering of huge protein sequence data sets.",
                "homepage": "https://github.com/soedinglab/MMseqs"
            }
        ],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Cluster genes with similar sequences\"",
            "container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"",
            "label 'mem_medium'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "diamondDedup": {
        "name_process": "diamondDedup",
        "string_process": "\nprocess diamondDedup {\n    tag \"Deduplicate sequences by alignment with DIAMOND\"\n    container \"quay.io/fhcrc-microbiome/famli:v1.5\"\n    label 'mem_veryhigh'\n    errorStrategy 'finish'\n    \n    input:\n    file \"input.genes.fasta.gz\"\n    \n    output:\n    file \"output.genes.fasta.gz\"\n    \n\"\"\"\n#!/bin/bash\n\nset -e\n\n# Make a DIAMOND database\ndiamond \\\n    makedb \\\n    --in input.genes.fasta.gz \\\n    --db input.genes.dmnd \\\n    --threads ${task.cpus}\n\n# Align the genes against themselves and filter any which align\ndiamond \\\n    blastp \\\n    --query input.genes.fasta.gz \\\n    --threads ${task.cpus} \\\n    --db input.genes.dmnd \\\n    --out input.genes.aln \\\n    --outfmt 6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qlen slen \\\n    --query-cover 90 \\\n    --id ${params.min_identity} \\\n    --top 0 \\\n    --block-size ${task.memory.toMega() / (1024 * 6)} \\\n    --unal 0\n\npython << END\n\nfrom Bio.SeqIO.FastaIO import SimpleFastaParser\nimport gzip\nimport sys\n\n# Keep track of the genes which have been filtered out\nduplicate_genes = set([])\n\n# Iterate over the alignment\nprint(\"Reading alignments\")\nix = 0\nfor line in open(\"input.genes.aln\", \"r\"):\n\n    ix += 1\n\n    if ix % 100000 == 0:\n        print(\"Read %d lines of alignment - found %d duplicated genes\" % (ix, len(duplicate_genes)))\n\n    qname, sname, _, _, _, _, _, _, _, _, _, _, qlen, slen = line.rstrip(\"\\\\n\").split(\"\\\\t\")\n\n    # Skip self alignments\n    if qname == sname:\n        continue\n    # If we have excluded either of the genes before, skip the line\n    if qname in duplicate_genes or sname in duplicate_genes:\n        continue\n    # For non-self alignments, remove the smaller of the two\n    if int(slen) < int(qlen):\n        duplicate_genes.add(sname)\n    else:\n        duplicate_genes.add(qname)\n\nassert ix > 0, \"Didn't read any alignments\"\nprint(\"Read %d lines of alignment - found %d duplicated genes\" % (ix, len(duplicate_genes)))\nprint(\"Done reading alignments\")\n\n# Now let's make the filtered FASTA with all duplicated genes removed\nn_found = 0\nn = 0\nwith gzip.open(\"input.genes.fasta.gz\", \"rt\") as handle_in:\n    with gzip.open(\"output.genes.fasta.gz\", \"wt\") as handle_out:\n        for header, seq in SimpleFastaParser(handle_in):\n\n            header = header.split(\" \", 1)[0]\n\n            n += 1\n            if header in duplicate_genes:\n                n_found += 1\n            else:\n                handle_out.write(\">%s\\\\n%s\\\\n\" % (header, seq))\n\n# Make sure that we encountered all of the duplicated genes\nprint(\"Read in %d sequences, filtered out %d, wrote out the rest\" % (n, n_found))\nassert n_found == len(duplicate_genes), \"%d != %d\" % (n_found, len(duplicate_genes))\n\nEND\n\necho \"Done\"\n\"\"\"\n}",
        "nb_lignes_process": 98,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\n# Make a DIAMOND database\ndiamond \\\n    makedb \\\n    --in input.genes.fasta.gz \\\n    --db input.genes.dmnd \\\n    --threads ${task.cpus}\n\n# Align the genes against themselves and filter any which align\ndiamond \\\n    blastp \\\n    --query input.genes.fasta.gz \\\n    --threads ${task.cpus} \\\n    --db input.genes.dmnd \\\n    --out input.genes.aln \\\n    --outfmt 6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qlen slen \\\n    --query-cover 90 \\\n    --id ${params.min_identity} \\\n    --top 0 \\\n    --block-size ${task.memory.toMega() / (1024 * 6)} \\\n    --unal 0\n\npython << END\n\nfrom Bio.SeqIO.FastaIO import SimpleFastaParser\nimport gzip\nimport sys\n\n# Keep track of the genes which have been filtered out\nduplicate_genes = set([])\n\n# Iterate over the alignment\nprint(\"Reading alignments\")\nix = 0\nfor line in open(\"input.genes.aln\", \"r\"):\n\n    ix += 1\n\n    if ix % 100000 == 0:\n        print(\"Read %d lines of alignment - found %d duplicated genes\" % (ix, len(duplicate_genes)))\n\n    qname, sname, _, _, _, _, _, _, _, _, _, _, qlen, slen = line.rstrip(\"\\\\n\").split(\"\\\\t\")\n\n    # Skip self alignments\n    if qname == sname:\n        continue\n    # If we have excluded either of the genes before, skip the line\n    if qname in duplicate_genes or sname in duplicate_genes:\n        continue\n    # For non-self alignments, remove the smaller of the two\n    if int(slen) < int(qlen):\n        duplicate_genes.add(sname)\n    else:\n        duplicate_genes.add(qname)\n\nassert ix > 0, \"Didn't read any alignments\"\nprint(\"Read %d lines of alignment - found %d duplicated genes\" % (ix, len(duplicate_genes)))\nprint(\"Done reading alignments\")\n\n# Now let's make the filtered FASTA with all duplicated genes removed\nn_found = 0\nn = 0\nwith gzip.open(\"input.genes.fasta.gz\", \"rt\") as handle_in:\n    with gzip.open(\"output.genes.fasta.gz\", \"wt\") as handle_out:\n        for header, seq in SimpleFastaParser(handle_in):\n\n            header = header.split(\" \", 1)[0]\n\n            n += 1\n            if header in duplicate_genes:\n                n_found += 1\n            else:\n                handle_out.write(\">%s\\\\n%s\\\\n\" % (header, seq))\n\n# Make sure that we encountered all of the duplicated genes\nprint(\"Read in %d sequences, filtered out %d, wrote out the rest\" % (n, n_found))\nassert n_found == len(duplicate_genes), \"%d != %d\" % (n_found, len(duplicate_genes))\n\nEND\n\necho \"Done\"\n\"\"\"",
        "nb_lignes_script": 85,
        "language_script": "bash",
        "tools": [
            "Diamond",
            "BLASTP-ACC",
            "FROMP",
            "tximport",
            "Mix",
            "GPU-CASSERT",
            "ENdb"
        ],
        "tools_url": [
            "https://bio.tools/diamond",
            "https://bio.tools/BLASTP-ACC",
            "https://bio.tools/fromp",
            "https://bio.tools/tximport",
            "https://bio.tools/mix",
            "https://bio.tools/gpu-cassert",
            "https://bio.tools/ENdb"
        ],
        "tools_dico": [
            {
                "name": "Diamond",
                "uri": "https://bio.tools/diamond",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0258",
                                    "term": "Sequence alignment analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Sequence aligner for protein and translated DNA searches and functions as a drop-in replacement for the NCBI BLAST software tools. It is suitable for protein-protein search as well as DNA-protein search on short reads and longer sequences including contigs and assemblies, providing a speedup of BLAST ranging up to x20,000.",
                "homepage": "https://github.com/bbuchfink/diamond"
            },
            {
                "name": "BLASTP-ACC",
                "uri": "https://bio.tools/BLASTP-ACC",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3297",
                            "term": "Biotechnology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Structure analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Structural bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Biomolecular structure"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0495",
                                    "term": "Local alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2421",
                                    "term": "Database search"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3802",
                                    "term": "Sorting"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0495",
                                    "term": "Local sequence alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0495",
                                    "term": "Sequence alignment (local)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2421",
                                    "term": "Search"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Parallel Architecture and Hardware Accelerator Design for BLAST-based Protein Sequence Alignment.\n\nIn this study, we design a hardware accelerator for a widely used sequence alignment algorithm, the basic local alignment search tool for proteins (BLASTP). The architecture of the proposed accelerator consists of five stages: a new systolic-array-based one-hit finding stage, a novel RAM-REG-based two-hit finding stage, a refined ungapped extension stage, a faster gapped extension stage, and a highly efficient parallel sorter. The system is implemented on an Altera Stratix V FPGA with a processing speed of more than 500 giga cell updates per second (GCUPS). It can receive a query sequence, compare it with the sequences in the database, and generate a list sorted in descending order of the similarity scores between the query sequence and the subject sequences.\n\n||| HOMEPAGE MISSING!.\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'accelerator', 'Altera', 'Stratix', 'RAM-REG-based'",
                "homepage": "https://www.ncbi.nlm.nih.gov/pubmed/?term=31581096"
            },
            {
                "name": "FROMP",
                "uri": "https://bio.tools/fromp",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2259",
                            "term": "Systems biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0602",
                            "term": "Molecular interactions, pathways and networks"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3172",
                            "term": "Metabolomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3660",
                                    "term": "Metabolic network modelling"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3660",
                                    "term": "http://edamontology.org/Metabolic%20pathway%20modelling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Software for mapping and visualizing enzyme annotations onto the Kyoto Encyclopedia of Genes and Genomes (KEGG) metabolic pathways or custom-made pathways and comparing the samples in terms of their Pathway Completeness Scores, their relative Activity Scores or enzyme enrichment odds ratios.",
                "homepage": "https://github.com/LaRocheLab/FROMP"
            },
            {
                "name": "tximport",
                "uri": "https://bio.tools/tximport",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "Gene transcripts"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Workflows"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "mRNA features"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Pipelines"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3680",
                                    "term": "RNA-Seq analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2497",
                                    "term": "Pathway or network analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An R/Bioconductor package that imports transcript-level abundance, estimated counts and transcript lengths, and summarizes into matrices for use with downstream gene-level analysis packages.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/tximport.html"
            },
            {
                "name": "Mix",
                "uri": "https://bio.tools/mix",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Tool that combines two or more draft assemblies, without relying on a reference genome and has the goal to reduce contig fragmentation and thus speed-up genome finishing. The algorithm builds an extension graph where vertices represent extremities of contigs and edges represent existing alignments between these extremities. These alignment edges are used for contig extension. The resulting output assembly corresponds to a path in the extension graph that maximizes the cumulative contig length.",
                "homepage": "https://github.com/cbib/MIX"
            },
            {
                "name": "GPU-CASSERT",
                "uri": "https://bio.tools/gpu-cassert",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_1317",
                            "term": "Structural biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Structure analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Structural bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Biomolecular structure"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0295",
                                    "term": "Structure alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0295",
                                    "term": "Structural alignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The GPU-based implementation of the CASSERT algorithm for protein 3D structure similarity searching. The algorithm is based on the two-phase alignment of protein structures when matching fragments of compared proteins.",
                "homepage": "http://zti.polsl.pl/dmrozek/science/gpucassert/cassert.htm"
            },
            {
                "name": "ENdb",
                "uri": "https://bio.tools/ENdb",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0219",
                            "term": "Data submission, annotation and curation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2885",
                            "term": "DNA polymorphism"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0749",
                            "term": "Transcription factors and regulatory sites"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "Gene transcripts"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "mRNA features"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0440",
                                    "term": "Promoter prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Query and retrieval"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression data analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Database retrieval"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A manually curated database of experimentally supported enhancers for human and mouse. Enhancers are a class of cis-regulatory elements that can increase gene transcription by forming loops in intergenic regions, introns and exons",
                "homepage": "http://www.licpathway.net/ENdb"
            }
        ],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Deduplicate sequences by alignment with DIAMOND\"",
            "container \"quay.io/fhcrc-microbiome/famli:v1.5\"",
            "label 'mem_veryhigh'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "joinFASTQ": {
        "name_process": "joinFASTQ",
        "string_process": "\nprocess joinFASTQ {\n    tag \"Join FASTQ files per-specimen\"\n    container \"${container__fastatools}\"\n    label = 'mem_medium'\n    errorStrategy 'finish'\n    maxRetries 10\n\n                                                                                        \n    publishDir path: \"${params.output_folder}qc/\", enabled: params.savereads, mode: \"copy\"\n\n    input:\n    tuple val(sample), file(\"R1.*.fastq.gz\"), file(\"R2.*.fastq.gz\")\n    \n    output:\n    tuple val(sample), file(\"${sample}.R1.fastq.gz\"), file(\"${sample}.R2.fastq.gz\")\n\n\"\"\"\nset -e\n\nls -lah *\n\ncombine_fastq_pairs.py \\\n-1 R1*fastq.gz \\\n-2 R2*fastq.gz \\\n--normalize-ids \\\n-o1 \"${sample}.R1.fastq.gz\" \\\n-o2 \"${sample}.R2.fastq.gz\"\n\n(( \\$(gunzip -c \"${sample}.R1.fastq.gz\" | head | wc -l) > 1 ))\n(( \\$(gunzip -c \"${sample}.R2.fastq.gz\" | head | wc -l) > 1 ))\n\n\"\"\"\n\n}",
        "nb_lignes_process": 33,
        "string_script": "\"\"\"\nset -e\n\nls -lah *\n\ncombine_fastq_pairs.py \\\n-1 R1*fastq.gz \\\n-2 R2*fastq.gz \\\n--normalize-ids \\\n-o1 \"${sample}.R1.fastq.gz\" \\\n-o2 \"${sample}.R2.fastq.gz\"\n\n(( \\$(gunzip -c \"${sample}.R1.fastq.gz\" | head | wc -l) > 1 ))\n(( \\$(gunzip -c \"${sample}.R2.fastq.gz\" | head | wc -l) > 1 ))\n\n\"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample"
        ],
        "nb_inputs": 1,
        "outputs": [
            "sample"
        ],
        "nb_outputs": 1,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Join FASTQ files per-specimen\"",
            "container \"${container__fastatools}\"",
            "label = 'mem_medium'",
            "errorStrategy 'finish'",
            "maxRetries 10",
            "publishDir path: \"${params.output_folder}qc/\", enabled: params.savereads, mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    },
    "outputManifest": {
        "name_process": "outputManifest",
        "string_process": "\nprocess outputManifest {\n    container \"${container__ubuntu}\"\n\n    publishDir path: \"${params.output_folder}qc/\", enabled: params.savereads, mode: \"copy\"\n\n    input:\n        val manifestStr\n    \n    output:\n        file 'manifest.qc.csv'\n\n    \"\"\"\n        echo \"${manifestStr}\" > manifest.qc.csv\n    \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "\"\"\"\n        echo \"${manifestStr}\" > manifest.qc.csv\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "manifestStr"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"${container__ubuntu}\"",
            "publishDir path: \"${params.output_folder}qc/\", enabled: params.savereads, mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    },
    "countReads": {
        "name_process": "countReads",
        "string_process": "\nprocess countReads {\n    tag \"Count the number of reads per sample\"\n    container \"${container__fastatools}\"\n    cpus 1\n    memory \"4 GB\"\n    errorStrategy \"finish\"\n\n    input:\n    tuple sample_name, file(R1), file(R2)\n\n    output:\n    file \"${sample_name}.countReads.csv\"\n\n\"\"\"\nset -e\n\n[[ -s ${R1} ]]\n[[ -s ${R2} ]]\n\nn=\\$(cat <(gunzip -c \"${R1}\") <(gunzip -c \"${R2}\") | awk 'NR % 4 == 1' | wc -l)\necho \"${sample_name},\\$n\" > \"${sample_name}.countReads.csv\"\n\"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "\"\"\"\nset -e\n\n[[ -s ${R1} ]]\n[[ -s ${R2} ]]\n\nn=\\$(cat <(gunzip -c \"${R1}\") <(gunzip -c \"${R2}\") | awk 'NR % 4 == 1' | wc -l)\necho \"${sample_name},\\$n\" > \"${sample_name}.countReads.csv\"\n\"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "R1",
            "R2",
            "sample_name"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Count the number of reads per sample\"",
            "container \"${container__fastatools}\"",
            "cpus 1",
            "memory \"4 GB\"",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "countReadsSummary": {
        "name_process": "countReadsSummary",
        "string_process": "\nprocess countReadsSummary {\n    tag \"Summarize the number of reads per sample\"\n    container \"${container__fastatools}\"\n                                                                                               \n    publishDir \"${params.output_folder}/qc/\", mode: 'copy'\n    errorStrategy \"finish\"\n\n    input:\n                                                                                                          \n    file readcount_csv_list\n\n    output:\n    file \"readcounts.csv\"\n\n\n\"\"\"\nset -e\n\necho specimen,n_reads > readcounts.csv\ncat ${readcount_csv_list} >> readcounts.csv\n\"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\nset -e\n\necho specimen,n_reads > readcounts.csv\ncat ${readcount_csv_list} >> readcounts.csv\n\"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "readcount_csv_list"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Summarize the number of reads per sample\"",
            "container \"${container__fastatools}\"",
            "publishDir \"${params.output_folder}/qc/\", mode: 'copy'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "concatenateFiles": {
        "name_process": "concatenateFiles",
        "string_process": "\nprocess concatenateFiles {\n    tag \"Directly combine a group of files\"\n    container \"${container__ubuntu}\"\n    label \"mem_medium\"\n    errorStrategy \"finish\"\n    \n    input:\n    file \"__INPUT*\"\n    val output_name\n\n    output:\n    file \"${output_name}\"\n\n\"\"\"\n# Break on any errors\nset -e\n\ncat __INPUT* > ${output_name}\n\"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "\"\"\"\n# Break on any errors\nset -e\n\ncat __INPUT* > ${output_name}\n\"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "output_name"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Directly combine a group of files\"",
            "container \"${container__ubuntu}\"",
            "label \"mem_medium\"",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "collectAbundances": {
        "name_process": "collectAbundances",
        "string_process": "\nprocess collectAbundances{\n    tag \"Add gene abundances to HDF\"\n    container \"${container__experiment_collection}\"\n    label 'mem_veryhigh'\n    errorStrategy 'finish'\n\n    input:\n        path cag_csv\n        path cag_abund_feather\n        path readcount_csv\n        path manifest_csv\n        path specimen_gene_count_csv\n        path specimen_reads_aligned_csv\n        path gene_length_csv\n        path breakaway_csv\n\n    output:\n        path \"${params.output_prefix}.results.hdf5\"\n\n\"\"\"\n#!/usr/bin/env python3\n\nimport gzip\nimport json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.stats.mstats import gmean\nfrom scipy.stats import entropy\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\ncag_csv = \"${cag_csv}\"\ncag_abund_feather = \"${cag_abund_feather}\"\nreadcount_csv = \"${readcount_csv}\"\nspecimen_gene_count_csv = \"${specimen_gene_count_csv}\"\nspecimen_reads_aligned_csv = \"${specimen_reads_aligned_csv}\"\ngene_length_csv = \"${gene_length_csv}\"\nbreakaway_csv = \"${breakaway_csv}\"\nmanifest_csv = \"${manifest_csv}\"\nsuffix=\".json.gz\"\n\n# Keep a list of descriptive statistics\nsummary_dict = dict([\n    (\"formula\", \"${params.formula}\"),\n    (\"distance_threshold\", \"${params.distance_threshold}\"),\n    (\"distance_threshold\", \"${params.distance_threshold}\"),\n    (\"linkage_type\", \"${params.linkage_type}\"),\n    (\"sd_mean_cutoff\", ${params.sd_mean_cutoff}),\n    (\"min_identity\", ${params.min_identity}),\n    (\"min_coverage\", ${params.min_coverage}),\n    (\"dmnd_min_identity\", ${params.dmnd_min_identity}),\n    (\"dmnd_min_coverage\", ${params.dmnd_min_coverage})\n])\n\n# Perform different types of ordination and include them in the output HDF\ndef run_ordination(abund_df, ordination_type):\n\n    if ordination_type == \"pca\":\n        return run_pca(abund_df)\n    elif ordination_type == \"tsne\":\n        return run_tsne(abund_df)\n    else:\n        assert \"No code available to run ordination type: %s\" % ordination_type\n\n# Dimensionality reduction with PCA\ndef run_pca(abund_df):\n    \n    # Initialize the PCA object\n    pca = PCA()\n    \n    # Fit to the data\n    pca.fit(abund_df)\n\n    # Make an output DataFrame\n    return pd.DataFrame(\n        pca.transform(\n            abund_df\n        ),\n        index=abund_df.index.values,\n        columns=[\n            \"PC%d (%s%s)\" % (\n                ix + 1,\n                round(100 * r, 1) if r > 0.01 else \"%.1E\" % (100 * r),\n                '%'\n            )\n            for ix, r in enumerate(\n                pca.explained_variance_ratio_\n            )\n        ]\n    ).reset_index(\n    ).rename(columns=dict([('index', 'specimen')]))\n\n# Dimensionality reduction with t-SNE\ndef run_tsne(abund_df, n_components=2):\n    \n    # Initialize the TSNE object\n    tsne = TSNE(\n        n_components=n_components\n    )\n\n    # Make an output DataFrame with the transformed data\n    return pd.DataFrame(\n        tsne.fit_transform(\n            abund_df\n        ),\n        index=abund_df.index.values,\n        columns=[\n            \"t-SNE %d\" % (\n                ix + 1\n            )\n            for ix in range(n_components)\n        ]\n    ).reset_index(\n    ).rename(columns=dict([('index', 'specimen')]))\n\n# Calculate pairwise distances (input has specimens in rows)\ndef calc_pdist(abund_df, metric=\"euclidean\"):\n    print(\"Calculating pairwise %s distances over %d samples and %d CAGs\" % (metric, abund_df.shape[0], abund_df.shape[1]))\n    \n    # Make an output DataFrame with the pairwise distances\n    return pd.DataFrame(\n        squareform(\n            pdist(\n                abund_df if metric != \"aitchison\" else clr_transform(abund_df),\n                metric=metric if metric != \"aitchison\" else \"euclidean\"\n            )\n        ),\n        index=abund_df.index.values,\n        columns=abund_df.index.values\n    ).reset_index(\n    ).rename(columns=dict([('index', 'specimen')]))\n\n# Calculate CLR-transformed abundances (input has specimens in rows)\ndef clr_transform(abund_df):\n    print(\"Calculating CLR over %d samples and %d CAGs\" % (abund_df.shape[0], abund_df.shape[1]))\n\n    # Make the input\n    specimen_col_abund_df = abund_df.T\n\n    # Calculate the geometric mean for every specimen\n    specimen_gmean = specimen_col_abund_df.apply(\n        lambda c: gmean(c.loc[c > 0])\n    )\n\n    # Find the global minimum value\n    min_abund = specimen_col_abund_df.apply(\n        lambda c: c.loc[c > 0].min()\n    ).min()\n\n    print(\"Minimum value is %d\" % min_abund)\n\n    # Fill in the minimum value\n    specimen_col_abund_df = specimen_col_abund_df.clip(\n        lower=min_abund\n    )\n\n    # Divide by the geometric mean\n    specimen_col_abund_df = specimen_col_abund_df / specimen_gmean\n    \n    # Take the log and return\n    return specimen_col_abund_df.applymap(np.log10).T\n\n# Read in the table with gene lengths\ngene_length_df = pd.read_csv(gene_length_csv).set_index(\"gene\")[\"length\"]\n\n# Open a connection to the HDF5\nwith pd.HDFStore(\"${params.output_prefix}.results.hdf5\", \"w\") as store:\n\n    # Read in the summary of the number of reads across all samples\n    readcount_df = pd.read_csv(readcount_csv)\n\n    # Read in the number of aligned reads per sample\n    aligned_reads_dict = pd.read_csv(\n        specimen_reads_aligned_csv\n    ).set_index(\n        \"specimen\"\n    )[\"n_reads_aligned\"]\n\n    # Add some descriptive statistics\n    summary_dict[\"num_samples\"] = readcount_df.shape[0]\n    summary_dict[\"total_reads\"] = readcount_df[\"n_reads\"].sum()\n    summary_dict[\"aligned_reads\"] = aligned_reads_dict.sum()\n\n    # Add the number of aligned reads per sample\n    readcount_df[\"aligned_reads\"] = readcount_df[\"specimen\"].apply(\n        aligned_reads_dict.get\n    )\n\n    # Write to HDF5\n    readcount_df.to_hdf(store, \"/summary/readcount\")\n\n    # Read in the summary of the number of genes detected by alignment per sample\n    specimen_gene_count_df = pd.read_csv(specimen_gene_count_csv)\n\n    # Write to HDF5\n    specimen_gene_count_df.to_hdf(store, \"/summary/genes_aligned\")\n\n    # Read in the gene richness estimate generated with breakaway\n    breakaway_df = pd.read_csv(breakaway_csv)\n\n    # Write to HDF5\n    breakaway_df.to_hdf(store, \"/summary/breakaway\")\n\n    # Write out a combined table\n    pd.concat(\n        [\n            readcount_df.set_index(\"specimen\"),\n            specimen_gene_count_df.set_index(\"specimen\"),\n            breakaway_df.set_index(\"specimen\")\n        ], \n        axis = 1, \n        sort = True\n    ).reset_index(\n    ).rename(\n        columns = dict([\n            (\"index\", \"specimen\")\n        ])\n    ).to_hdf(\n        store,\n        \"/summary/all\"\n    )\n\n    # Read in the table with the CAG-level abundances across all samples\n    cag_abund_df = pd.read_feather(\n        cag_abund_feather\n    ).rename(\n        columns={\"index\": \"CAG\"}\n    )\n    print(\n        \"Read in abundances for %d CAGs across %d samples\" %\n        (cag_abund_df.shape[0], cag_abund_df.shape[1] - 1)\n    )\n    # Add some descriptive statistics\n    summary_dict[\"num_cags\"] = cag_abund_df.shape[0]\n\n\n    # Write to HDF5\n    cag_abund_df.to_hdf(\n        store, \n        \"/abund/cag/wide\",\n        format = \"table\",\n        data_columns = [\"CAG\"],\n        complevel = 5\n    )\n\n    # Perform ordination, both PCA and t-SNE and write to store\n    for ordination_type in [\"pca\", \"tsne\"]:\n\n        # The ordination methods expect samples to be in rows\n        # and so we need to rotate the /abund/cags/wide object\n        run_ordination(\n            cag_abund_df.set_index(\"CAG\").T,\n            ordination_type\n        ).to_hdf(\n            store,\n            \"/ordination/%s\" % ordination_type\n        )\n\n    # Calculate pairwise distances and write to store\n    for metric in [\"euclidean\", \"braycurtis\", \"jaccard\", \"aitchison\"]:\n\n        # Calculate pairwise distances and write to the store\n        calc_pdist(\n            cag_abund_df.set_index(\"CAG\").T,\n            metric\n        ).to_hdf(\n            store,\n            \"/distances/%s\" % metric\n        )\n\n    # Read in the table describing which genes are grouped into which CAGs\n    # This is being called 'cag_df', but it's really a table of CAG annotations per-gene,\n    # so there is one row per gene.\n    cag_df = pd.read_csv(cag_csv)\n\n    print(\n        \"Read in CAG assignments for %d genes across %d CAGs\" % \n        (cag_df.shape[0], cag_df[\"CAG\"].unique().shape[0])\n    )\n\n    # Save the total number of genes\n    summary_dict[\"num_genes\"] = cag_df.shape[0]\n\n    # Write to HDF5\n    cag_df.to_hdf(\n        store, \n        \"/annot/gene/cag\",\n        format = \"table\",\n        data_columns = [\"gene\", \"CAG\"]\n    )\n\n    # Add that information on the gene abundance and prevalence to this gene summary table\n    cag_df = cag_df.assign(\n        length = cag_df[\"gene\"].apply(gene_length_df.get)\n    )\n\n    # Now create the `/annot/gene/all` table, which will be added to later\n    # with the taxonomic and functional annotations, if those are performed\n    cag_df.to_hdf(\n        store, \n        \"/annot/gene/all\",\n        format = \"table\",\n        data_columns = [\"gene\", \"CAG\"]\n    )\n\n    # Make a summary table describing each CAG with size, mean_abundance, and prevalence\n    # Save it in the HDF5 as \"/annot/cag/all\"\n    cag_abund_df.set_index(\"CAG\", inplace=True)\n    cag_summary_df = pd.DataFrame(dict([\n        (\"size\", cag_df[\"CAG\"].value_counts()),\n        (\"prevalence\", (cag_abund_df > 0).mean(axis=1)),\n        (\"mean_abundance\", cag_abund_df.mean(axis=1)),\n        (\"std_abundance\", cag_abund_df.std(axis=1)),\n        (\"entropy\", cag_abund_df.apply(entropy, axis=1)),\n    ])).reset_index(\n    ).rename(\n        columns=dict([(\"index\", \"CAG\")])\n    ).to_hdf(\n        store,\n        \"/annot/cag/all\"\n    )\n\n    # Write out the descriptive statistics\n    pd.DataFrame([\n        dict([\n            (\"variable\", k),\n            (\"value\", v)\n        ])\n        for k, v in summary_dict.items()\n    ]).to_hdf(\n        store,\n        \"/summary/experiment\"\n    )\n\n    # Read in the manifest provided by the user\n    manifest_df = pd.read_csv(\n        \"${manifest_csv}\"\n    )\n    # Drop the columns with paths to reads\n    for col_name in [\"R1\", \"R2\", \"I1\", \"I2\"]:\n        if col_name in manifest_df.columns.values:\n            manifest_df = manifest_df.drop(columns=col_name)\n\n    # Drop duplicated rows (multiple read pairs from the same set of samples)\n    manifest_df = manifest_df.drop_duplicates()\n\n    # Write out the manifest provided by the user\n    manifest_df.to_hdf(store, \"/manifest\")\n\n\"\"\"\n\n}",
        "nb_lignes_process": 354,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport gzip\nimport json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.stats.mstats import gmean\nfrom scipy.stats import entropy\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\ncag_csv = \"${cag_csv}\"\ncag_abund_feather = \"${cag_abund_feather}\"\nreadcount_csv = \"${readcount_csv}\"\nspecimen_gene_count_csv = \"${specimen_gene_count_csv}\"\nspecimen_reads_aligned_csv = \"${specimen_reads_aligned_csv}\"\ngene_length_csv = \"${gene_length_csv}\"\nbreakaway_csv = \"${breakaway_csv}\"\nmanifest_csv = \"${manifest_csv}\"\nsuffix=\".json.gz\"\n\n# Keep a list of descriptive statistics\nsummary_dict = dict([\n    (\"formula\", \"${params.formula}\"),\n    (\"distance_threshold\", \"${params.distance_threshold}\"),\n    (\"distance_threshold\", \"${params.distance_threshold}\"),\n    (\"linkage_type\", \"${params.linkage_type}\"),\n    (\"sd_mean_cutoff\", ${params.sd_mean_cutoff}),\n    (\"min_identity\", ${params.min_identity}),\n    (\"min_coverage\", ${params.min_coverage}),\n    (\"dmnd_min_identity\", ${params.dmnd_min_identity}),\n    (\"dmnd_min_coverage\", ${params.dmnd_min_coverage})\n])\n\n# Perform different types of ordination and include them in the output HDF\ndef run_ordination(abund_df, ordination_type):\n\n    if ordination_type == \"pca\":\n        return run_pca(abund_df)\n    elif ordination_type == \"tsne\":\n        return run_tsne(abund_df)\n    else:\n        assert \"No code available to run ordination type: %s\" % ordination_type\n\n# Dimensionality reduction with PCA\ndef run_pca(abund_df):\n    \n    # Initialize the PCA object\n    pca = PCA()\n    \n    # Fit to the data\n    pca.fit(abund_df)\n\n    # Make an output DataFrame\n    return pd.DataFrame(\n        pca.transform(\n            abund_df\n        ),\n        index=abund_df.index.values,\n        columns=[\n            \"PC%d (%s%s)\" % (\n                ix + 1,\n                round(100 * r, 1) if r > 0.01 else \"%.1E\" % (100 * r),\n                '%'\n            )\n            for ix, r in enumerate(\n                pca.explained_variance_ratio_\n            )\n        ]\n    ).reset_index(\n    ).rename(columns=dict([('index', 'specimen')]))\n\n# Dimensionality reduction with t-SNE\ndef run_tsne(abund_df, n_components=2):\n    \n    # Initialize the TSNE object\n    tsne = TSNE(\n        n_components=n_components\n    )\n\n    # Make an output DataFrame with the transformed data\n    return pd.DataFrame(\n        tsne.fit_transform(\n            abund_df\n        ),\n        index=abund_df.index.values,\n        columns=[\n            \"t-SNE %d\" % (\n                ix + 1\n            )\n            for ix in range(n_components)\n        ]\n    ).reset_index(\n    ).rename(columns=dict([('index', 'specimen')]))\n\n# Calculate pairwise distances (input has specimens in rows)\ndef calc_pdist(abund_df, metric=\"euclidean\"):\n    print(\"Calculating pairwise %s distances over %d samples and %d CAGs\" % (metric, abund_df.shape[0], abund_df.shape[1]))\n    \n    # Make an output DataFrame with the pairwise distances\n    return pd.DataFrame(\n        squareform(\n            pdist(\n                abund_df if metric != \"aitchison\" else clr_transform(abund_df),\n                metric=metric if metric != \"aitchison\" else \"euclidean\"\n            )\n        ),\n        index=abund_df.index.values,\n        columns=abund_df.index.values\n    ).reset_index(\n    ).rename(columns=dict([('index', 'specimen')]))\n\n# Calculate CLR-transformed abundances (input has specimens in rows)\ndef clr_transform(abund_df):\n    print(\"Calculating CLR over %d samples and %d CAGs\" % (abund_df.shape[0], abund_df.shape[1]))\n\n    # Make the input\n    specimen_col_abund_df = abund_df.T\n\n    # Calculate the geometric mean for every specimen\n    specimen_gmean = specimen_col_abund_df.apply(\n        lambda c: gmean(c.loc[c > 0])\n    )\n\n    # Find the global minimum value\n    min_abund = specimen_col_abund_df.apply(\n        lambda c: c.loc[c > 0].min()\n    ).min()\n\n    print(\"Minimum value is %d\" % min_abund)\n\n    # Fill in the minimum value\n    specimen_col_abund_df = specimen_col_abund_df.clip(\n        lower=min_abund\n    )\n\n    # Divide by the geometric mean\n    specimen_col_abund_df = specimen_col_abund_df / specimen_gmean\n    \n    # Take the log and return\n    return specimen_col_abund_df.applymap(np.log10).T\n\n# Read in the table with gene lengths\ngene_length_df = pd.read_csv(gene_length_csv).set_index(\"gene\")[\"length\"]\n\n# Open a connection to the HDF5\nwith pd.HDFStore(\"${params.output_prefix}.results.hdf5\", \"w\") as store:\n\n    # Read in the summary of the number of reads across all samples\n    readcount_df = pd.read_csv(readcount_csv)\n\n    # Read in the number of aligned reads per sample\n    aligned_reads_dict = pd.read_csv(\n        specimen_reads_aligned_csv\n    ).set_index(\n        \"specimen\"\n    )[\"n_reads_aligned\"]\n\n    # Add some descriptive statistics\n    summary_dict[\"num_samples\"] = readcount_df.shape[0]\n    summary_dict[\"total_reads\"] = readcount_df[\"n_reads\"].sum()\n    summary_dict[\"aligned_reads\"] = aligned_reads_dict.sum()\n\n    # Add the number of aligned reads per sample\n    readcount_df[\"aligned_reads\"] = readcount_df[\"specimen\"].apply(\n        aligned_reads_dict.get\n    )\n\n    # Write to HDF5\n    readcount_df.to_hdf(store, \"/summary/readcount\")\n\n    # Read in the summary of the number of genes detected by alignment per sample\n    specimen_gene_count_df = pd.read_csv(specimen_gene_count_csv)\n\n    # Write to HDF5\n    specimen_gene_count_df.to_hdf(store, \"/summary/genes_aligned\")\n\n    # Read in the gene richness estimate generated with breakaway\n    breakaway_df = pd.read_csv(breakaway_csv)\n\n    # Write to HDF5\n    breakaway_df.to_hdf(store, \"/summary/breakaway\")\n\n    # Write out a combined table\n    pd.concat(\n        [\n            readcount_df.set_index(\"specimen\"),\n            specimen_gene_count_df.set_index(\"specimen\"),\n            breakaway_df.set_index(\"specimen\")\n        ], \n        axis = 1, \n        sort = True\n    ).reset_index(\n    ).rename(\n        columns = dict([\n            (\"index\", \"specimen\")\n        ])\n    ).to_hdf(\n        store,\n        \"/summary/all\"\n    )\n\n    # Read in the table with the CAG-level abundances across all samples\n    cag_abund_df = pd.read_feather(\n        cag_abund_feather\n    ).rename(\n        columns={\"index\": \"CAG\"}\n    )\n    print(\n        \"Read in abundances for %d CAGs across %d samples\" %\n        (cag_abund_df.shape[0], cag_abund_df.shape[1] - 1)\n    )\n    # Add some descriptive statistics\n    summary_dict[\"num_cags\"] = cag_abund_df.shape[0]\n\n\n    # Write to HDF5\n    cag_abund_df.to_hdf(\n        store, \n        \"/abund/cag/wide\",\n        format = \"table\",\n        data_columns = [\"CAG\"],\n        complevel = 5\n    )\n\n    # Perform ordination, both PCA and t-SNE and write to store\n    for ordination_type in [\"pca\", \"tsne\"]:\n\n        # The ordination methods expect samples to be in rows\n        # and so we need to rotate the /abund/cags/wide object\n        run_ordination(\n            cag_abund_df.set_index(\"CAG\").T,\n            ordination_type\n        ).to_hdf(\n            store,\n            \"/ordination/%s\" % ordination_type\n        )\n\n    # Calculate pairwise distances and write to store\n    for metric in [\"euclidean\", \"braycurtis\", \"jaccard\", \"aitchison\"]:\n\n        # Calculate pairwise distances and write to the store\n        calc_pdist(\n            cag_abund_df.set_index(\"CAG\").T,\n            metric\n        ).to_hdf(\n            store,\n            \"/distances/%s\" % metric\n        )\n\n    # Read in the table describing which genes are grouped into which CAGs\n    # This is being called 'cag_df', but it's really a table of CAG annotations per-gene,\n    # so there is one row per gene.\n    cag_df = pd.read_csv(cag_csv)\n\n    print(\n        \"Read in CAG assignments for %d genes across %d CAGs\" % \n        (cag_df.shape[0], cag_df[\"CAG\"].unique().shape[0])\n    )\n\n    # Save the total number of genes\n    summary_dict[\"num_genes\"] = cag_df.shape[0]\n\n    # Write to HDF5\n    cag_df.to_hdf(\n        store, \n        \"/annot/gene/cag\",\n        format = \"table\",\n        data_columns = [\"gene\", \"CAG\"]\n    )\n\n    # Add that information on the gene abundance and prevalence to this gene summary table\n    cag_df = cag_df.assign(\n        length = cag_df[\"gene\"].apply(gene_length_df.get)\n    )\n\n    # Now create the `/annot/gene/all` table, which will be added to later\n    # with the taxonomic and functional annotations, if those are performed\n    cag_df.to_hdf(\n        store, \n        \"/annot/gene/all\",\n        format = \"table\",\n        data_columns = [\"gene\", \"CAG\"]\n    )\n\n    # Make a summary table describing each CAG with size, mean_abundance, and prevalence\n    # Save it in the HDF5 as \"/annot/cag/all\"\n    cag_abund_df.set_index(\"CAG\", inplace=True)\n    cag_summary_df = pd.DataFrame(dict([\n        (\"size\", cag_df[\"CAG\"].value_counts()),\n        (\"prevalence\", (cag_abund_df > 0).mean(axis=1)),\n        (\"mean_abundance\", cag_abund_df.mean(axis=1)),\n        (\"std_abundance\", cag_abund_df.std(axis=1)),\n        (\"entropy\", cag_abund_df.apply(entropy, axis=1)),\n    ])).reset_index(\n    ).rename(\n        columns=dict([(\"index\", \"CAG\")])\n    ).to_hdf(\n        store,\n        \"/annot/cag/all\"\n    )\n\n    # Write out the descriptive statistics\n    pd.DataFrame([\n        dict([\n            (\"variable\", k),\n            (\"value\", v)\n        ])\n        for k, v in summary_dict.items()\n    ]).to_hdf(\n        store,\n        \"/summary/experiment\"\n    )\n\n    # Read in the manifest provided by the user\n    manifest_df = pd.read_csv(\n        \"${manifest_csv}\"\n    )\n    # Drop the columns with paths to reads\n    for col_name in [\"R1\", \"R2\", \"I1\", \"I2\"]:\n        if col_name in manifest_df.columns.values:\n            manifest_df = manifest_df.drop(columns=col_name)\n\n    # Drop duplicated rows (multiple read pairs from the same set of samples)\n    manifest_df = manifest_df.drop_duplicates()\n\n    # Write out the manifest provided by the user\n    manifest_df.to_hdf(store, \"/manifest\")\n\n\"\"\"",
        "nb_lignes_script": 333,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "cag_csv",
            "cag_abund_feather",
            "readcount_csv",
            "manifest_csv",
            "specimen_gene_count_csv",
            "specimen_reads_aligned_csv",
            "gene_length_csv",
            "breakaway_csv"
        ],
        "nb_inputs": 8,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Add gene abundances to HDF\"",
            "container \"${container__experiment_collection}\"",
            "label 'mem_veryhigh'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "addGeneAssembly": {
        "name_process": "addGeneAssembly",
        "string_process": "\nprocess addGeneAssembly{\n    tag \"Add gene assembly data to HDF\"\n    container \"${container__experiment_collection}\"\n    label 'mem_veryhigh'\n    errorStrategy 'finish'\n\n    input:\n        path results_hdf\n        path detailed_hdf\n        path allele_assembly_csv_list\n\n    output:\n        path \"${results_hdf}\"\n        path \"${detailed_hdf}\"\n\n\"\"\"\n#!/usr/bin/env python3\n\nimport pandas as pd\nimport os\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\n# Count up the number of genes assembled per-specimen\nn_genes_assembled_per_specimen = dict()\n\n# Open a connection to the detailed HDF5 output store\ndetailed_store = pd.HDFStore(\"${detailed_hdf}\", \"a\")\n\n# Read in the summary of allele assembly for each sample\nfor fp in \"${allele_assembly_csv_list}\".split(\" \"):\n\n    # Make sure that the file path has the expected pattern\n    assert fp.endswith(\".csv.gz\"), fp\n    sample_name = fp.replace(\".csv.gz\", \"\")\n    print(\"Reading in assembly information for %s\" % sample_name)\n    assembly_df = pd.read_csv(fp)\n    print(\"Read in %d assembled genes\" % assembly_df.shape[0])\n\n    # Save the information on how many genes were assembled in each sample\n    n_genes_assembled_per_specimen[sample_name] = assembly_df.shape[0]\n\n    # Write out to the detailed HDF\n    assembly_df.to_hdf(\n        detailed_store,\n        \"/abund/allele/assembly/%s\" % sample_name\n    )\n\ndetailed_store.close()\n\nn_genes_assembled_per_specimen = pd.DataFrame(\n    dict(\n        [\n            (\n                \"n_genes_assembled\",\n                pd.Series(n_genes_assembled_per_specimen)\n            )\n        ]\n    )\n).reset_index(\n).rename(\n    columns = dict(\n        [\n            (\"index\", \"specimen\")\n        ]\n    )\n)\n\n# Open a connection to the HDF5\nwith pd.HDFStore(\"${results_hdf}\", \"a\") as store:\n\n    # Write the summary of the number of genes assembled per sample\n    n_genes_assembled_per_specimen.to_hdf(store, \"/summary/genes_assembled\")\n\n    # Add the number of genes assembled to the combined summary table\n    pd.concat([\n        pd.read_hdf(\n            store,\n            \"/summary/all\"\n        ).set_index(\n            \"specimen\"\n        ),\n        n_genes_assembled_per_specimen.set_index(\n            \"specimen\"\n        )\n    ], axis = 1, sort = True).reset_index(\n    ).rename(\n        columns = dict([\n            (\"index\", \"specimen\")\n        ])\n    ).to_hdf(\n        store,\n        \"/summary/all\"\n    )\n\n\"\"\"\n\n}",
        "nb_lignes_process": 97,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport pandas as pd\nimport os\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\n# Count up the number of genes assembled per-specimen\nn_genes_assembled_per_specimen = dict()\n\n# Open a connection to the detailed HDF5 output store\ndetailed_store = pd.HDFStore(\"${detailed_hdf}\", \"a\")\n\n# Read in the summary of allele assembly for each sample\nfor fp in \"${allele_assembly_csv_list}\".split(\" \"):\n\n    # Make sure that the file path has the expected pattern\n    assert fp.endswith(\".csv.gz\"), fp\n    sample_name = fp.replace(\".csv.gz\", \"\")\n    print(\"Reading in assembly information for %s\" % sample_name)\n    assembly_df = pd.read_csv(fp)\n    print(\"Read in %d assembled genes\" % assembly_df.shape[0])\n\n    # Save the information on how many genes were assembled in each sample\n    n_genes_assembled_per_specimen[sample_name] = assembly_df.shape[0]\n\n    # Write out to the detailed HDF\n    assembly_df.to_hdf(\n        detailed_store,\n        \"/abund/allele/assembly/%s\" % sample_name\n    )\n\ndetailed_store.close()\n\nn_genes_assembled_per_specimen = pd.DataFrame(\n    dict(\n        [\n            (\n                \"n_genes_assembled\",\n                pd.Series(n_genes_assembled_per_specimen)\n            )\n        ]\n    )\n).reset_index(\n).rename(\n    columns = dict(\n        [\n            (\"index\", \"specimen\")\n        ]\n    )\n)\n\n# Open a connection to the HDF5\nwith pd.HDFStore(\"${results_hdf}\", \"a\") as store:\n\n    # Write the summary of the number of genes assembled per sample\n    n_genes_assembled_per_specimen.to_hdf(store, \"/summary/genes_assembled\")\n\n    # Add the number of genes assembled to the combined summary table\n    pd.concat([\n        pd.read_hdf(\n            store,\n            \"/summary/all\"\n        ).set_index(\n            \"specimen\"\n        ),\n        n_genes_assembled_per_specimen.set_index(\n            \"specimen\"\n        )\n    ], axis = 1, sort = True).reset_index(\n    ).rename(\n        columns = dict([\n            (\"index\", \"specimen\")\n        ])\n    ).to_hdf(\n        store,\n        \"/summary/all\"\n    )\n\n\"\"\"",
        "nb_lignes_script": 80,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "results_hdf",
            "detailed_hdf",
            "allele_assembly_csv_list"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Add gene assembly data to HDF\"",
            "container \"${container__experiment_collection}\"",
            "label 'mem_veryhigh'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "addCorncobResults": {
        "name_process": "addCorncobResults",
        "string_process": "\nprocess addCorncobResults{\n    tag \"Add statistical analysis to HDF\"\n    container \"${container__pandas}\"\n    label 'mem_veryhigh'\n    errorStrategy 'finish'\n\n    input:\n        path results_hdf\n        path corncob_csv\n\n    output:\n        path \"${results_hdf}\"\n        path \"corncob.for.betta.*.csv.gz\" optional true\n\n\"\"\"\n#!/bin/bash\n\nadd_corncob_results.py \"${results_hdf}\" \"${corncob_csv}\" \"${params.fdr_method}\"\n\n\"\"\"\n\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\n#!/bin/bash\n\nadd_corncob_results.py \"${results_hdf}\" \"${corncob_csv}\" \"${params.fdr_method}\"\n\n\"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "results_hdf",
            "corncob_csv"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Add statistical analysis to HDF\"",
            "container \"${container__pandas}\"",
            "label 'mem_veryhigh'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "addMetaPhlAn2Results": {
        "name_process": "addMetaPhlAn2Results",
        "string_process": "\nprocess addMetaPhlAn2Results{\n    tag \"Add composition analysis to HDF\"\n    container \"${container__pandas}\"\n    label 'mem_medium'\n    errorStrategy 'finish'\n\n    input:\n        path results_hdf\n        path metaphlan_tsv_list\n\n    output:\n        path \"${results_hdf}\"\n\n\"\"\"\n#!/usr/bin/env python3\n\nimport pandas as pd\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\n# Open a connection to the HDF5\nwith pd.HDFStore(\"${results_hdf}\", \"a\") as store:\n\n    # Iterate over each input file\n    for fp in \"${metaphlan_tsv_list}\".split(\" \"):\n\n        # Make sure that the file has the expected suffix\n        assert fp.endswith(\".metaphlan2.tsv\"), fp\n\n        # Get the specimen name from the file name\n        specimen_name = fp.replace(\".metaphlan2.tsv\", \"\")\n\n        # Read in from the flat file\n        metaphlan_df = pd.read_csv(\n            fp,\n            skiprows=1,\n            sep=\"\\\\t\"\n        )\n\n        print(\"Read in %d taxa from %s\" % (metaphlan_df.shape[0], specimen_name))\n\n        # Write metaphlan results to HDF5\n        key = \"/composition/metaphlan/%s\" % specimen_name\n        print(\"Writing to %s\" % key)\n        metaphlan_df.to_hdf(store, key)\n\n    print(\"Closing store\")\n\nprint(\"Done\")\n\n\"\"\"\n\n}",
        "nb_lignes_process": 52,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport pandas as pd\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\n# Open a connection to the HDF5\nwith pd.HDFStore(\"${results_hdf}\", \"a\") as store:\n\n    # Iterate over each input file\n    for fp in \"${metaphlan_tsv_list}\".split(\" \"):\n\n        # Make sure that the file has the expected suffix\n        assert fp.endswith(\".metaphlan2.tsv\"), fp\n\n        # Get the specimen name from the file name\n        specimen_name = fp.replace(\".metaphlan2.tsv\", \"\")\n\n        # Read in from the flat file\n        metaphlan_df = pd.read_csv(\n            fp,\n            skiprows=1,\n            sep=\"\\\\t\"\n        )\n\n        print(\"Read in %d taxa from %s\" % (metaphlan_df.shape[0], specimen_name))\n\n        # Write metaphlan results to HDF5\n        key = \"/composition/metaphlan/%s\" % specimen_name\n        print(\"Writing to %s\" % key)\n        metaphlan_df.to_hdf(store, key)\n\n    print(\"Closing store\")\n\nprint(\"Done\")\n\n\"\"\"",
        "nb_lignes_script": 37,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "results_hdf",
            "metaphlan_tsv_list"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Add composition analysis to HDF\"",
            "container \"${container__pandas}\"",
            "label 'mem_medium'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "addEggnogResults": {
        "name_process": "addEggnogResults",
        "string_process": "\nprocess addEggnogResults {\n    tag \"Add functional predictions to HDF\"\n    container \"${container__experiment_collection}\"\n    label 'mem_veryhigh'\n    errorStrategy 'finish'\n\n    input:\n        path results_hdf\n        path \"genes.emapper.annotations.*.gz\"\n\n    output:\n        path \"${results_hdf}\"\n\n\"\"\"\n#!/usr/bin/env python3\n\nimport os\nimport pandas as pd\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\n# Get the list of files with eggNOG results\neggnog_csv_list = [\n    fp\n    for fp in os.listdir(\".\")\n    if fp.startswith(\"genes.emapper.annotations\")\n]\n\n# Make sure that every eggNOG file ends with '.gz'\n# If not, this is a sign that the file was staged incorrectly\nfor fp in eggnog_csv_list:\n    assert fp.endswith('.gz'), \"Unexpected: %s\" % fp\n\nprint(\"Found %d files with eggNOG results\" % len(eggnog_csv_list))\n\n# Read in the eggNOG-mapper results\neggnog_df = pd.concat([\n    pd.read_csv(\n        fp, \n        header=3, \n        sep=\"\\\\t\"\n    )\n    for fp in eggnog_csv_list\n]).rename(\n    columns = dict([(\"#query_name\", \"query_name\")])\n).reset_index(\n    drop=True\n)\n\ndef head_and_tail(df):\n    print(\"HEAD:\")\n    print(df.head())\n    print(\"\")\n    print(\"TAIL:\")\n    print(df.tail())\n    print(\"\")\n\nhead_and_tail(eggnog_df)\n\nassert 'query_name' in eggnog_df.columns.values\nprint(\"Read in %d lines of annotations\" % eggnog_df.shape[0])\n\n# Remove those rows with no \"seed_eggNOG_ortholog\" (which are internal runtime metrics)\neggnog_df = eggnog_df.reindex(\n    index=eggnog_df[\"seed_eggNOG_ortholog\"].dropna().index\n)\nprint(\"Read in annotations for %d genes\" % eggnog_df.shape[0])\n\nhead_and_tail(eggnog_df)\n\n# Set the index to be the gene name\neggnog_df.set_index(\"query_name\", inplace=True)\nprint(\"Set the index on 'query_name'\")\n\nhead_and_tail(eggnog_df)\n\nprint(\n    \"Read in eggnog results for %d genes\" % \n    eggnog_df.shape[0]\n)\n\n# Read in the existing set of gene annotations\nwith pd.HDFStore(\"${results_hdf}\", \"r\") as store:\n\n    # Set an index on the `gene` column\n    gene_annot = pd.read_hdf(\n        \"${results_hdf}\", \n        \"/annot/gene/all\"\n    ).set_index(\"gene\")\n\n# Add in a subset of the eggnog results\ngene_annot = gene_annot.assign(\n    eggNOG_ortholog = eggnog_df[\"seed_eggNOG_ortholog\"]\n).assign(\n    eggNOG_tax = eggnog_df[\"best_tax_level\"]\n).assign(\n    eggNOG_desc = eggnog_df[\"eggNOG free text desc.\"]\n).sort_values(\n    by=\"CAG\"\n).reset_index()\n\n# Open a connection to the HDF5\nwith pd.HDFStore(\"${results_hdf}\", \"a\") as store:\n\n    # Write eggnog results to HDF5\n    eggnog_df.reset_index().reindex(\n        columns = [\n            'query_name', \n            'seed_eggNOG_ortholog', \n            'best_tax_level', \n            'GOs', \n            'EC', \n            'KEGG_ko', \n            'best eggNOG OG', \n            'COG Functional cat.', \n            'eggNOG free text desc.'\n        ]\n    ).to_hdf(\n        store,\n        \"/annot/gene/eggnog\",\n        format = \"fixed\",\n    )\n    \n    # Write summary annotation table to HDF5\n    gene_annot.to_hdf(\n        store, \n        \"/annot/gene/all\",\n        format = \"table\",\n        data_columns = [\"CAG\"],\n    )\n\n\"\"\"\n\n}",
        "nb_lignes_process": 133,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport os\nimport pandas as pd\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\n# Get the list of files with eggNOG results\neggnog_csv_list = [\n    fp\n    for fp in os.listdir(\".\")\n    if fp.startswith(\"genes.emapper.annotations\")\n]\n\n# Make sure that every eggNOG file ends with '.gz'\n# If not, this is a sign that the file was staged incorrectly\nfor fp in eggnog_csv_list:\n    assert fp.endswith('.gz'), \"Unexpected: %s\" % fp\n\nprint(\"Found %d files with eggNOG results\" % len(eggnog_csv_list))\n\n# Read in the eggNOG-mapper results\neggnog_df = pd.concat([\n    pd.read_csv(\n        fp, \n        header=3, \n        sep=\"\\\\t\"\n    )\n    for fp in eggnog_csv_list\n]).rename(\n    columns = dict([(\"#query_name\", \"query_name\")])\n).reset_index(\n    drop=True\n)\n\ndef head_and_tail(df):\n    print(\"HEAD:\")\n    print(df.head())\n    print(\"\")\n    print(\"TAIL:\")\n    print(df.tail())\n    print(\"\")\n\nhead_and_tail(eggnog_df)\n\nassert 'query_name' in eggnog_df.columns.values\nprint(\"Read in %d lines of annotations\" % eggnog_df.shape[0])\n\n# Remove those rows with no \"seed_eggNOG_ortholog\" (which are internal runtime metrics)\neggnog_df = eggnog_df.reindex(\n    index=eggnog_df[\"seed_eggNOG_ortholog\"].dropna().index\n)\nprint(\"Read in annotations for %d genes\" % eggnog_df.shape[0])\n\nhead_and_tail(eggnog_df)\n\n# Set the index to be the gene name\neggnog_df.set_index(\"query_name\", inplace=True)\nprint(\"Set the index on 'query_name'\")\n\nhead_and_tail(eggnog_df)\n\nprint(\n    \"Read in eggnog results for %d genes\" % \n    eggnog_df.shape[0]\n)\n\n# Read in the existing set of gene annotations\nwith pd.HDFStore(\"${results_hdf}\", \"r\") as store:\n\n    # Set an index on the `gene` column\n    gene_annot = pd.read_hdf(\n        \"${results_hdf}\", \n        \"/annot/gene/all\"\n    ).set_index(\"gene\")\n\n# Add in a subset of the eggnog results\ngene_annot = gene_annot.assign(\n    eggNOG_ortholog = eggnog_df[\"seed_eggNOG_ortholog\"]\n).assign(\n    eggNOG_tax = eggnog_df[\"best_tax_level\"]\n).assign(\n    eggNOG_desc = eggnog_df[\"eggNOG free text desc.\"]\n).sort_values(\n    by=\"CAG\"\n).reset_index()\n\n# Open a connection to the HDF5\nwith pd.HDFStore(\"${results_hdf}\", \"a\") as store:\n\n    # Write eggnog results to HDF5\n    eggnog_df.reset_index().reindex(\n        columns = [\n            'query_name', \n            'seed_eggNOG_ortholog', \n            'best_tax_level', \n            'GOs', \n            'EC', \n            'KEGG_ko', \n            'best eggNOG OG', \n            'COG Functional cat.', \n            'eggNOG free text desc.'\n        ]\n    ).to_hdf(\n        store,\n        \"/annot/gene/eggnog\",\n        format = \"fixed\",\n    )\n    \n    # Write summary annotation table to HDF5\n    gene_annot.to_hdf(\n        store, \n        \"/annot/gene/all\",\n        format = \"table\",\n        data_columns = [\"CAG\"],\n    )\n\n\"\"\"",
        "nb_lignes_script": 118,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "results_hdf"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Add functional predictions to HDF\"",
            "container \"${container__experiment_collection}\"",
            "label 'mem_veryhigh'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "readTaxonomy": {
        "name_process": "readTaxonomy",
        "string_process": "\nprocess readTaxonomy {\n    tag \"Read the NCBI taxonomy\"\n    container \"${container__experiment_collection}\"\n    label 'mem_medium'\n    errorStrategy 'finish'\n\n    input:\n        path taxdump_tar_gz\n\n    output:\n        path \"ncbi_taxonomy.csv.gz\"\n\n\"\"\"\n#!/usr/bin/env python3\n\nimport gzip\nfrom io import BytesIO\nimport pandas as pd\nimport tarfile\n\nprint(\"Opening taxdump tar file\")\ntar = tarfile.open(\n    \"${taxdump_tar_gz}\", \n    \"r:gz\", \n    encoding='utf-8'\n)\n\n# Read in the table of merged taxids\nprint(\"Reading in merged taxids\")\nmerged_df = pd.read_csv(\n    BytesIO(\n        tar.extractfile(\n            \"merged.dmp\"\n        ).read()\n    ),\n    sep=\"\\\\t\",\n    header=None\n).rename(columns=dict([\n    (0, \"old\"),\n    (2, \"new\"),\n    (6, \"level\")\n])).reindex(\n    columns=[\"old\", \"new\"]\n)\n\n# Get the name of each taxon\nprint(\"Reading in names\")\nnames_df = pd.read_csv(\n    BytesIO(\n        tar.extractfile(\n            \"names.dmp\"\n        ).read()\n    ),\n    sep=\"\\\\t\",\n    header=None\n).rename(columns=dict([\n    (0, \"tax_id\"),\n    (2, \"name\"),\n    (6, \"level\")\n])).query(\n    \"level == 'scientific name'\"\n).reindex(\n    columns=[\"tax_id\", \"name\"]\n)\n\n# Get the rank and parent of every taxon\nprint(\"Reading in nodes\")\nnodes_df = pd.read_csv(\n    BytesIO(\n        tar.extractfile(\n            \"nodes.dmp\"\n        ).read()\n    ),\n    sep=\"\\\\t\",\n    header=None\n).rename(columns=dict([\n    (0, \"tax_id\"),\n    (2, \"parent\"),\n    (4, \"rank\")\n])).reindex(\n    columns=[\"tax_id\", \"parent\", \"rank\"]\n)\n\n# Join the names and the nodes\nprint(\"Joining names and nodes\")\ntax_df = pd.concat([\n    names_df.set_index(\"tax_id\"),\n    nodes_df.set_index(\"tax_id\")\n], axis=1, sort=True)\n\n# Add in the merged taxids\nprint(\"Adding in merged tax IDs\")\ntax_df = pd.concat([\n    tax_df,\n    tax_df.reindex(\n        index=merged_df[\"old\"].values\n    ).dropna(\n    ).apply(\n        lambda v: v.apply(merged_df.set_index(\"old\")[\"new\"].get) if v.name == \"tax_id\" else v\n    )\n]).reset_index()\nassert tax_df[\"tax_id\"].apply(lambda n: \"\\\\n\" not in str(n)).all()\n\n# Write to CSV\nprint(\"Writing out final CSV\")\ntax_df.to_csv(\n    \"ncbi_taxonomy.csv.gz\",\n    compression=\"gzip\",\n    index=None\n)\n\"\"\"\n\n}",
        "nb_lignes_process": 112,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport gzip\nfrom io import BytesIO\nimport pandas as pd\nimport tarfile\n\nprint(\"Opening taxdump tar file\")\ntar = tarfile.open(\n    \"${taxdump_tar_gz}\", \n    \"r:gz\", \n    encoding='utf-8'\n)\n\n# Read in the table of merged taxids\nprint(\"Reading in merged taxids\")\nmerged_df = pd.read_csv(\n    BytesIO(\n        tar.extractfile(\n            \"merged.dmp\"\n        ).read()\n    ),\n    sep=\"\\\\t\",\n    header=None\n).rename(columns=dict([\n    (0, \"old\"),\n    (2, \"new\"),\n    (6, \"level\")\n])).reindex(\n    columns=[\"old\", \"new\"]\n)\n\n# Get the name of each taxon\nprint(\"Reading in names\")\nnames_df = pd.read_csv(\n    BytesIO(\n        tar.extractfile(\n            \"names.dmp\"\n        ).read()\n    ),\n    sep=\"\\\\t\",\n    header=None\n).rename(columns=dict([\n    (0, \"tax_id\"),\n    (2, \"name\"),\n    (6, \"level\")\n])).query(\n    \"level == 'scientific name'\"\n).reindex(\n    columns=[\"tax_id\", \"name\"]\n)\n\n# Get the rank and parent of every taxon\nprint(\"Reading in nodes\")\nnodes_df = pd.read_csv(\n    BytesIO(\n        tar.extractfile(\n            \"nodes.dmp\"\n        ).read()\n    ),\n    sep=\"\\\\t\",\n    header=None\n).rename(columns=dict([\n    (0, \"tax_id\"),\n    (2, \"parent\"),\n    (4, \"rank\")\n])).reindex(\n    columns=[\"tax_id\", \"parent\", \"rank\"]\n)\n\n# Join the names and the nodes\nprint(\"Joining names and nodes\")\ntax_df = pd.concat([\n    names_df.set_index(\"tax_id\"),\n    nodes_df.set_index(\"tax_id\")\n], axis=1, sort=True)\n\n# Add in the merged taxids\nprint(\"Adding in merged tax IDs\")\ntax_df = pd.concat([\n    tax_df,\n    tax_df.reindex(\n        index=merged_df[\"old\"].values\n    ).dropna(\n    ).apply(\n        lambda v: v.apply(merged_df.set_index(\"old\")[\"new\"].get) if v.name == \"tax_id\" else v\n    )\n]).reset_index()\nassert tax_df[\"tax_id\"].apply(lambda n: \"\\\\n\" not in str(n)).all()\n\n# Write to CSV\nprint(\"Writing out final CSV\")\ntax_df.to_csv(\n    \"ncbi_taxonomy.csv.gz\",\n    compression=\"gzip\",\n    index=None\n)\n\"\"\"",
        "nb_lignes_script": 98,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "taxdump_tar_gz"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Read the NCBI taxonomy\"",
            "container \"${container__experiment_collection}\"",
            "label 'mem_medium'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "addTaxResults": {
        "name_process": "addTaxResults",
        "string_process": "\nprocess addTaxResults {\n    tag \"Add taxonomic annotations to HDF\"\n    container \"${container__experiment_collection}\"\n    label 'mem_veryhigh'\n    errorStrategy 'finish'\n\n    input:\n        path results_hdf\n        path \"genes.tax.aln.*.gz\"\n        path taxonomy_csv\n\n    output:\n        path \"${results_hdf}\"\n\n\"\"\"\n#!/usr/bin/env python3\n\nimport os\nimport pandas as pd\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\ndiamond_tax_csv_list = [\n    fp\n    for fp in os.listdir(\".\")\n    if fp.startswith(\"genes.tax.aln\")\n]\nfor fp in diamond_tax_csv_list:\n    assert fp.endswith(\".gz\")\n\nprint(\"Found %d taxonomy results CSVs to import\" % len(diamond_tax_csv_list))\n\n# Read in the DIAMOND-tax results\ntax_df = pd.concat([\n    pd.read_csv(\n        fp, \n        sep=\"\\\\t\", \n        header=None\n    ).rename(\n        columns=dict([\n            (0, \"gene\"), \n            (1, \"tax_id\"), \n            (2, \"evalue\")\n        ])\n    )\n    for fp in diamond_tax_csv_list\n])\n\nprint(\n    \"Read in taxonomic results for %d genes\" % \n    tax_df.shape[0]\n)\n\n# Read in the existing set of gene annotations\nwith pd.HDFStore(\"${results_hdf}\", \"r\") as store:\n\n    # Set an index on the `gene` column\n    gene_annot = pd.read_hdf(\n        \"${results_hdf}\", \n        \"/annot/gene/all\"\n    ).set_index(\"gene\")\n\n# Add the taxonomic annotation results\ngene_annot = gene_annot.assign(\n    tax_id = tax_df.set_index(\"gene\")[\"tax_id\"].apply(str)\n).reset_index()\n\n# Read the taxonomy CSV\ntaxonomy_df = pd.read_csv(\n    \"${taxonomy_csv}\"\n).applymap(\n    str\n)\n\n# Add the name of the assigned tax_id\ngene_annot = gene_annot.assign(\n    tax_name = gene_annot[\"tax_id\"].apply(\n        taxonomy_df.set_index(\"tax_id\")[\"name\"].get\n    )\n).assign(\n    tax_rank = gene_annot[\"tax_id\"].apply(\n        taxonomy_df.set_index(\"tax_id\")[\"rank\"].get\n    )\n)\n\n\n# Open a connection to the HDF5\nwith pd.HDFStore(\"${results_hdf}\", \"a\") as store:\n\n    # Write taxonomic results to HDF5\n    tax_df.to_hdf(store, \"/annot/gene/tax\")\n\n    # Write summary gene annotation table to HDF5\n    gene_annot.to_hdf(\n        store, \n        \"/annot/gene/all\",\n        format = \"table\",\n        data_columns = [\"gene\", \"CAG\"]\n    )\n\n    # Write the taxonomy table\n    taxonomy_df.to_hdf(\n        store,\n        \"/ref/taxonomy\"\n    )\n\n\"\"\"\n\n}",
        "nb_lignes_process": 108,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport os\nimport pandas as pd\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\ndiamond_tax_csv_list = [\n    fp\n    for fp in os.listdir(\".\")\n    if fp.startswith(\"genes.tax.aln\")\n]\nfor fp in diamond_tax_csv_list:\n    assert fp.endswith(\".gz\")\n\nprint(\"Found %d taxonomy results CSVs to import\" % len(diamond_tax_csv_list))\n\n# Read in the DIAMOND-tax results\ntax_df = pd.concat([\n    pd.read_csv(\n        fp, \n        sep=\"\\\\t\", \n        header=None\n    ).rename(\n        columns=dict([\n            (0, \"gene\"), \n            (1, \"tax_id\"), \n            (2, \"evalue\")\n        ])\n    )\n    for fp in diamond_tax_csv_list\n])\n\nprint(\n    \"Read in taxonomic results for %d genes\" % \n    tax_df.shape[0]\n)\n\n# Read in the existing set of gene annotations\nwith pd.HDFStore(\"${results_hdf}\", \"r\") as store:\n\n    # Set an index on the `gene` column\n    gene_annot = pd.read_hdf(\n        \"${results_hdf}\", \n        \"/annot/gene/all\"\n    ).set_index(\"gene\")\n\n# Add the taxonomic annotation results\ngene_annot = gene_annot.assign(\n    tax_id = tax_df.set_index(\"gene\")[\"tax_id\"].apply(str)\n).reset_index()\n\n# Read the taxonomy CSV\ntaxonomy_df = pd.read_csv(\n    \"${taxonomy_csv}\"\n).applymap(\n    str\n)\n\n# Add the name of the assigned tax_id\ngene_annot = gene_annot.assign(\n    tax_name = gene_annot[\"tax_id\"].apply(\n        taxonomy_df.set_index(\"tax_id\")[\"name\"].get\n    )\n).assign(\n    tax_rank = gene_annot[\"tax_id\"].apply(\n        taxonomy_df.set_index(\"tax_id\")[\"rank\"].get\n    )\n)\n\n\n# Open a connection to the HDF5\nwith pd.HDFStore(\"${results_hdf}\", \"a\") as store:\n\n    # Write taxonomic results to HDF5\n    tax_df.to_hdf(store, \"/annot/gene/tax\")\n\n    # Write summary gene annotation table to HDF5\n    gene_annot.to_hdf(\n        store, \n        \"/annot/gene/all\",\n        format = \"table\",\n        data_columns = [\"gene\", \"CAG\"]\n    )\n\n    # Write the taxonomy table\n    taxonomy_df.to_hdf(\n        store,\n        \"/ref/taxonomy\"\n    )\n\n\"\"\"",
        "nb_lignes_script": 92,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "results_hdf",
            "taxonomy_csv"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Add taxonomic annotations to HDF\"",
            "container \"${container__experiment_collection}\"",
            "label 'mem_veryhigh'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "repackHDF": {
        "name_process": "repackHDF",
        "string_process": "\nprocess repackHDF {\n\n    container \"${container__pandas}\"\n    tag \"Compress HDF store\"\n    label \"mem_veryhigh\"\n    errorStrategy \"finish\"\n    publishDir \"${params.output_folder}\", mode: \"copy\", overwrite: true\n    \n    input:\n    file output_hdf5\n        \n    output:\n    file \"${output_hdf5}\"\n\n    \"\"\"\n#!/bin/bash\n\nset -e\n\n[ -s ${output_hdf5} ]\n\nh5repack -f GZIP=5 ${output_hdf5} TEMP && mv TEMP ${output_hdf5}\n    \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\n[ -s ${output_hdf5} ]\n\nh5repack -f GZIP=5 ${output_hdf5} TEMP && mv TEMP ${output_hdf5}\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "output_hdf5"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"${container__pandas}\"",
            "tag \"Compress HDF store\"",
            "label \"mem_veryhigh\"",
            "errorStrategy \"finish\"",
            "publishDir \"${params.output_folder}\", mode: \"copy\", overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "publish": {
        "name_process": "publish",
        "string_process": "\nprocess publish {\n    container \"ubuntu:20.04\"\n    label \"io_limited\"\n    publishDir \"${params.output_folder}\", mode: \"copy\", overwrite: true\n\n    input:\n    file input_fp\n\n    output:\n    file \"${input_fp}\"\n\n    \"\"\"#!/bin/bash\nset -e\necho \"Publishing ${input_fp}\"\n    \"\"\"\n\n}",
        "nb_lignes_process": 16,
        "string_script": "\"\"\"#!/bin/bash\nset -e\necho \"Publishing ${input_fp}\"\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "input_fp"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"ubuntu:20.04\"",
            "label \"io_limited\"",
            "publishDir \"${params.output_folder}\", mode: \"copy\", overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "getSRAlist": {
        "name_process": "getSRAlist",
        "string_process": "\nprocess getSRAlist {\n    container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"\n    label \"io_net\"\n    errorStrategy 'finish'\n    \n    input:\n    val accession\n\n    output:\n    file \"accession_list.txt\"\n\n\"\"\"\n#!/usr/bin/env python3\nimport requests\nfrom time import sleep\nimport xml.etree.ElementTree as ET\nimport pandas as pd\n\naccession =\"${accession}\"\nprint(\"Fetching paired-end data from %s\" % accession)\n\n# Make a function to fetch data from the Entrez API\n# while retrying if any errors are encountered\ndef request_url(url, max_retries=10, pause_seconds=0.5):\n    # Try the request `max_retries` times, with a `pause_seconds` pause in-between\n    assert isinstance(max_retries, int)\n    assert max_retries > 0\n    for i in range(max_retries):\n        r = requests.get(url)\n        if r.status_code == 200:\n            return r.text\n        print(\"Caught an error on attempt #%d, retrying\" % (i + 1))\n        sleep(pause_seconds)\n\n# Format an Entrez query, execute the request, and return the result\ndef entrez(mode, **kwargs):\n    assert mode in [\"efetch\", \"esearch\", \"elink\"]\n\n    kwargs_str = \"&\".join([\"%s=%s\" % (k, v) for k, v in kwargs.items()])\n    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/%s.fcgi?%s\"\n    url = url % (mode, kwargs_str)\n\n    # Get the text response\n    response_text = request_url(url)\n\n    # Parse the XML\n    return ET.fromstring(response_text)\n\n# Make a list of SRA records which will be added to\nsra_id_list = []\n\n# First, get the ID(s) for the BioProject from the accession\nbioproject_result = entrez('esearch', db=\"bioproject\", term=accession)\n\n# Iterate over each of the IDs linked to the BioProject\nfor i in bioproject_result.find(\n    \"IdList\"\n).findall(\n    \"Id\"\n):\n    # Looking at this particular ID\n    bioproject_id = i.text\n\n    print(\"Getting BioProject %s = %s\" % (accession, bioproject_id))\n\n    # Get all of the SRA Runs for this BioProject\n    elink_results = entrez(\n        'elink',\n        dbfrom=\"bioproject\", \n        id=int(bioproject_id), \n        linkname=\"bioproject_sra\"\n    )\n\n    # Make sure that there are some links in this set of results, or move on\n    if elink_results.find(\"LinkSet\") is None:\n        print(\"No SRA accessions found for %s = %s\" % (accession, bioproject_id))\n        continue\n    if elink_results.find(\"LinkSet\").find(\"LinkSetDb\") is None:\n        print(\"No SRA accessions found for %s = %s\" % (accession, bioproject_id))\n        continue\n\n    # Parse all of the SRA records from this BioProject\n    sra_id_list.extend([\n        child.find(\"Id\").text\n        for child in elink_results.find(\"LinkSet\").find(\"LinkSetDb\")\n        if child.find(\"Id\") is not None\n    ])\n\nprint(\"Found %d SRA accessions from this BioProject\" % (len(sra_id_list)))\n\nassert len(sra_id_list) > 0\n\n# Save the accession list\nwith open(\"accession_list.txt\", \"wt\") as fo:\n    fo.write(\"\\\\n\".join(sra_id_list))\n\n\"\"\"\n}",
        "nb_lignes_process": 97,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport requests\nfrom time import sleep\nimport xml.etree.ElementTree as ET\nimport pandas as pd\n\naccession =\"${accession}\"\nprint(\"Fetching paired-end data from %s\" % accession)\n\n# Make a function to fetch data from the Entrez API\n# while retrying if any errors are encountered\ndef request_url(url, max_retries=10, pause_seconds=0.5):\n    # Try the request `max_retries` times, with a `pause_seconds` pause in-between\n    assert isinstance(max_retries, int)\n    assert max_retries > 0\n    for i in range(max_retries):\n        r = requests.get(url)\n        if r.status_code == 200:\n            return r.text\n        print(\"Caught an error on attempt #%d, retrying\" % (i + 1))\n        sleep(pause_seconds)\n\n# Format an Entrez query, execute the request, and return the result\ndef entrez(mode, **kwargs):\n    assert mode in [\"efetch\", \"esearch\", \"elink\"]\n\n    kwargs_str = \"&\".join([\"%s=%s\" % (k, v) for k, v in kwargs.items()])\n    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/%s.fcgi?%s\"\n    url = url % (mode, kwargs_str)\n\n    # Get the text response\n    response_text = request_url(url)\n\n    # Parse the XML\n    return ET.fromstring(response_text)\n\n# Make a list of SRA records which will be added to\nsra_id_list = []\n\n# First, get the ID(s) for the BioProject from the accession\nbioproject_result = entrez('esearch', db=\"bioproject\", term=accession)\n\n# Iterate over each of the IDs linked to the BioProject\nfor i in bioproject_result.find(\n    \"IdList\"\n).findall(\n    \"Id\"\n):\n    # Looking at this particular ID\n    bioproject_id = i.text\n\n    print(\"Getting BioProject %s = %s\" % (accession, bioproject_id))\n\n    # Get all of the SRA Runs for this BioProject\n    elink_results = entrez(\n        'elink',\n        dbfrom=\"bioproject\", \n        id=int(bioproject_id), \n        linkname=\"bioproject_sra\"\n    )\n\n    # Make sure that there are some links in this set of results, or move on\n    if elink_results.find(\"LinkSet\") is None:\n        print(\"No SRA accessions found for %s = %s\" % (accession, bioproject_id))\n        continue\n    if elink_results.find(\"LinkSet\").find(\"LinkSetDb\") is None:\n        print(\"No SRA accessions found for %s = %s\" % (accession, bioproject_id))\n        continue\n\n    # Parse all of the SRA records from this BioProject\n    sra_id_list.extend([\n        child.find(\"Id\").text\n        for child in elink_results.find(\"LinkSet\").find(\"LinkSetDb\")\n        if child.find(\"Id\") is not None\n    ])\n\nprint(\"Found %d SRA accessions from this BioProject\" % (len(sra_id_list)))\n\nassert len(sra_id_list) > 0\n\n# Save the accession list\nwith open(\"accession_list.txt\", \"wt\") as fo:\n    fo.write(\"\\\\n\".join(sra_id_list))\n\n\"\"\"",
        "nb_lignes_script": 85,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "accession"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"",
            "label \"io_net\"",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "getMetadata": {
        "name_process": "getMetadata",
        "string_process": "\nprocess getMetadata {\n    container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"\n    label \"io_net\"\n    errorStrategy 'finish'\n    \n    input:\n    val sra_id\n\n    output:\n    file \"*.metadata.json.gz\"\n\n\"\"\"\n#!/usr/bin/env python3\nimport json\nimport gzip\nimport requests\nfrom time import sleep\nimport xml.etree.ElementTree as ET\nimport pandas as pd\n\nprint(\"Fetching metadata for SRA ID ${sra_id}\")\n\n# Make a function to fetch data from the Entrez API\n# while retrying if any errors are encountered\ndef request_url(url, max_retries=10, pause_seconds=0.5):\n    # Try the request `max_retries` times, with a `pause_seconds` pause in-between\n    assert isinstance(max_retries, int)\n    assert max_retries > 0\n    for i in range(max_retries):\n        r = requests.get(url)\n        if r.status_code == 200:\n            return r.text\n        print(\"Caught an error on attempt #%d, retrying\" % (i + 1))\n        sleep(pause_seconds)\n\n# Format an Entrez query, execute the request, and return the result\ndef entrez(mode, **kwargs):\n    assert mode in [\"efetch\", \"esearch\", \"elink\"]\n\n    kwargs_str = \"&\".join([\"%s=%s\" % (k, v) for k, v in kwargs.items()])\n    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/%s.fcgi?%s\"\n    url = url % (mode, kwargs_str)\n\n    # Get the text response\n    response_text = request_url(url)\n\n    # Parse the XML\n    return ET.fromstring(response_text)\n\n# Function to iteratively parse the XML record\ndef parse_record(r, prefix=[]):\n\n    if r.text is not None:\n        if len(r.text.replace(\"\\\\n\", \"\")) > 0:\n            yield prefix + [r.tag], r.text.replace(\"\\\\n\", \"\")\n\n    for k, v in r.attrib.items():\n        yield prefix + [k], v\n\n    for child in r:\n\n        if child.tag == \"TAG\":\n            if r.find(\"VALUE\") is not None:\n                yield prefix + [child.text], r.find(\"VALUE\").text\n                continue\n\n        if child.tag == \"VALUE\":\n            if r.find(\"TAG\") is not None:\n                continue\n\n        for i in parse_record(child, prefix + [r.tag]):\n            yield i\n\n# Function to get metadata from an SRA accession\ndef fetch_sra_metadata(sra_id):\n    print(\"Fetching metadata for %s\" % sra_id)\n    record = entrez(\n        'efetch',\n        db=\"sra\",\n        id=sra_id\n    )\n\n    # Keep track of the data as a simple dict\n    dat = {}\n\n    dat[\"LAYOUT\"] = record.find(\"EXPERIMENT_PACKAGE\").find(\"EXPERIMENT\").find(\"DESIGN\").find(\"LIBRARY_DESCRIPTOR\").find(\"LIBRARY_LAYOUT\")[0].tag\n\n    # Iterate over every entry in the record\n    for prefix, val in parse_record(record):\n\n        # Lots of things to skip as not being worth saving\n        if len(prefix) < 2:\n            continue\n        if prefix[0] != \"EXPERIMENT_PACKAGE_SET\":\n            continue\n        if prefix[1] != \"EXPERIMENT_PACKAGE\":\n            continue\n        if len(val) == 0:\n            continue\n        if prefix[-1] == \"namespace\":\n            continue\n        if \"SRAFiles\" in prefix:\n            continue\n        if \"CloudFiles\" in prefix:\n            continue\n        if \"Statistics\" in prefix:\n            continue\n        if \"Databases\" in prefix:\n            continue\n\n        # This item is worth saving\n        dat[\"_\".join(prefix[2:])] = val\n\n\n    # Make sure that we have the RUN_SET_accession\n    assert \"RUN_SET_accession\" in dat\n\n    return dat\n\n# Fetch the metadata\ndat = fetch_sra_metadata(\"${sra_id}\")\n\n# Get the SRR accession from the ID that we used as the input\nsra_accession = dat[\"RUN_SET_accession\"]\n\n# Write out to a file\nwith gzip.open(\"%s.metadata.json.gz\" % sra_accession, \"wt\") as fo:\n    fo.write(\n        json.dumps(\n            dat,\n            indent=4\n        )\n    )\n\n\"\"\"\n}",
        "nb_lignes_process": 135,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport json\nimport gzip\nimport requests\nfrom time import sleep\nimport xml.etree.ElementTree as ET\nimport pandas as pd\n\nprint(\"Fetching metadata for SRA ID ${sra_id}\")\n\n# Make a function to fetch data from the Entrez API\n# while retrying if any errors are encountered\ndef request_url(url, max_retries=10, pause_seconds=0.5):\n    # Try the request `max_retries` times, with a `pause_seconds` pause in-between\n    assert isinstance(max_retries, int)\n    assert max_retries > 0\n    for i in range(max_retries):\n        r = requests.get(url)\n        if r.status_code == 200:\n            return r.text\n        print(\"Caught an error on attempt #%d, retrying\" % (i + 1))\n        sleep(pause_seconds)\n\n# Format an Entrez query, execute the request, and return the result\ndef entrez(mode, **kwargs):\n    assert mode in [\"efetch\", \"esearch\", \"elink\"]\n\n    kwargs_str = \"&\".join([\"%s=%s\" % (k, v) for k, v in kwargs.items()])\n    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/%s.fcgi?%s\"\n    url = url % (mode, kwargs_str)\n\n    # Get the text response\n    response_text = request_url(url)\n\n    # Parse the XML\n    return ET.fromstring(response_text)\n\n# Function to iteratively parse the XML record\ndef parse_record(r, prefix=[]):\n\n    if r.text is not None:\n        if len(r.text.replace(\"\\\\n\", \"\")) > 0:\n            yield prefix + [r.tag], r.text.replace(\"\\\\n\", \"\")\n\n    for k, v in r.attrib.items():\n        yield prefix + [k], v\n\n    for child in r:\n\n        if child.tag == \"TAG\":\n            if r.find(\"VALUE\") is not None:\n                yield prefix + [child.text], r.find(\"VALUE\").text\n                continue\n\n        if child.tag == \"VALUE\":\n            if r.find(\"TAG\") is not None:\n                continue\n\n        for i in parse_record(child, prefix + [r.tag]):\n            yield i\n\n# Function to get metadata from an SRA accession\ndef fetch_sra_metadata(sra_id):\n    print(\"Fetching metadata for %s\" % sra_id)\n    record = entrez(\n        'efetch',\n        db=\"sra\",\n        id=sra_id\n    )\n\n    # Keep track of the data as a simple dict\n    dat = {}\n\n    dat[\"LAYOUT\"] = record.find(\"EXPERIMENT_PACKAGE\").find(\"EXPERIMENT\").find(\"DESIGN\").find(\"LIBRARY_DESCRIPTOR\").find(\"LIBRARY_LAYOUT\")[0].tag\n\n    # Iterate over every entry in the record\n    for prefix, val in parse_record(record):\n\n        # Lots of things to skip as not being worth saving\n        if len(prefix) < 2:\n            continue\n        if prefix[0] != \"EXPERIMENT_PACKAGE_SET\":\n            continue\n        if prefix[1] != \"EXPERIMENT_PACKAGE\":\n            continue\n        if len(val) == 0:\n            continue\n        if prefix[-1] == \"namespace\":\n            continue\n        if \"SRAFiles\" in prefix:\n            continue\n        if \"CloudFiles\" in prefix:\n            continue\n        if \"Statistics\" in prefix:\n            continue\n        if \"Databases\" in prefix:\n            continue\n\n        # This item is worth saving\n        dat[\"_\".join(prefix[2:])] = val\n\n\n    # Make sure that we have the RUN_SET_accession\n    assert \"RUN_SET_accession\" in dat\n\n    return dat\n\n# Fetch the metadata\ndat = fetch_sra_metadata(\"${sra_id}\")\n\n# Get the SRR accession from the ID that we used as the input\nsra_accession = dat[\"RUN_SET_accession\"]\n\n# Write out to a file\nwith gzip.open(\"%s.metadata.json.gz\" % sra_accession, \"wt\") as fo:\n    fo.write(\n        json.dumps(\n            dat,\n            indent=4\n        )\n    )\n\n\"\"\"",
        "nb_lignes_script": 123,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sra_id"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"",
            "label \"io_net\"",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "joinMetadata": {
        "name_process": "joinMetadata",
        "string_process": "\nprocess joinMetadata {\n    container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"\n    label \"mem_medium\"\n    errorStrategy 'retry'\n    \n    input:\n    file metadata_json_list\n\n    output:\n    file \"${params.accession}.metadata.csv\"\n\n\"\"\"\n#!/usr/bin/env python3\nimport os\nimport json\nimport gzip\nimport pandas as pd\n\n# Parse the list of files to read in\nfp_list = []\nfor fp in os.listdir(\".\"):\n    if \".metadata.json.gz\" in fp:\n        assert fp.endswith(\".metadata.json.gz\")\n        fp_list.append(fp)\nprint(\"Reading in a set of %d metadata files\" % len(fp_list))\n\n# Iterate over all of the SRA IDs from this BioProject\nmetadata_df = pd.DataFrame([\n    json.load(gzip.open(fp, \"rt\"))\n    for fp in fp_list\n])\nprint(\"Found records for %d SRA accessions\" % metadata_df.shape[0])\n\n# Filter down to just the PAIRED records\nmetadata_df = metadata_df.query(\"LAYOUT == 'PAIRED'\")\nprint(\"Kept records for %d PAIRED SRA accessions\" % metadata_df.shape[0])\n\n# Make sure that we have a column for the SRR* accession\nassert \"RUN_SET_accession\" in metadata_df.columns.values\nassert metadata_df[\"RUN_SET_accession\"].isnull().sum() == 0\n\n# Save the complete metadata to a file\nmetadata_df.to_csv(\"${params.accession}.metadata.csv\", index=None)\n\n\"\"\"\n}",
        "nb_lignes_process": 45,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport os\nimport json\nimport gzip\nimport pandas as pd\n\n# Parse the list of files to read in\nfp_list = []\nfor fp in os.listdir(\".\"):\n    if \".metadata.json.gz\" in fp:\n        assert fp.endswith(\".metadata.json.gz\")\n        fp_list.append(fp)\nprint(\"Reading in a set of %d metadata files\" % len(fp_list))\n\n# Iterate over all of the SRA IDs from this BioProject\nmetadata_df = pd.DataFrame([\n    json.load(gzip.open(fp, \"rt\"))\n    for fp in fp_list\n])\nprint(\"Found records for %d SRA accessions\" % metadata_df.shape[0])\n\n# Filter down to just the PAIRED records\nmetadata_df = metadata_df.query(\"LAYOUT == 'PAIRED'\")\nprint(\"Kept records for %d PAIRED SRA accessions\" % metadata_df.shape[0])\n\n# Make sure that we have a column for the SRR* accession\nassert \"RUN_SET_accession\" in metadata_df.columns.values\nassert metadata_df[\"RUN_SET_accession\"].isnull().sum() == 0\n\n# Save the complete metadata to a file\nmetadata_df.to_csv(\"${params.accession}.metadata.csv\", index=None)\n\n\"\"\"",
        "nb_lignes_script": 33,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "metadata_json_list"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"",
            "label \"mem_medium\"",
            "errorStrategy 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "downloadSRA": {
        "name_process": "downloadSRA",
        "string_process": "\nprocess downloadSRA {\n    container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"\n    label \"io_net\"\n    errorStrategy 'finish'\n    \n    input:\n    val accession\n\n    output:\n    path \"${accession}\", optional: true\n\n\n\"\"\"\nset -e\n\necho \"Getting the URL for the SRA file\"\nACC=$accession\ncurl -o \\${ACC}.json -s -X POST \"https://www.ncbi.nlm.nih.gov/Traces/sdl/1/retrieve?acc=\\${ACC}&location=s3.us-west-2\"\n\nsra_url=\"\\$(cat \\${ACC}.json | jq '.[0] | .files | .[0] | .link' | tr -d '\"')\"\necho \"Download URL is \\$sra_url\"\n\nif [[ \\$sra_url == null ]]; then\n    cat \\${ACC}.json\n    echo \"Stopping\"\nelse\n    echo \"Downloading\"\n    wget -O \\${ACC} \\${sra_url}\n\n    echo \"Done\"\nfi\n\"\"\"\n\n}",
        "nb_lignes_process": 33,
        "string_script": "\"\"\"\nset -e\n\necho \"Getting the URL for the SRA file\"\nACC=$accession\ncurl -o \\${ACC}.json -s -X POST \"https://www.ncbi.nlm.nih.gov/Traces/sdl/1/retrieve?acc=\\${ACC}&location=s3.us-west-2\"\n\nsra_url=\"\\$(cat \\${ACC}.json | jq '.[0] | .files | .[0] | .link' | tr -d '\"')\"\necho \"Download URL is \\$sra_url\"\n\nif [[ \\$sra_url == null ]]; then\n    cat \\${ACC}.json\n    echo \"Stopping\"\nelse\n    echo \"Downloading\"\n    wget -O \\${ACC} \\${sra_url}\n\n    echo \"Done\"\nfi\n\"\"\"",
        "nb_lignes_script": 19,
        "language_script": "bash",
        "tools": [
            "CURLS"
        ],
        "tools_url": [
            "https://bio.tools/CURLS"
        ],
        "tools_dico": [
            {
                "name": "CURLS",
                "uri": "https://bio.tools/CURLS",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3335",
                            "term": "Cardiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3421",
                            "term": "Surgery"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "Pathology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3335",
                            "term": "Cardiovascular medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3421",
                            "term": "https://en.wikipedia.org/wiki/Surgery"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "Disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "https://en.wikipedia.org/wiki/Pathology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "towards a wider use of basic echo applications in Africa.\n\nBACKGROUND:Point-of-care ultrasound is increasingly being used as a diagnostic tool in resource-limited settings. The majority of existing ultrasound protocols have been developed and implemented in high-resource settings. In sub-Saharan Africa (SSA), patients with heart failure of various etiologies commonly present late in the disease process, with a similar syndrome of dyspnea, edema and cardiomegaly on chest X-ray. The causes of heart failure in SSA differ from those in high-resource settings. Point-of-care ultrasound has the potential to identify the underlying etiology of heart failure, and lead to targeted therapy.\n\n||| HOMEPAGE MISSING!.\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'ultrasound', 'Cardiac ultrasound resource-limited settings', 'high-resource', 'cardiomegaly SSA'",
                "homepage": "https://www.ncbi.nlm.nih.gov/pubmed/?term=31883027"
            }
        ],
        "inputs": [
            "accession"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"",
            "label \"io_net\"",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "extractSRA": {
        "name_process": "extractSRA",
        "string_process": "\nprocess extractSRA {\n    container \"quay.io/fhcrc-microbiome/get_sra:v0.4\"\n    label \"io_limited\"\n    errorStrategy 'finish'\n    publishDir \"${output_folder}\", mode: \"copy\", overwrite: \"true\"\n    \n    input:\n    file accession\n\n    output:\n    tuple val(\"${accession.name}\"), file(\"*fastq.gz\")\n\n\n    \"\"\"\n    set -e\n\n    fastq-dump \\\n        --split-files \\\n        --outdir ./ \\\n        ${accession}\n\n    rm ${accession}\n\n    echo \"Compressing downloaded FASTQ files\"\n    pigz ${accession.name}*\n\n    echo \"Done\"\n    \"\"\"\n\n}",
        "nb_lignes_process": 29,
        "string_script": "\"\"\"\n    set -e\n\n    fastq-dump \\\n        --split-files \\\n        --outdir ./ \\\n        ${accession}\n\n    rm ${accession}\n\n    echo \"Compressing downloaded FASTQ files\"\n    pigz ${accession.name}*\n\n    echo \"Done\"\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "accession"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/get_sra:v0.4\"",
            "label \"io_limited\"",
            "errorStrategy 'finish'",
            "publishDir \"${output_folder}\", mode: \"copy\", overwrite: \"true\""
        ],
        "when": "",
        "stub": ""
    },
    "gatherReadnames": {
        "name_process": "gatherReadnames",
        "string_process": "\nprocess gatherReadnames {\n    container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"\n    label \"io_limited\"\n    errorStrategy 'finish'\n    publishDir \"${output_folder}\", mode: \"copy\", overwrite: \"true\"\n\n    input:\n        val manifestStr\n        file metadata_csv\n    \n    output:\n        file \"${params.accession}.csv\"\n\n\"\"\"\n#!/usr/bin/env python3\nimport pandas as pd\n\nwith open(\"manifest.csv\", \"wt\") as fo:\n    fo.write(\\\"\\\"\\\"${manifestStr}\\\"\\\"\\\")\n\nmanifest_df = pd.read_csv(\"manifest.csv\")\nprint(manifest_df)\n\n# Read in the metadata\nmetadata_df = pd.read_csv(\"${metadata_csv}\")\nprint(metadata_df)\n\n# Join the two together\nmetadata_df = pd.concat([\n    manifest_df.set_index(\"specimen\"),\n    metadata_df.set_index(\"RUN_SET_accession\")\n], axis=1, sort=True)\n\n# Write out to a file\nmetadata_df.reset_index(\n).rename(\n    columns=dict([(\"index\", \"specimen\")])\n).to_csv(\n    \"${params.accession}.csv\", \n    index=None\n)\n\"\"\"\n}",
        "nb_lignes_process": 42,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport pandas as pd\n\nwith open(\"manifest.csv\", \"wt\") as fo:\n    fo.write(\\\"\\\"\\\"${manifestStr}\\\"\\\"\\\")\n\nmanifest_df = pd.read_csv(\"manifest.csv\")\nprint(manifest_df)\n\n# Read in the metadata\nmetadata_df = pd.read_csv(\"${metadata_csv}\")\nprint(metadata_df)\n\n# Join the two together\nmetadata_df = pd.concat([\n    manifest_df.set_index(\"specimen\"),\n    metadata_df.set_index(\"RUN_SET_accession\")\n], axis=1, sort=True)\n\n# Write out to a file\nmetadata_df.reset_index(\n).rename(\n    columns=dict([(\"index\", \"specimen\")])\n).to_csv(\n    \"${params.accession}.csv\", \n    index=None\n)\n\"\"\"",
        "nb_lignes_script": 28,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "manifestStr",
            "metadata_csv"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"",
            "label \"io_limited\"",
            "errorStrategy 'finish'",
            "publishDir \"${output_folder}\", mode: \"copy\", overwrite: \"true\""
        ],
        "when": "",
        "stub": ""
    },
    "assembly": {
        "name_process": "assembly",
        "string_process": "\nprocess assembly {\n    tag \"De novo metagenomic assembly\"\n    container \"${container__assembler}\"\n    label 'mem_veryhigh'\n    errorStrategy \"finish\"\n\n    publishDir \"${params.output_folder}/assembly/${specimen}\", mode: \"copy\"\n\n    input:\n        tuple specimen, file(R1), file(R2)\n    \n    output:\n        tuple specimen, file(\"${specimen}.contigs.fasta.gz\"), file(\"${specimen}.megahit.log\")\n    \n\"\"\"\nset -e \n\ndate\necho -e \"Running Megahit\\\\n\"\n\nmegahit \\\n    -1 ${R1} -2 ${R2} \\\n    -o OUTPUT \\\n    -t ${task.cpus}\n\ndate\necho -e \"\\\\nMaking sure output files are not empty\\\\n\"\n[[ \\$(cat OUTPUT/final.contigs.fa | wc -l) > 0 ]]\n\ndate\necho -e \"\\\\nRenaming output files\\\\n\"\n\nmv OUTPUT/log \"${specimen}.megahit.log\"\n\n# Add the specimen name to the contig name\ncat OUTPUT/final.contigs.fa | sed 's/>/>${specimen}__GENE__/' | sed 's/ /__/g' | gzip -c > \"${specimen}.contigs.fasta.gz\"\n\ndate\necho -e \"\\\\nDone\\\\n\"\n\"\"\"\n}",
        "nb_lignes_process": 40,
        "string_script": "\"\"\"\nset -e \n\ndate\necho -e \"Running Megahit\\\\n\"\n\nmegahit \\\n    -1 ${R1} -2 ${R2} \\\n    -o OUTPUT \\\n    -t ${task.cpus}\n\ndate\necho -e \"\\\\nMaking sure output files are not empty\\\\n\"\n[[ \\$(cat OUTPUT/final.contigs.fa | wc -l) > 0 ]]\n\ndate\necho -e \"\\\\nRenaming output files\\\\n\"\n\nmv OUTPUT/log \"${specimen}.megahit.log\"\n\n# Add the specimen name to the contig name\ncat OUTPUT/final.contigs.fa | sed 's/>/>${specimen}__GENE__/' | sed 's/ /__/g' | gzip -c > \"${specimen}.contigs.fasta.gz\"\n\ndate\necho -e \"\\\\nDone\\\\n\"\n\"\"\"",
        "nb_lignes_script": 25,
        "language_script": "bash",
        "tools": [
            "datelife",
            "MEGAHIT"
        ],
        "tools_url": [
            "https://bio.tools/datelife",
            "https://bio.tools/megahit"
        ],
        "tools_dico": [
            {
                "name": "datelife",
                "uri": "https://bio.tools/datelife",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3068",
                            "term": "Literature and language"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0194",
                            "term": "Phylogenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3068",
                            "term": "Language"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3068",
                            "term": "Literature"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Query and retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Imputation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Database retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic tree reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Data imputation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Leveraging databases and analytical tools to reveal the dated Tree of Life.\n\nR package containing datelife's core functionality.\n\nDashboard \u22c5 phylotastic/datelife.\n\nGet a phylogenetic tree with branch lengths proportional to geologic time (aka a chronogram) of any two or more lineages of interest to you: use this R package or go to www.datelife.org to make a query of chronograms available for your lineages in the Open Tree of Life\u2019s tree store.\n\nWelcome to the DateLife project.\n\nAn R package, datelife for doing the calculations.\n\nCode coverage done right",
                "homepage": "http://www.datelife.org/"
            },
            {
                "name": "MEGAHIT",
                "uri": "https://bio.tools/megahit",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0610",
                            "term": "Ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Single node assembler for large and complex metagenomics NGS reads, such as soil. It makes use of succinct de Bruijn graph to achieve low memory usage, whereas its goal is not to make memory usage as low as possible.",
                "homepage": "https://github.com/voutcn/megahit"
            }
        ],
        "inputs": [
            "R1",
            "R2",
            "specimen"
        ],
        "nb_inputs": 3,
        "outputs": [
            "specimen"
        ],
        "nb_outputs": 1,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"De novo metagenomic assembly\"",
            "container \"${container__assembler}\"",
            "label 'mem_veryhigh'",
            "errorStrategy \"finish\"",
            "publishDir \"${params.output_folder}/assembly/${specimen}\", mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    },
    "prodigal": {
        "name_process": "prodigal",
        "string_process": "\nprocess prodigal {\n    tag \"Identify protein-coding genes\"\n    container 'quay.io/biocontainers/prodigal:2.6.3--h516909a_2'\n    label 'mem_medium'\n    errorStrategy \"finish\"\n    publishDir \"${params.output_folder}/assembly/${specimen}/\", mode: \"copy\"\n\n    input:\n        tuple val(specimen), file(contigs), file(spades_log)\n    \n    output:\n        tuple val(specimen), file(\"${specimen}.faa.gz\")\n        file \"${specimen}.gff.gz\"\n    \n\"\"\"\nset -e \n\ngunzip -c ${contigs} > ${specimen}.contigs.fasta\n\nprodigal \\\n    -a ${specimen}.faa \\\n    -i  ${specimen}.contigs.fasta \\\n    -f gff \\\n    -o ${specimen}.gff \\\n    -p meta\n\ngzip ${specimen}.gff\ngzip ${specimen}.faa\n\n\"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "\"\"\"\nset -e \n\ngunzip -c ${contigs} > ${specimen}.contigs.fasta\n\nprodigal \\\n    -a ${specimen}.faa \\\n    -i  ${specimen}.contigs.fasta \\\n    -f gff \\\n    -o ${specimen}.gff \\\n    -p meta\n\ngzip ${specimen}.gff\ngzip ${specimen}.faa\n\n\"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "contigs",
            "spades_log"
        ],
        "nb_inputs": 3,
        "outputs": [
            "specimen"
        ],
        "nb_outputs": 1,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Identify protein-coding genes\"",
            "container 'quay.io/biocontainers/prodigal:2.6.3--h516909a_2'",
            "label 'mem_medium'",
            "errorStrategy \"finish\"",
            "publishDir \"${params.output_folder}/assembly/${specimen}/\", mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    },
    "parseGeneAnnotations": {
        "name_process": "parseGeneAnnotations",
        "string_process": "\nprocess parseGeneAnnotations {\n    tag \"Summarize every assembled gene\"\n    container \"${container__pandas}\"\n    label 'mem_medium'\n    errorStrategy 'finish'\n    \n    input:\n    tuple val(specimen), file(faa)\n    \n    output:\n    tuple val(specimen), file(\"${specimen}.gene_annotations.csv.gz\")\n\n\"\"\"\n#!/usr/bin/env python3\nimport gzip\nimport pandas as pd\n\n# Function to parse a FASTA file with information encoded by Prodigal and Megahit in the header\ndef parse_prodigal_faa(fp):\n\n    # Keep track of all of the information for each assembled gene\n    dat = []\n\n    # Open a connection to the file\n    with gzip.open(fp, \"rt\") as f:\n\n        # Iterate over every line\n        for line in f:\n\n            # Skip the non-header lines\n            if line.startswith(\">\") is False:\n                continue\n\n            # Add the information for this header\n            dat.append(parse_header(line))\n\n    return pd.DataFrame(dat)\n\n# Function to parse a single header\ndef parse_header(line):\n\n    # The header line follows a format somewhat like this:\n    # >Mock__15__GENE__k21_0__flag=1__multi=20.5919__len=48411_1 # 510 # 716 # -1 # ID=1_1;partial=00;start_type=ATG;rbs_motif=GGAG/GAGG;rbs_spacer=5-10bp;gc_cont=0.386\n    #  ------------------------GENE_NAME------------------------   START STOP  STRAND\n    #  SPECIMEN        CONTIG -----------NAME_DETAILS-----------                    -----------------------------------HEADER_DETAILS-----------------------------------\n\n    # First let's just get the gene name\n    gene_name, header = line[1:].rstrip(\"\\\\n\").lstrip(\">\").split(\" \", 1)\n\n    # The \"__GENE__\" separator is used to preserve the specimen name within the gene name\n    specimen, gene_remainder = gene_name.split(\"__GENE__\", 1)\n\n    # The contig name is the next field encoded in the gene name\n    contig, gene_remainder = gene_remainder.split(\"__\", 1)\n\n    # The rest of the gene details are encoded in the gene_remainder (delimited by __)\n    # as well as the final string in the header (delimited by ;)\n\n    # We can parse the gene_remainder details with the __ delimiter and the =\n    # We have to make sure to strip off the gene index number\n\n    output_dat = dict([\n        (field.split(\"=\",1)[0], field.split(\"=\",1)[1].split(\"_\", 1)[0])\n        for field in gene_remainder.split(\"__\")\n    ])\n\n    # Now let's add in the details from the header\n    start, stop, strand, header = header.strip(\" \").strip(\"#\").split(\" # \")\n\n    # Iterate over each of the elements provided\n    for f in header.split(\";\"):\n        if \"=\" in f:\n            k, v = f.split(\"=\", 1)\n            output_dat[k] = v\n    assert \"gc_cont\" in output_dat, (output_dat, line)\n\n    # Add the other metrics to the output\n    output_dat[\"gene_name\"] = gene_name\n    output_dat[\"start\"] = int(start)\n    output_dat[\"stop\"] = int(stop)\n    output_dat[\"strand\"] = strand\n    output_dat[\"specimen\"] = specimen\n    output_dat[\"contig\"] = contig\n\n    # Make some customizations to the data types\n    del output_dat[\"ID\"]\n    for k, t in [\n        (\"strand\", int), \n        (\"len\", int), \n        (\"multi\", float), \n        (\"gc_cont\", float)\n    ]:\n        assert k in output_dat, (output_dat, line)\n        output_dat[k] = t(output_dat[k])\n\n    return output_dat\n\n# Parse and write out to a file\nparse_prodigal_faa(\"${faa}\").to_csv(\n    \"${specimen}.gene_annotations.csv.gz\",\n    index = None,\n    compression = \"gzip\"\n)\n\n\n\"\"\"\n}",
        "nb_lignes_process": 106,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport gzip\nimport pandas as pd\n\n# Function to parse a FASTA file with information encoded by Prodigal and Megahit in the header\ndef parse_prodigal_faa(fp):\n\n    # Keep track of all of the information for each assembled gene\n    dat = []\n\n    # Open a connection to the file\n    with gzip.open(fp, \"rt\") as f:\n\n        # Iterate over every line\n        for line in f:\n\n            # Skip the non-header lines\n            if line.startswith(\">\") is False:\n                continue\n\n            # Add the information for this header\n            dat.append(parse_header(line))\n\n    return pd.DataFrame(dat)\n\n# Function to parse a single header\ndef parse_header(line):\n\n    # The header line follows a format somewhat like this:\n    # >Mock__15__GENE__k21_0__flag=1__multi=20.5919__len=48411_1 # 510 # 716 # -1 # ID=1_1;partial=00;start_type=ATG;rbs_motif=GGAG/GAGG;rbs_spacer=5-10bp;gc_cont=0.386\n    #  ------------------------GENE_NAME------------------------   START STOP  STRAND\n    #  SPECIMEN        CONTIG -----------NAME_DETAILS-----------                    -----------------------------------HEADER_DETAILS-----------------------------------\n\n    # First let's just get the gene name\n    gene_name, header = line[1:].rstrip(\"\\\\n\").lstrip(\">\").split(\" \", 1)\n\n    # The \"__GENE__\" separator is used to preserve the specimen name within the gene name\n    specimen, gene_remainder = gene_name.split(\"__GENE__\", 1)\n\n    # The contig name is the next field encoded in the gene name\n    contig, gene_remainder = gene_remainder.split(\"__\", 1)\n\n    # The rest of the gene details are encoded in the gene_remainder (delimited by __)\n    # as well as the final string in the header (delimited by ;)\n\n    # We can parse the gene_remainder details with the __ delimiter and the =\n    # We have to make sure to strip off the gene index number\n\n    output_dat = dict([\n        (field.split(\"=\",1)[0], field.split(\"=\",1)[1].split(\"_\", 1)[0])\n        for field in gene_remainder.split(\"__\")\n    ])\n\n    # Now let's add in the details from the header\n    start, stop, strand, header = header.strip(\" \").strip(\"#\").split(\" # \")\n\n    # Iterate over each of the elements provided\n    for f in header.split(\";\"):\n        if \"=\" in f:\n            k, v = f.split(\"=\", 1)\n            output_dat[k] = v\n    assert \"gc_cont\" in output_dat, (output_dat, line)\n\n    # Add the other metrics to the output\n    output_dat[\"gene_name\"] = gene_name\n    output_dat[\"start\"] = int(start)\n    output_dat[\"stop\"] = int(stop)\n    output_dat[\"strand\"] = strand\n    output_dat[\"specimen\"] = specimen\n    output_dat[\"contig\"] = contig\n\n    # Make some customizations to the data types\n    del output_dat[\"ID\"]\n    for k, t in [\n        (\"strand\", int), \n        (\"len\", int), \n        (\"multi\", float), \n        (\"gc_cont\", float)\n    ]:\n        assert k in output_dat, (output_dat, line)\n        output_dat[k] = t(output_dat[k])\n\n    return output_dat\n\n# Parse and write out to a file\nparse_prodigal_faa(\"${faa}\").to_csv(\n    \"${specimen}.gene_annotations.csv.gz\",\n    index = None,\n    compression = \"gzip\"\n)\n\n\n\"\"\"",
        "nb_lignes_script": 93,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "faa"
        ],
        "nb_inputs": 2,
        "outputs": [
            "specimen"
        ],
        "nb_outputs": 1,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Summarize every assembled gene\"",
            "container \"${container__pandas}\"",
            "label 'mem_medium'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "annotateAssemblies": {
        "name_process": "annotateAssemblies",
        "string_process": "\nprocess annotateAssemblies {\n    tag \"Summarize every assembled gene\"\n    container \"${container__pandas}\"\n    label 'mem_medium'\n    errorStrategy 'finish'\n\n    publishDir \"${params.output_folder}/assembly/${specimen}\", mode: \"copy\"\n    \n    input:\n    tuple val(specimen), file(assembly_csv), file(alignment_tsv)\n    \n    output:\n    file \"${specimen}.csv.gz\"\n\n\"\"\"\n#!/usr/bin/env python3\nimport pandas as pd\n\nprint(\"Reading in assembly information for ${specimen}\")\nassembly_df = pd.read_csv(\"${assembly_csv}\")\nprint(\"Read in %d assembled genes\" % assembly_df.shape[0])\n\n# Read in the table linking each allele to the gene catalog\ngene_alignments = pd.read_csv(\n    \"${alignment_tsv}\",\n    sep = \"\\\\t\",\n    header = None,\n    names = [\n        \"allele\",\n        \"gene\",\n        \"pct_iden\",\n        \"alignment_len\",\n        \"allele_len\",\n        \"gene_len\"\n    ],\n    compression = \"gzip\"\n)\n\n# Add the gene name to the gene annotation table\nassembly_df[\"catalog_gene\"] = assembly_df[\"gene_name\"].apply(\n    gene_alignments.groupby(\n        \"allele\"\n    ).head(\n        1\n    ).set_index(\n        \"allele\"\n    )[\n        \"gene\"\n    ].get\n)\nprint(\"%d / %d genes have an annotation in the gene catalog\" % (assembly_df[\"catalog_gene\"].dropna().shape[0], assembly_df.shape[0]))\n\n# Save and exit\nassembly_df.to_csv(\n    \"${specimen}.csv.gz\",\n    index = None,\n    compression = \"gzip\"\n)\n\n\"\"\"\n}",
        "nb_lignes_process": 60,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport pandas as pd\n\nprint(\"Reading in assembly information for ${specimen}\")\nassembly_df = pd.read_csv(\"${assembly_csv}\")\nprint(\"Read in %d assembled genes\" % assembly_df.shape[0])\n\n# Read in the table linking each allele to the gene catalog\ngene_alignments = pd.read_csv(\n    \"${alignment_tsv}\",\n    sep = \"\\\\t\",\n    header = None,\n    names = [\n        \"allele\",\n        \"gene\",\n        \"pct_iden\",\n        \"alignment_len\",\n        \"allele_len\",\n        \"gene_len\"\n    ],\n    compression = \"gzip\"\n)\n\n# Add the gene name to the gene annotation table\nassembly_df[\"catalog_gene\"] = assembly_df[\"gene_name\"].apply(\n    gene_alignments.groupby(\n        \"allele\"\n    ).head(\n        1\n    ).set_index(\n        \"allele\"\n    )[\n        \"gene\"\n    ].get\n)\nprint(\"%d / %d genes have an annotation in the gene catalog\" % (assembly_df[\"catalog_gene\"].dropna().shape[0], assembly_df.shape[0]))\n\n# Save and exit\nassembly_df.to_csv(\n    \"${specimen}.csv.gz\",\n    index = None,\n    compression = \"gzip\"\n)\n\n\"\"\"",
        "nb_lignes_script": 45,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "assembly_csv",
            "alignment_tsv"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Summarize every assembled gene\"",
            "container \"${container__pandas}\"",
            "label 'mem_medium'",
            "errorStrategy 'finish'",
            "publishDir \"${params.output_folder}/assembly/${specimen}\", mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    },
    "shard_genes": {
        "name_process": "shard_genes",
        "string_process": "\nprocess shard_genes {\n    tag \"Split the gene catalog into smaller shards\"\n    container \"ubuntu:18.04\"\n    label 'mem_medium'\n    errorStrategy 'finish'\n    \n    input:\n    file fasta_gz\n    \n    output:\n    file \"genes.shard.*.fasta.gz\"\n    \n\"\"\"\n#!/bin/bash\n\nset -e\n\nsplit --additional-suffix .fasta -l 1000000 <(gunzip -c ${fasta_gz}) genes.shard.\n\ngzip genes.shard.*.fasta\n\"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\nsplit --additional-suffix .fasta -l 1000000 <(gunzip -c ${fasta_gz}) genes.shard.\n\ngzip genes.shard.*.fasta\n\"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fasta_gz"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Split the gene catalog into smaller shards\"",
            "container \"ubuntu:18.04\"",
            "label 'mem_medium'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "diamond_tax": {
        "name_process": "diamond_tax",
        "string_process": "\nprocess diamond_tax {\n    tag \"Annotate genes by taxonomy\"\n    container \"quay.io/fhcrc-microbiome/famli:v1.5\"\n    label 'mem_veryhigh'\n\n    input:\n    file query\n    file diamond_tax_db\n    \n    output:\n    file \"genes.tax.aln.gz\"\n\n    \n\"\"\"\nset -e\n\ndiamond \\\n    blastp \\\n    --db ${diamond_tax_db} \\\n    --query ${query} \\\n    --out genes.tax.aln.gz \\\n    --outfmt 102 \\\n    --id ${params.min_identity} \\\n    --top ${100 - params.min_identity} \\\n    --block-size ${task.memory.toMega() / (1024 * 6)} \\\n    --threads ${task.cpus} \\\n    --compress 1\n\nrm ${diamond_tax_db}\n\"\"\"\n\n}",
        "nb_lignes_process": 31,
        "string_script": "\"\"\"\nset -e\n\ndiamond \\\n    blastp \\\n    --db ${diamond_tax_db} \\\n    --query ${query} \\\n    --out genes.tax.aln.gz \\\n    --outfmt 102 \\\n    --id ${params.min_identity} \\\n    --top ${100 - params.min_identity} \\\n    --block-size ${task.memory.toMega() / (1024 * 6)} \\\n    --threads ${task.cpus} \\\n    --compress 1\n\nrm ${diamond_tax_db}\n\"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [
            "Diamond",
            "BLASTP-ACC"
        ],
        "tools_url": [
            "https://bio.tools/diamond",
            "https://bio.tools/BLASTP-ACC"
        ],
        "tools_dico": [
            {
                "name": "Diamond",
                "uri": "https://bio.tools/diamond",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0258",
                                    "term": "Sequence alignment analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Sequence aligner for protein and translated DNA searches and functions as a drop-in replacement for the NCBI BLAST software tools. It is suitable for protein-protein search as well as DNA-protein search on short reads and longer sequences including contigs and assemblies, providing a speedup of BLAST ranging up to x20,000.",
                "homepage": "https://github.com/bbuchfink/diamond"
            },
            {
                "name": "BLASTP-ACC",
                "uri": "https://bio.tools/BLASTP-ACC",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3297",
                            "term": "Biotechnology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Structure analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Structural bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Biomolecular structure"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0495",
                                    "term": "Local alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2421",
                                    "term": "Database search"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3802",
                                    "term": "Sorting"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0495",
                                    "term": "Local sequence alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0495",
                                    "term": "Sequence alignment (local)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2421",
                                    "term": "Search"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Parallel Architecture and Hardware Accelerator Design for BLAST-based Protein Sequence Alignment.\n\nIn this study, we design a hardware accelerator for a widely used sequence alignment algorithm, the basic local alignment search tool for proteins (BLASTP). The architecture of the proposed accelerator consists of five stages: a new systolic-array-based one-hit finding stage, a novel RAM-REG-based two-hit finding stage, a refined ungapped extension stage, a faster gapped extension stage, and a highly efficient parallel sorter. The system is implemented on an Altera Stratix V FPGA with a processing speed of more than 500 giga cell updates per second (GCUPS). It can receive a query sequence, compare it with the sequences in the database, and generate a list sorted in descending order of the similarity scores between the query sequence and the subject sequences.\n\n||| HOMEPAGE MISSING!.\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'accelerator', 'Altera', 'Stratix', 'RAM-REG-based'",
                "homepage": "https://www.ncbi.nlm.nih.gov/pubmed/?term=31581096"
            }
        ],
        "inputs": [
            "query",
            "diamond_tax_db"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Annotate genes by taxonomy\"",
            "container \"quay.io/fhcrc-microbiome/famli:v1.5\"",
            "label 'mem_veryhigh'"
        ],
        "when": "",
        "stub": ""
    },
    "join_tax": {
        "name_process": "join_tax",
        "string_process": "\nprocess join_tax {\n    tag \"Concatenate taxonomy annotation files\"\n    container \"ubuntu:18.04\"\n    label 'mem_medium'\n    publishDir \"${params.output_folder}/annot/\", mode: \"copy\"\n\n    input:\n    file \"genes.tax.aln.*.gz\"\n    \n    output:\n    file \"genes.tax.aln.gz\"\n\n    \n\"\"\"\nset -e\n\nfor fp in genes.tax.aln.*.gz; do\n\n    cat \\$fp\n    rm \\$fp\n\ndone > genes.tax.aln.gz\n\"\"\"\n\n}",
        "nb_lignes_process": 24,
        "string_script": "\"\"\"\nset -e\n\nfor fp in genes.tax.aln.*.gz; do\n\n    cat \\$fp\n    rm \\$fp\n\ndone > genes.tax.aln.gz\n\"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Concatenate taxonomy annotation files\"",
            "container \"ubuntu:18.04\"",
            "label 'mem_medium'",
            "publishDir \"${params.output_folder}/annot/\", mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    },
    "eggnog": {
        "name_process": "eggnog",
        "string_process": "\nprocess eggnog {\n    tag \"Annotate genes by predicted function\"\n    container \"quay.io/biocontainers/eggnog-mapper:2.0.1--py_1\"\n    label 'mem_veryhigh'\n    \n    input:\n    path query\n    path eggnog_db\n    path eggnog_dmnd\n\n    output:\n    path \"genes.emapper.annotations.gz\"\n\n    \n    \"\"\"\nset -e\n\nmkdir data\nmkdir TEMP\nmkdir SCRATCH\n\nmv ${eggnog_db} data/eggnog.db\nmv ${eggnog_dmnd} data/eggnog_proteins.dmnd\n\nemapper.py \\\n    -i ${query} \\\n    --output genes \\\n    -m \"diamond\" \\\n    --cpu ${task.cpus} \\\n    --data_dir data/ \\\n    --scratch_dir SCRATCH/ \\\n    --temp_dir TEMP/\n\ngzip genes.emapper.annotations\n    \n    \"\"\"\n\n}",
        "nb_lignes_process": 37,
        "string_script": "\"\"\"\nset -e\n\nmkdir data\nmkdir TEMP\nmkdir SCRATCH\n\nmv ${eggnog_db} data/eggnog.db\nmv ${eggnog_dmnd} data/eggnog_proteins.dmnd\n\nemapper.py \\\n    -i ${query} \\\n    --output genes \\\n    -m \"diamond\" \\\n    --cpu ${task.cpus} \\\n    --data_dir data/ \\\n    --scratch_dir SCRATCH/ \\\n    --temp_dir TEMP/\n\ngzip genes.emapper.annotations\n    \n    \"\"\"",
        "nb_lignes_script": 21,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "query",
            "eggnog_db",
            "eggnog_dmnd"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Annotate genes by predicted function\"",
            "container \"quay.io/biocontainers/eggnog-mapper:2.0.1--py_1\"",
            "label 'mem_veryhigh'"
        ],
        "when": "",
        "stub": ""
    },
    "renameGenes": {
        "name_process": "renameGenes",
        "string_process": "\nprocess renameGenes {\n    tag \"Make concise unique gene names\"\n    container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"\n    label 'io_limited'\n    errorStrategy 'finish'\n    publishDir \"${params.output_folder}/ref/\", mode: \"copy\"\n\n    input:\n    file \"input.genes.fasta.gz\"\n\n    output:\n    file \"genes.fasta.gz\"\n\n\"\"\"\n#!/usr/bin/env python3\n\nfrom Bio.SeqIO.FastaIO import SimpleFastaParser\nimport gzip\nimport uuid\n\ndef random_string(n=8):\n    return str(uuid.uuid4())[:n]\n\nused_strings = set([])\n\nwith gzip.open(\"genes.fasta.gz\", \"wt\") as fo:\n    with gzip.open(\"input.genes.fasta.gz\", \"rt\") as fi:\n        for header, seq in SimpleFastaParser(fi):\n            new_string = random_string()\n            while new_string in used_strings:\n                new_string = random_string()\n            used_strings.add(new_string)\n            fo.write(\">gene_%s_%daa\\\\n%s\\\\n\" % (new_string, len(seq), seq))\n\n\"\"\"\n}",
        "nb_lignes_process": 35,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nfrom Bio.SeqIO.FastaIO import SimpleFastaParser\nimport gzip\nimport uuid\n\ndef random_string(n=8):\n    return str(uuid.uuid4())[:n]\n\nused_strings = set([])\n\nwith gzip.open(\"genes.fasta.gz\", \"wt\") as fo:\n    with gzip.open(\"input.genes.fasta.gz\", \"rt\") as fi:\n        for header, seq in SimpleFastaParser(fi):\n            new_string = random_string()\n            while new_string in used_strings:\n                new_string = random_string()\n            used_strings.add(new_string)\n            fo.write(\">gene_%s_%daa\\\\n%s\\\\n\" % (new_string, len(seq), seq))\n\n\"\"\"",
        "nb_lignes_script": 21,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Make concise unique gene names\"",
            "container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"",
            "label 'io_limited'",
            "errorStrategy 'finish'",
            "publishDir \"${params.output_folder}/ref/\", mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    },
    "alignAlleles": {
        "name_process": "alignAlleles",
        "string_process": "\nprocess alignAlleles {\n    tag \"Match alleles to gene centroids\"\n    container \"quay.io/fhcrc-microbiome/famli:v1.5\"\n    label 'mem_medium'\n    errorStrategy 'finish'\n    \n    input:\n    tuple val(specimen), file(alleles_fasta)\n    file refdb\n    \n    output:\n    tuple val(specimen), file(\"${specimen}.gene_alignments.tsv.gz\") optional true\n\n    \"\"\"\n    set -e\n\n    echo \"Aligning ${alleles_fasta}\"\n\n    # Check to see if there are any reads\n    if (( \\$( gunzip -c ${alleles_fasta} | wc -l ) <= 1 )); then\n    \n        echo \"No alleles found in ${alleles_fasta}, skipping\"\n    \n    else\n\n        # Make the output filepath\n        fo=\"${specimen}.gene_alignments.tsv.gz\"\n        echo \"Writing out to \\$fo\"\n\n        diamond \\\n        blastp \\\n        --query ${alleles_fasta} \\\n        --out \\$fo \\\n        --threads ${task.cpus} \\\n        --db ${refdb} \\\n        --outfmt 6 qseqid sseqid pident length qlen slen \\\n        --query-cover ${params.min_coverage} \\\n        --id ${params.min_identity} \\\n        --top 0 \\\n        --block-size ${task.memory.toMega() / (1024 * 6)} \\\n        --compress 1 \\\n        --unal 0\n\n    fi\n    \"\"\"\n\n}",
        "nb_lignes_process": 46,
        "string_script": "\"\"\"\n    set -e\n\n    echo \"Aligning ${alleles_fasta}\"\n\n    # Check to see if there are any reads\n    if (( \\$( gunzip -c ${alleles_fasta} | wc -l ) <= 1 )); then\n    \n        echo \"No alleles found in ${alleles_fasta}, skipping\"\n    \n    else\n\n        # Make the output filepath\n        fo=\"${specimen}.gene_alignments.tsv.gz\"\n        echo \"Writing out to \\$fo\"\n\n        diamond \\\n        blastp \\\n        --query ${alleles_fasta} \\\n        --out \\$fo \\\n        --threads ${task.cpus} \\\n        --db ${refdb} \\\n        --outfmt 6 qseqid sseqid pident length qlen slen \\\n        --query-cover ${params.min_coverage} \\\n        --id ${params.min_identity} \\\n        --top 0 \\\n        --block-size ${task.memory.toMega() / (1024 * 6)} \\\n        --compress 1 \\\n        --unal 0\n\n    fi\n    \"\"\"",
        "nb_lignes_script": 31,
        "language_script": "bash",
        "tools": [
            "Diamond",
            "BLASTP-ACC"
        ],
        "tools_url": [
            "https://bio.tools/diamond",
            "https://bio.tools/BLASTP-ACC"
        ],
        "tools_dico": [
            {
                "name": "Diamond",
                "uri": "https://bio.tools/diamond",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0258",
                                    "term": "Sequence alignment analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Sequence aligner for protein and translated DNA searches and functions as a drop-in replacement for the NCBI BLAST software tools. It is suitable for protein-protein search as well as DNA-protein search on short reads and longer sequences including contigs and assemblies, providing a speedup of BLAST ranging up to x20,000.",
                "homepage": "https://github.com/bbuchfink/diamond"
            },
            {
                "name": "BLASTP-ACC",
                "uri": "https://bio.tools/BLASTP-ACC",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3297",
                            "term": "Biotechnology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Structure analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Structural bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Biomolecular structure"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0495",
                                    "term": "Local alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2421",
                                    "term": "Database search"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3802",
                                    "term": "Sorting"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0495",
                                    "term": "Local sequence alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0495",
                                    "term": "Sequence alignment (local)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2421",
                                    "term": "Search"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Parallel Architecture and Hardware Accelerator Design for BLAST-based Protein Sequence Alignment.\n\nIn this study, we design a hardware accelerator for a widely used sequence alignment algorithm, the basic local alignment search tool for proteins (BLASTP). The architecture of the proposed accelerator consists of five stages: a new systolic-array-based one-hit finding stage, a novel RAM-REG-based two-hit finding stage, a refined ungapped extension stage, a faster gapped extension stage, and a highly efficient parallel sorter. The system is implemented on an Altera Stratix V FPGA with a processing speed of more than 500 giga cell updates per second (GCUPS). It can receive a query sequence, compare it with the sequences in the database, and generate a list sorted in descending order of the similarity scores between the query sequence and the subject sequences.\n\n||| HOMEPAGE MISSING!.\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'accelerator', 'Altera', 'Stratix', 'RAM-REG-based'",
                "homepage": "https://www.ncbi.nlm.nih.gov/pubmed/?term=31581096"
            }
        ],
        "inputs": [
            "specimen",
            "alleles_fasta",
            "refdb"
        ],
        "nb_inputs": 3,
        "outputs": [
            "specimen"
        ],
        "nb_outputs": 1,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Match alleles to gene centroids\"",
            "container \"quay.io/fhcrc-microbiome/famli:v1.5\"",
            "label 'mem_medium'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "replaceManifest": {
        "name_process": "replaceManifest",
        "string_process": "\nprocess replaceManifest {\n    container \"${container__pandas}\"\n    label 'mem_medium'\n    errorStrategy 'finish'\n\n    input:\n        path \"input.hdf5\"\n        path manifest_csv\n\n    output:\n        path \"${params.output_prefix}.results.hdf5\"\n\n\"\"\"\n#!/usr/bin/env python3\n\nimport pandas as pd\nimport shutil\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\n# Read in the new manifest\nnew_manifest = pd.read_csv(\"${manifest_csv}\", sep=\",\")\n\n# Make sure that the new manifest has the three required columns\nfor k in [\"specimen\", \"R1\", \"R2\"]:\n    assert k in new_manifest.columns.values, \"New manifest must contain a %s column\" % k\n\n# Remove the read path columns, if present\nfor k in [\"R1\", \"R2\", \"I1\", \"I2\"]:\n    if k in new_manifest.columns.values:\n        new_manifest = new_manifest.drop(columns=k)\n\n# Assign the new name for the output file\nnew_file_name = \"${params.output_prefix}.results.hdf5\"\n\n# Copy the input file to a new output file object\nprint(\"Copying input.hdf5 to %s\" % new_file_name)\nshutil.copyfile(\"input.hdf5\", new_file_name)\n\n# Open a connection to the store\nwith pd.HDFStore(new_file_name, \"a\") as store:\n    \n    # Read in the existing manifest\n    old_manifest = pd.read_hdf(store, \"/manifest\")\n\n    # Make sure that the new and old manifest have the same set of specimens\n    old_specimen_set = set(old_manifest[\"specimen\"].tolist())\n    new_specimen_set = set(new_manifest[\"specimen\"].tolist())\n\n    for s, msg in [\n        (old_specimen_set - new_specimen_set, \"Specimens missing from old specimen list\"),\n        (new_specimen_set - old_specimen_set, \"Specimens missing from new specimen list\"),\n    ]:\n        assert len(s) == 0, \"%s: %s\" % (msg, \", \".join(list(s)))\n\n    # Write new manifest to the store\n    new_manifest.to_hdf(store, \"/manifest\")\n\nprint(\"Done\")\n\"\"\"\n\n}",
        "nb_lignes_process": 61,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport pandas as pd\nimport shutil\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\n# Read in the new manifest\nnew_manifest = pd.read_csv(\"${manifest_csv}\", sep=\",\")\n\n# Make sure that the new manifest has the three required columns\nfor k in [\"specimen\", \"R1\", \"R2\"]:\n    assert k in new_manifest.columns.values, \"New manifest must contain a %s column\" % k\n\n# Remove the read path columns, if present\nfor k in [\"R1\", \"R2\", \"I1\", \"I2\"]:\n    if k in new_manifest.columns.values:\n        new_manifest = new_manifest.drop(columns=k)\n\n# Assign the new name for the output file\nnew_file_name = \"${params.output_prefix}.results.hdf5\"\n\n# Copy the input file to a new output file object\nprint(\"Copying input.hdf5 to %s\" % new_file_name)\nshutil.copyfile(\"input.hdf5\", new_file_name)\n\n# Open a connection to the store\nwith pd.HDFStore(new_file_name, \"a\") as store:\n    \n    # Read in the existing manifest\n    old_manifest = pd.read_hdf(store, \"/manifest\")\n\n    # Make sure that the new and old manifest have the same set of specimens\n    old_specimen_set = set(old_manifest[\"specimen\"].tolist())\n    new_specimen_set = set(new_manifest[\"specimen\"].tolist())\n\n    for s, msg in [\n        (old_specimen_set - new_specimen_set, \"Specimens missing from old specimen list\"),\n        (new_specimen_set - old_specimen_set, \"Specimens missing from new specimen list\"),\n    ]:\n        assert len(s) == 0, \"%s: %s\" % (msg, \", \".join(list(s)))\n\n    # Write new manifest to the store\n    new_manifest.to_hdf(store, \"/manifest\")\n\nprint(\"Done\")\n\"\"\"",
        "nb_lignes_script": 47,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "manifest_csv"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"${container__pandas}\"",
            "label 'mem_medium'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "mockData": {
        "name_process": "mockData",
        "string_process": "\nprocess mockData {\n    tag \"Simulate dataset for validation\"\n    container \"${container__pandas}\"\n    label 'io_limited'\n\n    input:\n    path manifest_csv\n\n    output:\n    path \"random.counts.csv.gz\"\n\n\"\"\"\n#!/usr/bin/env python3\n\nimport numpy as np\nimport pandas as pd\n\n# Get the user-provided manifest\nmanifest = pd.read_csv(\"${manifest_csv}\")\n\n# Make sure that 'specimen' is in the header\nassert 'specimen' in manifest.columns.values, \"Must provide a column named 'specimen'\"\n\n# Get the list of specimen names\nspecimen_names = list(set(manifest['specimen'].tolist()))\n\n# Make a new DataFrame with random numbers\ndf = pd.DataFrame(\n    np.random.randint(\n        1000, high=2000, size=[len(specimen_names), 5], dtype=int\n    ),\n    index=specimen_names,\n    columns=[\n        \"CAG-%d\" % i\n        for i in range(5)\n    ]\n)\n\n# Add a total column\ndf[\"total\"] = df.sum(axis=1)\n\n# Write out to a file\ndf.reset_index(\n).rename(\n    columns=dict([(\"index\", \"specimen\")])\n).to_csv(\n    \"random.counts.csv.gz\",\n    index=None,\n    compression=\"gzip\"\n)\n\n\"\"\"\n\n}",
        "nb_lignes_process": 53,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport numpy as np\nimport pandas as pd\n\n# Get the user-provided manifest\nmanifest = pd.read_csv(\"${manifest_csv}\")\n\n# Make sure that 'specimen' is in the header\nassert 'specimen' in manifest.columns.values, \"Must provide a column named 'specimen'\"\n\n# Get the list of specimen names\nspecimen_names = list(set(manifest['specimen'].tolist()))\n\n# Make a new DataFrame with random numbers\ndf = pd.DataFrame(\n    np.random.randint(\n        1000, high=2000, size=[len(specimen_names), 5], dtype=int\n    ),\n    index=specimen_names,\n    columns=[\n        \"CAG-%d\" % i\n        for i in range(5)\n    ]\n)\n\n# Add a total column\ndf[\"total\"] = df.sum(axis=1)\n\n# Write out to a file\ndf.reset_index(\n).rename(\n    columns=dict([(\"index\", \"specimen\")])\n).to_csv(\n    \"random.counts.csv.gz\",\n    index=None,\n    compression=\"gzip\"\n)\n\n\"\"\"",
        "nb_lignes_script": 40,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "manifest_csv"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Simulate dataset for validation\"",
            "container \"${container__pandas}\"",
            "label 'io_limited'"
        ],
        "when": "",
        "stub": ""
    },
    "validateFormula": {
        "name_process": "validateFormula",
        "string_process": "\nprocess validateFormula {\n    tag \"Validate user-provided formula\"\n    container \"${container__pandas}\"\n    label 'io_limited'\n\n    input:\n    path corncob_output_csv\n    path manifest_csv\n\n    output:\n    path \"${manifest_csv}\"\n\n\"\"\"\n#!/usr/bin/env python3\n\nimport pandas as pd\n\n# Open up the corncob results for the simulated random data\ndf = pd.read_csv(\"${corncob_output_csv}\")\n\n# Make sure that we have results for every CAG\nassert set(df[\"CAG\"].tolist()) == set([\"CAG-%d\" % i for i in range(5)])\n\n# Check to see if any CAGs returned 'failed'\nif \"failed\" in df[\"type\"].values:\n    n_failed_cags = df.query(\"type == 'failed'\")[\"CAG\"].unique().shape[0]\n    n_total_cags = df[\"CAG\"].unique().shape[0]\n    msg = \"%d / %d CAGs failed processing with this formula\" % (n_failed_cags, n_total_cags)\n    assert False, msg\n\n# Make sure that every CAG has each expected row\nprint(\"Making sure that every CAG has results for p_value, std_error, and estimate\")\nfor cag_id, cag_df in df.groupby(\"CAG\"):\n\n    msg = \"%s: Found %s\" % (cag_id, \", t\".join(cag_df[\"type\"].tolist()))\n    assert set(cag_df[\"type\"].tolist()) == set([\"p_value\", \"std_error\", \"estimate\"]), msg\n\"\"\"\n\n}",
        "nb_lignes_process": 38,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport pandas as pd\n\n# Open up the corncob results for the simulated random data\ndf = pd.read_csv(\"${corncob_output_csv}\")\n\n# Make sure that we have results for every CAG\nassert set(df[\"CAG\"].tolist()) == set([\"CAG-%d\" % i for i in range(5)])\n\n# Check to see if any CAGs returned 'failed'\nif \"failed\" in df[\"type\"].values:\n    n_failed_cags = df.query(\"type == 'failed'\")[\"CAG\"].unique().shape[0]\n    n_total_cags = df[\"CAG\"].unique().shape[0]\n    msg = \"%d / %d CAGs failed processing with this formula\" % (n_failed_cags, n_total_cags)\n    assert False, msg\n\n# Make sure that every CAG has each expected row\nprint(\"Making sure that every CAG has results for p_value, std_error, and estimate\")\nfor cag_id, cag_df in df.groupby(\"CAG\"):\n\n    msg = \"%s: Found %s\" % (cag_id, \", t\".join(cag_df[\"type\"].tolist()))\n    assert set(cag_df[\"type\"].tolist()) == set([\"p_value\", \"std_error\", \"estimate\"]), msg\n\"\"\"",
        "nb_lignes_script": 24,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "corncob_output_csv",
            "manifest_csv"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Validate user-provided formula\"",
            "container \"${container__pandas}\"",
            "label 'io_limited'"
        ],
        "when": "",
        "stub": ""
    },
    "extractCounts": {
        "name_process": "extractCounts",
        "string_process": "\nprocess extractCounts {\n    tag \"Make CAG ~ sample read-count matrix\"\n    container \"${container__pandas}\"\n    label 'mem_veryhigh'\n    publishDir \"${params.output_folder}/abund/\", mode: \"copy\"\n    \n    input:\n    file famli_json_list\n    file cag_csv\n\n    output:\n    file \"CAG.readcounts.csv.gz\"\n\n\n\"\"\"\n#!/usr/bin/env python3\n\nfrom collections import defaultdict\nimport gzip\nimport json\nimport logging\nimport os\nimport pandas as pd\n\n# Set up logging\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [extractCounts] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write logs to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\n# Read in the CAG assignment for each gene\nlogging.info(\"Reading in CAG assignments\")\ncags = pd.read_csv(\n    \"${cag_csv}\"\n).set_index(\n    \"gene\"\n)[\"CAG\"]\n\n# Make an object to hold the number of reads per CAG, per sample\ncag_counts = dict()\n\n# Read in each of the FAMLI output objects\nfor fp in \"${famli_json_list}\".split(\" \"):\n\n    logging.info(\"Processing %s\" % fp)\n\n    # Get the sample name\n    assert fp.endswith(\".json.gz\"), fp\n    sample_name = fp[:-len(\".json.gz\")]\n\n    # Make sure the file was staged correctly\n    assert os.path.exists(fp), \"%s not found\" % fp\n\n    # Add the counts\n    cag_counts[sample_name] = defaultdict(int)\n\n    for i in json.load(\n        gzip.open(\n            fp,\n            \"rt\"\n        )\n    ):\n\n        # Add the value in 'nreads' to the CAG assigned to this gene\n        cag_counts[\n            sample_name\n        ][\n            cags[\n                i[\n                    \"id\"\n                ]\n            ]\n        ] += i[\n            \"nreads\"\n        ]\n\n# Format as a DataFrame\nlogging.info(\"Making a DataFrame\")\ncag_counts = pd.DataFrame(\n    cag_counts\n).fillna(\n    0\n).applymap(\n    int\n)\n\n# Transform so that samples are rows\ncag_counts = cag_counts.T\n\n# Add a \"total\" column\ncag_counts[\"total\"] = cag_counts.sum(axis=1)\n\n# Reset the index and add a name indicating that the rows \n# correspond to specimens from the manifest\ncag_counts = cag_counts.reset_index(\n).rename(\n    columns=dict([(\"index\", \"specimen\")])\n)\n\n# Save to a file\nlogging.info(\"Writing to disk\")\ncag_counts.to_csv(\n    \"CAG.readcounts.csv.gz\",\n    compression=\"gzip\",\n    index=None\n)\n\nlogging.info(\"Done\")\n\"\"\"\n}",
        "nb_lignes_process": 115,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nfrom collections import defaultdict\nimport gzip\nimport json\nimport logging\nimport os\nimport pandas as pd\n\n# Set up logging\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [extractCounts] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write logs to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\n# Read in the CAG assignment for each gene\nlogging.info(\"Reading in CAG assignments\")\ncags = pd.read_csv(\n    \"${cag_csv}\"\n).set_index(\n    \"gene\"\n)[\"CAG\"]\n\n# Make an object to hold the number of reads per CAG, per sample\ncag_counts = dict()\n\n# Read in each of the FAMLI output objects\nfor fp in \"${famli_json_list}\".split(\" \"):\n\n    logging.info(\"Processing %s\" % fp)\n\n    # Get the sample name\n    assert fp.endswith(\".json.gz\"), fp\n    sample_name = fp[:-len(\".json.gz\")]\n\n    # Make sure the file was staged correctly\n    assert os.path.exists(fp), \"%s not found\" % fp\n\n    # Add the counts\n    cag_counts[sample_name] = defaultdict(int)\n\n    for i in json.load(\n        gzip.open(\n            fp,\n            \"rt\"\n        )\n    ):\n\n        # Add the value in 'nreads' to the CAG assigned to this gene\n        cag_counts[\n            sample_name\n        ][\n            cags[\n                i[\n                    \"id\"\n                ]\n            ]\n        ] += i[\n            \"nreads\"\n        ]\n\n# Format as a DataFrame\nlogging.info(\"Making a DataFrame\")\ncag_counts = pd.DataFrame(\n    cag_counts\n).fillna(\n    0\n).applymap(\n    int\n)\n\n# Transform so that samples are rows\ncag_counts = cag_counts.T\n\n# Add a \"total\" column\ncag_counts[\"total\"] = cag_counts.sum(axis=1)\n\n# Reset the index and add a name indicating that the rows \n# correspond to specimens from the manifest\ncag_counts = cag_counts.reset_index(\n).rename(\n    columns=dict([(\"index\", \"specimen\")])\n)\n\n# Save to a file\nlogging.info(\"Writing to disk\")\ncag_counts.to_csv(\n    \"CAG.readcounts.csv.gz\",\n    compression=\"gzip\",\n    index=None\n)\n\nlogging.info(\"Done\")\n\"\"\"",
        "nb_lignes_script": 100,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "famli_json_list",
            "cag_csv"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Make CAG ~ sample read-count matrix\"",
            "container \"${container__pandas}\"",
            "label 'mem_veryhigh'",
            "publishDir \"${params.output_folder}/abund/\", mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    },
    "splitCorncob": {
        "name_process": "splitCorncob",
        "string_process": "\nprocess splitCorncob {\n    container \"${container__pandas}\"\n    label \"mem_veryhigh\"\n    errorStrategy \"finish\"\n\n    input:\n    file readcounts_csv_gz\n\n    output:\n    file \"readcounts.*.csv.gz\"\n\n\"\"\"#!/usr/bin/env python3\n\nimport pandas as pd\n\n# Read in the entire set of readcounts\nprint(\"Reading in ${readcounts_csv_gz}\")\ndf = pd.read_csv(\"${readcounts_csv_gz}\")\nprint(\"Read in %d rows and %d columns\" % (df.shape[0], df.shape[1]))\n\n# Write out shards of the data\nfor shard_ix in range(${params.corncob_batches}):\n\n    # Get the list of CAGs to write out\n    cag_list = [\n        n\n        for ix, n in enumerate(df.columns.values)\n        if ix % ${params.corncob_batches} == shard_ix or n in [\"specimen\", \"total\"]\n    ]\n\n    # Skip if there are too few CAGs for this shard\n    if len(cag_list) <= 2:\n        print(\"Skipping shard %d, too few CAGs to populate\" % shard_ix)\n        continue\n\n    # Write out the shard\n    print(\"Writing out %d columns to shard %d\" % (len(cag_list), shard_ix))\n\n    df.reindex(columns=cag_list).to_csv(\"readcounts.%d.csv.gz\" % shard_ix, index=None)\n    print(\"Done with shard %d\" % shard_ix)\n\nprint(\"Done with all shards\")\n\"\"\"\n}",
        "nb_lignes_process": 43,
        "string_script": "\"\"\"#!/usr/bin/env python3\n\nimport pandas as pd\n\n# Read in the entire set of readcounts\nprint(\"Reading in ${readcounts_csv_gz}\")\ndf = pd.read_csv(\"${readcounts_csv_gz}\")\nprint(\"Read in %d rows and %d columns\" % (df.shape[0], df.shape[1]))\n\n# Write out shards of the data\nfor shard_ix in range(${params.corncob_batches}):\n\n    # Get the list of CAGs to write out\n    cag_list = [\n        n\n        for ix, n in enumerate(df.columns.values)\n        if ix % ${params.corncob_batches} == shard_ix or n in [\"specimen\", \"total\"]\n    ]\n\n    # Skip if there are too few CAGs for this shard\n    if len(cag_list) <= 2:\n        print(\"Skipping shard %d, too few CAGs to populate\" % shard_ix)\n        continue\n\n    # Write out the shard\n    print(\"Writing out %d columns to shard %d\" % (len(cag_list), shard_ix))\n\n    df.reindex(columns=cag_list).to_csv(\"readcounts.%d.csv.gz\" % shard_ix, index=None)\n    print(\"Done with shard %d\" % shard_ix)\n\nprint(\"Done with all shards\")\n\"\"\"",
        "nb_lignes_script": 31,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "readcounts_csv_gz"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"${container__pandas}\"",
            "label \"mem_veryhigh\"",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "runCorncob": {
        "name_process": "runCorncob",
        "string_process": "\nprocess runCorncob {\n    tag \"Perform statistical analysis\"\n    container \"quay.io/fhcrc-microbiome/corncob\"\n    label \"mem_medium\"\n    errorStrategy \"finish\"\n    \n    input:\n    file readcounts_csv_gz\n    file metadata_csv\n    each formula\n\n    output:\n    file \"corncob.results.csv\"\n\n\n    \"\"\"\n#!/usr/bin/env Rscript\n\n# Get the arguments passed in by the user\n\nlibrary(tidyverse)\nlibrary(corncob)\nlibrary(parallel)\n\n## By default, use 10% of the available memory to read in data\nconnectionSize = 100000 * ${task.memory.toMega()}\nprint(\"Using VROOM_CONNECTION_SIZE =\")\nprint(connectionSize)\nSys.setenv(\"VROOM_CONNECTION_SIZE\" = format(connectionSize, scientific=F))\n\nnumCores = ${task.cpus}\n\n##  READCOUNTS CSV should have columns `specimen` (first col) and `total` (last column).\n##  METADATA CSV should have columns `specimen` (which matches up with `specimen` from\n##         the recounts file), and additional columns with covariates matching `formula`\n\n##  corncob analysis (coefficients and p-values) are written to OUTPUT CSV on completion\n\nprint(\"Reading in ${metadata_csv}\")\nmetadata <- vroom::vroom(\"${metadata_csv}\", delim=\",\")\n\nprint(\"Removing columns which are not in the formula\")\nfor(column_name in names(metadata)){\n    if(column_name == \"specimen\" || grepl(column_name, \"${formula}\", fixed=TRUE) ){\n        print(paste(\"Keeping column\", column_name))\n    } else {\n        print(paste(\"Removing column\", column_name))\n        metadata <- metadata %>% select(-column_name)\n    }\n}\nmetadata <- metadata %>% unique %>% drop_na\nprint(\"Filtered and deduplicated manifest:\")\nprint(metadata)\n\nprint(\"Reading in ${readcounts_csv_gz}\")\ncounts <- vroom::vroom(\"${readcounts_csv_gz}\", delim=\",\")\ntotal_counts <- counts[,c(\"specimen\", \"total\")]\n\nprint(\"Adding total counts to manifest\")\nprint(head(total_counts))\n\nprint(\"Merging total counts with metadata\")\ntotal_and_meta <- metadata %>% \n  left_join(total_counts, by = c(\"specimen\" = \"specimen\"))\n\n#### Run the analysis for every individual CAG (in this shard)\nprint(sprintf(\"Starting to process %s columns (CAGs)\", length(c(2:(dim(counts)[2] - 1)))))\ncorn_tib <- do.call(rbind, mclapply(\n    c(2:(dim(counts)[2] - 1)),\n    function(i){\n        try_bbdml <- try(\n            counts[,c(1, i)] %>%\n            rename(W = 2) %>%\n            right_join(\n                total_and_meta, \n                by = c(\"specimen\" = \"specimen\")\n            ) %>%\n            corncob::bbdml(\n                formula = cbind(W, total - W) ~ ${formula},\n                phi.formula = ~ 1,\n                data = .\n            )\n        )\n\n      if (class(try_bbdml) == \"bbdml\") {\n        return(\n            summary(\n                try_bbdml\n            )\\$coef %>%\n            as_tibble %>%\n            mutate(\"parameter\" = summary(try_bbdml)\\$coef %>% row.names) %>%\n            rename(\n                \"estimate\" = Estimate,\n                \"std_error\" = `Std. Error`,\n                \"p_value\" = `Pr(>|t|)`\n            ) %>%\n            select(-`t value`) %>%\n            gather(key = type, ...=estimate:p_value) %>%\n            mutate(\"CAG\" = names(counts)[i])\n        )\n      } else {\n          return(\n              tibble(\n                  \"parameter\" = \"all\",\n                  \"type\" = \"failed\", \n                  \"value\" = NA, \n                  \"CAG\" = names(counts)[i]\n              )\n          )\n      }   \n    },\n    mc.cores = numCores\n  ))\n\nprint(head(corn_tib))\n\nprint(\"Adding a column with the formula used here\")\ncorn_tib <- corn_tib %>% add_column(formula = \"${formula}\")\n\nprint(head(corn_tib))\n\nprint(sprintf(\"Writing out %s rows to corncob.results.csv\", nrow(corn_tib)))\nwrite_csv(corn_tib, \"corncob.results.csv\")\nprint(\"Done\")\n    \"\"\"\n\n}",
        "nb_lignes_process": 126,
        "string_script": "\"\"\"\n#!/usr/bin/env Rscript\n\n# Get the arguments passed in by the user\n\nlibrary(tidyverse)\nlibrary(corncob)\nlibrary(parallel)\n\n## By default, use 10% of the available memory to read in data\nconnectionSize = 100000 * ${task.memory.toMega()}\nprint(\"Using VROOM_CONNECTION_SIZE =\")\nprint(connectionSize)\nSys.setenv(\"VROOM_CONNECTION_SIZE\" = format(connectionSize, scientific=F))\n\nnumCores = ${task.cpus}\n\n##  READCOUNTS CSV should have columns `specimen` (first col) and `total` (last column).\n##  METADATA CSV should have columns `specimen` (which matches up with `specimen` from\n##         the recounts file), and additional columns with covariates matching `formula`\n\n##  corncob analysis (coefficients and p-values) are written to OUTPUT CSV on completion\n\nprint(\"Reading in ${metadata_csv}\")\nmetadata <- vroom::vroom(\"${metadata_csv}\", delim=\",\")\n\nprint(\"Removing columns which are not in the formula\")\nfor(column_name in names(metadata)){\n    if(column_name == \"specimen\" || grepl(column_name, \"${formula}\", fixed=TRUE) ){\n        print(paste(\"Keeping column\", column_name))\n    } else {\n        print(paste(\"Removing column\", column_name))\n        metadata <- metadata %>% select(-column_name)\n    }\n}\nmetadata <- metadata %>% unique %>% drop_na\nprint(\"Filtered and deduplicated manifest:\")\nprint(metadata)\n\nprint(\"Reading in ${readcounts_csv_gz}\")\ncounts <- vroom::vroom(\"${readcounts_csv_gz}\", delim=\",\")\ntotal_counts <- counts[,c(\"specimen\", \"total\")]\n\nprint(\"Adding total counts to manifest\")\nprint(head(total_counts))\n\nprint(\"Merging total counts with metadata\")\ntotal_and_meta <- metadata %>% \n  left_join(total_counts, by = c(\"specimen\" = \"specimen\"))\n\n#### Run the analysis for every individual CAG (in this shard)\nprint(sprintf(\"Starting to process %s columns (CAGs)\", length(c(2:(dim(counts)[2] - 1)))))\ncorn_tib <- do.call(rbind, mclapply(\n    c(2:(dim(counts)[2] - 1)),\n    function(i){\n        try_bbdml <- try(\n            counts[,c(1, i)] %>%\n            rename(W = 2) %>%\n            right_join(\n                total_and_meta, \n                by = c(\"specimen\" = \"specimen\")\n            ) %>%\n            corncob::bbdml(\n                formula = cbind(W, total - W) ~ ${formula},\n                phi.formula = ~ 1,\n                data = .\n            )\n        )\n\n      if (class(try_bbdml) == \"bbdml\") {\n        return(\n            summary(\n                try_bbdml\n            )\\$coef %>%\n            as_tibble %>%\n            mutate(\"parameter\" = summary(try_bbdml)\\$coef %>% row.names) %>%\n            rename(\n                \"estimate\" = Estimate,\n                \"std_error\" = `Std. Error`,\n                \"p_value\" = `Pr(>|t|)`\n            ) %>%\n            select(-`t value`) %>%\n            gather(key = type, ...=estimate:p_value) %>%\n            mutate(\"CAG\" = names(counts)[i])\n        )\n      } else {\n          return(\n              tibble(\n                  \"parameter\" = \"all\",\n                  \"type\" = \"failed\", \n                  \"value\" = NA, \n                  \"CAG\" = names(counts)[i]\n              )\n          )\n      }   \n    },\n    mc.cores = numCores\n  ))\n\nprint(head(corn_tib))\n\nprint(\"Adding a column with the formula used here\")\ncorn_tib <- corn_tib %>% add_column(formula = \"${formula}\")\n\nprint(head(corn_tib))\n\nprint(sprintf(\"Writing out %s rows to corncob.results.csv\", nrow(corn_tib)))\nwrite_csv(corn_tib, \"corncob.results.csv\")\nprint(\"Done\")\n    \"\"\"",
        "nb_lignes_script": 109,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "readcounts_csv_gz",
            "metadata_csv",
            "formula"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Perform statistical analysis\"",
            "container \"quay.io/fhcrc-microbiome/corncob\"",
            "label \"mem_medium\"",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "breakaway": {
        "name_process": "breakaway",
        "string_process": "\nprocess breakaway {\n    tag \"Estimate richness\"\n    container \"quay.io/fhcrc-microbiome/breakaway\"\n    label \"io_limited\"\n    errorStrategy \"retry\"\n    \n    input:\n    file famli_json_gz\n\n    output:\n    file \"*.breakaway.json\"\n\n\n\"\"\"\n#!/usr/bin/env Rscript\n\nlibrary(jsonlite)\nlibrary(breakaway)\n\n# Read in the input data\ngene_readcounts <- fromJSON(\"${famli_json_gz}\")\\$nreads\n\n# Run breakaway\nr <- breakaway(gene_readcounts, plot = FALSE, output = FALSE, answers = TRUE)\n\nprint(r)\n\n# Make a new output object with only the data objects which are strictly needed\noutput <- list(\n    estimate = r\\$estimate,\n    error = r\\$error,\n    interval = r\\$interval,\n    reasonable = r\\$reasonable,\n    estimand = r\\$estimand\n)\n\n# Save the results to a file\noutput_filename <- sub(\".json.gz\", \".breakaway.json\", \"${famli_json_gz}\")\nwrite(\n    toJSON(\n        output,\n        force = TRUE\n    ),\n    file = output_filename\n)\n\n\"\"\"\n\n}",
        "nb_lignes_process": 48,
        "string_script": "\"\"\"\n#!/usr/bin/env Rscript\n\nlibrary(jsonlite)\nlibrary(breakaway)\n\n# Read in the input data\ngene_readcounts <- fromJSON(\"${famli_json_gz}\")\\$nreads\n\n# Run breakaway\nr <- breakaway(gene_readcounts, plot = FALSE, output = FALSE, answers = TRUE)\n\nprint(r)\n\n# Make a new output object with only the data objects which are strictly needed\noutput <- list(\n    estimate = r\\$estimate,\n    error = r\\$error,\n    interval = r\\$interval,\n    reasonable = r\\$reasonable,\n    estimand = r\\$estimand\n)\n\n# Save the results to a file\noutput_filename <- sub(\".json.gz\", \".breakaway.json\", \"${famli_json_gz}\")\nwrite(\n    toJSON(\n        output,\n        force = TRUE\n    ),\n    file = output_filename\n)\n\n\"\"\"",
        "nb_lignes_script": 33,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "famli_json_gz"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Estimate richness\"",
            "container \"quay.io/fhcrc-microbiome/breakaway\"",
            "label \"io_limited\"",
            "errorStrategy \"retry\""
        ],
        "when": "",
        "stub": ""
    },
    "collectBreakaway": {
        "name_process": "collectBreakaway",
        "string_process": "\nprocess collectBreakaway {\n    tag \"Join richness tables\"\n    container \"quay.io/fhcrc-microbiome/python-pandas:v1.0.3\"\n    label \"io_limited\"\n    errorStrategy \"finish\"\n    publishDir \"${params.output_folder}stats\", mode: \"copy\", overwrite: true\n    \n    input:\n    file breakaway_json_list\n\n    output:\n    file \"${params.output_prefix}.breakaway.csv.gz\"\n\n\n\"\"\"\n#!/usr/bin/env python3\n\nimport json\nimport pandas as pd\n\n# Get the list of files, with the samples encoded in the file names\nsamples = dict([\n    (fp.replace(\".breakaway.json\", \"\"), fp)\n    for fp in \"${breakaway_json_list}\".split(\" \")\n])\n\nprint(\"Reading in breakaway results for %d samples\" % len(samples))\n\n# Function to read in breakaway results\ndef read_breakaway(fp):\n    dat = json.load(open(fp, \"r\"))\n    return dict([\n        (\"estimate\", dat[\"estimate\"][0]),\n        (\"error\", dat[\"error\"][0]),\n        (\"interval_lower\", dat[\"interval\"][0]),\n        (\"interval_upper\", dat[\"interval\"][1]),\n        (\"reasonable\", dat[\"reasonable\"][0]),\n        (\"estimand\", dat[\"estimand\"][0])\n    ])\n\noutput = pd.DataFrame(dict([\n    (sample_name, read_breakaway(fp))\n    for sample_name, fp in samples.items()\n])).T.reset_index(\n).rename(columns=dict([(\"index\", \"specimen\")]))\n\noutput.to_csv(\"${params.output_prefix}.breakaway.csv.gz\", index=None)\n\n\"\"\"\n\n}",
        "nb_lignes_process": 50,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport json\nimport pandas as pd\n\n# Get the list of files, with the samples encoded in the file names\nsamples = dict([\n    (fp.replace(\".breakaway.json\", \"\"), fp)\n    for fp in \"${breakaway_json_list}\".split(\" \")\n])\n\nprint(\"Reading in breakaway results for %d samples\" % len(samples))\n\n# Function to read in breakaway results\ndef read_breakaway(fp):\n    dat = json.load(open(fp, \"r\"))\n    return dict([\n        (\"estimate\", dat[\"estimate\"][0]),\n        (\"error\", dat[\"error\"][0]),\n        (\"interval_lower\", dat[\"interval\"][0]),\n        (\"interval_upper\", dat[\"interval\"][1]),\n        (\"reasonable\", dat[\"reasonable\"][0]),\n        (\"estimand\", dat[\"estimand\"][0])\n    ])\n\noutput = pd.DataFrame(dict([\n    (sample_name, read_breakaway(fp))\n    for sample_name, fp in samples.items()\n])).T.reset_index(\n).rename(columns=dict([(\"index\", \"specimen\")]))\n\noutput.to_csv(\"${params.output_prefix}.breakaway.csv.gz\", index=None)\n\n\"\"\"",
        "nb_lignes_script": 34,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "breakaway_json_list"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Join richness tables\"",
            "container \"quay.io/fhcrc-microbiome/python-pandas:v1.0.3\"",
            "label \"io_limited\"",
            "errorStrategy \"finish\"",
            "publishDir \"${params.output_folder}stats\", mode: \"copy\", overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "joinCorncob": {
        "name_process": "joinCorncob",
        "string_process": "\nprocess joinCorncob {\n    container \"quay.io/fhcrc-microbiome/python-pandas:v1.0.3\"\n    label \"io_limited\"\n    errorStrategy \"finish\"\n    publishDir \"${params.output_folder}/stats/\", mode: \"copy\"\n    \n    input:\n    file \"corncob.results.*.csv\"\n\n    output:\n    file \"corncob.results.csv\"\n\n\n\"\"\"\n#!/usr/bin/env python3\n\nimport os\nimport pandas as pd\n\n# Get the list of files to join\nfp_list = [\n    fp\n    for fp in os.listdir(\".\")\n    if fp.startswith(\"corncob.results.\") and fp.endswith(\".csv\")\n]\n\nprint(\"Reading in corncob results for %d formula(s)\" % len(fp_list))\n\ndf = pd.concat([\n    pd.read_csv(fp)\n    for fp in fp_list\n])\n\nprint(\"Writing out to corncob.results.csv\")\ndf.to_csv(\"corncob.results.csv\", index=None)\nprint(\"Done\")\n\"\"\"\n\n}",
        "nb_lignes_process": 38,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport os\nimport pandas as pd\n\n# Get the list of files to join\nfp_list = [\n    fp\n    for fp in os.listdir(\".\")\n    if fp.startswith(\"corncob.results.\") and fp.endswith(\".csv\")\n]\n\nprint(\"Reading in corncob results for %d formula(s)\" % len(fp_list))\n\ndf = pd.concat([\n    pd.read_csv(fp)\n    for fp in fp_list\n])\n\nprint(\"Writing out to corncob.results.csv\")\ndf.to_csv(\"corncob.results.csv\", index=None)\nprint(\"Done\")\n\"\"\"",
        "nb_lignes_script": 23,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/python-pandas:v1.0.3\"",
            "label \"io_limited\"",
            "errorStrategy \"finish\"",
            "publishDir \"${params.output_folder}/stats/\", mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    },
    "runBetta": {
        "name_process": "runBetta",
        "string_process": "\nprocess runBetta {\n    container \"quay.io/fhcrc-microbiome/breakaway:latest\"\n    label \"mem_medium\"\n    errorStrategy \"finish\"\n    \n    input:\n    path labelled_corncob_csv\n\n    output:\n    file \"${labelled_corncob_csv}.betta.csv.gz\"\n\n\n\"\"\"\n#!/usr/bin/env Rscript\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(reshape2)\nlibrary(breakaway)\n\n# Use vroom to read in the table\nSys.setenv(\"VROOM_CONNECTION_SIZE\" = 131072 * 20)\n\n# Read in all of the data for a single covariate\nprint(\"Reading in $labelled_corncob_csv\")\ndf <- vroom::vroom(\"$labelled_corncob_csv\", delim=\",\")\ndf <- as.tibble(df)\n\nprint(head(df))\n\n# Make a function which will trim off the trailing digit\noptions(scipen=999)\nnum_decimal_places <- function(v){nchar(strsplit(as.character(v), \"\\\\\\\\.\")[[1]][2])}\ntrim_trailing <- function(v){round(v, num_decimal_places(v) - 1)}\n\n# Make a function to run betta while tolerating faults\nfault_tolerant_betta <- function(df, f){\n    if(nrow(df) == 1){\n        return(\n            data.frame(\n                estimate=df[1,\"estimate\"],\n                std_error=df[1,\"std_error\"],\n                p_value=df[1,\"p_value\"]\n            )\n        )\n    }\n    chats <- df\\$estimate\n    ses <- df\\$std_error\n    r <- NULL\n    for(ix in c(1:10)){\n        r <- tryCatch({\n            breakaway::betta(chats=chats, ses=ses)\\$table\n            },\n            error=function(cond) {\n                print(\"We have encountered an error:\")\n                print(cond)\n                print(\"The input data which caused the error was:\")\n                print(chats)\n                print(ses)\n                return(NULL)\n            })\n        if(!is.null(r)){\n            return(\n                data.frame(\n                    estimate=r[1,\"Estimates\"],\n                    std_error=r[1,\"Standard Errors\"],\n                    p_value=r[1,\"p-values\"]\n                )\n            )\n        } else {\n            print(\"Trimming down the input data\")\n            chats <- trim_trailing(chats)\n            ses <- trim_trailing(ses)\n            print(chats)\n            print(ses)\n        }\n    }\n}\n\n# If there is a single dummy row, skip the entire process\nif(nrow(df) == 1){\n\n    write.table(df, file=gzfile(\"${labelled_corncob_csv}.betta.csv.gz\"), sep=\",\", row.names=FALSE)\n\n} else{\n\n    # Perform meta-analysis combining the results for each label, and each parameter\n    results <- df %>% group_by(annotation, label, parameter) %>% group_modify(fault_tolerant_betta)\n\n    # Write out to a CSV\n    write.table(results, file=gzfile(\"${labelled_corncob_csv}.betta.csv.gz\"), sep=\",\", row.names=FALSE)\n\n}\n\n\"\"\"\n\n}",
        "nb_lignes_process": 95,
        "string_script": "\"\"\"\n#!/usr/bin/env Rscript\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(reshape2)\nlibrary(breakaway)\n\n# Use vroom to read in the table\nSys.setenv(\"VROOM_CONNECTION_SIZE\" = 131072 * 20)\n\n# Read in all of the data for a single covariate\nprint(\"Reading in $labelled_corncob_csv\")\ndf <- vroom::vroom(\"$labelled_corncob_csv\", delim=\",\")\ndf <- as.tibble(df)\n\nprint(head(df))\n\n# Make a function which will trim off the trailing digit\noptions(scipen=999)\nnum_decimal_places <- function(v){nchar(strsplit(as.character(v), \"\\\\\\\\.\")[[1]][2])}\ntrim_trailing <- function(v){round(v, num_decimal_places(v) - 1)}\n\n# Make a function to run betta while tolerating faults\nfault_tolerant_betta <- function(df, f){\n    if(nrow(df) == 1){\n        return(\n            data.frame(\n                estimate=df[1,\"estimate\"],\n                std_error=df[1,\"std_error\"],\n                p_value=df[1,\"p_value\"]\n            )\n        )\n    }\n    chats <- df\\$estimate\n    ses <- df\\$std_error\n    r <- NULL\n    for(ix in c(1:10)){\n        r <- tryCatch({\n            breakaway::betta(chats=chats, ses=ses)\\$table\n            },\n            error=function(cond) {\n                print(\"We have encountered an error:\")\n                print(cond)\n                print(\"The input data which caused the error was:\")\n                print(chats)\n                print(ses)\n                return(NULL)\n            })\n        if(!is.null(r)){\n            return(\n                data.frame(\n                    estimate=r[1,\"Estimates\"],\n                    std_error=r[1,\"Standard Errors\"],\n                    p_value=r[1,\"p-values\"]\n                )\n            )\n        } else {\n            print(\"Trimming down the input data\")\n            chats <- trim_trailing(chats)\n            ses <- trim_trailing(ses)\n            print(chats)\n            print(ses)\n        }\n    }\n}\n\n# If there is a single dummy row, skip the entire process\nif(nrow(df) == 1){\n\n    write.table(df, file=gzfile(\"${labelled_corncob_csv}.betta.csv.gz\"), sep=\",\", row.names=FALSE)\n\n} else{\n\n    # Perform meta-analysis combining the results for each label, and each parameter\n    results <- df %>% group_by(annotation, label, parameter) %>% group_modify(fault_tolerant_betta)\n\n    # Write out to a CSV\n    write.table(results, file=gzfile(\"${labelled_corncob_csv}.betta.csv.gz\"), sep=\",\", row.names=FALSE)\n\n}\n\n\"\"\"",
        "nb_lignes_script": 81,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "labelled_corncob_csv"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/breakaway:latest\"",
            "label \"mem_medium\"",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "addBetta": {
        "name_process": "addBetta",
        "string_process": "\nprocess addBetta{\n    tag \"Add meta-analysis to HDF\"\n    container \"${container__pandas}\"\n    label 'mem_medium'\n    errorStrategy 'finish'\n\n    input:\n        path results_hdf\n        path betta_csv_list\n\n    output:\n        path \"${results_hdf}\"\n\n\"\"\"\n#!/usr/bin/env python3\n\nimport os\nimport pandas as pd\nfrom statsmodels.stats.multitest import multipletests\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\nbetta_csv_list = \"${betta_csv_list}\".split(\" \")\n\nfor betta_csv in betta_csv_list:\n    if len(betta_csv) > 1:\n        assert os.path.exists(betta_csv)\n\n# Read in from the flat file\ndf = pd.concat([\n    pd.read_csv(betta_csv)\n    for betta_csv in betta_csv_list\n    if len(betta_csv) > 1\n])\n\nprint(\"Read in {:,} lines from {}\".format(\n    df.shape[0],\n    betta_csv\n))\n\n# If there are real results (not just a dummy file), write to HDF5\nif df.shape[0] > 1:\n\n    # Add the q-values\n    df = df.assign(\n        q_value = multipletests(\n            df[\"p_value\"],\n            0.2,\n            \"${params.fdr_method}\"\n        )[1]\n    )\n\n    # Add the wald\n    df = df.assign(\n        wald = df[\"estimate\"] / df[\"std_error\"],\n    )\n\n    # Open a connection to the HDF5\n    with pd.HDFStore(\"${results_hdf}\", \"a\") as store:\n\n        # Write to HDF5\n        key = \"/stats/enrichment/betta\"\n        print(\"Writing to %s\" % key)\n        \n        # Write to HDF\n        df.to_hdf(store, key)\n\n        print(\"Closing store\")\n\n    print(\"Done\")\n\nelse:\n    print(\"No betta results found -- returning unopened results HDF\")\n\n\"\"\"\n\n}",
        "nb_lignes_process": 76,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\nimport os\nimport pandas as pd\nfrom statsmodels.stats.multitest import multipletests\nimport pickle\npickle.HIGHEST_PROTOCOL = 4\n\nbetta_csv_list = \"${betta_csv_list}\".split(\" \")\n\nfor betta_csv in betta_csv_list:\n    if len(betta_csv) > 1:\n        assert os.path.exists(betta_csv)\n\n# Read in from the flat file\ndf = pd.concat([\n    pd.read_csv(betta_csv)\n    for betta_csv in betta_csv_list\n    if len(betta_csv) > 1\n])\n\nprint(\"Read in {:,} lines from {}\".format(\n    df.shape[0],\n    betta_csv\n))\n\n# If there are real results (not just a dummy file), write to HDF5\nif df.shape[0] > 1:\n\n    # Add the q-values\n    df = df.assign(\n        q_value = multipletests(\n            df[\"p_value\"],\n            0.2,\n            \"${params.fdr_method}\"\n        )[1]\n    )\n\n    # Add the wald\n    df = df.assign(\n        wald = df[\"estimate\"] / df[\"std_error\"],\n    )\n\n    # Open a connection to the HDF5\n    with pd.HDFStore(\"${results_hdf}\", \"a\") as store:\n\n        # Write to HDF5\n        key = \"/stats/enrichment/betta\"\n        print(\"Writing to %s\" % key)\n        \n        # Write to HDF\n        df.to_hdf(store, key)\n\n        print(\"Closing store\")\n\n    print(\"Done\")\n\nelse:\n    print(\"No betta results found -- returning unopened results HDF\")\n\n\"\"\"",
        "nb_lignes_script": 61,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "results_hdf",
            "betta_csv_list"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Golob-Minot__geneshot",
        "directive": [
            "tag \"Add meta-analysis to HDF\"",
            "container \"${container__pandas}\"",
            "label 'mem_medium'",
            "errorStrategy 'finish'"
        ],
        "when": "",
        "stub": ""
    }
}