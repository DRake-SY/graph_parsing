{
    "checkm": {
        "name_process": "checkm",
        "string_process": "\nprocess checkm {\n    tag \"${id}\"\n    publishDir \"data/${id}\"\n\n    input:\n    file db\n    set val(id), file(genome) from checkm_genomes\n\n    output:\n    set id, \"${id}.ssu.fna\" into rRNA_fasta\n    set id, \"${id}.checkm.tsv\" into checkm_lineage\n\n    script:\n    checkm_db = \"${db}/checkm/\"\n\n    \"\"\"\n    # Set CheckM root data location\n    echo ${checkm_db} | checkm data setRoot ${checkm_db}\n\n    # Extract 16S rRNA gene sequences with Nhmmer\n    checkm ssu_finder -t ${task.cpus} -x ${params.x} ${genome} . ssu_finder 2>&1\n    ln -s ssu_finder/ssu.fna ${id}.ssu.fna\n\n    # Phylogenetic placement onto reduced reference tree\n    checkm lineage_wf -t ${task.cpus} --reduced_tree -x ${params.x} . checkm\n    checkm qa -o 2 --tab_table checkm/lineage.ms checkm > ${id}.checkm.tsv\n    \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "    checkm_db = \"${db}/checkm/\"\n\n    \"\"\"\n    # Set CheckM root data location\n    echo ${checkm_db} | checkm data setRoot ${checkm_db}\n\n    # Extract 16S rRNA gene sequences with Nhmmer\n    checkm ssu_finder -t ${task.cpus} -x ${params.x} ${genome} . ssu_finder 2>&1\n    ln -s ssu_finder/ssu.fna ${id}.ssu.fna\n\n    # Phylogenetic placement onto reduced reference tree\n    checkm lineage_wf -t ${task.cpus} --reduced_tree -x ${params.x} . checkm\n    checkm qa -o 2 --tab_table checkm/lineage.ms checkm > ${id}.checkm.tsv\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "db",
            "checkm_genomes"
        ],
        "nb_inputs": 2,
        "outputs": [
            "rRNA_fasta",
            "checkm_lineage"
        ],
        "nb_outputs": 2,
        "name_workflow": "CAMI-challenge__CAMITAX",
        "directive": [
            "tag \"${id}\"",
            "publishDir \"data/${id}\""
        ],
        "when": "",
        "stub": ""
    },
    "mash": {
        "name_process": "mash",
        "string_process": "\nprocess mash {\n    tag \"${id}\"\n    publishDir \"data/${id}\"\n\n    input:\n    file db\n    set val(id), file(genome) from mash_genomes\n\n    output:\n    set id, \"${id}.mash.ANImax.txt\", \"${id}.mash.taxIDs.txt\" into mash_ANImax_taxIDs\n\n    script:\n    mash_index = \"${db}/mash/RefSeq_20190108.msh\"\n\n    \"\"\"\n    mash dist ${mash_index} ${genome} | sort -gk3 > ${genome.baseName}.mash.sorted\n    head -n 1 ${genome.baseName}.mash.sorted | awk '{print 1-\\$3}' > ${genome.baseName}.mash.ANImax.txt\n    awk '\\$3 <= 0.05' ${genome.baseName}.mash.sorted | cut -f1 -d'_' > ${genome.baseName}.mash.taxIDs.txt\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "    mash_index = \"${db}/mash/RefSeq_20190108.msh\"\n\n    \"\"\"\n    mash dist ${mash_index} ${genome} | sort -gk3 > ${genome.baseName}.mash.sorted\n    head -n 1 ${genome.baseName}.mash.sorted | awk '{print 1-\\$3}' > ${genome.baseName}.mash.ANImax.txt\n    awk '\\$3 <= 0.05' ${genome.baseName}.mash.sorted | cut -f1 -d'_' > ${genome.baseName}.mash.taxIDs.txt\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "Mash"
        ],
        "tools_url": [
            "https://bio.tools/mash"
        ],
        "tools_dico": [
            {
                "name": "Mash",
                "uri": "https://bio.tools/mash",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2533",
                            "term": "DNA mutation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix generation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Phylogenetic distance matrix generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Fast genome and metagenome distance estimation using MinHash.",
                "homepage": "https://github.com/marbl/mash"
            }
        ],
        "inputs": [
            "db",
            "mash_genomes"
        ],
        "nb_inputs": 2,
        "outputs": [
            "mash_ANImax_taxIDs"
        ],
        "nb_outputs": 1,
        "name_workflow": "CAMI-challenge__CAMITAX",
        "directive": [
            "tag \"${id}\"",
            "publishDir \"data/${id}\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2": {
        "name_process": "dada2",
        "string_process": "\nprocess dada2 {\n    tag \"${id}\"\n    publishDir \"data/${id}\"\n\n    input:\n    file db\n    set val(id), file(fasta) from rRNA_fasta\n\n    output:\n    set id, \"${id}.dada2.txt\" into dada2_lineage\n\n    script:\n    if ( params.dada2_db == 'silva' ) {\n        dada2_train_set = \"${db}/dada2/silva_nr_v132_train_set.fa.gz\"\n        dada2_species_assignment = \"${db}/dada2/silva_species_assignment_v132.fa.gz\"\n    } else if ( params.dada2_db == 'rdp' ) {\n        dada2_train_set = \"${db}/dada2/rdp_train_set_16.fa.gz\"\n        dada2_species_assignment = \"${db}/dada2/rdp_species_assignment_16.fa.gz\"\n    } else\n        error \"Invalid database for dada2 specified: ${params.dada2_db}\"\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    set.seed(42)\n\n    seqs <- paste(Biostrings::readDNAStringSet(\"${fasta}\"))\n    tt <- data.frame(Batman = \"NA;NA;NA;NA;NA;NA;NA\")\n    try(tt <- addSpecies(assignTaxonomy(seqs, \"${dada2_train_set}\", minBoot=80), \"${dada2_species_assignment}\"))\n    write.table(tt, \"${id}.dada2.txt\", quote=F, sep=\";\", row.names=F, col.names=F)\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "    if ( params.dada2_db == 'silva' ) {\n        dada2_train_set = \"${db}/dada2/silva_nr_v132_train_set.fa.gz\"\n        dada2_species_assignment = \"${db}/dada2/silva_species_assignment_v132.fa.gz\"\n    } else if ( params.dada2_db == 'rdp' ) {\n        dada2_train_set = \"${db}/dada2/rdp_train_set_16.fa.gz\"\n        dada2_species_assignment = \"${db}/dada2/rdp_species_assignment_16.fa.gz\"\n    } else\n        error \"Invalid database for dada2 specified: ${params.dada2_db}\"\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    set.seed(42)\n\n    seqs <- paste(Biostrings::readDNAStringSet(\"${fasta}\"))\n    tt <- data.frame(Batman = \"NA;NA;NA;NA;NA;NA;NA\")\n    try(tt <- addSpecies(assignTaxonomy(seqs, \"${dada2_train_set}\", minBoot=80), \"${dada2_species_assignment}\"))\n    write.table(tt, \"${id}.dada2.txt\", quote=F, sep=\";\", row.names=F, col.names=F)\n    \"\"\"",
        "nb_lignes_script": 19,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "db",
            "rRNA_fasta"
        ],
        "nb_inputs": 2,
        "outputs": [
            "dada2_lineage"
        ],
        "nb_outputs": 1,
        "name_workflow": "CAMI-challenge__CAMITAX",
        "directive": [
            "tag \"${id}\"",
            "publishDir \"data/${id}\""
        ],
        "when": "",
        "stub": ""
    },
    "prodigal": {
        "name_process": "prodigal",
        "string_process": "\nprocess prodigal {\n    tag \"${id}\"\n    publishDir \"data/${id}\"\n\n    input:\n    set val(id), file(genome) from prodigal_genomes\n\n    output:\n    set id, \"${id}.genes.faa\" into faa_kaiju\n    set id, \"${id}.genes.fna\" into fna_centrifuge\n    set id, \"${id}.genes.cnt\" into genes_cnt\n\n    \"\"\"\n    prodigal -a ${id}.genes.faa -d ${id}.genes.fna -i ${genome}\n    grep -c \"^>\" ${id}.genes.faa > ${id}.genes.cnt\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "\"\"\"\n    prodigal -a ${id}.genes.faa -d ${id}.genes.fna -i ${genome}\n    grep -c \"^>\" ${id}.genes.faa > ${id}.genes.cnt\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "prodigal_genomes"
        ],
        "nb_inputs": 1,
        "outputs": [
            "faa_kaiju",
            "fna_centrifuge",
            "genes_cnt"
        ],
        "nb_outputs": 3,
        "name_workflow": "CAMI-challenge__CAMITAX",
        "directive": [
            "tag \"${id}\"",
            "publishDir \"data/${id}\""
        ],
        "when": "",
        "stub": ""
    },
    "centrifuge": {
        "name_process": "centrifuge",
        "string_process": "\nprocess centrifuge {\n    tag \"${id}\"\n    publishDir \"data/${id}\"\n\n    input:\n    file db\n    set val(id), file(genes) from fna_centrifuge\n\n    output:\n    set id, \"${id}.centrifuge.taxIDs.txt\" into centrifuge_taxIDs\n\n    script:\n    centrifuge_index = \"${db}/centrifuge/proGenomes_20190108\"\n\n    \"\"\"\n    centrifuge -p ${task.cpus} -f -x ${centrifuge_index} ${genes} > ${id}.centrifuge.out\n    awk '\\$4+1 > 250' ${id}.centrifuge.out | cut -f3 > ${id}.centrifuge.taxIDs.txt\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "    centrifuge_index = \"${db}/centrifuge/proGenomes_20190108\"\n\n    \"\"\"\n    centrifuge -p ${task.cpus} -f -x ${centrifuge_index} ${genes} > ${id}.centrifuge.out\n    awk '\\$4+1 > 250' ${id}.centrifuge.out | cut -f3 > ${id}.centrifuge.taxIDs.txt\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "Centrifuge"
        ],
        "tools_url": [
            "https://bio.tools/centrifuge"
        ],
        "tools_dico": [
            {
                "name": "Centrifuge",
                "uri": "https://bio.tools/centrifuge",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Nucleic acid sequence analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Sequence analysis (nucleic acid)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A very rapid and memory-efficient system for the classification of DNA sequences from microbial samples. The system uses a novel indexing scheme based on the Burrows-Wheeler transform and the Ferragina-Manzini index, optimized specifically for the metagenomic classification problem. Together these advances enable timely and accurate analysis of large metagenomics data sets on conventional desktop computers.",
                "homepage": "https://ccb.jhu.edu/software/centrifuge/"
            }
        ],
        "inputs": [
            "db",
            "fna_centrifuge"
        ],
        "nb_inputs": 2,
        "outputs": [
            "centrifuge_taxIDs"
        ],
        "nb_outputs": 1,
        "name_workflow": "CAMI-challenge__CAMITAX",
        "directive": [
            "tag \"${id}\"",
            "publishDir \"data/${id}\""
        ],
        "when": "",
        "stub": ""
    },
    "kaiju": {
        "name_process": "kaiju",
        "string_process": "\nprocess kaiju {\n    tag \"${id}\"\n    publishDir \"data/${id}\"\n\n    input:\n    file db\n    set val(id), file(genes) from faa_kaiju\n\n    output:\n    set id, \"${id}.kaiju.taxIDs.txt\" into kaiju_taxIDs\n\n    script:\n    kaiju_index = \"${db}/kaiju/proGenomes_20190108.fmi\"\n    ncbi_nodes = \"${db}/taxonomy/nodes_20190108.dmp\"\n\n    \"\"\"\n    kaiju -p -z ${task.cpus} -t ${ncbi_nodes} -f ${kaiju_index} -i ${genes} > ${id}.kaiju.out\n    grep \"^C\" ${id}.kaiju.out | cut -f3 > ${id}.kaiju.taxIDs.txt\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "    kaiju_index = \"${db}/kaiju/proGenomes_20190108.fmi\"\n    ncbi_nodes = \"${db}/taxonomy/nodes_20190108.dmp\"\n\n    \"\"\"\n    kaiju -p -z ${task.cpus} -t ${ncbi_nodes} -f ${kaiju_index} -i ${genes} > ${id}.kaiju.out\n    grep \"^C\" ${id}.kaiju.out | cut -f3 > ${id}.kaiju.taxIDs.txt\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "Kaiju"
        ],
        "tools_url": [
            "https://bio.tools/kaiju"
        ],
        "tools_dico": [
            {
                "name": "Kaiju",
                "uri": "https://bio.tools/kaiju",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2975",
                                "term": "Nucleic acid sequence (raw)"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3028",
                                "term": "Taxonomy"
                            }
                        ]
                    }
                ],
                "description": "Program for the taxonomic assignment of high-throughput sequencing reads, e.g., Illumina or Roche/454, from whole-genome sequencing of metagenomic DNA. Reads are directly assigned to taxa using the NCBI taxonomy and a reference database of protein sequences from Bacteria, Archaea, Fungi, microbial eukaryotes and viruses.",
                "homepage": "http://kaiju.binf.ku.dk"
            }
        ],
        "inputs": [
            "db",
            "faa_kaiju"
        ],
        "nb_inputs": 2,
        "outputs": [
            "kaiju_taxIDs"
        ],
        "nb_outputs": 1,
        "name_workflow": "CAMI-challenge__CAMITAX",
        "directive": [
            "tag \"${id}\"",
            "publishDir \"data/${id}\""
        ],
        "when": "",
        "stub": ""
    },
    "taxonomy": {
        "name_process": "taxonomy",
        "string_process": "\nprocess taxonomy {\n    tag \"${id}\"\n    publishDir \"data/${id}\"\n\n    input:\n    file db\n    set val(id), file(mash_ANImax), file(mash_taxIDs), file(dada2_lineage), file(centrifuge_taxIDs), file(kaiju_taxIDs), file(checkm_lineage), file(genes_cnt) from id_collection\n\n    output:\n    file \"${id}.summary\" into camitax_summaries\n\n    script:\n    mash_ids = \"${db}/mash/RefSeq_20190108.ids\"\n    ncbi_names = \"${db}/taxonomy/names_20190108.dmp\"\n    ncbi_nodes = \"${db}/taxonomy/nodes_20190108.dmp\"\n\n    \"\"\"\n    camitaxonomy.py --names ${ncbi_names} \\\n                    --nodes ${ncbi_nodes} \\\n                    --mash ${mash_taxIDs} \\\n                    --dada2 ${dada2_lineage} \\\n                    --centrifuge ${centrifuge_taxIDs} \\\n                    --kaiju ${kaiju_taxIDs} \\\n                    --checkm ${checkm_lineage} \\\n                    --known ${mash_ids} \\\n                    --animax ${mash_ANImax} \\\n                    --genes ${genes_cnt} \\\n                    ${id} > ${id}.summary\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "    mash_ids = \"${db}/mash/RefSeq_20190108.ids\"\n    ncbi_names = \"${db}/taxonomy/names_20190108.dmp\"\n    ncbi_nodes = \"${db}/taxonomy/nodes_20190108.dmp\"\n\n    \"\"\"\n    camitaxonomy.py --names ${ncbi_names} \\\n                    --nodes ${ncbi_nodes} \\\n                    --mash ${mash_taxIDs} \\\n                    --dada2 ${dada2_lineage} \\\n                    --centrifuge ${centrifuge_taxIDs} \\\n                    --kaiju ${kaiju_taxIDs} \\\n                    --checkm ${checkm_lineage} \\\n                    --known ${mash_ids} \\\n                    --animax ${mash_ANImax} \\\n                    --genes ${genes_cnt} \\\n                    ${id} > ${id}.summary\n    \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "db",
            "id_collection"
        ],
        "nb_inputs": 2,
        "outputs": [
            "camitax_summaries"
        ],
        "nb_outputs": 1,
        "name_workflow": "CAMI-challenge__CAMITAX",
        "directive": [
            "tag \"${id}\"",
            "publishDir \"data/${id}\""
        ],
        "when": "",
        "stub": ""
    },
    "summary": {
        "name_process": "summary",
        "string_process": "\nprocess summary {\n    tag 'The Final Countdown'\n    publishDir 'data'\n\n    input:\n    file summaryList from camitax_summaries.collect()\n\n    output:\n    file \"camitax.tsv\"\n\n    \"\"\"\n    for summary in ${summaryList}\n    do\n        head -n 1 \\$summary > camitax.tsv\n        break\n    done\n    for summary in ${summaryList}\n    do\n        tail -n +2 \\$summary >> camitax.tsv\n    done\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\n    for summary in ${summaryList}\n    do\n        head -n 1 \\$summary > camitax.tsv\n        break\n    done\n    for summary in ${summaryList}\n    do\n        tail -n +2 \\$summary >> camitax.tsv\n    done\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [
            "BreakSeq"
        ],
        "tools_url": [
            "https://bio.tools/breakseq"
        ],
        "tools_dico": [
            {
                "name": "BreakSeq",
                "uri": "https://bio.tools/breakseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "Structural variation"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "Genomic structural variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "DNA structural variation"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Database of known human breakpoint junctions and software to search short reads against them.",
                "homepage": "http://sv.gersteinlab.org/breakseq/"
            }
        ],
        "inputs": [
            "camitax_summaries"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "CAMI-challenge__CAMITAX",
        "directive": [
            "tag 'The Final Countdown'",
            "publishDir 'data'"
        ],
        "when": "",
        "stub": ""
    },
    "cache_taxonomy": {
        "name_process": "cache_taxonomy",
        "string_process": "\nprocess cache_taxonomy {\n    storeDir \"${params.db}\"\n                     \n\n    output:\n    file 'taxonomy/names_20190108.dmp'\n    file 'taxonomy/nodes_20190108.dmp'\n\n    \"\"\"\n    wget https://zenodo.org/record/3237483/files/taxonomy_20190108.tar.gz\n    tar xfv taxonomy_*.tar.gz && rm taxonomy_*.tar.gz\n    \"\"\"\n}",
        "nb_lignes_process": 12,
        "string_script": "\"\"\"\n    wget https://zenodo.org/record/3237483/files/taxonomy_20190108.tar.gz\n    tar xfv taxonomy_*.tar.gz && rm taxonomy_*.tar.gz\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "CAMI-challenge__CAMITAX",
        "directive": [
            "storeDir \"${params.db}\""
        ],
        "when": "",
        "stub": ""
    },
    "cache_mash_db": {
        "name_process": "cache_mash_db",
        "string_process": "\nprocess cache_mash_db {\n    storeDir \"${params.db}\"\n                     \n\n    output:\n    file 'mash/RefSeq_20190108.ids'\n    file 'mash/RefSeq_20190108.msh'\n\n    \"\"\"\n    wget https://zenodo.org/record/3237483/files/mash_20190108.tar.gz\n    tar xfv mash_*.tar.gz && rm mash_*.tar.gz\n    \"\"\"\n}",
        "nb_lignes_process": 12,
        "string_script": "\"\"\"\n    wget https://zenodo.org/record/3237483/files/mash_20190108.tar.gz\n    tar xfv mash_*.tar.gz && rm mash_*.tar.gz\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "CAMI-challenge__CAMITAX",
        "directive": [
            "storeDir \"${params.db}\""
        ],
        "when": "",
        "stub": ""
    },
    "cache_dada2_db": {
        "name_process": "cache_dada2_db",
        "string_process": "\nprocess cache_dada2_db {\n    storeDir \"${params.db}/dada2\"\n                     \n\n    output:\n    file 'rdp_train_set_16.fa.gz'\n    file 'rdp_species_assignment_16.fa.gz'\n    file 'silva_nr_v132_train_set.fa.gz'\n    file 'silva_species_assignment_v132.fa.gz'\n\n    \"\"\"\n    wget https://zenodo.org/record/801828/files/rdp_train_set_16.fa.gz\n    wget https://zenodo.org/record/801828/files/rdp_species_assignment_16.fa.gz\n    wget https://zenodo.org/record/1172783/files/silva_nr_v132_train_set.fa.gz\n    wget https://zenodo.org/record/1172783/files/silva_species_assignment_v132.fa.gz\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "\"\"\"\n    wget https://zenodo.org/record/801828/files/rdp_train_set_16.fa.gz\n    wget https://zenodo.org/record/801828/files/rdp_species_assignment_16.fa.gz\n    wget https://zenodo.org/record/1172783/files/silva_nr_v132_train_set.fa.gz\n    wget https://zenodo.org/record/1172783/files/silva_species_assignment_v132.fa.gz\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "CAMI-challenge__CAMITAX",
        "directive": [
            "storeDir \"${params.db}/dada2\""
        ],
        "when": "",
        "stub": ""
    },
    "cache_centrifuge_db": {
        "name_process": "cache_centrifuge_db",
        "string_process": "\nprocess cache_centrifuge_db {\n    storeDir \"${params.db}\"\n                     \n\n    output:\n    file 'centrifuge/proGenomes_20190108.1.cf'\n    file 'centrifuge/proGenomes_20190108.2.cf'\n    file 'centrifuge/proGenomes_20190108.3.cf'\n    file 'centrifuge/proGenomes_20190108.4.cf'\n\n    \"\"\"\n    wget https://zenodo.org/record/3237483/files/centrifuge_20190108.tar.gz\n    tar xfv centrifuge_*.tar.gz && rm centrifuge_*.tar.gz\n    \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "\"\"\"\n    wget https://zenodo.org/record/3237483/files/centrifuge_20190108.tar.gz\n    tar xfv centrifuge_*.tar.gz && rm centrifuge_*.tar.gz\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "CAMI-challenge__CAMITAX",
        "directive": [
            "storeDir \"${params.db}\""
        ],
        "when": "",
        "stub": ""
    },
    "cache_kaiju_db": {
        "name_process": "cache_kaiju_db",
        "string_process": "\nprocess cache_kaiju_db {\n    storeDir \"${params.db}\"\n                     \n\n    output:\n    file 'kaiju/proGenomes_20190108.fmi'\n\n    \"\"\"\n    wget https://zenodo.org/record/3237483/files/kaiju_20190108.tar.gz\n    tar xfv kaiju_*.tar.gz && rm kaiju_*.tar.gz\n    \"\"\"\n}",
        "nb_lignes_process": 11,
        "string_script": "\"\"\"\n    wget https://zenodo.org/record/3237483/files/kaiju_20190108.tar.gz\n    tar xfv kaiju_*.tar.gz && rm kaiju_*.tar.gz\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "CAMI-challenge__CAMITAX",
        "directive": [
            "storeDir \"${params.db}\""
        ],
        "when": "",
        "stub": ""
    },
    "cache_checkm_db": {
        "name_process": "cache_checkm_db",
        "string_process": "\nprocess cache_checkm_db {\n    storeDir \"${params.db}/checkm\"\n                     \n\n    output:\n    file 'distributions/cd_dist.txt'\n    file 'distributions/gc_dist.txt'\n    file 'distributions/td_dist.txt'\n    file 'genome_tree/genome_tree.derep.txt'\n    file 'genome_tree/genome_tree.metadata.tsv'\n    file 'genome_tree/genome_tree.taxonomy.tsv'\n    file 'genome_tree/genome_tree_full.refpkg/CONTENTS.json'\n    file 'genome_tree/genome_tree_full.refpkg/genome_tree.fasta'\n    file 'genome_tree/genome_tree_full.refpkg/genome_tree.log'\n    file 'genome_tree/genome_tree_full.refpkg/genome_tree.tre'\n    file 'genome_tree/genome_tree_full.refpkg/phylo_modelEcOyPk.json'\n    file 'genome_tree/genome_tree_reduced.refpkg/CONTENTS.json'\n    file 'genome_tree/genome_tree_reduced.refpkg/genome_tree.fasta'\n    file 'genome_tree/genome_tree_reduced.refpkg/genome_tree.log'\n    file 'genome_tree/genome_tree_reduced.refpkg/genome_tree.tre'\n    file 'genome_tree/genome_tree_reduced.refpkg/phylo_modelJqWx6_.json'\n    file 'genome_tree/missing_duplicate_genes_50.tsv'\n    file 'genome_tree/missing_duplicate_genes_97.tsv'\n    file 'hmms/checkm.hmm'\n    file 'hmms/checkm.hmm.ssi'\n    file 'hmms/phylo.hmm'\n    file 'hmms/phylo.hmm.ssi'\n    file 'hmms_ssu/createHMMs.py'\n    file 'hmms_ssu/SSU_archaea.hmm'\n    file 'hmms_ssu/SSU_bacteria.hmm'\n    file 'hmms_ssu/SSU_euk.hmm'\n    file 'img/img_metadata.tsv'\n    file 'pfam/Pfam-A.hmm.dat'\n    file 'pfam/tigrfam2pfam.tsv'\n    file 'selected_marker_sets.tsv'\n    file 'taxon_marker_sets.tsv'\n    file 'test_data/637000110.fna'\n\n    \"\"\"\n    wget https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz\n    tar xfv checkm_*.tar.gz && rm checkm_*.tar.gz\n    \"\"\"\n}",
        "nb_lignes_process": 42,
        "string_script": "\"\"\"\n    wget https://data.ace.uq.edu.au/public/CheckM_databases/checkm_data_2015_01_16.tar.gz\n    tar xfv checkm_*.tar.gz && rm checkm_*.tar.gz\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "CAMI-challenge__CAMITAX",
        "directive": [
            "storeDir \"${params.db}/checkm\""
        ],
        "when": "",
        "stub": ""
    }
}