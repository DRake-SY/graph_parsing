{
    "roary": {
        "name_process": "roary",
        "string_process": "\nprocess roary {\n    cpus 20\n    time '47h' \n    queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'\n    publishDir 'roary', mode: 'copy', overwrite: true\n \n    input:\n    file(genome) from gff_roary.collect()\n\n    output:\n    file \"roary_out/*\" into roary_all_results\n\n    script:\n    \"\"\"\n    roary -p ${task.cpus} -ne -f roary_out ${genome}\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    roary -p ${task.cpus} -ne -f roary_out ${genome}\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "Roary"
        ],
        "tools_url": [
            "https://bio.tools/roary"
        ],
        "tools_dico": [
            {
                "name": "Roary",
                "uri": "https://bio.tools/roary",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A high speed stand alone pan genome pipeline, which takes annotated assemblies in GFF3 format (produced by Prokka (Seemann, 2014)) and calculates the pan genome.",
                "homepage": "http://sanger-pathogens.github.io/Roary/"
            }
        ],
        "inputs": [
            "gff_roary"
        ],
        "nb_inputs": 1,
        "outputs": [
            "roary_all_results"
        ],
        "nb_outputs": 1,
        "name_workflow": "happykhan__nf-scripts",
        "directive": [
            "cpus 20",
            "time '47h'",
            "queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'",
            "publishDir 'roary', mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "ariba": {
        "name_process": "ariba",
        "string_process": "\nprocess ariba {\n    cpus 20\n    time '8h' \n    queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'\n    publishDir 'ariba_resfinder', mode: 'copy', overwrite: true\n \n    input:\n    set val(name), file(reads) from reads_ariba_res\n    file db \n\n    output:\n    file \"${name}.tsv\" into ariba_res_results\n    file \"${name}\" into ariba_all_results\n\n    script:\n    \"\"\"\n    ariba run --threads ${task.cpus} ${db}  ${reads[0]}  ${reads[1]} ${name} \n    cp ${name}/report.tsv  ${name}.tsv \n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "    \"\"\"\n    ariba run --threads ${task.cpus} ${db}  ${reads[0]}  ${reads[1]} ${name} \n    cp ${name}/report.tsv  ${name}.tsv \n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "reads_ariba_res",
            "db"
        ],
        "nb_inputs": 2,
        "outputs": [
            "ariba_res_results",
            "ariba_all_results"
        ],
        "nb_outputs": 2,
        "name_workflow": "happykhan__nf-scripts",
        "directive": [
            "cpus 20",
            "time '8h'",
            "queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'",
            "publishDir 'ariba_resfinder', mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "aribasum": {
        "name_process": "aribasum",
        "string_process": "\nprocess aribasum {\n    cpus 10\n    time '4h' \n    queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'\n    publishDir 'ariba_resfinder/', mode: 'copy', overwrite: true\n \n    input:\n    file(reports) from ariba_res_results.collect()\n\n    output:\n    file \"*\" into aribares_out\n\n    script:\n    \"\"\"\n    ariba summary --preset all resfinder  $reports \n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    ariba summary --preset all resfinder  $reports \n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ariba_res_results"
        ],
        "nb_inputs": 1,
        "outputs": [
            "aribares_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "happykhan__nf-scripts",
        "directive": [
            "cpus 10",
            "time '4h'",
            "queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'",
            "publishDir 'ariba_resfinder/', mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "aribaplas": {
        "name_process": "aribaplas",
        "string_process": "\nprocess aribaplas {\n    cpus 20\n    time '8h' \n    queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'\n    publishDir 'ariba_plasmidfinder', mode: 'copy', overwrite: true\n \n    input:\n    set val(name), file(reads) from reads_ariba_plas\n    file plasdb \n\n    output:\n    file \"${name}.tsv\" into ariba_plas_results\n    file \"${name}\" into aribaplas_all_results\n\n    script:\n    \"\"\"\n    ariba run --threads ${task.cpus} ${plasdb}  ${reads[0]}  ${reads[1]} ${name} \n    cp ${name}/report.tsv  ${name}.tsv \n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "    \"\"\"\n    ariba run --threads ${task.cpus} ${plasdb}  ${reads[0]}  ${reads[1]} ${name} \n    cp ${name}/report.tsv  ${name}.tsv \n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "reads_ariba_plas",
            "plasdb"
        ],
        "nb_inputs": 2,
        "outputs": [
            "ariba_plas_results",
            "aribaplas_all_results"
        ],
        "nb_outputs": 2,
        "name_workflow": "happykhan__nf-scripts",
        "directive": [
            "cpus 20",
            "time '8h'",
            "queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'",
            "publishDir 'ariba_plasmidfinder', mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "aribaplassum": {
        "name_process": "aribaplassum",
        "string_process": "\nprocess aribaplassum {\n    cpus 10\n    time '4h' \n    queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'\n    publishDir 'ariba_plasmidfinder/', mode: 'copy', overwrite: true\n \n    input:\n    file(reports) from ariba_plas_results.collect()\n\n    output:\n    file \"*\" into aribaplas_out\n\n    script:\n    \"\"\"\n    ariba summary --preset all plasmidfinder  $reports \n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    ariba summary --preset all plasmidfinder  $reports \n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ariba_plas_results"
        ],
        "nb_inputs": 1,
        "outputs": [
            "aribaplas_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "happykhan__nf-scripts",
        "directive": [
            "cpus 10",
            "time '4h'",
            "queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'",
            "publishDir 'ariba_plasmidfinder/', mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "aribacard": {
        "name_process": "aribacard",
        "string_process": "\nprocess aribacard {\n    cpus 20\n    time '8h' \n    queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'\n    publishDir 'ariba_card', mode: 'copy', overwrite: true\n \n    input:\n    set val(name), file(reads) from reads_ariba_card\n    file carddb \n\n    output:\n    file \"${name}.tsv\" into ariba_card_results\n    file \"${name}\" into aribacard_all_results\n\n    script:\n    \"\"\"\n    ariba run --threads ${task.cpus} ${carddb}  ${reads[0]}  ${reads[1]} ${name} \n    cp ${name}/report.tsv  ${name}.tsv \n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "    \"\"\"\n    ariba run --threads ${task.cpus} ${carddb}  ${reads[0]}  ${reads[1]} ${name} \n    cp ${name}/report.tsv  ${name}.tsv \n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "reads_ariba_card",
            "carddb"
        ],
        "nb_inputs": 2,
        "outputs": [
            "ariba_card_results",
            "aribacard_all_results"
        ],
        "nb_outputs": 2,
        "name_workflow": "happykhan__nf-scripts",
        "directive": [
            "cpus 20",
            "time '8h'",
            "queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'",
            "publishDir 'ariba_card', mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "aribacardsum": {
        "name_process": "aribacardsum",
        "string_process": "\nprocess aribacardsum {\n    cpus 10\n    time '4h' \n    queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'\n    publishDir 'ariba_card/', mode: 'copy', overwrite: true\n \n    input:\n    file(reports) from ariba_card_results.collect()\n\n    output:\n    file \"*\" into aribacard_out\n\n    script:\n    \"\"\"\n    ariba summary --preset all card  $reports \n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    ariba summary --preset all card  $reports \n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ariba_card_results"
        ],
        "nb_inputs": 1,
        "outputs": [
            "aribacard_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "happykhan__nf-scripts",
        "directive": [
            "cpus 10",
            "time '4h'",
            "queue 'qib-long,qib-medium,qib-short,nbi-medium,nbi-short,nbi-long'",
            "publishDir 'ariba_card/', mode: 'copy', overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "snippy": {
        "name_process": "snippy",
        "string_process": "\nprocess snippy {\n   cpus 20\n   queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'\n   executor 'slurm'\n   memory { 80.GB * task.attempt }\n\n   errorStrategy {  'retry' }\n   maxRetries 3\n\n   input:\n   set name, file(read1), file(read2) from reads_snippy\n   file ref \n\n   output:\n   file \"${name}\" into core_aln_results\n\n   script:\n   \"\"\"\n   snippy --cpus ${task.cpus} --ref ${ref} --R1 ${read1} --R2 ${read2} --outdir ${name} --cleanup\n   \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "   \"\"\"\n   snippy --cpus ${task.cpus} --ref ${ref} --R1 ${read1} --R2 ${read2} --outdir ${name} --cleanup\n   \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "snippy"
        ],
        "tools_url": [
            "https://bio.tools/snippy"
        ],
        "tools_dico": [
            {
                "name": "snippy",
                "uri": "https://bio.tools/snippy",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2885",
                            "term": "DNA polymorphism"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phylogenetic inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3227",
                                    "term": "Variant calling"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phlyogenetic tree construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phylogenetic tree generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3227",
                                    "term": "Variant mapping"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Rapid haploid variant calling and core SNP phylogeny generation.",
                "homepage": "https://github.com/tseemann/snippy"
            }
        ],
        "inputs": [
            "reads_snippy",
            "ref"
        ],
        "nb_inputs": 2,
        "outputs": [
            "core_aln_results"
        ],
        "nb_outputs": 1,
        "name_workflow": "happykhan__nf-scripts",
        "directive": [
            "cpus 20",
            "queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'",
            "executor 'slurm'",
            "memory { 80.GB * task.attempt }",
            "errorStrategy { 'retry' }",
            "maxRetries 3"
        ],
        "when": "",
        "stub": ""
    },
    "snippycore": {
        "name_process": "snippycore",
        "string_process": "\nprocess snippycore {\n    publishDir 'snippy', mode: 'copy', overwrite: true\n    cpus 10 \n    queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'\n\n    input:\n    file(snippy) from core_aln_results.collect()\n\n    output:\n    file \"core.full.aln\" into snipclean\n    file \"core*\" into snippyout\n\n    script:\n    \"\"\"\n    snippy-core --mask-char=N --ref ${params.reference} ${snippy}\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    snippy-core --mask-char=N --ref ${params.reference} ${snippy}\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "core_aln_results"
        ],
        "nb_inputs": 1,
        "outputs": [
            "snipclean",
            "snippyout"
        ],
        "nb_outputs": 2,
        "name_workflow": "happykhan__nf-scripts",
        "directive": [
            "publishDir 'snippy', mode: 'copy', overwrite: true",
            "cpus 10",
            "queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'"
        ],
        "when": "",
        "stub": ""
    },
    "snippyclean": {
        "name_process": "snippyclean",
        "string_process": "\nprocess snippyclean {\n    publishDir 'snippy', mode: 'copy', overwrite: true\n    queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'\n    executor 'slurm'\n\n    input: \n    file(core) from snipclean    \n\n    output:\n    file \"clean.full.aln\" into ( iq_align, nj_core_align, ft_core_align, clonal_align, fast_align)\n\n    script:\n    \"\"\"\n    snippy-clean_full_aln ${core} > clean.full.aln\n    \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "    \"\"\"\n    snippy-clean_full_aln ${core} > clean.full.aln\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "snipclean"
        ],
        "nb_inputs": 1,
        "outputs": [
            ""
        ],
        "nb_outputs": 1,
        "name_workflow": "happykhan__nf-scripts",
        "directive": [
            "publishDir 'snippy', mode: 'copy', overwrite: true",
            "queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'",
            "executor 'slurm'"
        ],
        "when": "",
        "stub": ""
    },
    "rapidnj": {
        "name_process": "rapidnj",
        "string_process": "\nprocess rapidnj  {\n    publishDir 'rapidnj', mode: 'copy', overwrite: true\n    queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'\n    executor 'slurm'\n \n    input:\n    file core from nj_core_align \n\n    output:\n    file 'rapidnj.tree' into njtree \n\n    script:\n    \"\"\"\n    rapidnj -n -i fa ${core} > rapidnj.tree\n    \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "    \"\"\"\n    rapidnj -n -i fa ${core} > rapidnj.tree\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "RapidNJ"
        ],
        "tools_url": [
            "https://bio.tools/rapidnj"
        ],
        "tools_dico": [
            {
                "name": "RapidNJ",
                "uri": "https://bio.tools/rapidnj",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phylogenetic inference"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phlyogenetic tree construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0323",
                                    "term": "Phylogenetic tree generation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A tool for fast canonical neighbor-joining tree construction.",
                "homepage": "http://birc.au.dk/software/rapidnj/"
            }
        ],
        "inputs": [
            "nj_core_align"
        ],
        "nb_inputs": 1,
        "outputs": [
            "njtree"
        ],
        "nb_outputs": 1,
        "name_workflow": "happykhan__nf-scripts",
        "directive": [
            "publishDir 'rapidnj', mode: 'copy', overwrite: true",
            "queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'",
            "executor 'slurm'"
        ],
        "when": "",
        "stub": ""
    },
    "iqtreefast": {
        "name_process": "iqtreefast",
        "string_process": "\nprocess iqtreefast{\n   cpus 20\n   publishDir 'iqtree_fast', mode: 'copy', overwrite: true\n   time '47h'\n   queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'\n   executor 'slurm'\n   memory { 100.GB * task.attempt }\n   time { 2.d * task.attempt }\n\n   errorStrategy { task.exitStatus in 137..140 ? 'retry' : 'terminate' }\n   maxRetries 3\n\n   when:\n   params.tree\n\n   input:\n   file align from fast_align\n\n   output:\n   file 'iqtree_fast*' into iqfastout\n   file 'iqtree_fast.treefile' into iqtreefastout\n\n   script:\n   \"\"\"\n   iqtree -s ${align} -pre iqtree_fast -nt ${task.cpus} -m HKY -fast\n   \"\"\"\n\n}",
        "nb_lignes_process": 27,
        "string_script": "   \"\"\"\n   iqtree -s ${align} -pre iqtree_fast -nt ${task.cpus} -m HKY -fast\n   \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fast_align"
        ],
        "nb_inputs": 1,
        "outputs": [
            "iqfastout",
            "iqtreefastout"
        ],
        "nb_outputs": 2,
        "name_workflow": "happykhan__nf-scripts",
        "directive": [
            "cpus 20",
            "publishDir 'iqtree_fast', mode: 'copy', overwrite: true",
            "time '47h'",
            "queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'",
            "executor 'slurm'",
            "memory { 100.GB * task.attempt }",
            "time { 2.d * task.attempt }",
            "errorStrategy { task.exitStatus in 137..140 ? 'retry' : 'terminate' }",
            "maxRetries 3"
        ],
        "when": "params.tree",
        "stub": ""
    },
    "iqtree": {
        "name_process": "iqtree",
        "string_process": "\nprocess iqtree{\n   cpus 20\n   publishDir 'iqtree', mode: 'copy', overwrite: true    \n   time '3d'\n   queue 'qib-long,nbi-long'\n   executor 'slurm'\n   memory { 100.GB * task.attempt }\n   time { 3.d * task.attempt }\n\n   errorStrategy { 'retry' }\n   maxRetries 3\n\n   when:\n   params.tree\n \n   input: \n   file align from iq_align \n   \n   output: \n   file 'iqtree*' into iqout\n   file 'iqtree.treefile' into iqtreeout\n   \n   script:\n   \"\"\"\n   iqtree -s ${align} -pre iqtree -nt ${task.cpus} -m GTR+G \n   \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "   \"\"\"\n   iqtree -s ${align} -pre iqtree -nt ${task.cpus} -m GTR+G \n   \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "iq_align"
        ],
        "nb_inputs": 1,
        "outputs": [
            "iqout",
            "iqtreeout"
        ],
        "nb_outputs": 2,
        "name_workflow": "happykhan__nf-scripts",
        "directive": [
            "cpus 20",
            "publishDir 'iqtree', mode: 'copy', overwrite: true",
            "time '3d'",
            "queue 'qib-long,nbi-long'",
            "executor 'slurm'",
            "memory { 100.GB * task.attempt }",
            "time { 3.d * task.attempt }",
            "errorStrategy { 'retry' }",
            "maxRetries 3"
        ],
        "when": "params.tree",
        "stub": ""
    },
    "clonal": {
        "name_process": "clonal",
        "string_process": "\nprocess clonal{\n   publishDir 'clonal', mode: 'copy', overwrite: true    \n   time '5d'\n   cpus 5 \n   memory '120 GB'   \n   queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'\n   executor 'slurm'\n   memory { 120.GB * task.attempt }\n   time { 2.d * task.attempt }\n\n   errorStrategy {  'retry' }\n   maxRetries 3\n\n   when:\n   params.tree\n\n   input: \n   file tree from iqtreefastout\n   file align from clonal_align\n   \n   output:\n   file 'clonal*' into clonalout\n\n   script:\n   \"\"\"\n   ClonalFrameML ${tree} ${align} clonal\n   \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "   \"\"\"\n   ClonalFrameML ${tree} ${align} clonal\n   \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "ClonalFrameML"
        ],
        "tools_url": [
            "https://bio.tools/clonalframeml"
        ],
        "tools_dico": [
            {
                "name": "ClonalFrameML",
                "uri": "https://bio.tools/clonalframeml",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acids"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0194",
                            "term": "Phylogenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2403",
                                    "term": "Sequence analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0539",
                                    "term": "Phylogenetic inference (method centric)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Nucleic acid sequence analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2403",
                                    "term": "Sequence analysis (general)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0539",
                                    "term": "Phylogenetic tree construction (method centric)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0539",
                                    "term": "Phylogenetic tree generation (method centric)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Sequence analysis (nucleic acid)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "ClonalFrameML is a maximum likelihood implementation of the Bayesian software ClonalFrame which was previously described by Didelot and Falush (2007). The recombination model underpinning ClonalFrameML is exactly the same as for ClonalFrame, but this new implementation is a lot faster, is able to deal with much larger genomic dataset, and does not suffer from MCMC convergence issues",
                "homepage": "https://github.com/xavierdidelot/ClonalFrameML"
            }
        ],
        "inputs": [
            "iqtreefastout",
            "clonal_align"
        ],
        "nb_inputs": 2,
        "outputs": [
            "clonalout"
        ],
        "nb_outputs": 1,
        "name_workflow": "happykhan__nf-scripts",
        "directive": [
            "publishDir 'clonal', mode: 'copy', overwrite: true",
            "time '5d'",
            "cpus 5",
            "memory '120 GB'",
            "queue 'nbi-largemem,nbi-medium,nbi-short,nbi-long,qib-long,qib-medium,qib-short'",
            "executor 'slurm'",
            "memory { 120.GB * task.attempt }",
            "time { 2.d * task.attempt }",
            "errorStrategy { 'retry' }",
            "maxRetries 3"
        ],
        "when": "params.tree",
        "stub": ""
    }
}