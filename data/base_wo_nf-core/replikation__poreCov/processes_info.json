{
    "snp_sites": {
        "name_process": "snp_sites",
        "string_process": "process snp_sites {\n    label \"snp_sites\"\n    input:\n        file(sites)\n    output:\n        file(\"clean.core.aln\")\n    script:\n        \"\"\"\n        snp-sites -c ${sites} > clean.core.aln\n        \"\"\"\n}",
        "nb_lignes_process": 9,
        "string_script": "        \"\"\"\n        snp-sites -c ${sites} > clean.core.aln\n        \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sites"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label \"snp_sites\""
        ],
        "when": "",
        "stub": ""
    },
    "artic_medaka": {
        "name_process": "artic_medaka",
        "string_process": "process artic_medaka {\n        label 'artic'\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"*.consensus.fasta\"\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}_mapped_*.primertrimmed.sorted.bam*\"\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}.trimmed.rg.sorted.bam\"\n        publishDir \"${params.output}/${params.genomedir}/all_consensus_sequences/\", mode: 'copy', pattern: \"*.consensus.fasta\"\n        publishDir \"${params.output}/${params.lineagedir}/${name}/\", mode: 'copy', pattern: \"SNP_${name}.pass.vcf\"\n\n    input:\n        tuple val(name), path(reads), path(external_scheme)\n    output:\n        tuple val(name), path(\"*.consensus.fasta\"), emit: fasta\n        tuple val(name), path(\"${name}_mapped_*.primertrimmed.sorted.bam\"), path(\"${name}_mapped_*.primertrimmed.sorted.bam.bai\"), emit: reference_bam\n        tuple val(name), path(\"SNP_${name}.pass.vcf\"), emit: vcf\n        tuple val(name), path(\"${name}.pass.vcf.gz\"), path(\"${name}.coverage_mask.txt.*1.depths\"), path(\"${name}.coverage_mask.txt.*2.depths\"), emit: covarplot\n        tuple val(name), path(\"${name}.trimmed.rg.sorted.bam\"), emit: fullbam\n    script:   \n        \"\"\"\n        artic minion    --medaka \\\n                        --medaka-model ${params.medaka_model} \\\n                        --min-depth ${params.min_depth} \\\n                        --normalise 500 \\\n                        --threads ${task.cpus} \\\n                        --scheme-directory ${external_scheme} \\\n                        --read-file ${reads} \\\n                        nCoV-2019/${params.primerV} ${name}\n\n        # generate depth files\n        artic_make_depth_mask --depth ${params.min_depth} \\\n            --store-rg-depths ${external_scheme}/nCoV-2019/${params.primerV}/nCoV-2019.reference.fasta \\\n            ${name}.primertrimmed.rg.sorted.bam \\\n            ${name}.coverage_mask.txt\n\n        zcat ${name}.pass.vcf.gz > SNP_${name}.pass.vcf\n\n        sed -i \"1s/.*/>${name}/\" *.consensus.fasta\n\n        # get reference FASTA ID to rename BAM\n        REF=\\$(samtools view -H ${name}.primertrimmed.rg.sorted.bam | awk 'BEGIN{FS=\"\\\\t\"};{if(\\$1==\"@SQ\"){print \\$2}}' | sed 's/SN://g')\n        mv ${name}.primertrimmed.rg.sorted.bam ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        samtools index ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        \"\"\"\n        stub:\n        \"\"\"\n        touch genome.consensus.fasta \\\n            ${name}_mapped_1.primertrimmed.sorted.bam \\\n            ${name}_mapped_1.primertrimmed.sorted.bam.bai \\\n            SNP_${name}.pass.vcf \\\n            ${name}.pass.vcf.gz \\\n            ${name}.coverage_mask.txt.nCoV-2019_1.depths \\\n            ${name}.coverage_mask.txt.nCoV-2019_2.depths \\\n            ${name}.trimmed.rg.sorted.bam\n        \"\"\"\n}",
        "nb_lignes_process": 52,
        "string_script": "        \"\"\"\n        artic minion    --medaka \\\n                        --medaka-model ${params.medaka_model} \\\n                        --min-depth ${params.min_depth} \\\n                        --normalise 500 \\\n                        --threads ${task.cpus} \\\n                        --scheme-directory ${external_scheme} \\\n                        --read-file ${reads} \\\n                        nCoV-2019/${params.primerV} ${name}\n\n        # generate depth files\n        artic_make_depth_mask --depth ${params.min_depth} \\\n            --store-rg-depths ${external_scheme}/nCoV-2019/${params.primerV}/nCoV-2019.reference.fasta \\\n            ${name}.primertrimmed.rg.sorted.bam \\\n            ${name}.coverage_mask.txt\n\n        zcat ${name}.pass.vcf.gz > SNP_${name}.pass.vcf\n\n        sed -i \"1s/.*/>${name}/\" *.consensus.fasta\n\n        # get reference FASTA ID to rename BAM\n        REF=\\$(samtools view -H ${name}.primertrimmed.rg.sorted.bam | awk 'BEGIN{FS=\"\\\\t\"};{if(\\$1==\"@SQ\"){print \\$2}}' | sed 's/SN://g')\n        mv ${name}.primertrimmed.rg.sorted.bam ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        samtools index ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        \"\"\"",
        "nb_lignes_script": 24,
        "language_script": "bash",
        "tools": [
            "ARTIC",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/artic",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "ARTIC",
                "uri": "https://bio.tools/artic",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_1383",
                                "term": "Nucleic acid sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3498",
                                "term": "Sequence variations"
                            }
                        ]
                    }
                ],
                "description": "A bioinformatics pipeline for working with virus sequencing data sequenced with nanopore",
                "homepage": "https://github.com/artic-network/fieldbioinformatics"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "name",
            "reads",
            "external_scheme"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'artic'",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"*.consensus.fasta\"",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}_mapped_*.primertrimmed.sorted.bam*\"",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}.trimmed.rg.sorted.bam\"",
            "publishDir \"${params.output}/${params.genomedir}/all_consensus_sequences/\", mode: 'copy', pattern: \"*.consensus.fasta\"",
            "publishDir \"${params.output}/${params.lineagedir}/${name}/\", mode: 'copy', pattern: \"SNP_${name}.pass.vcf\""
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch genome.consensus.fasta \\\n            ${name}_mapped_1.primertrimmed.sorted.bam \\\n            ${name}_mapped_1.primertrimmed.sorted.bam.bai \\\n            SNP_${name}.pass.vcf \\\n            ${name}.pass.vcf.gz \\\n            ${name}.coverage_mask.txt.nCoV-2019_1.depths \\\n            ${name}.coverage_mask.txt.nCoV-2019_2.depths \\\n            ${name}.trimmed.rg.sorted.bam\n        \"\"\""
    },
    "artic_medaka_custom_bed": {
        "name_process": "artic_medaka_custom_bed",
        "string_process": "\nprocess artic_medaka_custom_bed {\n        label 'artic'\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"*.consensus.fasta\"\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}_mapped_*.primertrimmed.sorted.bam*\"\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}.trimmed.rg.sorted.bam\"\n        publishDir \"${params.output}/${params.genomedir}/all_consensus_sequences/\", mode: 'copy', pattern: \"*.consensus.fasta\"\n        publishDir \"${params.output}/${params.lineagedir}/${name}/\", mode: 'copy', pattern: \"SNP_${name}.pass.vcf\"\n\n    input:\n        tuple val(name), path(reads), path(external_scheme), path(primerBed)\n    output:\n        tuple val(name), path(\"*.consensus.fasta\"), emit: fasta\n        tuple val(name), path(\"${name}_mapped_*.primertrimmed.sorted.bam\"), path(\"${name}_mapped_*.primertrimmed.sorted.bam.bai\"), emit: reference_bam\n        tuple val(name), path(\"SNP_${name}.pass.vcf\"), emit: vcf\n        tuple val(name), path(\"${name}.pass.vcf.gz\"), path(\"${name}.coverage_mask.txt.*1.depths\"), path(\"${name}.coverage_mask.txt.*2.depths\"), emit: covarplot\n        tuple val(name), path(\"${name}.trimmed.rg.sorted.bam\"), emit: fullbam\n    script:   \n        \"\"\"\n        # create a new primer dir as input for artic\n        mkdir -p primer_scheme/nCoV-2019\n        cp -r ${external_scheme}/nCoV-2019/V_custom primer_scheme/nCoV-2019/\n\n        # clean up bed file: replace first colum with MN908947.3, remove empty lines and sort by 4th column (primer names) \n        cut -f2- ${primerBed} |\\\n            sed '/^[[:space:]]*\\$/d' |\\\n            sed -e \\$'s/^/MN908947.3\\\\t/' |\\\n            sort -k4 > primer_scheme/nCoV-2019/V_custom/nCoV-2019.scheme.bed\n\n        # start artic\n        artic minion    --medaka \\\n                        --medaka-model ${params.medaka_model} \\\n                        --min-depth ${params.min_depth} \\\n                        --normalise 500 \\\n                        --threads ${task.cpus} \\\n                        --scheme-directory primer_scheme \\\n                        --read-file ${reads} \\\n                        nCoV-2019/V_custom ${name}\n\n        # generate depth files\n        artic_make_depth_mask --depth ${params.min_depth} \\\n            --store-rg-depths primer_scheme/nCoV-2019/V_custom/nCoV-2019.reference.fasta \\\n            ${name}.primertrimmed.rg.sorted.bam \\\n            ${name}.coverage_mask.txt\n\n        zcat ${name}.pass.vcf.gz > SNP_${name}.pass.vcf\n\n        sed -i \"1s/.*/>${name}/\" *.consensus.fasta\n\n        # get reference FASTA ID to rename BAM\n        REF=\\$(samtools view -H ${name}.primertrimmed.rg.sorted.bam | awk 'BEGIN{FS=\"\\\\t\"};{if(\\$1==\"@SQ\"){print \\$2}}' | sed 's/SN://g')\n        mv ${name}.primertrimmed.rg.sorted.bam ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        samtools index ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        \"\"\"\n        stub:\n        \"\"\"\n        touch genome.consensus.fasta \\\n            ${name}_mapped_1.primertrimmed.sorted.bam \\\n            ${name}_mapped_1.primertrimmed.sorted.bam.bai \\\n            SNP_${name}.pass.vcf \\\n            ${name}.pass.vcf.gz \\\n            ${name}.coverage_mask.txt.nCoV-2019_1.depths \\\n            ${name}.coverage_mask.txt.nCoV-2019_2.depths \\\n            ${name}.trimmed.rg.sorted.bam\n        \"\"\"\n}",
        "nb_lignes_process": 64,
        "string_script": "        \"\"\"\n        # create a new primer dir as input for artic\n        mkdir -p primer_scheme/nCoV-2019\n        cp -r ${external_scheme}/nCoV-2019/V_custom primer_scheme/nCoV-2019/\n\n        # clean up bed file: replace first colum with MN908947.3, remove empty lines and sort by 4th column (primer names) \n        cut -f2- ${primerBed} |\\\n            sed '/^[[:space:]]*\\$/d' |\\\n            sed -e \\$'s/^/MN908947.3\\\\t/' |\\\n            sort -k4 > primer_scheme/nCoV-2019/V_custom/nCoV-2019.scheme.bed\n\n        # start artic\n        artic minion    --medaka \\\n                        --medaka-model ${params.medaka_model} \\\n                        --min-depth ${params.min_depth} \\\n                        --normalise 500 \\\n                        --threads ${task.cpus} \\\n                        --scheme-directory primer_scheme \\\n                        --read-file ${reads} \\\n                        nCoV-2019/V_custom ${name}\n\n        # generate depth files\n        artic_make_depth_mask --depth ${params.min_depth} \\\n            --store-rg-depths primer_scheme/nCoV-2019/V_custom/nCoV-2019.reference.fasta \\\n            ${name}.primertrimmed.rg.sorted.bam \\\n            ${name}.coverage_mask.txt\n\n        zcat ${name}.pass.vcf.gz > SNP_${name}.pass.vcf\n\n        sed -i \"1s/.*/>${name}/\" *.consensus.fasta\n\n        # get reference FASTA ID to rename BAM\n        REF=\\$(samtools view -H ${name}.primertrimmed.rg.sorted.bam | awk 'BEGIN{FS=\"\\\\t\"};{if(\\$1==\"@SQ\"){print \\$2}}' | sed 's/SN://g')\n        mv ${name}.primertrimmed.rg.sorted.bam ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        samtools index ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        \"\"\"",
        "nb_lignes_script": 35,
        "language_script": "bash",
        "tools": [
            "ARTIC",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/artic",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "ARTIC",
                "uri": "https://bio.tools/artic",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_1383",
                                "term": "Nucleic acid sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3498",
                                "term": "Sequence variations"
                            }
                        ]
                    }
                ],
                "description": "A bioinformatics pipeline for working with virus sequencing data sequenced with nanopore",
                "homepage": "https://github.com/artic-network/fieldbioinformatics"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "name",
            "reads",
            "external_scheme",
            "primerBed"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'artic'",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"*.consensus.fasta\"",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}_mapped_*.primertrimmed.sorted.bam*\"",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}.trimmed.rg.sorted.bam\"",
            "publishDir \"${params.output}/${params.genomedir}/all_consensus_sequences/\", mode: 'copy', pattern: \"*.consensus.fasta\"",
            "publishDir \"${params.output}/${params.lineagedir}/${name}/\", mode: 'copy', pattern: \"SNP_${name}.pass.vcf\""
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch genome.consensus.fasta \\\n            ${name}_mapped_1.primertrimmed.sorted.bam \\\n            ${name}_mapped_1.primertrimmed.sorted.bam.bai \\\n            SNP_${name}.pass.vcf \\\n            ${name}.pass.vcf.gz \\\n            ${name}.coverage_mask.txt.nCoV-2019_1.depths \\\n            ${name}.coverage_mask.txt.nCoV-2019_2.depths \\\n            ${name}.trimmed.rg.sorted.bam\n        \"\"\""
    },
    "artic_nanopolish": {
        "name_process": "artic_nanopolish",
        "string_process": "\nprocess artic_nanopolish {\n        label 'artic'\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"*.consensus.fasta\"\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}_mapped_*.primertrimmed.sorted.bam*\"\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}.trimmed.rg.sorted.bam\"        \n        publishDir \"${params.output}/${params.genomedir}/all_consensus_sequences/\", mode: 'copy', pattern: \"*.consensus.fasta\"\n        publishDir \"${params.output}/${params.lineagedir}/${name}/\", mode: 'copy', pattern: \"SNP_${name}.pass.vcf\"\n\n    input:\n        tuple val(name), path(reads), path(external_scheme), path(fast5_dir), path(txt_files)\n    output:\n        tuple val(name), path(\"*.consensus.fasta\"), emit: fasta\n        tuple val(name), path(\"${name}_mapped_*.primertrimmed.sorted.bam\"), path(\"${name}_mapped_*.primertrimmed.sorted.bam.bai\"), emit: reference_bam\n        tuple val(name), path(\"SNP_${name}.pass.vcf\"), emit: vcf\n        tuple val(name), path(\"${name}.pass.vcf.gz\"), path(\"${name}.coverage_mask.txt.*1.depths\"), path(\"${name}.coverage_mask.txt.*2.depths\"), emit: covarplot\n        tuple val(name), path(\"${name}.trimmed.rg.sorted.bam\"), emit: fullbam\n    script:   \n        \"\"\"\n        artic minion --minimap2 --normalise 500 \\\n            --threads ${task.cpus} \\\n            --scheme-directory ${external_scheme} \\\n            --read-file ${reads} \\\n            --fast5-directory ${fast5_dir} \\\n            --sequencing-summary sequencing_summary*.txt \\\n            nCoV-2019/${params.primerV} ${name}\n\n        # generate depth files\n        artic_make_depth_mask --depth ${params.min_depth} \\\n            --store-rg-depths ${external_scheme}/nCoV-2019/${params.primerV}/nCoV-2019.reference.fasta \\\n            ${name}.primertrimmed.rg.sorted.bam \\\n            ${name}.coverage_mask.txt\n\n        zcat ${name}.pass.vcf.gz > SNP_${name}.pass.vcf\n\n        sed -i \"1s/.*/>${name}/\" *.consensus.fasta\n\n        # get reference FASTA ID to rename BAM\n        REF=\\$(samtools view -H ${name}.primertrimmed.rg.sorted.bam | awk 'BEGIN{FS=\"\\\\t\"};{if(\\$1==\"@SQ\"){print \\$2}}' | sed 's/SN://g')\n        mv ${name}.primertrimmed.rg.sorted.bam ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        samtools index ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        \"\"\"\n        stub:\n        \"\"\"\n        touch genome.consensus.fasta \\\n            ${name}_mapped_1.primertrimmed.sorted.bam \\\n            ${name}_mapped_1.primertrimmed.sorted.bam.bai \\\n            SNP_${name}.pass.vcf \\\n            ${name}.pass.vcf.gz \\\n            ${name}.coverage_mask.txt.nCoV-2019_1.depths \\\n            ${name}.coverage_mask.txt.nCoV-2019_2.depths \\\n            ${name}.trimmed.rg.sorted.bam\n        \"\"\"\n}",
        "nb_lignes_process": 52,
        "string_script": "        \"\"\"\n        artic minion --minimap2 --normalise 500 \\\n            --threads ${task.cpus} \\\n            --scheme-directory ${external_scheme} \\\n            --read-file ${reads} \\\n            --fast5-directory ${fast5_dir} \\\n            --sequencing-summary sequencing_summary*.txt \\\n            nCoV-2019/${params.primerV} ${name}\n\n        # generate depth files\n        artic_make_depth_mask --depth ${params.min_depth} \\\n            --store-rg-depths ${external_scheme}/nCoV-2019/${params.primerV}/nCoV-2019.reference.fasta \\\n            ${name}.primertrimmed.rg.sorted.bam \\\n            ${name}.coverage_mask.txt\n\n        zcat ${name}.pass.vcf.gz > SNP_${name}.pass.vcf\n\n        sed -i \"1s/.*/>${name}/\" *.consensus.fasta\n\n        # get reference FASTA ID to rename BAM\n        REF=\\$(samtools view -H ${name}.primertrimmed.rg.sorted.bam | awk 'BEGIN{FS=\"\\\\t\"};{if(\\$1==\"@SQ\"){print \\$2}}' | sed 's/SN://g')\n        mv ${name}.primertrimmed.rg.sorted.bam ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        samtools index ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        \"\"\"",
        "nb_lignes_script": 23,
        "language_script": "bash",
        "tools": [
            "ARTIC",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/artic",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "ARTIC",
                "uri": "https://bio.tools/artic",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_1383",
                                "term": "Nucleic acid sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3498",
                                "term": "Sequence variations"
                            }
                        ]
                    }
                ],
                "description": "A bioinformatics pipeline for working with virus sequencing data sequenced with nanopore",
                "homepage": "https://github.com/artic-network/fieldbioinformatics"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "name",
            "reads",
            "external_scheme",
            "fast5_dir",
            "txt_files"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'artic'",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"*.consensus.fasta\"",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}_mapped_*.primertrimmed.sorted.bam*\"",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}.trimmed.rg.sorted.bam\"",
            "publishDir \"${params.output}/${params.genomedir}/all_consensus_sequences/\", mode: 'copy', pattern: \"*.consensus.fasta\"",
            "publishDir \"${params.output}/${params.lineagedir}/${name}/\", mode: 'copy', pattern: \"SNP_${name}.pass.vcf\""
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch genome.consensus.fasta \\\n            ${name}_mapped_1.primertrimmed.sorted.bam \\\n            ${name}_mapped_1.primertrimmed.sorted.bam.bai \\\n            SNP_${name}.pass.vcf \\\n            ${name}.pass.vcf.gz \\\n            ${name}.coverage_mask.txt.nCoV-2019_1.depths \\\n            ${name}.coverage_mask.txt.nCoV-2019_2.depths \\\n            ${name}.trimmed.rg.sorted.bam\n        \"\"\""
    },
    "artic_nanopolish_custom_bed": {
        "name_process": "artic_nanopolish_custom_bed",
        "string_process": "\nprocess artic_nanopolish_custom_bed {\n        label 'artic'\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"*.consensus.fasta\"\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}_mapped_*.primertrimmed.sorted.bam*\"\n        publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}.trimmed.rg.sorted.bam\"        \n        publishDir \"${params.output}/${params.genomedir}/all_consensus_sequences/\", mode: 'copy', pattern: \"*.consensus.fasta\"\n        publishDir \"${params.output}/${params.lineagedir}/${name}/\", mode: 'copy', pattern: \"SNP_${name}.pass.vcf\"\n\n    input:\n        tuple val(name), path(reads), path(external_scheme), path(fast5_dir), path(txt_files), path(primerBed)\n    output:\n        tuple val(name), path(\"*.consensus.fasta\"), emit: fasta\n        tuple val(name), path(\"${name}_mapped_*.primertrimmed.sorted.bam\"), path(\"${name}_mapped_*.primertrimmed.sorted.bam.bai\"), emit: reference_bam\n        tuple val(name), path(\"SNP_${name}.pass.vcf\"), emit: vcf\n        tuple val(name), path(\"${name}.pass.vcf.gz\"), path(\"${name}.coverage_mask.txt.*1.depths\"), path(\"${name}.coverage_mask.txt.*2.depths\"), emit: covarplot\n        tuple val(name), path(\"${name}.trimmed.rg.sorted.bam\"), emit: fullbam\n    script:   \n        \"\"\"\n        # create a new primer dir as input for artic\n        mkdir -p primer_scheme/nCoV-2019\n        cp -r ${external_scheme}/nCoV-2019/V_custom primer_scheme/nCoV-2019/\n\n        # clean up bed file: replace first colum with MN908947.3, remove empty lines and sort by 4th column (primer names) \n        cut -f2- ${primerBed} |\\\n            sed '/^[[:space:]]*\\$/d' |\\\n            sed -e \\$'s/^/MN908947.3\\\\t/' |\\\n            sort -k4 > primer_scheme/nCoV-2019/V_custom/nCoV-2019.scheme.bed\n\n        # start artic\n        artic minion --minimap2 --normalise 500 \\\n            --threads ${task.cpus} \\\n            --scheme-directory primer_scheme \\\n            --read-file ${reads} \\\n            --fast5-directory ${fast5_dir} \\\n            --sequencing-summary sequencing_summary*.txt \\\n            nCoV-2019/V_custom ${name}\n\n        # generate depth files\n        artic_make_depth_mask --depth ${params.min_depth} \\\n            --store-rg-depths primer_scheme/nCoV-2019/V_custom/nCoV-2019.reference.fasta \\\n            ${name}.primertrimmed.rg.sorted.bam \\\n            ${name}.coverage_mask.txt\n\n        zcat ${name}.pass.vcf.gz > SNP_${name}.pass.vcf\n\n        sed -i \"1s/.*/>${name}/\" *.consensus.fasta\n\n        # get reference FASTA ID to rename BAM\n        REF=\\$(samtools view -H ${name}.primertrimmed.rg.sorted.bam | awk 'BEGIN{FS=\"\\\\t\"};{if(\\$1==\"@SQ\"){print \\$2}}' | sed 's/SN://g')\n        mv ${name}.primertrimmed.rg.sorted.bam ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        samtools index ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        \"\"\"\n        stub:\n        \"\"\"\n        touch genome.consensus.fasta \\\n            ${name}_mapped_1.primertrimmed.sorted.bam \\\n            ${name}_mapped_1.primertrimmed.sorted.bam.bai \\\n            SNP_${name}.pass.vcf \\\n            ${name}.pass.vcf.gz \\\n            ${name}.coverage_mask.txt.nCoV-2019_1.depths \\\n            ${name}.coverage_mask.txt.nCoV-2019_2.depths \\\n            ${name}.trimmed.rg.sorted.bam\n        \"\"\"\n}",
        "nb_lignes_process": 63,
        "string_script": "        \"\"\"\n        # create a new primer dir as input for artic\n        mkdir -p primer_scheme/nCoV-2019\n        cp -r ${external_scheme}/nCoV-2019/V_custom primer_scheme/nCoV-2019/\n\n        # clean up bed file: replace first colum with MN908947.3, remove empty lines and sort by 4th column (primer names) \n        cut -f2- ${primerBed} |\\\n            sed '/^[[:space:]]*\\$/d' |\\\n            sed -e \\$'s/^/MN908947.3\\\\t/' |\\\n            sort -k4 > primer_scheme/nCoV-2019/V_custom/nCoV-2019.scheme.bed\n\n        # start artic\n        artic minion --minimap2 --normalise 500 \\\n            --threads ${task.cpus} \\\n            --scheme-directory primer_scheme \\\n            --read-file ${reads} \\\n            --fast5-directory ${fast5_dir} \\\n            --sequencing-summary sequencing_summary*.txt \\\n            nCoV-2019/V_custom ${name}\n\n        # generate depth files\n        artic_make_depth_mask --depth ${params.min_depth} \\\n            --store-rg-depths primer_scheme/nCoV-2019/V_custom/nCoV-2019.reference.fasta \\\n            ${name}.primertrimmed.rg.sorted.bam \\\n            ${name}.coverage_mask.txt\n\n        zcat ${name}.pass.vcf.gz > SNP_${name}.pass.vcf\n\n        sed -i \"1s/.*/>${name}/\" *.consensus.fasta\n\n        # get reference FASTA ID to rename BAM\n        REF=\\$(samtools view -H ${name}.primertrimmed.rg.sorted.bam | awk 'BEGIN{FS=\"\\\\t\"};{if(\\$1==\"@SQ\"){print \\$2}}' | sed 's/SN://g')\n        mv ${name}.primertrimmed.rg.sorted.bam ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        samtools index ${name}_mapped_\\${REF}.primertrimmed.sorted.bam\n        \"\"\"",
        "nb_lignes_script": 34,
        "language_script": "bash",
        "tools": [
            "ARTIC",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/artic",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "ARTIC",
                "uri": "https://bio.tools/artic",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_1383",
                                "term": "Nucleic acid sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3498",
                                "term": "Sequence variations"
                            }
                        ]
                    }
                ],
                "description": "A bioinformatics pipeline for working with virus sequencing data sequenced with nanopore",
                "homepage": "https://github.com/artic-network/fieldbioinformatics"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "name",
            "reads",
            "external_scheme",
            "fast5_dir",
            "txt_files",
            "primerBed"
        ],
        "nb_inputs": 6,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'artic'",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"*.consensus.fasta\"",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}_mapped_*.primertrimmed.sorted.bam*\"",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"${name}.trimmed.rg.sorted.bam\"",
            "publishDir \"${params.output}/${params.genomedir}/all_consensus_sequences/\", mode: 'copy', pattern: \"*.consensus.fasta\"",
            "publishDir \"${params.output}/${params.lineagedir}/${name}/\", mode: 'copy', pattern: \"SNP_${name}.pass.vcf\""
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch genome.consensus.fasta \\\n            ${name}_mapped_1.primertrimmed.sorted.bam \\\n            ${name}_mapped_1.primertrimmed.sorted.bam.bai \\\n            SNP_${name}.pass.vcf \\\n            ${name}.pass.vcf.gz \\\n            ${name}.coverage_mask.txt.nCoV-2019_1.depths \\\n            ${name}.coverage_mask.txt.nCoV-2019_2.depths \\\n            ${name}.trimmed.rg.sorted.bam\n        \"\"\""
    },
    "rki_report": {
        "name_process": "rki_report",
        "string_process": "process rki_report {\n    label \"ubuntu\"\n    publishDir \"${params.output}/${params.rkidir}/valid\", mode: 'copy', pattern: \"rki_valid_report.csv\"\n    publishDir \"${params.output}/${params.rkidir}\", mode: 'copy', pattern: \"${readme}\"\n    input:\n        path(president_data)\n        path(readme)\n    output:\n        path(\"rki_valid_report.csv\"), emit: report\n        path(\"${readme}\"), emit: readme\n    script:\n        \"\"\"\n        cat ${president_data} | sed '/(ignoring Ns)/d' | awk -F'\\\\t' '{print \\$1 \"\\\\t\" \\$8}' | sed '/\\\\False\\\\b/d' >> all.csv\n\n        if [ -s all.csv ]; then\n            rki_report_parser.sh all.csv rki_valid_report.csv\n        else\n            touch rki_valid_report.csv\n        fi\n        \"\"\"\n    stub:\n        \"\"\"\n        touch rki_valid_report.csv ${readme}\n        \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "        \"\"\"\n        cat ${president_data} | sed '/(ignoring Ns)/d' | awk -F'\\\\t' '{print \\$1 \"\\\\t\" \\$8}' | sed '/\\\\False\\\\b/d' >> all.csv\n\n        if [ -s all.csv ]; then\n            rki_report_parser.sh all.csv rki_valid_report.csv\n        else\n            touch rki_valid_report.csv\n        fi\n        \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "president_data",
            "readme"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label \"ubuntu\"",
            "publishDir \"${params.output}/${params.rkidir}/valid\", mode: 'copy', pattern: \"rki_valid_report.csv\"",
            "publishDir \"${params.output}/${params.rkidir}\", mode: 'copy', pattern: \"${readme}\""
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch rki_valid_report.csv ${readme}\n        \"\"\""
    },
    "rki_report_extended": {
        "name_process": "rki_report_extended",
        "string_process": "\nprocess rki_report_extended {\n    label \"ubuntu\"\n    publishDir \"${params.output}/${params.rkidir}/valid\", mode: 'copy', pattern: \"rki_valid_report.csv\"\n    publishDir \"${params.output}/${params.rkidir}\", mode: 'copy', pattern: \"${readme}\"\n    input:\n        path(president_data)\n        path(readme)\n        path(extended_table)\n    output:\n        path(\"rki_valid_report.csv\"), emit: report\n        path(\"${readme}\"), emit: readme\n    script:\n        \"\"\"\n        cat ${president_data} | sed '/(ignoring Ns)/d' | awk -F'\\\\t' '{print \\$1 \"\\\\t\" \\$8}' | sed '/\\\\False\\\\b/d' >> all.csv\n\n        if [ -s all.csv ]; then\n            rki_report_parser_extended.sh all.csv rki_valid_report.csv ${extended_table}\n        else\n            touch rki_valid_report.csv\n        fi\n        \"\"\"\n    stub:\n        \"\"\"\n        touch rki_valid_report.csv ${readme}\n        \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "        \"\"\"\n        cat ${president_data} | sed '/(ignoring Ns)/d' | awk -F'\\\\t' '{print \\$1 \"\\\\t\" \\$8}' | sed '/\\\\False\\\\b/d' >> all.csv\n\n        if [ -s all.csv ]; then\n            rki_report_parser_extended.sh all.csv rki_valid_report.csv ${extended_table}\n        else\n            touch rki_valid_report.csv\n        fi\n        \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "president_data",
            "readme",
            "extended_table"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label \"ubuntu\"",
            "publishDir \"${params.output}/${params.rkidir}/valid\", mode: 'copy', pattern: \"rki_valid_report.csv\"",
            "publishDir \"${params.output}/${params.rkidir}\", mode: 'copy', pattern: \"${readme}\""
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch rki_valid_report.csv ${readme}\n        \"\"\""
    },
    "get_fasta": {
        "name_process": "get_fasta",
        "string_process": "\nprocess get_fasta {\n  \tcontainer = 'nanozoo/template:3.8--d089809'\n\toutput:\n\tpath(\"SARSCoV2.fasta\") \n\tscript:\n\t\"\"\"\n    wget --no-check-certificate https://osf.io/87bc9/download -O SARSCoV2.fasta\n\t\"\"\"\n}",
        "nb_lignes_process": 8,
        "string_script": "\t\"\"\"\n    wget --no-check-certificate https://osf.io/87bc9/download -O SARSCoV2.fasta\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "container = 'nanozoo/template:3.8--d089809'"
        ],
        "when": "",
        "stub": ""
    },
    "plot_coverages": {
        "name_process": "plot_coverages",
        "string_process": "\nprocess plot_coverages {\n  \tlabel 'fastcov'\n    input:\n        path(alignment_files)\n\t\tpath(index_files)\n\toutput:\n\t    path(\"coverages_*.png\")\n\tshell:\n\t'''\n    fastcov.py -l -o coverages_$(echo !{alignment_files} | tr ' ' '_').png !{alignment_files}\n\t'''\n\tstub:\n\t\"\"\"\n\ttouch coverages_1.png\n\t\"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "\t'''\n    fastcov.py -l -o coverages_$(echo !{alignment_files} | tr ' ' '_').png !{alignment_files}\n\t'''",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "alignment_files",
            "index_files"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'fastcov'"
        ],
        "when": "",
        "stub": "\n\t\"\"\"\n\ttouch coverages_1.png\n\t\"\"\""
    },
    "get_fast5": {
        "name_process": "get_fast5",
        "string_process": "\nprocess get_fast5 {\n  \tcontainer = 'nanozoo/template:3.8--d089809'\n\toutput:\n\tpath(\"fast5\") \n\tscript:\n\t\"\"\"\n    wget --no-check-certificate https://osf.io/vxd7f/download -O SARSCoV2.tar.gz\n    tar xzf SARSCoV2.tar.gz \n\t\"\"\"\n}",
        "nb_lignes_process": 9,
        "string_script": "\t\"\"\"\n    wget --no-check-certificate https://osf.io/vxd7f/download -O SARSCoV2.tar.gz\n    tar xzf SARSCoV2.tar.gz \n\t\"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "container = 'nanozoo/template:3.8--d089809'"
        ],
        "when": "",
        "stub": ""
    },
    "coverage_plot": {
        "name_process": "coverage_plot",
        "string_process": "process coverage_plot {\n    label \"ggplot2\"\n    publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy'\n    input:\n        tuple val(name), path(coverage_info)\n    output:\n        tuple val(name), path(\"${name}_coverage.pdf\"), path(\"${name}_coverage.svg\")\n    script:\n        \"\"\"\n        coverage_plot.R\n        mv overview.pdf ${name}_coverage.pdf \n        mv overview.svg ${name}_coverage.svg \n        \"\"\"\n}",
        "nb_lignes_process": 12,
        "string_script": "        \"\"\"\n        coverage_plot.R\n        mv overview.pdf ${name}_coverage.pdf \n        mv overview.svg ${name}_coverage.svg \n        \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "coverage_info"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label \"ggplot2\"",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "get_nanopore_fastq": {
        "name_process": "get_nanopore_fastq",
        "string_process": "process get_nanopore_fastq {\n  \tcontainer = 'nanozoo/template:3.8--d089809'\n\toutput:\n\tpath(\"SARSCoV2.fastq.gz\") \n\tscript:\n\t\"\"\"\n    wget --no-check-certificate https://osf.io/kf54a/download -O SARSCoV2.fastq.gz\n\t\"\"\"\n}",
        "nb_lignes_process": 7,
        "string_script": "\t\"\"\"\n    wget --no-check-certificate https://osf.io/kf54a/download -O SARSCoV2.fastq.gz\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "container = 'nanozoo/template:3.8--d089809'"
        ],
        "when": "",
        "stub": ""
    },
    "pycoqc": {
        "name_process": "pycoqc",
        "string_process": "process pycoqc {\n        label 'pycoqc'  \n        publishDir \"${params.output}/${params.readqcdir}/\", mode: 'copy', pattern: \"${name}_sequencing_performance.html\"\n    input:\n        tuple val(name), path(txt_files)\n    output:\n        tuple val(name), path(\"${name}_sequencing_performance.html\") \n    script:\n    if (params.single)\n        \"\"\"\n        pycoQC -f sequencing_summary.txt -o ${name}_sequencing_performance.html \n        \"\"\"\n    else\n        \"\"\"\n        pycoQC -f sequencing_summary.txt -b barcoding_summary.txt -o ${name}_sequencing_performance.html         \n        \"\"\"\n    stub:\n        \"\"\"\n        touch ${name}_sequencing_performance.html\n        \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "    if (params.single)\n        \"\"\"\n        pycoQC -f sequencing_summary.txt -o ${name}_sequencing_performance.html \n        \"\"\"\n    else\n        \"\"\"\n        pycoQC -f sequencing_summary.txt -b barcoding_summary.txt -o ${name}_sequencing_performance.html         \n        \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "txt_files"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'pycoqc'",
            "publishDir \"${params.output}/${params.readqcdir}/\", mode: 'copy', pattern: \"${name}_sequencing_performance.html\""
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch ${name}_sequencing_performance.html\n        \"\"\""
    },
    "lcs_ucsc_markers_table": {
        "name_process": "lcs_ucsc_markers_table",
        "string_process": "process lcs_ucsc_markers_table {\n    label 'lcs_sc2'\n\n    input:\n    path(variant_group_tsv)\n\n    output:\n    path(\"LCS/outputs/variants_table/ucsc-markers-table-*.tsv\")\n\n    script:\n    if ( params.lcs_ucsc_update || params.lcs_ucsc_version != 'predefined')\n        \"\"\"\n        git clone https://github.com/rki-mf1/LCS.git\n\n        if [[ \"${variant_group_tsv}\" != default ]]; then\n            rm -rf LCS/data/variant_groups.tsv\n            cp ${variant_group_tsv} LCS/data/variant_groups.tsv\n        fi\n\n        cd LCS\n        ## change settings\n        sed -i \"s/PB_VERSION=.*/PB_VERSION='${params.lcs_ucsc}'/\" rules/config.py\n        sed -i \"s/NUM_SAMPLE=.*/NUM_SAMPLE=${params.lcs_ucsc_downsampling}/\" rules/config.py\n        mem=\\$(echo ${task.memory} | cut -d' ' -f1)\n        ## run pipeline\n        snakemake --cores ${task.cpus} --resources mem_gb=\\$mem --config dataset=somestring markers=ucsc -- ucsc_gather_tables\n        ## output\n        mv outputs/variants_table/ucsc-markers-table.tsv outputs/variants_table/ucsc-markers-table-${params.lcs_ucsc}.tsv \n        \"\"\"\n    else if ( params.lcs_ucsc_version == 'predefined' )\n        \"\"\"\n        git clone https://github.com/rki-mf1/LCS.git\n        mkdir -p LCS/outputs/variants_table\n        zcat LCS/data/pre-generated-marker-tables/ucsc-markers-${params.lcs_ucsc_predefined}.tsv.gz > LCS/outputs/variants_table/ucsc-markers-table.tsv\n        mv LCS/outputs/variants_table/ucsc-markers-table.tsv LCS/outputs/variants_table/ucsc-markers-table-predefined.tsv \n        \"\"\"\n    stub:\n    \"\"\"\n    mkdir -p LCS/outputs/variants_table/\n    touch LCS/outputs/variants_table/ucsc-markers-table-42.tsv\n    \"\"\"\n}",
        "nb_lignes_process": 40,
        "string_script": "    if ( params.lcs_ucsc_update || params.lcs_ucsc_version != 'predefined')\n        \"\"\"\n        git clone https://github.com/rki-mf1/LCS.git\n\n        if [[ \"${variant_group_tsv}\" != default ]]; then\n            rm -rf LCS/data/variant_groups.tsv\n            cp ${variant_group_tsv} LCS/data/variant_groups.tsv\n        fi\n\n        cd LCS\n        ## change settings\n        sed -i \"s/PB_VERSION=.*/PB_VERSION='${params.lcs_ucsc}'/\" rules/config.py\n        sed -i \"s/NUM_SAMPLE=.*/NUM_SAMPLE=${params.lcs_ucsc_downsampling}/\" rules/config.py\n        mem=\\$(echo ${task.memory} | cut -d' ' -f1)\n        ## run pipeline\n        snakemake --cores ${task.cpus} --resources mem_gb=\\$mem --config dataset=somestring markers=ucsc -- ucsc_gather_tables\n        ## output\n        mv outputs/variants_table/ucsc-markers-table.tsv outputs/variants_table/ucsc-markers-table-${params.lcs_ucsc}.tsv \n        \"\"\"\n    else if ( params.lcs_ucsc_version == 'predefined' )\n        \"\"\"\n        git clone https://github.com/rki-mf1/LCS.git\n        mkdir -p LCS/outputs/variants_table\n        zcat LCS/data/pre-generated-marker-tables/ucsc-markers-${params.lcs_ucsc_predefined}.tsv.gz > LCS/outputs/variants_table/ucsc-markers-table.tsv\n        mv LCS/outputs/variants_table/ucsc-markers-table.tsv LCS/outputs/variants_table/ucsc-markers-table-predefined.tsv \n        \"\"\"",
        "nb_lignes_script": 25,
        "language_script": "bash",
        "tools": [
            "Snakemake"
        ],
        "tools_url": [
            "https://bio.tools/snakemake"
        ],
        "tools_dico": [
            {
                "name": "Snakemake",
                "uri": "https://bio.tools/snakemake",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3372",
                            "term": "Software engineering"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0091",
                            "term": "Bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Workflows"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3372",
                            "term": "Computer programming"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3372",
                            "term": "Software development"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Pipelines"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3762",
                                    "term": "Service composition"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Workflow engine and language. It aims to reduce the complexity of creating workflows by providing a fast and comfortable execution environment, together with a clean and modern domain specific specification language (DSL) in python style.",
                "homepage": "https://snakemake.readthedocs.io/en/stable/index.html"
            }
        ],
        "inputs": [
            "variant_group_tsv"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'lcs_sc2'"
        ],
        "when": "",
        "stub": "\n    \"\"\"\n    mkdir -p LCS/outputs/variants_table/\n    touch LCS/outputs/variants_table/ucsc-markers-table-42.tsv\n    \"\"\""
    },
    "lcs_sc2": {
        "name_process": "lcs_sc2",
        "string_process": "\nprocess lcs_sc2 {\n    label 'lcs_sc2'\n    publishDir \"${params.output}/${params.lineagedir}/${name}/lineage-proportion-by-reads\", mode: 'copy'\n    input:\n    tuple val(name), path(reads), path(ucsc_markers_table)\n  \toutput:\n    tuple val(name), path(\"${name}.lcs.tsv\")\n  \tscript:\n    \"\"\"\n    git clone https://github.com/rki-mf1/LCS.git\n\n    mkdir -p LCS/outputs/variants_table\n    mv ${ucsc_markers_table} LCS/outputs/variants_table/ucsc-markers-table.tsv  \n\n    mkdir -p LCS/data/fastq\n    cp ${reads} LCS/data/fastq/\n\n    BN=\\$(basename ${reads} .fastq.gz)\n    echo \\$BN >> LCS/data/tags_pool_mypool\n\n    cd LCS\n    snakemake --config markers=ucsc dataset=mypool --cores ${task.cpus} --resources mem_gb=8000 --set-threads pool_mutect=${task.cpus}\n    cd ..\n\n    cp LCS/outputs/decompose/mypool.out ${name}.lcs.tsv\n    rm -rf LCS/data/fastq\n    \"\"\"\n    stub:\n    \"\"\"\n    touch ${name}.lcs.tsv\n    \"\"\"\n  }",
        "nb_lignes_process": 31,
        "string_script": "    \"\"\"\n    git clone https://github.com/rki-mf1/LCS.git\n\n    mkdir -p LCS/outputs/variants_table\n    mv ${ucsc_markers_table} LCS/outputs/variants_table/ucsc-markers-table.tsv  \n\n    mkdir -p LCS/data/fastq\n    cp ${reads} LCS/data/fastq/\n\n    BN=\\$(basename ${reads} .fastq.gz)\n    echo \\$BN >> LCS/data/tags_pool_mypool\n\n    cd LCS\n    snakemake --config markers=ucsc dataset=mypool --cores ${task.cpus} --resources mem_gb=8000 --set-threads pool_mutect=${task.cpus}\n    cd ..\n\n    cp LCS/outputs/decompose/mypool.out ${name}.lcs.tsv\n    rm -rf LCS/data/fastq\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [
            "Snakemake"
        ],
        "tools_url": [
            "https://bio.tools/snakemake"
        ],
        "tools_dico": [
            {
                "name": "Snakemake",
                "uri": "https://bio.tools/snakemake",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3372",
                            "term": "Software engineering"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0091",
                            "term": "Bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Workflows"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3372",
                            "term": "Computer programming"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3372",
                            "term": "Software development"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Pipelines"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3762",
                                    "term": "Service composition"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Workflow engine and language. It aims to reduce the complexity of creating workflows by providing a fast and comfortable execution environment, together with a clean and modern domain specific specification language (DSL) in python style.",
                "homepage": "https://snakemake.readthedocs.io/en/stable/index.html"
            }
        ],
        "inputs": [
            "name",
            "reads",
            "ucsc_markers_table"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'lcs_sc2'",
            "publishDir \"${params.output}/${params.lineagedir}/${name}/lineage-proportion-by-reads\", mode: 'copy'"
        ],
        "when": "",
        "stub": "\n    \"\"\"\n    touch ${name}.lcs.tsv\n    \"\"\""
    },
    "lcs_plot": {
        "name_process": "lcs_plot",
        "string_process": "\nprocess lcs_plot {\n  label \"ggplot2\"\n  publishDir \"${params.output}/${params.lineagedir}/\", mode: 'copy'\n\n  input:\n  path(tsv)\n  val(cutoff)\n  \n  output:\n  path(\"*.png\")\n  \n  script:\n  \"\"\"\n  lcs_bar_plot.R '${tsv}' ${cutoff}\n  \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "  \"\"\"\n  lcs_bar_plot.R '${tsv}' ${cutoff}\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "tsv",
            "cutoff"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label \"ggplot2\"",
            "publishDir \"${params.output}/${params.lineagedir}/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "guppy_gpu": {
        "name_process": "guppy_gpu",
        "string_process": "process guppy_gpu {\n    label 'guppy_gpu'\n        if (!params.localguppy) {\n\t    if (workflow.profile.contains('slurm')) {\n\t\tclusterOptions = '--gpus=1 --time=06:00:00'\n\t    }\n            if (workflow.profile.contains('docker')) {\n                container = 'nanozoo/guppy_gpu:5.0.7-1--ec2c6e7'\n                containerOptions '--gpus all'\n            }\n            else if (workflow.profile.contains('singularity')) {\n                container = 'nanozoo/guppy_gpu:5.0.7-1--ec2c6e7'\n                containerOptions '--nv'\n            }\n            else if (workflow.profile.contains('ukj_cloud') || workflow.profile.contains('nanozoo')) {\n            accelerator 2, type: 'nvidia-tesla-p100'\n            container = 'nanozoo/guppy_gpu:5.0.7-1--ec2c6e7'\n            containerOptions '--gpus all'\n            }\n        }\n        else if (params.localguppy) {\n            if (workflow.profile.contains('ukj_cloud') || workflow.profile.contains('nanozoo')) {\n                executor = \"local\"\n                container = 'nanozoo/guppy_gpu:5.0.7-1--ec2c6e7'\n                containerOptions '--gpus all'\n            }\n            else {\n                executor = \"local\"\n            }\n        }\n\n        errorStrategy { if ( task.exitStatus == 127) { 'retry' ; exit 1, \"Could not find the guppy basecaller\"  }\n                    else if (task.exitStatus == 255) { 'retry' ; exit 1, \"nvidia docker toolkit not installed (correctly)?\" }\n                    else if (task.exitStatus == 125) { 'retry' ; exit 1, \"nvidia cuda driver not found\" } }\n\n        publishDir \"${params.output}/${params.readsdir}/\", mode: 'copy'\n    input:\n        tuple val(name), path(dir)\n    output:\n        tuple val(name), path(\"*.fastq.gz\"), emit: reads\n        tuple val(name), path(\"fastq_tmp/*.txt\"), emit: summary\n    script:       \n        if (params.rapid) {\n            guppy_arrangement_files = 'barcode_arrs_rbk4.cfg barcode_arrs_rbk096.cfg'\n            barcoding_option = '  '\n            }\n        else {\n            guppy_arrangement_files = 'barcode_arrs_nb12.cfg barcode_arrs_nb24.cfg barcode_arrs_nb96.cfg'\n            barcoding_option = '--require_barcodes_both_ends'\n            }\n        if (params.one_end) {\n            barcoding_option = '  '\n            }\n        if (params.single)\n        \"\"\"\n        guppy_basecaller -c ${params.guppy_model} -i ${dir} -s fastq -x auto -r --trim_strategy dna -q 0 --disable_pings\n\n        find -L fastq -name '*.fastq' -exec cat {} +  | gzip > ${name}.fastq.gz\n        \n        mkdir -p fastq_tmp/\n        cp fastq/*.txt fastq_tmp\n        \"\"\"\n        else\n        \"\"\"\n        guppy_basecaller -c ${params.guppy_model} -i ${dir} -s fastq_tmp -x auto -r --disable_pings\n        guppy_barcoder -t ${task.cpus} -r ${barcoding_option} -i fastq_tmp -s fastq --detect_mid_strand_barcodes --min_score_mid_barcodes 50 --arrangements_files \"${guppy_arrangement_files}\" --disable_pings\n\n        for barcodes in fastq/barcode??; do\n            find -L \\${barcodes} -name '*.fastq' -exec cat {} + | gzip > \\${barcodes##*/}.fastq.gz\n        done\n\n        cp fastq/*.txt fastq_tmp\n        \"\"\"\n    stub:\n        \"\"\"\n        touch ${name}.fastq.gz\n        mkdir -p fastq_tmp/\n        touch fastq_tmp/sequencesummary.txt\n        \"\"\"\n\n}",
        "nb_lignes_process": 79,
        "string_script": "        if (params.rapid) {\n            guppy_arrangement_files = 'barcode_arrs_rbk4.cfg barcode_arrs_rbk096.cfg'\n            barcoding_option = '  '\n            }\n        else {\n            guppy_arrangement_files = 'barcode_arrs_nb12.cfg barcode_arrs_nb24.cfg barcode_arrs_nb96.cfg'\n            barcoding_option = '--require_barcodes_both_ends'\n            }\n        if (params.one_end) {\n            barcoding_option = '  '\n            }\n        if (params.single)\n        \"\"\"\n        guppy_basecaller -c ${params.guppy_model} -i ${dir} -s fastq -x auto -r --trim_strategy dna -q 0 --disable_pings\n\n        find -L fastq -name '*.fastq' -exec cat {} +  | gzip > ${name}.fastq.gz\n        \n        mkdir -p fastq_tmp/\n        cp fastq/*.txt fastq_tmp\n        \"\"\"\n        else\n        \"\"\"\n        guppy_basecaller -c ${params.guppy_model} -i ${dir} -s fastq_tmp -x auto -r --disable_pings\n        guppy_barcoder -t ${task.cpus} -r ${barcoding_option} -i fastq_tmp -s fastq --detect_mid_strand_barcodes --min_score_mid_barcodes 50 --arrangements_files \"${guppy_arrangement_files}\" --disable_pings\n\n        for barcodes in fastq/barcode??; do\n            find -L \\${barcodes} -name '*.fastq' -exec cat {} + | gzip > \\${barcodes##*/}.fastq.gz\n        done\n\n        cp fastq/*.txt fastq_tmp\n        \"\"\"",
        "nb_lignes_script": 30,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "dir"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'guppy_gpu' if (!params.localguppy) { if (workflow.profile.contains('slurm')) { clusterOptions = '--gpus=1 --time=06:00:00' } if (workflow.profile.contains('docker')) { container = 'nanozoo/guppy_gpu:5.0.7-1--ec2c6e7' containerOptions '--gpus all' } else if (workflow.profile.contains('singularity')) { container = 'nanozoo/guppy_gpu:5.0.7-1--ec2c6e7' containerOptions '--nv' } else if (workflow.profile.contains('ukj_cloud') || workflow.profile.contains('nanozoo')) { accelerator 2, type: 'nvidia-tesla-p100' container = 'nanozoo/guppy_gpu:5.0.7-1--ec2c6e7' containerOptions '--gpus all' } } else if (params.localguppy) { if (workflow.profile.contains('ukj_cloud') || workflow.profile.contains('nanozoo')) { executor = \"local\" container = 'nanozoo/guppy_gpu:5.0.7-1--ec2c6e7' containerOptions '--gpus all' } else { executor = \"local\" } }",
            "errorStrategy { if ( task.exitStatus == 127) { 'retry' ; exit 1, \"Could not find the guppy basecaller\" } else if (task.exitStatus == 255) { 'retry' ; exit 1, \"nvidia docker toolkit not installed (correctly)?\" } else if (task.exitStatus == 125) { 'retry' ; exit 1, \"nvidia cuda driver not found\" } }",
            "publishDir \"${params.output}/${params.readsdir}/\", mode: 'copy'"
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch ${name}.fastq.gz\n        mkdir -p fastq_tmp/\n        touch fastq_tmp/sequencesummary.txt\n        \"\"\""
    },
    "guppy_cpu": {
        "name_process": "guppy_cpu",
        "string_process": "\nprocess guppy_cpu {\n        label 'guppy_cpu'\n        if (!params.localguppy && workflow.profile.contains('docker') || workflow.profile.contains('singularity') ) {\n            container = 'nanozoo/guppy_cpu:5.0.7-1--47b84be'\n        }\n        publishDir \"${params.output}/${params.readsdir}/\", mode: 'copy'\n    input:\n        tuple val(name), path(dir)\n    output:\n        tuple val(name), path(\"*.fastq.gz\"), emit: reads\n        tuple val(name), path(\"fastq_tmp/*.txt\"), emit: summary\n    script:\n        if (params.rapid) {\n            guppy_arrangement_files = 'barcode_arrs_rbk4.cfg barcode_arrs_rbk096.cfg'\n            barcoding_option = '  '\n            }\n        else {\n            guppy_arrangement_files = 'barcode_arrs_nb12.cfg barcode_arrs_nb24.cfg barcode_arrs_nb96.cfg'\n            barcoding_option = '--require_barcodes_both_ends'\n            }\n        if (params.one_end) {\n            barcoding_option = '  '\n            }\n        if (params.single)\n        \"\"\"\n        guppy_basecaller -c ${params.guppy_model} -i ${dir} -s fastq  --num_callers ${task.cpus} --cpu_threads_per_caller 1 -r --trim_strategy dna -q 0 --disable_pings\n\n        find -L fastq -name '*.fastq' -exec cat {} +  | gzip > ${name}.fastq.gz\n        \n        mkdir -p fastq_tmp/\n        cp fastq/*.txt fastq_tmp\n        \"\"\"\n        else\n        \"\"\"\n        guppy_basecaller -c ${params.guppy_model} -i ${dir} -s fastq_tmp  --num_callers ${task.cpus} --cpu_threads_per_caller 1 -r --disable_pings\n        guppy_barcoder -t ${task.cpus} -r ${barcoding_option} -i fastq_tmp -s fastq  --detect_mid_strand_barcodes --min_score_mid_barcodes 50 --arrangements_files \"${guppy_arrangement_files}\" --disable_pings\n\n        for barcodes in fastq/barcode??; do\n            find -L \\${barcodes} -name '*.fastq' -exec cat {} + | gzip > \\${barcodes##*/}.fastq.gz\n        done\n\n        cp fastq/*.txt fastq_tmp\n        \"\"\"\n    stub:\n        \"\"\"\n        touch ${name}.fastq.gz\n        mkdir -p fastq_tmp/\n        touch fastq_tmp/sequencesummary.txt\n        \"\"\"\n}",
        "nb_lignes_process": 49,
        "string_script": "        if (params.rapid) {\n            guppy_arrangement_files = 'barcode_arrs_rbk4.cfg barcode_arrs_rbk096.cfg'\n            barcoding_option = '  '\n            }\n        else {\n            guppy_arrangement_files = 'barcode_arrs_nb12.cfg barcode_arrs_nb24.cfg barcode_arrs_nb96.cfg'\n            barcoding_option = '--require_barcodes_both_ends'\n            }\n        if (params.one_end) {\n            barcoding_option = '  '\n            }\n        if (params.single)\n        \"\"\"\n        guppy_basecaller -c ${params.guppy_model} -i ${dir} -s fastq  --num_callers ${task.cpus} --cpu_threads_per_caller 1 -r --trim_strategy dna -q 0 --disable_pings\n\n        find -L fastq -name '*.fastq' -exec cat {} +  | gzip > ${name}.fastq.gz\n        \n        mkdir -p fastq_tmp/\n        cp fastq/*.txt fastq_tmp\n        \"\"\"\n        else\n        \"\"\"\n        guppy_basecaller -c ${params.guppy_model} -i ${dir} -s fastq_tmp  --num_callers ${task.cpus} --cpu_threads_per_caller 1 -r --disable_pings\n        guppy_barcoder -t ${task.cpus} -r ${barcoding_option} -i fastq_tmp -s fastq  --detect_mid_strand_barcodes --min_score_mid_barcodes 50 --arrangements_files \"${guppy_arrangement_files}\" --disable_pings\n\n        for barcodes in fastq/barcode??; do\n            find -L \\${barcodes} -name '*.fastq' -exec cat {} + | gzip > \\${barcodes##*/}.fastq.gz\n        done\n\n        cp fastq/*.txt fastq_tmp\n        \"\"\"",
        "nb_lignes_script": 30,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "dir"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'guppy_cpu' if (!params.localguppy && workflow.profile.contains('docker') || workflow.profile.contains('singularity') ) { container = 'nanozoo/guppy_cpu:5.0.7-1--47b84be' }",
            "publishDir \"${params.output}/${params.readsdir}/\", mode: 'copy'"
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch ${name}.fastq.gz\n        mkdir -p fastq_tmp/\n        touch fastq_tmp/sequencesummary.txt\n        \"\"\""
    },
    "filter_fastq_by_length": {
        "name_process": "filter_fastq_by_length",
        "string_process": "process filter_fastq_by_length {\n        label 'ubuntu'\n        publishDir \"${params.output}/${params.readsdir}/filtered_reads/\", mode: 'copy', pattern: \"${name}_filtered.fastq.gz\"\n    input:\n        tuple val(name), path(reads) \n    output:\n\t    tuple val(name), path(\"${name}_filtered.fastq.gz\") optional true\n    script:\n    read_min_length = params.minLength\n    read_max_length = params.maxLength\n\n    if ( params.primerV.matches('V1200')) {\n        if ( !params.minLength ) { read_min_length = 100 }\n        if ( !params.maxLength ) { read_max_length = 1500 }\n    }\n    else {\n        if ( !params.minLength ) { read_min_length = 100 }\n        if ( !params.maxLength ) { read_max_length = 700 }\n    }\n    \n                                                                                              \n        \"\"\"\n        case \"${reads}\" in\n            *.fastq.gz ) \n                zcat ${reads} | paste - - - - | awk -F\"\\\\t\" 'length(\\$2)  >= ${read_min_length}' |\\\n                    awk -F\"\\\\t\" 'length(\\$2)  <= ${read_max_length}' | sed 's/\\\\t/\\\\n/g' | gzip > \"${name}_filtered.fastq.gz\"\n            ;;\n            *.fastq)\n                cat ${reads} | paste - - - - | awk -F\"\\\\t\" 'length(\\$2)  >= ${read_min_length}' |\\\n                awk -F\"\\\\t\" 'length(\\$2)  <= ${read_max_length}' | sed 's/\\\\t/\\\\n/g' | gzip > \"${name}_filtered.fastq.gz\"\n            ;;\n        esac\n        \n        if [ ${params.samples} == false ]; then\n            find . -name \"${name}_filtered.fastq.gz\" -type 'f' -size -1500k -delete\n        fi\n        \"\"\"\n    stub:\n        \"\"\"\n        touch ${name}_filtered.fastq.gz\n        \"\"\"    \n}",
        "nb_lignes_process": 40,
        "string_script": "    read_min_length = params.minLength\n    read_max_length = params.maxLength\n\n    if ( params.primerV.matches('V1200')) {\n        if ( !params.minLength ) { read_min_length = 100 }\n        if ( !params.maxLength ) { read_max_length = 1500 }\n    }\n    else {\n        if ( !params.minLength ) { read_min_length = 100 }\n        if ( !params.maxLength ) { read_max_length = 700 }\n    }\n    \n                                                                                              \n        \"\"\"\n        case \"${reads}\" in\n            *.fastq.gz ) \n                zcat ${reads} | paste - - - - | awk -F\"\\\\t\" 'length(\\$2)  >= ${read_min_length}' |\\\n                    awk -F\"\\\\t\" 'length(\\$2)  <= ${read_max_length}' | sed 's/\\\\t/\\\\n/g' | gzip > \"${name}_filtered.fastq.gz\"\n            ;;\n            *.fastq)\n                cat ${reads} | paste - - - - | awk -F\"\\\\t\" 'length(\\$2)  >= ${read_min_length}' |\\\n                awk -F\"\\\\t\" 'length(\\$2)  <= ${read_max_length}' | sed 's/\\\\t/\\\\n/g' | gzip > \"${name}_filtered.fastq.gz\"\n            ;;\n        esac\n        \n        if [ ${params.samples} == false ]; then\n            find . -name \"${name}_filtered.fastq.gz\" -type 'f' -size -1500k -delete\n        fi\n        \"\"\"",
        "nb_lignes_script": 28,
        "language_script": "bash",
        "tools": [
            "CASE"
        ],
        "tools_url": [
            "https://bio.tools/CASE"
        ],
        "tools_dico": [
            {
                "name": "CASE",
                "uri": "https://bio.tools/CASE",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0749",
                            "term": "Transcription factors and regulatory sites"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0089",
                            "term": "Ontology and terminology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3436",
                                    "term": "Aggregation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3435",
                                    "term": "Standardisation and normalisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3559",
                                    "term": "Ontology visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3559",
                                    "term": "Ontology browsing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Advancing Coordinated Cyber-investigations and Tool Interoperability using a Community Developed Specification Language.\n\nSource files for the CASE website.\n\nAPI used for instantiating CASE objects (includes ontological verification and type checking).\n\nCyber-investigation Analysis Standard Expression (CASE).\n\nRead the CASE Wiki tab to learn everything you need to know about the Cyber-investigation Analysis Standard Expression (CASE) ontology. For learning about the Unified Cyber Ontology, CASE's parent, see UCO.\n\n\"@vocab\": \"http://case.example.org/core#\",.\n\nDET ER DINE PENGER DET DREIER SEG OM...\n\nVi er ikke st\ufffdrst, men garanterer effektiv behandling.\n\nLast ned v\ufffdr brosjyre i PDF format.\n\n||| COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/pymzml (GITHUB.COM).\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'UCO', 'cyber-investigation', 'cyber-investigations', 'plaso'",
                "homepage": "http://CASE.as"
            }
        ],
        "inputs": [
            "name",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'ubuntu'",
            "publishDir \"${params.output}/${params.readsdir}/filtered_reads/\", mode: 'copy', pattern: \"${name}_filtered.fastq.gz\""
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch ${name}_filtered.fastq.gz\n        \"\"\""
    },
    "collect_fastq": {
        "name_process": "collect_fastq",
        "string_process": "process collect_fastq {\n        label 'demultiplex'\n    input:\n        tuple val(name), path(dir)\n    output:\n        tuple val(name), path(\"*.fastq.gz\"), emit: reads\n    script:\n        if (params.rapid) {\n            guppy_arrangement_files = 'barcode_arrs_rbk4.cfg barcode_arrs_rbk096.cfg'\n            barcoding_option = '  '\n            }\n        else {\n            guppy_arrangement_files = 'barcode_arrs_nb12.cfg barcode_arrs_nb24.cfg barcode_arrs_nb96.cfg'\n            barcoding_option = '--require_barcodes_both_ends'\n            }\n        if (params.one_end) {\n            barcoding_option = '  '\n            }\n        if (params.single)\n        \"\"\"\n        find -L ${dir} -name '*.fastq' -exec cat {} +  | gzip > ${name}.fastq.gz\n        find -L ${dir} -name '*.fastq.gz' -exec zcat {} + | gzip >> ${name}.fastq.gz\n        \"\"\"\n        else if (!params.single)\n        \"\"\"\n        BARCODE_DIRS=\\$(find -L ${dir} -name \"barcode??\" -type d)\n        \n        if [ -z \"\\${BARCODE_DIRS}\" ]; then \n            guppy_barcoder -t ${task.cpus} -r ${barcoding_option} -i ${dir} -s fastq_porecov  --detect_mid_strand_barcodes --min_score_mid_barcodes 50 --arrangements_files \"${guppy_arrangement_files}\"\n\n            for barcodes in fastq_porecov/barcode??; do\n                find -L \\${barcodes} -name '*.fastq' -exec cat {} + | gzip >> \\${barcodes##*/}.fastq.gz\n            done\n        else\n            for barcodes in \\${BARCODE_DIRS}; do\n                find -L \\${barcodes} -name '*.fastq' -exec cat {} + | gzip >> \\${barcodes##*/}.fastq.gz\n                find -L \\${barcodes} -name '*.fastq.gz' -exec zcat {} + | gzip >> \\${barcodes##*/}.fastq.gz\n            done\n        fi\n        \"\"\"\n        stub:\n        \"\"\"\n        touch ${name}.fastq.gz\n        \"\"\"\n}",
        "nb_lignes_process": 43,
        "string_script": "        if (params.rapid) {\n            guppy_arrangement_files = 'barcode_arrs_rbk4.cfg barcode_arrs_rbk096.cfg'\n            barcoding_option = '  '\n            }\n        else {\n            guppy_arrangement_files = 'barcode_arrs_nb12.cfg barcode_arrs_nb24.cfg barcode_arrs_nb96.cfg'\n            barcoding_option = '--require_barcodes_both_ends'\n            }\n        if (params.one_end) {\n            barcoding_option = '  '\n            }\n        if (params.single)\n        \"\"\"\n        find -L ${dir} -name '*.fastq' -exec cat {} +  | gzip > ${name}.fastq.gz\n        find -L ${dir} -name '*.fastq.gz' -exec zcat {} + | gzip >> ${name}.fastq.gz\n        \"\"\"\n        else if (!params.single)\n        \"\"\"\n        BARCODE_DIRS=\\$(find -L ${dir} -name \"barcode??\" -type d)\n        \n        if [ -z \"\\${BARCODE_DIRS}\" ]; then \n            guppy_barcoder -t ${task.cpus} -r ${barcoding_option} -i ${dir} -s fastq_porecov  --detect_mid_strand_barcodes --min_score_mid_barcodes 50 --arrangements_files \"${guppy_arrangement_files}\"\n\n            for barcodes in fastq_porecov/barcode??; do\n                find -L \\${barcodes} -name '*.fastq' -exec cat {} + | gzip >> \\${barcodes##*/}.fastq.gz\n            done\n        else\n            for barcodes in \\${BARCODE_DIRS}; do\n                find -L \\${barcodes} -name '*.fastq' -exec cat {} + | gzip >> \\${barcodes##*/}.fastq.gz\n                find -L \\${barcodes} -name '*.fastq.gz' -exec zcat {} + | gzip >> \\${barcodes##*/}.fastq.gz\n            done\n        fi\n        \"\"\"",
        "nb_lignes_script": 32,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "dir"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'demultiplex'"
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch ${name}.fastq.gz\n        \"\"\""
    },
    "cat_fastq": {
        "name_process": "cat_fastq",
        "string_process": "process cat_fastq {\n        label 'ubuntu'\n        echo true\n    input:\n        tuple val(name), path(fastq_dir) \n    output:\n\t    tuple val(name), path(\"*.fastq.gz\") \n    script:\n    if (!params.barcodes)\n    \"\"\"\n    find -L ${fastq_dir} -name '*.fastq' -exec cat {} +  | gzip > ${name}.fastq.gz\n    \"\"\"\n    else if (params.barcodes)\n    \"\"\"\n    barcode_dirs=\\$(find -L ${fastq_dir} -name 'barcode??')\n    echo \\$barcode_dirs\n    for barcodes in \\$barcode_dirs; do\n        find -L \\${barcodes} -name '*.fastq' -exec cat {} + | gzip > \\${barcodes##*/}.fastq.gz\n    done\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "    if (!params.barcodes)\n    \"\"\"\n    find -L ${fastq_dir} -name '*.fastq' -exec cat {} +  | gzip > ${name}.fastq.gz\n    \"\"\"\n    else if (params.barcodes)\n    \"\"\"\n    barcode_dirs=\\$(find -L ${fastq_dir} -name 'barcode??')\n    echo \\$barcode_dirs\n    for barcodes in \\$barcode_dirs; do\n        find -L \\${barcodes} -name '*.fastq' -exec cat {} + | gzip > \\${barcodes##*/}.fastq.gz\n    done\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "fastq_dir"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'ubuntu'",
            "echo true"
        ],
        "when": "",
        "stub": ""
    },
    "nextclade": {
        "name_process": "nextclade",
        "string_process": "process nextclade {\n    label 'nextclade'\n    container = params.nextcladedocker\n    publishDir \"${params.output}/${params.lineagedir}/${name}/\", mode: 'copy', pattern: \"${name}_clade.tsv\"\n    input:\n        tuple val(name), path(consensus)\n    output:\n        tuple val(name), path(\"${name}_clade.tsv\")\n    script:\n    \"\"\"\n    nextclade run --input-fasta ${consensus} --input-dataset /data/sars-cov-2_MN908947 --output-tsv tmp.tsv\n    cat tmp.tsv | tr -d \"\\r\" > ${name}_clade.tsv\n    \"\"\"\n    stub:\n    \"\"\"\n    touch ${name}_clade.tsv\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    nextclade run --input-fasta ${consensus} --input-dataset /data/sars-cov-2_MN908947 --output-tsv tmp.tsv\n    cat tmp.tsv | tr -d \"\\r\" > ${name}_clade.tsv\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "consensus"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'nextclade'",
            "container = params.nextcladedocker",
            "publishDir \"${params.output}/${params.lineagedir}/${name}/\", mode: 'copy', pattern: \"${name}_clade.tsv\""
        ],
        "when": "",
        "stub": "\n    \"\"\"\n    touch ${name}_clade.tsv\n    \"\"\""
    },
    "treetime": {
        "name_process": "treetime",
        "string_process": "process treetime {\n    label \"treetime\"\n    publishDir \"${params.output}/${clonal_complex}/\", mode: 'copy', pattern: \"${clonal_complex}_tree_corrected.newick\" \n    input:\n        tuple val(name), file(clean_core_alignment), file(tree), file(dates)\n    output:\n        tuple val(clonal_complex), val(ref_name), file(\"${clonal_complex}_tree_corrected.newick\")\n    script:\n        \"\"\"\n        treetime --keep-root --max-iter 5 --gtr infer --tip-labels \\\n            --aln ${clean_core_alignment} --tree ${tree} --dates ${dates} --outdir .\n\n        # convert nexus to newick (from bin/)\n        nexus2newick.py timetree.nexus ${clonal_complex}_tree.nwk\n\n        #sed 's/'Reference'/'${ref_name}'/' ${clonal_complex}_tree.nwk  > ${clonal_complex}_tree_corrected.newick\n        \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "        \"\"\"\n        treetime --keep-root --max-iter 5 --gtr infer --tip-labels \\\n            --aln ${clean_core_alignment} --tree ${tree} --dates ${dates} --outdir .\n\n        # convert nexus to newick (from bin/)\n        nexus2newick.py timetree.nexus ${clonal_complex}_tree.nwk\n\n        #sed 's/'Reference'/'${ref_name}'/' ${clonal_complex}_tree.nwk  > ${clonal_complex}_tree_corrected.newick\n        \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "clean_core_alignment",
            "tree",
            "dates"
        ],
        "nb_inputs": 4,
        "outputs": [
            "ref_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label \"treetime\"",
            "publishDir \"${params.output}/${clonal_complex}/\", mode: 'copy', pattern: \"${clonal_complex}_tree_corrected.newick\""
        ],
        "when": "",
        "stub": ""
    },
    "json_report": {
        "name_process": "json_report",
        "string_process": "process json_report {\n        publishDir \"${params.output}/${params.jsondir}/\", mode: 'copy'\n        label 'fastcov'\n    input:\n        tuple val(name), path(pangolin_result), path(president_result), path(nextclade_result)\n    output:\n\t    tuple val(name), path(\"*.json\")\n    script:\n    if ((params.fasta || workflow.profile.contains('test_fasta')))\n    \"\"\"\n    json_parser.py -i ${name} \\\n        -p ${pangolin_result} \\\n        -n ${nextclade_result} \\\n        -q ${president_result} \\\n    \"\"\"\n    else if (!params.fasta )\n    \"\"\"\n    json_parser.py -i ${name} \\\n        -p ${pangolin_result} \\\n        -n ${nextclade_result} \\\n        -q ${president_result} \\\n        -v ${params.primerV}\n    \"\"\"\n    stub:\n    \"\"\"\n    touch ${name}.json\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "    if ((params.fasta || workflow.profile.contains('test_fasta')))\n    \"\"\"\n    json_parser.py -i ${name} \\\n        -p ${pangolin_result} \\\n        -n ${nextclade_result} \\\n        -q ${president_result} \\\n    \"\"\"\n    else if (!params.fasta )\n    \"\"\"\n    json_parser.py -i ${name} \\\n        -p ${pangolin_result} \\\n        -n ${nextclade_result} \\\n        -q ${president_result} \\\n        -v ${params.primerV}\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "pangolin_result",
            "president_result",
            "nextclade_result"
        ],
        "nb_inputs": 4,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "publishDir \"${params.output}/${params.jsondir}/\", mode: 'copy'",
            "label 'fastcov'"
        ],
        "when": "",
        "stub": "\n    \"\"\"\n    touch ${name}.json\n    \"\"\""
    },
    "fasttree": {
        "name_process": "fasttree",
        "string_process": "process fasttree {\n    label \"fasttree\"\n    publishDir \"${params.output}/tree/\", mode: 'copy', pattern: \"clean.core.tree.nwk\"\n    input:\n        path(clean_core_alignment)\n    output:\n        path(\"clean.core.tree.nwk\")\n    script:\n        \"\"\"\n        FastTree -gtr -nt ${clean_core_alignment} > clean.core.tree.nwk\n        \"\"\"\n}",
        "nb_lignes_process": 10,
        "string_script": "        \"\"\"\n        FastTree -gtr -nt ${clean_core_alignment} > clean.core.tree.nwk\n        \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "FastTree"
        ],
        "tools_url": [
            "https://bio.tools/fasttree"
        ],
        "tools_dico": [
            {
                "name": "FastTree",
                "uri": "https://bio.tools/fasttree",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0547",
                                    "term": "Phylogenetic inference (maximum likelihood and Bayesian methods)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0540",
                                    "term": "Phylogenetic inference (from molecular sequences)"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0547",
                                    "term": "Phylogenetic tree construction (maximum likelihood and Bayesian methods)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0547",
                                    "term": "Phylogenetic tree generation (maximum likelihood and Bayesian methods)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0540",
                                    "term": "Phylogenetic tree construction (from molecular sequences)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0540",
                                    "term": "Phylogenetic tree generation (from molecular sequences)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Infers approximately-maximum-likelihood phylogenetic trees from alignments of nucleotide or protein sequences.",
                "homepage": "http://www.microbesonline.org/fasttree/"
            }
        ],
        "inputs": [
            "clean_core_alignment"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label \"fasttree\"",
            "publishDir \"${params.output}/tree/\", mode: 'copy', pattern: \"clean.core.tree.nwk\""
        ],
        "when": "",
        "stub": ""
    },
    "quality_genome_filter": {
        "name_process": "quality_genome_filter",
        "string_process": "process quality_genome_filter {\n    label 'ubuntu'\n    publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"error_report_*.txt\"\n  input:\n    tuple val(name), path(fasta)\n  output:\n    tuple val(name), path(\"qc_${fasta}\") optional true\n    tuple val(name), path(\"error_report_*.txt\") optional true\n  script:\n    \"\"\"\n    genome_integrity.sh ${fasta} ${params.maskBegin} ${params.maskEnd} ${params.rm_N_genome}\n\n    if [ ! -f error_report_*.txt ]\n      then\n          mv ${fasta} qc_${fasta}         \n    fi\n    \"\"\"\n  stub:\n    \"\"\"\n    touch qc_${fasta} error_report_1.txt\n    \"\"\"\n  }",
        "nb_lignes_process": 20,
        "string_script": "    \"\"\"\n    genome_integrity.sh ${fasta} ${params.maskBegin} ${params.maskEnd} ${params.rm_N_genome}\n\n    if [ ! -f error_report_*.txt ]\n      then\n          mv ${fasta} qc_${fasta}         \n    fi\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name",
            "name"
        ],
        "nb_outputs": 2,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'ubuntu'",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: \"error_report_*.txt\""
        ],
        "when": "",
        "stub": "\n    \"\"\"\n    touch qc_${fasta} error_report_1.txt\n    \"\"\""
    },
    "summary_report": {
        "name_process": "summary_report",
        "string_process": "process summary_report {\n\n    publishDir \"${params.output}/\", mode: 'copy'\n    label 'fastcov'\n    \n    input:\n        path(version_config)\n        path(variants_table)\n        path(pangolin_results)\n        path(president_results)\n        path(nextclade_results)\n        file(kraken2_results)\n        file(coverage_plots)\n        file(samples_table)\n    output:\n\t    path(\"poreCov_summary_report_*.html\")\n        path(\"poreCov_summary_report_*.xlsx\")\n        path(\"poreCov_summary_report_*.tsv\")\n\n    shell:\n        guppyused = ((params.fast5 && !(params.fastq || params.fastq_pass)) || workflow.profile.contains('test_fast5'))\n\n        '''\n        echo 'sample,num_unclassified,num_sarscov2,num_human' > kraken2_results.csv\n        for KF in !{kraken2_results}; do\n        NUNCLASS=$(awk -v ORS= '$5==\"0\" {print $3}' $KF)\n        NSARS=$(awk -v ORS= '$5==\"2697049\" {print $3}' $KF)\n        NHUM=$(awk '$5==\"9606\" {print $3}' $KF)\n        echo \"${KF%.kreport},${NUNCLASS:-0},${NSARS:-0},${NHUM:-0}\" >> kraken2_results.csv\n        done\n\n        summary_report.py \\\n            -v !{version_config} \\\n            --variants_table !{variants_table} \\\n            --porecov_version !{workflow.revision}:!{workflow.commitId}:!{workflow.scriptId} \\\n            --nextclade_docker !{params.nextcladedocker} \\\n            --guppy_used !{guppyused} \\\n            --guppy_model !{params.guppy_model} \\\n            --medaka_model !{params.medaka_model} \\\n            --nf_commandline '!{workflow.commandLine}' \\\n            --primer !{params.primerV} \\\n            -p !{pangolin_results} \\\n            -q !{president_results} \\\n            -n !{nextclade_results} \\\n            -k kraken2_results.csv \\\n            -c $(echo !{coverage_plots} | tr ' ' ',') \\\n            -s !{samples_table}\n        '''\n    stub:\n        \"\"\"\n        touch poreCov_summary_report_1.html poreCov_summary_report_1.xlsx poreCov_summary_report_1.tsv\n        \"\"\"\n}",
        "nb_lignes_process": 51,
        "string_script": "        guppyused = ((params.fast5 && !(params.fastq || params.fastq_pass)) || workflow.profile.contains('test_fast5'))\n\n        '''\n        echo 'sample,num_unclassified,num_sarscov2,num_human' > kraken2_results.csv\n        for KF in !{kraken2_results}; do\n        NUNCLASS=$(awk -v ORS= '$5==\"0\" {print $3}' $KF)\n        NSARS=$(awk -v ORS= '$5==\"2697049\" {print $3}' $KF)\n        NHUM=$(awk '$5==\"9606\" {print $3}' $KF)\n        echo \"${KF%.kreport},${NUNCLASS:-0},${NSARS:-0},${NHUM:-0}\" >> kraken2_results.csv\n        done\n\n        summary_report.py \\\n            -v !{version_config} \\\n            --variants_table !{variants_table} \\\n            --porecov_version !{workflow.revision}:!{workflow.commitId}:!{workflow.scriptId} \\\n            --nextclade_docker !{params.nextcladedocker} \\\n            --guppy_used !{guppyused} \\\n            --guppy_model !{params.guppy_model} \\\n            --medaka_model !{params.medaka_model} \\\n            --nf_commandline '!{workflow.commandLine}' \\\n            --primer !{params.primerV} \\\n            -p !{pangolin_results} \\\n            -q !{president_results} \\\n            -n !{nextclade_results} \\\n            -k kraken2_results.csv \\\n            -c $(echo !{coverage_plots} | tr ' ' ',') \\\n            -s !{samples_table}\n        '''",
        "nb_lignes_script": 27,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "version_config",
            "variants_table",
            "pangolin_results",
            "president_results",
            "nextclade_results",
            "kraken2_results",
            "coverage_plots",
            "samples_table"
        ],
        "nb_inputs": 8,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "publishDir \"${params.output}/\", mode: 'copy'",
            "label 'fastcov'"
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch poreCov_summary_report_1.html poreCov_summary_report_1.xlsx poreCov_summary_report_1.tsv\n        \"\"\""
    },
    "summary_report_default": {
        "name_process": "summary_report_default",
        "string_process": "\nprocess summary_report_default {\n\n    publishDir \"${params.output}/\", mode: 'copy'\n    label 'fastcov'\n    \n    input:\n        path(version_config)\n        path(variants_table)\n        path(pangolin_results)\n        path(president_results)\n        path(nextclade_results)\n        path(kraken2_results)\n        path(coverage_plots)\n    output:\n\t    path(\"poreCov_summary_report_*.html\")\n        path(\"poreCov_summary_report_*.xlsx\")\n        path(\"poreCov_summary_report_*.tsv\")\n\n    shell:\n        guppyused = ((params.fast5 && !(params.fastq || params.fastq_pass)) || workflow.profile.contains('test_fast5'))\n\n        '''\n        echo 'sample,num_unclassified,num_sarscov2,num_human' > kraken2_results.csv\n        for KF in !{kraken2_results}; do\n        NUNCLASS=$(awk -v ORS= '$5==\"0\" {print $3}' $KF)\n        NSARS=$(awk -v ORS= '$5==\"2697049\" {print $3}' $KF)\n        NHUM=$(awk '$5==\"9606\" {print $3}' $KF)\n        echo \"${KF%.kreport},${NUNCLASS:-0},${NSARS:-0},${NHUM:-0}\" >> kraken2_results.csv\n        done\n\n        summary_report.py \\\n            -v !{version_config} \\\n            --variants_table !{variants_table} \\\n            --porecov_version !{workflow.revision}:!{workflow.commitId}:!{workflow.scriptId} \\\n            --guppy_used !{guppyused} \\\n            --guppy_model !{params.guppy_model} \\\n            --medaka_model !{params.medaka_model} \\\n            --nf_commandline '!{workflow.commandLine}' \\\n            --nextclade_docker !{params.nextcladedocker} \\\n            --primer !{params.primerV} \\\n            -p !{pangolin_results} \\\n            -q !{president_results} \\\n            -n !{nextclade_results} \\\n            -k kraken2_results.csv \\\n            -c $(echo !{coverage_plots} | tr ' ' ',') \n        '''\n    stub:\n        \"\"\"\n        touch poreCov_summary_report_1.html poreCov_summary_report_1.xlsx poreCov_summary_report_1.tsv\n        \"\"\"\n}",
        "nb_lignes_process": 50,
        "string_script": "        guppyused = ((params.fast5 && !(params.fastq || params.fastq_pass)) || workflow.profile.contains('test_fast5'))\n\n        '''\n        echo 'sample,num_unclassified,num_sarscov2,num_human' > kraken2_results.csv\n        for KF in !{kraken2_results}; do\n        NUNCLASS=$(awk -v ORS= '$5==\"0\" {print $3}' $KF)\n        NSARS=$(awk -v ORS= '$5==\"2697049\" {print $3}' $KF)\n        NHUM=$(awk '$5==\"9606\" {print $3}' $KF)\n        echo \"${KF%.kreport},${NUNCLASS:-0},${NSARS:-0},${NHUM:-0}\" >> kraken2_results.csv\n        done\n\n        summary_report.py \\\n            -v !{version_config} \\\n            --variants_table !{variants_table} \\\n            --porecov_version !{workflow.revision}:!{workflow.commitId}:!{workflow.scriptId} \\\n            --guppy_used !{guppyused} \\\n            --guppy_model !{params.guppy_model} \\\n            --medaka_model !{params.medaka_model} \\\n            --nf_commandline '!{workflow.commandLine}' \\\n            --nextclade_docker !{params.nextcladedocker} \\\n            --primer !{params.primerV} \\\n            -p !{pangolin_results} \\\n            -q !{president_results} \\\n            -n !{nextclade_results} \\\n            -k kraken2_results.csv \\\n            -c $(echo !{coverage_plots} | tr ' ' ',') \n        '''",
        "nb_lignes_script": 26,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "version_config",
            "variants_table",
            "pangolin_results",
            "president_results",
            "nextclade_results",
            "kraken2_results",
            "coverage_plots"
        ],
        "nb_inputs": 7,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "publishDir \"${params.output}/\", mode: 'copy'",
            "label 'fastcov'"
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch poreCov_summary_report_1.html poreCov_summary_report_1.xlsx poreCov_summary_report_1.tsv\n        \"\"\""
    },
    "summary_report_fasta": {
        "name_process": "summary_report_fasta",
        "string_process": "\nprocess summary_report_fasta {\n        publishDir \"${params.output}/\", mode: 'copy'\n        label 'fastcov'\n    input:\n        path(version_config)\n        path(variants_table)\n        path(pangolin_results)\n        path(president_results)\n        path(nextclade_results)\n    output:\n\t    path(\"poreCov_summary_report_*.html\")\n        path(\"poreCov_summary_report_*.xlsx\")\n        path(\"poreCov_summary_report_*.tsv\")\n\n    script:          \n        \"\"\"\n        summary_report.py \\\n            -v ${version_config} \\\n            --variants_table ${variants_table} \\\n            --porecov_version ${workflow.revision}:${workflow.commitId}:${workflow.scriptId} \\\n            --nf_commandline '${workflow.commandLine}' \\\n            --nextclade_docker ${params.nextcladedocker} \\\n            -p ${pangolin_results} \\\n            -q ${president_results} \\\n            -n ${nextclade_results} \\\n            -s \"deactivated\"\n        \"\"\"\n    stub:\n        \"\"\"\n        touch poreCov_summary_report_1.html poreCov_summary_report_1.xlsx poreCov_summary_report_1.tsv\n        \"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "        \"\"\"\n        summary_report.py \\\n            -v ${version_config} \\\n            --variants_table ${variants_table} \\\n            --porecov_version ${workflow.revision}:${workflow.commitId}:${workflow.scriptId} \\\n            --nf_commandline '${workflow.commandLine}' \\\n            --nextclade_docker ${params.nextcladedocker} \\\n            -p ${pangolin_results} \\\n            -q ${president_results} \\\n            -n ${nextclade_results} \\\n            -s \"deactivated\"\n        \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "version_config",
            "variants_table",
            "pangolin_results",
            "president_results",
            "nextclade_results"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "publishDir \"${params.output}/\", mode: 'copy'",
            "label 'fastcov'"
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch poreCov_summary_report_1.html poreCov_summary_report_1.xlsx poreCov_summary_report_1.tsv\n        \"\"\""
    },
    "download_database_kraken2": {
        "name_process": "download_database_kraken2",
        "string_process": "process download_database_kraken2 {\n        label \"ubuntu\"\n        storeDir \"${params.databases}/kraken2\"\n        errorStrategy 'retry'\n        maxRetries 1\n    output:\n        path(\"kraken.tar.gz\")\n    script:\n    if (task.attempt.toInteger() == 1)\n        \"\"\"\n        echo ${task.attempt}\n        wget --no-check-certificate https://zenodo.org/record/6333909/files/GRCh38.p13_SC2_2022-03-01.tar.gz -O kraken.tar.gz\n        \"\"\"\n    else if (task.attempt.toInteger() > 1)\n        \"\"\"\n        echo ${task.attempt}\n        wget --no-check-certificate https://osf.io/eprfq/download -O kraken.tar.gz\n        \"\"\"\n    stub:\n        \"\"\"\n        touch kraken.tar.gz\n        \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "    if (task.attempt.toInteger() == 1)\n        \"\"\"\n        echo ${task.attempt}\n        wget --no-check-certificate https://zenodo.org/record/6333909/files/GRCh38.p13_SC2_2022-03-01.tar.gz -O kraken.tar.gz\n        \"\"\"\n    else if (task.attempt.toInteger() > 1)\n        \"\"\"\n        echo ${task.attempt}\n        wget --no-check-certificate https://osf.io/eprfq/download -O kraken.tar.gz\n        \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label \"ubuntu\"",
            "storeDir \"${params.databases}/kraken2\"",
            "errorStrategy 'retry'",
            "maxRetries 1"
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch kraken.tar.gz\n        \"\"\""
    },
    "align_to_reference": {
        "name_process": "align_to_reference",
        "string_process": "process align_to_reference {\n    label 'minimap2'\n    input:\n        tuple(val(name), path(fastq_file), path(fasta_reference))\n    output:\n        tuple(path(\"${name}.bam\"), path(\"${name}.bam.bai\"))\n    script:\n    \"\"\"\n    set -o pipefail\n\t\n    minimap2 -t ${task.cpus} -ax map-ont ${fasta_reference} ${fastq_file} | samtools view -hbF4 | samtools sort -@ ${task.cpus} > ${name}.bam\n\n    samtools index ${name}.bam\n    \"\"\"\n    stub:\n\t\"\"\"\n\ttouch ${name}.bam ${name}.bam.bai\n\t\"\"\"\n    \n}",
        "nb_lignes_process": 18,
        "string_script": "    \"\"\"\n    set -o pipefail\n\t\n    minimap2 -t ${task.cpus} -ax map-ont ${fasta_reference} ${fastq_file} | samtools view -hbF4 | samtools sort -@ ${task.cpus} > ${name}.bam\n\n    samtools index ${name}.bam\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "Minimap2",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/minimap2",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "Minimap2",
                "uri": "https://bio.tools/minimap2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0491",
                                    "term": "Pairwise sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0491",
                                    "term": "Pairwise alignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Pairwise aligner for genomic and spliced nucleotide sequences",
                "homepage": "https://github.com/lh3/minimap2"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "name",
            "fastq_file",
            "fasta_reference"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'minimap2'"
        ],
        "when": "",
        "stub": "\n\t\"\"\"\n\ttouch ${name}.bam ${name}.bam.bai\n\t\"\"\""
    },
    "split_fasta": {
        "name_process": "split_fasta",
        "string_process": "process split_fasta {\n  \tlabel 'fastcov'\n\tinput:\n\t\tpath(fasta_input_raw)\n\toutput:\n\t\tpath(\"split_fasta/*.fasta\")\n\tscript:\n\t\"\"\"\n\tmkdir -p split_fasta\n    split_fasta.py ${fasta_input_raw}\n\t\"\"\"\n\tstub:\n\t\"\"\"\n\tmkdir split_fasta\n\ttouch split_fasta/genome.fasta\n\t\"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "\t\"\"\"\n\tmkdir -p split_fasta\n    split_fasta.py ${fasta_input_raw}\n\t\"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fasta_input_raw"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'fastcov'"
        ],
        "when": "",
        "stub": "\n\t\"\"\"\n\tmkdir split_fasta\n\ttouch split_fasta/genome.fasta\n\t\"\"\""
    },
    "pangolin": {
        "name_process": "pangolin",
        "string_process": "process pangolin {\n    label 'pangolin'\n    container = params.pangolindocker\n    publishDir \"${params.output}/${params.lineagedir}/${name}/\", mode: 'copy', pattern: \"lineage_report_${name}.csv\"\n  input:\n    tuple val(name), path(fasta)\n  output:\n    tuple val(name), path(\"lineage_report_${name}.csv\") optional true\n  script:\n    \"\"\"\n    pangolin -t ${task.cpus} ${fasta}\n\n    mv lineage_report.csv lineage_report_${name}.csv\n    \n    find . -name \"*.csv\" -size  0 -print -delete\n    \"\"\"\n    stub:\n    \"\"\"\n    echo \"taxon,lineage,conflict,ambiguity_score,scorpio_call,scorpio_support,scorpio_conflict,version,pangolin_version,pangoLEARN_version,pango_version,status,note\" > lineage_report_${name}.csv\n    echo \"barcode13_ARTIC_medaka,B.1.177,,,,,,PANGO-v1.2.12,3.0.5,2021-06-05,v1.2.12,passed_qc,Assigned from designation hash.\" >> lineage_report_${name}.csv\n    \"\"\"\n  }",
        "nb_lignes_process": 20,
        "string_script": "    \"\"\"\n    pangolin -t ${task.cpus} ${fasta}\n\n    mv lineage_report.csv lineage_report_${name}.csv\n    \n    find . -name \"*.csv\" -size  0 -print -delete\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'pangolin'",
            "container = params.pangolindocker",
            "publishDir \"${params.output}/${params.lineagedir}/${name}/\", mode: 'copy', pattern: \"lineage_report_${name}.csv\""
        ],
        "when": "",
        "stub": "\n    \"\"\"\n    echo \"taxon,lineage,conflict,ambiguity_score,scorpio_call,scorpio_support,scorpio_conflict,version,pangolin_version,pangoLEARN_version,pango_version,status,note\" > lineage_report_${name}.csv\n    echo \"barcode13_ARTIC_medaka,B.1.177,,,,,,PANGO-v1.2.12,3.0.5,2021-06-05,v1.2.12,passed_qc,Assigned from designation hash.\" >> lineage_report_${name}.csv\n    \"\"\""
    },
    "president": {
        "name_process": "president",
        "string_process": "process president {\n    label \"president\"\n    publishDir \"${params.output}/${params.genomedir}/${name}\", mode: 'copy',\n        saveAs: { filename -> if (filename.endsWith(\"${name}_report.tsv\")) \"${name}_seq_ident_check.tsv\" }\n    input:\n        tuple val(name), path(fasta), path(reference_fasta)\n    output:\n        tuple val(name), path(\"${name}_report.tsv\"), path(\"${name}_valid.fasta\"), emit: valid\n        tuple val(name), path(\"${name}_report.tsv\"), path(\"${name}_invalid.fasta\"), emit: invalid\n    script:\n        \"\"\"\n        president -r ${reference_fasta} -t ${task.cpus} -q ${fasta} -x ${params.seq_threshold} -n ${params.n_threshold} -p . -f ${name}_\n        \"\"\"\n    stub:\n        \"\"\"\n        touch ${name}_report.tsv ${name}_valid.fasta ${name}_invalid.fasta\n        \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "        \"\"\"\n        president -r ${reference_fasta} -t ${task.cpus} -q ${fasta} -x ${params.seq_threshold} -n ${params.n_threshold} -p . -f ${name}_\n        \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "fasta",
            "reference_fasta"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label \"president\"",
            "publishDir \"${params.output}/${params.genomedir}/${name}\", mode: 'copy' , saveAs: { filename -> if (filename.endsWith(\"${name}_report.tsv\")) \"${name}_seq_ident_check.tsv\" }"
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch ${name}_report.tsv ${name}_valid.fasta ${name}_invalid.fasta\n        \"\"\""
    },
    "nanoplot": {
        "name_process": "nanoplot",
        "string_process": "process nanoplot {\n    label 'nanoplot'\n    publishDir \"${params.output}/${params.readqcdir}/${name}/\", mode: 'copy', pattern: \"${name}_read_quality_report.html\"\n    publishDir \"${params.output}/${params.readqcdir}/${name}/\", mode: 'copy', pattern: \"${name}_read_quality.txt\"\n    publishDir \"${params.output}/${params.readqcdir}/${name}/figures\", mode: 'copy', pattern: \"*.png\"\n    publishDir \"${params.output}/${params.readqcdir}/${name}/vector_figures\", mode: 'copy', pattern: \"*.pdf\"\n    input:\n        tuple val(name), path(reads)\n    output:\n        tuple val(name), path(\"*.html\"), path(\"*.pdf\") optional true\n        tuple val(name), path(\"${name}_read_quality.txt\"), path(\"*.png\") optional true\n    script:\n    \"\"\"\n    NanoPlot -t ${task.cpus} --fastq ${reads} --title '${name}' --color darkslategrey --N50 --plots hex --loglength -f png --store\n    NanoPlot -t ${task.cpus} --pickle NanoPlot-data.pickle --title '${name}' --color darkslategrey --N50 --plots hex --loglength -f pdf\n    mv NanoPlot-report.html ${name}_read_quality_report.html\n    mv NanoStats.txt ${name}_read_quality.txt\n    \"\"\"\n    stub:\n    \"\"\"\n    touch ${name}_read_quality_report.html \\\n        ${name}_read_quality.txt \\\n        ${name}.png \\\n        ${name}.pdf\n    \"\"\"\n}",
        "nb_lignes_process": 24,
        "string_script": "    \"\"\"\n    NanoPlot -t ${task.cpus} --fastq ${reads} --title '${name}' --color darkslategrey --N50 --plots hex --loglength -f png --store\n    NanoPlot -t ${task.cpus} --pickle NanoPlot-data.pickle --title '${name}' --color darkslategrey --N50 --plots hex --loglength -f pdf\n    mv NanoPlot-report.html ${name}_read_quality_report.html\n    mv NanoStats.txt ${name}_read_quality.txt\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name",
            "name"
        ],
        "nb_outputs": 2,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'nanoplot'",
            "publishDir \"${params.output}/${params.readqcdir}/${name}/\", mode: 'copy', pattern: \"${name}_read_quality_report.html\"",
            "publishDir \"${params.output}/${params.readqcdir}/${name}/\", mode: 'copy', pattern: \"${name}_read_quality.txt\"",
            "publishDir \"${params.output}/${params.readqcdir}/${name}/figures\", mode: 'copy', pattern: \"*.png\"",
            "publishDir \"${params.output}/${params.readqcdir}/${name}/vector_figures\", mode: 'copy', pattern: \"*.pdf\""
        ],
        "when": "",
        "stub": "\n    \"\"\"\n    touch ${name}_read_quality_report.html \\\n        ${name}_read_quality.txt \\\n        ${name}.png \\\n        ${name}.pdf\n    \"\"\""
    },
    "covarplot": {
        "name_process": "covarplot",
        "string_process": "process covarplot {\n    label \"covarplot\"\n    publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy'\n    input:\n        tuple val(name), path(vcf), path(depth1), path(depth2), path(primerbed)\n    output:\n        tuple val(name), path(\"${name}_amplicon_coverage.png\"), path(\"${name}_amplicon_coverage_log.png\")\n    script:\n        \"\"\"\n        covarplot.py -v ${vcf} -d1 ${depth1} -d2 ${depth2} -b ${primerbed}/nCoV-2019/${params.primerV}/nCoV-2019.scheme.bed -s .\n        mv ${name}.CoVarPlot.png ${name}_amplicon_coverage.png\n        covarplot.py -v ${vcf} -d1 ${depth1} -d2 ${depth2} -b ${primerbed}/nCoV-2019/${params.primerV}/nCoV-2019.scheme.bed -s . --log\n        mv ${name}.CoVarPlot.png ${name}_amplicon_coverage_log.png\n        \"\"\"\n    stub:\n        \"\"\"\n        touch ${name}_amplicon_coverage.png ${name}_amplicon_coverage_log.png\n        \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "        \"\"\"\n        covarplot.py -v ${vcf} -d1 ${depth1} -d2 ${depth2} -b ${primerbed}/nCoV-2019/${params.primerV}/nCoV-2019.scheme.bed -s .\n        mv ${name}.CoVarPlot.png ${name}_amplicon_coverage.png\n        covarplot.py -v ${vcf} -d1 ${depth1} -d2 ${depth2} -b ${primerbed}/nCoV-2019/${params.primerV}/nCoV-2019.scheme.bed -s . --log\n        mv ${name}.CoVarPlot.png ${name}_amplicon_coverage_log.png\n        \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "vcf",
            "depth1",
            "depth2",
            "primerbed"
        ],
        "nb_inputs": 5,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label \"covarplot\"",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy'"
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch ${name}_amplicon_coverage.png ${name}_amplicon_coverage_log.png\n        \"\"\""
    },
    "covarplot_custom_bed": {
        "name_process": "covarplot_custom_bed",
        "string_process": "\nprocess covarplot_custom_bed {\n    label \"covarplot\"\n    publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy'\n    input:\n        tuple val(name), path(vcf), path(depth1), path(depth2), path(primerbed)\n    output:\n        tuple val(name), path(\"${name}_amplicon_coverage.png\"), path(\"${name}_amplicon_coverage_log.png\")\n    script:\n        \"\"\"\n        # clean up bed file: replace first colum with MN908947.3, remove empty lines and sort by 4th column (primer names) \n        cut -f2- ${primerbed} |\\\n            sed '/^[[:space:]]*\\$/d' |\\\n            sed -e \\$'s/^/MN908947.3\\\\t/' |\\\n            sort -k4 > nCoV-2019-plot.scheme.bed\n\n\n        covarplot.py -v ${vcf} -d1 ${depth1} -d2 ${depth2} -b nCoV-2019-plot.scheme.bed -s .\n        mv ${name}.CoVarPlot.png ${name}_amplicon_coverage.png\n        covarplot.py -v ${vcf} -d1 ${depth1} -d2 ${depth2} -b nCoV-2019-plot.scheme.bed -s . --log\n        mv ${name}.CoVarPlot.png ${name}_amplicon_coverage_log.png\n        \"\"\"\n    stub:\n        \"\"\"\n        touch ${name}_amplicon_coverage.png ${name}_amplicon_coverage_log.png\n        \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "        \"\"\"\n        # clean up bed file: replace first colum with MN908947.3, remove empty lines and sort by 4th column (primer names) \n        cut -f2- ${primerbed} |\\\n            sed '/^[[:space:]]*\\$/d' |\\\n            sed -e \\$'s/^/MN908947.3\\\\t/' |\\\n            sort -k4 > nCoV-2019-plot.scheme.bed\n\n\n        covarplot.py -v ${vcf} -d1 ${depth1} -d2 ${depth2} -b nCoV-2019-plot.scheme.bed -s .\n        mv ${name}.CoVarPlot.png ${name}_amplicon_coverage.png\n        covarplot.py -v ${vcf} -d1 ${depth1} -d2 ${depth2} -b nCoV-2019-plot.scheme.bed -s . --log\n        mv ${name}.CoVarPlot.png ${name}_amplicon_coverage_log.png\n        \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "vcf",
            "depth1",
            "depth2",
            "primerbed"
        ],
        "nb_inputs": 5,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label \"covarplot\"",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy'"
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch ${name}_amplicon_coverage.png ${name}_amplicon_coverage_log.png\n        \"\"\""
    },
    "split_reference": {
        "name_process": "split_reference",
        "string_process": "process split_reference {\n    label 'ubuntu'\n  input:\n    tuple val(name), path(fasta)\n  output:\n    path(\"${name}_contigs/*.fa\")\n  script:\n    \"\"\"\n    split.sh ${name} ${fasta}\n    \"\"\"\n  }",
        "nb_lignes_process": 9,
        "string_script": "    \"\"\"\n    split.sh ${name} ${fasta}\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'ubuntu'"
        ],
        "when": "",
        "stub": ""
    },
    "mafft": {
        "name_process": "mafft",
        "string_process": "process mafft {\n    label \"mafft\"\n    input:\n        path(samples)\n        tuple val(reference_name), path(references)\n    output:\n        path(\"clean.full.aln\")\n    script:\n        \"\"\"\n        # merge sequence and add counter for uniq names\n        cat ${samples} ${references} | tr -d \"\\\\r\" | tr \"|\" \"_\" | tr \"/\" \"_\" | tr \" \" \"_\" | awk '/^>/{\\$0=\\$0\"_\"(++i)}1' > combined.fasta\n        mafft --thread ${task.cpus} combined.fasta > clean.full.aln\n        \"\"\"\n}",
        "nb_lignes_process": 12,
        "string_script": "        \"\"\"\n        # merge sequence and add counter for uniq names\n        cat ${samples} ${references} | tr -d \"\\\\r\" | tr \"|\" \"_\" | tr \"/\" \"_\" | tr \" \" \"_\" | awk '/^>/{\\$0=\\$0\"_\"(++i)}1' > combined.fasta\n        mafft --thread ${task.cpus} combined.fasta > clean.full.aln\n        \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "MAFFT"
        ],
        "tools_url": [
            "https://bio.tools/MAFFT"
        ],
        "tools_dico": [
            {
                "name": "MAFFT",
                "uri": "https://bio.tools/MAFFT",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0492",
                                    "term": "Multiple sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0492",
                                    "term": "Multiple alignment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            }
                        ]
                    }
                ],
                "description": "MAFFT (Multiple Alignment using Fast Fourier Transform) is a high speed multiple sequence alignment program.",
                "homepage": "http://mafft.cbrc.jp/alignment/server/index.html"
            }
        ],
        "inputs": [
            "samples",
            "reference_name",
            "references"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label \"mafft\""
        ],
        "when": "",
        "stub": ""
    },
    "bwa_samtools": {
        "name_process": "bwa_samtools",
        "string_process": "process bwa_samtools {\n    label \"bwa\"\n    publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: '*.sorted.bam*'\n    input:\n        tuple val(name), path(fasta), path(reads)\n    output:\n        tuple val(name), path(\"coverage_info.txt\"), optional: true\n        tuple val(name), path(\"*.sorted.bam\"), path(\"*.sorted.bam.bai\")\n    script:\n        \"\"\"\n        bwa index ${fasta}\n        bwa mem -t ${task.cpus} ${fasta} ${reads} | samtools view -bS - | samtools sort -@ ${task.cpus} - > ${name}.sorted.bam\n        samtools index -@ ${task.cpus} ${name}.sorted.bam\n\n        samtools mpileup ${name}.sorted.bam | awk '{print \\$1\"\\\\t\"\\$2\"\\\\t\"\\$4}' > coverage_info.txt\n\n        if [ ! -s coverage_info.txt ] ; then\n            rm coverage_info.txt\n        fi\n\n        # report stats\n\n        \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "        \"\"\"\n        bwa index ${fasta}\n        bwa mem -t ${task.cpus} ${fasta} ${reads} | samtools view -bS - | samtools sort -@ ${task.cpus} - > ${name}.sorted.bam\n        samtools index -@ ${task.cpus} ${name}.sorted.bam\n\n        samtools mpileup ${name}.sorted.bam | awk '{print \\$1\"\\\\t\"\\$2\"\\\\t\"\\$4}' > coverage_info.txt\n\n        if [ ! -s coverage_info.txt ] ; then\n            rm coverage_info.txt\n        fi\n\n        # report stats\n\n        \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "BWA",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/bwa",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "name",
            "fasta",
            "reads"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name",
            "name"
        ],
        "nb_outputs": 2,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label \"bwa\"",
            "publishDir \"${params.output}/${params.genomedir}/${name}/\", mode: 'copy', pattern: '*.sorted.bam*'"
        ],
        "when": "",
        "stub": ""
    },
    "get_variants_classification": {
        "name_process": "get_variants_classification",
        "string_process": "\nprocess get_variants_classification {\n  \tcontainer = 'nanozoo/template:3.8--d089809'\n\tpublishDir \"${params.output}/\", mode: 'copy'\n\tcache false\n\t                         \n\t                              \n\toutput:\n\tpath(\"SARSCoV2_variants_*.csv\")\n\tshell:\n\t'''\n\tDATE=`date +\"%Y-%m-%d--%H-%M-%S\"`\n    wget --no-check-certificate https://raw.githubusercontent.com/3dgiordano/SARS-CoV-2-Variants/main/data/variants.csv -O SARSCoV2_variants_${DATE}.csv || \\\n\t{ echo Using fallback from ./data/; \\\n\t rm SARSCoV2_variants_${DATE}.csv; \\\n\t cp !{workflow.projectDir}/data/variants_SARSCoV2/SARSCoV2_variants_fallback_*.csv .; }\n\t'''\n}",
        "nb_lignes_process": 16,
        "string_script": "\t'''\n\tDATE=`date +\"%Y-%m-%d--%H-%M-%S\"`\n    wget --no-check-certificate https://raw.githubusercontent.com/3dgiordano/SARS-CoV-2-Variants/main/data/variants.csv -O SARSCoV2_variants_${DATE}.csv || \\\n\t{ echo Using fallback from ./data/; \\\n\t rm SARSCoV2_variants_${DATE}.csv; \\\n\t cp !{workflow.projectDir}/data/variants_SARSCoV2/SARSCoV2_variants_fallback_*.csv .; }\n\t'''",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "container = 'nanozoo/template:3.8--d089809'",
            "publishDir \"${params.output}/\", mode: 'copy'",
            "cache false"
        ],
        "when": "",
        "stub": ""
    },
    "seqrs": {
        "name_process": "seqrs",
        "string_process": "process seqrs {\n        label 'seqrs'\n        publishDir \"${params.output}/${params.seqrepair}/\", mode: 'copy'\n    input:\n        tuple val(name), path(fasta), path(primerbed)\n  \toutput:\n    \ttuple val(name), file(\"*${name}.tsv\")\n  \tscript:\n        \"\"\"\n        seqrs --genomes ${fasta} --primerbed ${primerbed}/V3/nCoV-2019.bed --results V3-primer-to-repair-Ns-for_${name}.tsv -a 400\n        seqrs --genomes ${fasta} --primerbed ${primerbed}/V4/nCoV-2019.primer.bed --results V4-primer-to-repair-Ns-for_${name}.tsv -a 400\n        seqrs --genomes ${fasta} --primerbed ${primerbed}/V4.1/nCoV-2019.primer.bed --results V4-1-primer-to-repair-Ns-for_${name}.tsv -a 400\n        seqrs --genomes ${fasta} --primerbed ${primerbed}/V1200/nCoV-2019.bed --results V1200-primer-to-repair-Ns-for_${name}.tsv -a 1200\n        \"\"\"\n    stub:\n        \"\"\"\n        touch 1${name}.tsv\n        \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "        \"\"\"\n        seqrs --genomes ${fasta} --primerbed ${primerbed}/V3/nCoV-2019.bed --results V3-primer-to-repair-Ns-for_${name}.tsv -a 400\n        seqrs --genomes ${fasta} --primerbed ${primerbed}/V4/nCoV-2019.primer.bed --results V4-primer-to-repair-Ns-for_${name}.tsv -a 400\n        seqrs --genomes ${fasta} --primerbed ${primerbed}/V4.1/nCoV-2019.primer.bed --results V4-1-primer-to-repair-Ns-for_${name}.tsv -a 400\n        seqrs --genomes ${fasta} --primerbed ${primerbed}/V1200/nCoV-2019.bed --results V1200-primer-to-repair-Ns-for_${name}.tsv -a 1200\n        \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "fasta",
            "primerbed"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'seqrs'",
            "publishDir \"${params.output}/${params.seqrepair}/\", mode: 'copy'"
        ],
        "when": "",
        "stub": "\n        \"\"\"\n        touch 1${name}.tsv\n        \"\"\""
    },
    "kraken2": {
        "name_process": "kraken2",
        "string_process": "process kraken2 {\n        label 'kraken2'\n        publishDir \"${params.output}/${params.readqcdir}/${name}/tax_read_classification\", mode: 'copy'\n    input:\n        tuple val(name), path(reads)\n        path(database)\n  \toutput:\n    \ttuple val(name), path(\"${name}.kraken.out\"), path(\"${name}.kreport\")\n  \tscript:\n    \"\"\"\n    mkdir -p kraken_db && tar xzf ${database} -C kraken_db --strip-components 1\n\n    # mask possible primer regions\n    case \"${reads}\" in\n        *.gz) \n            zcat ${reads} | sed '1b ; s/..............................\\$/NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN/ ; n ' |\\\n            sed '1b ; s/^............................../NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN/ ; n ' > masked_reads.fastq\n            ;;\n        *.fastq)\n            cat ${reads} | sed '1b ; s/..............................\\$/NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN/ ; n ' |\\\n            sed '1b ; s/^............................../NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN/ ; n ' > masked_reads.fastq\n            ;;\n        *)\n            echo \"file format not supported...what the ...(.fastq .fastq.gz is supported)\"\n            exit 1\n    esac\n\n    kraken2 --db kraken_db --threads ${task.cpus} --output ${name}.kraken.out --report ${name}.kreport masked_reads.fastq\n\n    # cleanup to reduce footprint\n    rm -rf kraken_db/\n    \"\"\"\n    stub:\n    \"\"\"\n    touch ${name}.kraken.out ${name}.kreport\n    \"\"\"\n  }",
        "nb_lignes_process": 35,
        "string_script": "    \"\"\"\n    mkdir -p kraken_db && tar xzf ${database} -C kraken_db --strip-components 1\n\n    # mask possible primer regions\n    case \"${reads}\" in\n        *.gz) \n            zcat ${reads} | sed '1b ; s/..............................\\$/NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN/ ; n ' |\\\n            sed '1b ; s/^............................../NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN/ ; n ' > masked_reads.fastq\n            ;;\n        *.fastq)\n            cat ${reads} | sed '1b ; s/..............................\\$/NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN/ ; n ' |\\\n            sed '1b ; s/^............................../NNNNNNNNNNNNNNNNNNNNNNNNNNNNNN/ ; n ' > masked_reads.fastq\n            ;;\n        *)\n            echo \"file format not supported...what the ...(.fastq .fastq.gz is supported)\"\n            exit 1\n    esac\n\n    kraken2 --db kraken_db --threads ${task.cpus} --output ${name}.kraken.out --report ${name}.kreport masked_reads.fastq\n\n    # cleanup to reduce footprint\n    rm -rf kraken_db/\n    \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [
            "CASE",
            "kraken2"
        ],
        "tools_url": [
            "https://bio.tools/CASE",
            "https://bio.tools/kraken2"
        ],
        "tools_dico": [
            {
                "name": "CASE",
                "uri": "https://bio.tools/CASE",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0749",
                            "term": "Transcription factors and regulatory sites"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0089",
                            "term": "Ontology and terminology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3436",
                                    "term": "Aggregation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3435",
                                    "term": "Standardisation and normalisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3559",
                                    "term": "Ontology visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3559",
                                    "term": "Ontology browsing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Advancing Coordinated Cyber-investigations and Tool Interoperability using a Community Developed Specification Language.\n\nSource files for the CASE website.\n\nAPI used for instantiating CASE objects (includes ontological verification and type checking).\n\nCyber-investigation Analysis Standard Expression (CASE).\n\nRead the CASE Wiki tab to learn everything you need to know about the Cyber-investigation Analysis Standard Expression (CASE) ontology. For learning about the Unified Cyber Ontology, CASE's parent, see UCO.\n\n\"@vocab\": \"http://case.example.org/core#\",.\n\nDET ER DINE PENGER DET DREIER SEG OM...\n\nVi er ikke st\ufffdrst, men garanterer effektiv behandling.\n\nLast ned v\ufffdr brosjyre i PDF format.\n\n||| COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/pymzml (GITHUB.COM).\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'UCO', 'cyber-investigation', 'cyber-investigations', 'plaso'",
                "homepage": "http://CASE.as"
            },
            {
                "name": "kraken2",
                "uri": "https://bio.tools/kraken2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3028",
                                "term": "Taxonomy"
                            }
                        ]
                    }
                ],
                "description": "Kraken 2 is the newest version of Kraken, a taxonomic classification system using exact k-mer matches to achieve high accuracy and fast classification speeds. This classifier matches each k-mer within a query sequence to the lowest common ancestor (LCA) of all genomes containing the given k-mer. The k-mer assignments inform the classification algorithm.",
                "homepage": "https://ccb.jhu.edu/software/kraken2/"
            }
        ],
        "inputs": [
            "name",
            "reads",
            "database"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'kraken2'",
            "publishDir \"${params.output}/${params.readqcdir}/${name}/tax_read_classification\", mode: 'copy'"
        ],
        "when": "",
        "stub": "\n    \"\"\"\n    touch ${name}.kraken.out ${name}.kreport\n    \"\"\""
    },
    "krona": {
        "name_process": "krona",
        "string_process": "process krona {\n        label 'krona'\n        publishDir \"${params.output}/${params.readqcdir}/${name}/\", mode: 'copy'\n    input:\n        tuple val(name), path(kraken2), path(kreport)\n  \toutput:\n    \ttuple val(name), file(\"${name}_krona.html\")\n  \tscript:\n    \"\"\"\n    cat ${kreport} | cut -f 3,5 > file.krona\n    ktImportTaxonomy file.krona -m 1\n    mv *.html ${name}_krona.html\n    \"\"\"\n    stub:\n    \"\"\"\n    touch ${name}_krona.html\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    cat ${kreport} | cut -f 3,5 > file.krona\n    ktImportTaxonomy file.krona -m 1\n    mv *.html ${name}_krona.html\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "kraken2",
            "kreport"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "replikation__poreCov",
        "directive": [
            "label 'krona'",
            "publishDir \"${params.output}/${params.readqcdir}/${name}/\", mode: 'copy'"
        ],
        "when": "",
        "stub": "\n    \"\"\"\n    touch ${name}_krona.html\n    \"\"\""
    }
}