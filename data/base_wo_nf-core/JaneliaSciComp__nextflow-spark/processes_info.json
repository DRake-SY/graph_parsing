{
    "prepare_spark_work_dir": {
        "name_process": "prepare_spark_work_dir",
        "string_process": "process prepare_spark_work_dir {\n    container = \"${params.spark_container_repo}/${params.spark_container_name}:${params.spark_container_version}\"\n    label 'small'\n\n    input:\n    val(spark_work_dir)\n    val(terminate_name)\n\n    output:\n    val(spark_work_dir)\n\n    script:\n    def terminate_file_name = get_terminate_file_name(spark_work_dir, terminate_name)\n    def write_session_id = create_write_session_id_script(spark_work_dir)\n    log.debug \"Spark local directory: ${params.spark_local_dir}\"\n    log.debug \"Spark work directory: ${spark_work_dir}\"\n    \"\"\"\n    mkdir -p \"${params.spark_local_dir}\"\n    if [[ ! -d \"${spark_work_dir}\" ]] ; then\n        mkdir -p \"${spark_work_dir}\"\n    else\n        rm -f ${spark_work_dir}/* || true\n    fi\n    ${write_session_id}\n    \"\"\"\n}",
        "nb_lignes_process": 24,
        "string_script": "    def terminate_file_name = get_terminate_file_name(spark_work_dir, terminate_name)\n    def write_session_id = create_write_session_id_script(spark_work_dir)\n    log.debug \"Spark local directory: ${params.spark_local_dir}\"\n    log.debug \"Spark work directory: ${spark_work_dir}\"\n    \"\"\"\n    mkdir -p \"${params.spark_local_dir}\"\n    if [[ ! -d \"${spark_work_dir}\" ]] ; then\n        mkdir -p \"${spark_work_dir}\"\n    else\n        rm -f ${spark_work_dir}/* || true\n    fi\n    ${write_session_id}\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "spark_work_dir",
            "terminate_name"
        ],
        "nb_inputs": 2,
        "outputs": [
            "spark_work_dir"
        ],
        "nb_outputs": 1,
        "name_workflow": "JaneliaSciComp__nextflow-spark",
        "directive": [
            "container = \"${params.spark_container_repo}/${params.spark_container_name}:${params.spark_container_version}\"",
            "label 'small'"
        ],
        "when": "",
        "stub": ""
    },
    "spark_master": {
        "name_process": "spark_master",
        "string_process": "\nprocess spark_master {\n    container = \"${params.spark_container_repo}/${params.spark_container_name}:${params.spark_container_version}\"\n    label 'small'\n\n    input:\n    val(spark_conf)\n    val(spark_work_dir)\n    val(terminate_name)\n\n    output:\n\n    script:\n    def spark_master_log_file = get_spark_master_log(spark_work_dir)\n    def spark_config_name = get_spark_config_name(spark_conf, spark_work_dir)\n    def terminate_file_name = get_terminate_file_name(spark_work_dir, terminate_name)\n    def create_spark_config\n    def spark_config_env\n    def spark_config_arg\n    if (spark_config_name != '') {\n        create_spark_config = create_default_spark_config(spark_config_name)\n        spark_config_arg = \"--properties-file ${spark_config_name}\"\n        spark_config_env = \"\"\n    } else {\n        create_spark_config = \"\"\n        spark_config_arg = \"\"\n        spark_config_env = \"export SPARK_CONF_DIR=${spark_conf}\"\n    }\n    def spark_env = create_spark_env(spark_work_dir, spark_config_env, task.ext.sparkLocation)\n    def lookup_ip_script = create_lookup_ip_script()\n    def check_session_id = create_check_session_id_script(spark_work_dir)\n    \"\"\"\n    echo \"Starting spark master - logging to ${spark_master_log_file}\"\n    ${check_session_id}\n\n    rm -f ${spark_master_log_file} || true\n\n    ${create_spark_config}\n    ${spark_env}\n    ${lookup_ip_script}\n\n    ${task.ext.sparkLocation}/bin/spark-class org.apache.spark.deploy.master.Master \\\n    -h \\$SPARK_LOCAL_IP \\\n    ${spark_config_arg} \\\n    &> ${spark_master_log_file} &\n    spid=\\$!\n\n    ${wait_to_terminate('spid', terminate_file_name, spark_master_log_file)}\n    \"\"\"\n}",
        "nb_lignes_process": 48,
        "string_script": "    def spark_master_log_file = get_spark_master_log(spark_work_dir)\n    def spark_config_name = get_spark_config_name(spark_conf, spark_work_dir)\n    def terminate_file_name = get_terminate_file_name(spark_work_dir, terminate_name)\n    def create_spark_config\n    def spark_config_env\n    def spark_config_arg\n    if (spark_config_name != '') {\n        create_spark_config = create_default_spark_config(spark_config_name)\n        spark_config_arg = \"--properties-file ${spark_config_name}\"\n        spark_config_env = \"\"\n    } else {\n        create_spark_config = \"\"\n        spark_config_arg = \"\"\n        spark_config_env = \"export SPARK_CONF_DIR=${spark_conf}\"\n    }\n    def spark_env = create_spark_env(spark_work_dir, spark_config_env, task.ext.sparkLocation)\n    def lookup_ip_script = create_lookup_ip_script()\n    def check_session_id = create_check_session_id_script(spark_work_dir)\n    \"\"\"\n    echo \"Starting spark master - logging to ${spark_master_log_file}\"\n    ${check_session_id}\n\n    rm -f ${spark_master_log_file} || true\n\n    ${create_spark_config}\n    ${spark_env}\n    ${lookup_ip_script}\n\n    ${task.ext.sparkLocation}/bin/spark-class org.apache.spark.deploy.master.Master \\\n    -h \\$SPARK_LOCAL_IP \\\n    ${spark_config_arg} \\\n    &> ${spark_master_log_file} &\n    spid=\\$!\n\n    ${wait_to_terminate('spid', terminate_file_name, spark_master_log_file)}\n    \"\"\"",
        "nb_lignes_script": 35,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "spark_conf",
            "spark_work_dir",
            "terminate_name"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "JaneliaSciComp__nextflow-spark",
        "directive": [
            "container = \"${params.spark_container_repo}/${params.spark_container_name}:${params.spark_container_version}\"",
            "label 'small'"
        ],
        "when": "",
        "stub": ""
    },
    "wait_for_master": {
        "name_process": "wait_for_master",
        "string_process": "\nprocess wait_for_master {\n    container { \"${params.spark_container_repo}/${params.spark_container_name}:${params.spark_container_version}\" }\n    label 'small'\n\n    input:\n    val(spark_work_dir)\n    val(terminate_name)\n\n    output:\n    tuple val(spark_work_dir), val(terminate_name), env(spark_uri)\n\n    script:\n    def spark_master_log_name = get_spark_master_log(spark_work_dir)\n    def terminate_file_name = get_terminate_file_name(spark_work_dir, terminate_name)\n    def check_session_id = create_check_session_id_script(spark_work_dir)\n    \"\"\"\n    ${check_session_id}\n\n    while true; do\n\n        if [[ -e ${spark_master_log_name} ]]; then\n            test_uri=`grep -o \"\\\\(spark://.*\\$\\\\)\" ${spark_master_log_name} || true`\n            if [[ ! -z \\${test_uri} ]]; then\n                echo \"Spark master started at \\${test_uri}\"\n                break\n            fi\n        fi\n\n        if [[ -e \"${terminate_file_name}\" ]]; then\n            echo \"Terminate file ${terminate_file_name} found\"\n            exit 1\n        fi\n\n        if (( \\${SECONDS} > \\${MAX_WAIT_SECS} )); then\n            echo \"Timed out after \\${SECONDS} seconds while waiting for spark master <- ${spark_master_log_name}\"\n            cat ${spark_master_log_name} >&2\n            exit 2\n        fi\n\n        sleep \\${SLEEP_SECS}\n        SECONDS=\\$(( \\${SECONDS} + \\${SLEEP_SECS} ))\n\n\n    done\n    spark_uri=\\${test_uri}\n    \"\"\"\n}",
        "nb_lignes_process": 46,
        "string_script": "    def spark_master_log_name = get_spark_master_log(spark_work_dir)\n    def terminate_file_name = get_terminate_file_name(spark_work_dir, terminate_name)\n    def check_session_id = create_check_session_id_script(spark_work_dir)\n    \"\"\"\n    ${check_session_id}\n\n    while true; do\n\n        if [[ -e ${spark_master_log_name} ]]; then\n            test_uri=`grep -o \"\\\\(spark://.*\\$\\\\)\" ${spark_master_log_name} || true`\n            if [[ ! -z \\${test_uri} ]]; then\n                echo \"Spark master started at \\${test_uri}\"\n                break\n            fi\n        fi\n\n        if [[ -e \"${terminate_file_name}\" ]]; then\n            echo \"Terminate file ${terminate_file_name} found\"\n            exit 1\n        fi\n\n        if (( \\${SECONDS} > \\${MAX_WAIT_SECS} )); then\n            echo \"Timed out after \\${SECONDS} seconds while waiting for spark master <- ${spark_master_log_name}\"\n            cat ${spark_master_log_name} >&2\n            exit 2\n        fi\n\n        sleep \\${SLEEP_SECS}\n        SECONDS=\\$(( \\${SECONDS} + \\${SLEEP_SECS} ))\n\n\n    done\n    spark_uri=\\${test_uri}\n    \"\"\"",
        "nb_lignes_script": 33,
        "language_script": "bash",
        "tools": [
            "BreakSeq"
        ],
        "tools_url": [
            "https://bio.tools/breakseq"
        ],
        "tools_dico": [
            {
                "name": "BreakSeq",
                "uri": "https://bio.tools/breakseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "Structural variation"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "Genomic structural variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "DNA structural variation"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Database of known human breakpoint junctions and software to search short reads against them.",
                "homepage": "http://sv.gersteinlab.org/breakseq/"
            }
        ],
        "inputs": [
            "spark_work_dir",
            "terminate_name"
        ],
        "nb_inputs": 2,
        "outputs": [
            "terminate_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "JaneliaSciComp__nextflow-spark",
        "directive": [
            "container { \"${params.spark_container_repo}/${params.spark_container_name}:${params.spark_container_version}\" }",
            "label 'small'"
        ],
        "when": "",
        "stub": ""
    },
    "spark_worker": {
        "name_process": "spark_worker",
        "string_process": "\nprocess spark_worker {\n    container = \"${params.spark_container_repo}/${params.spark_container_name}:${params.spark_container_version}\"\n    cpus { worker_cores }\n                                                                         \n    memory \"${worker_mem_in_gb+1} GB\"\n\n    input:\n    val(spark_master_uri)\n    val(worker_id)\n    val(spark_conf)\n    val(spark_work_dir)\n    val(worker_cores)\n    val(worker_mem_in_gb)\n    val(terminate_name)\n    \n    output:\n\n    script:\n    def terminate_file_name = get_terminate_file_name(spark_work_dir, terminate_name)\n    def spark_worker_log_file = get_spark_worker_log(spark_work_dir, worker_id)\n    def spark_config_name = get_spark_config_name(spark_conf, spark_work_dir)\n    def spark_config_env\n    def spark_config_arg\n    def spark_worker_opts_list = []\n    spark_worker_opts_list << \"spark.worker.cleanup.enabled=true\"\n    spark_worker_opts_list << \"spark.worker.cleanup.interval=30\"\n    spark_worker_opts_list << \"spark.worker.cleanup.appDataTtl=1\"\n    spark_worker_opts_list << \"spark.port.maxRetries=${params.max_connect_retries}\"\n    def spark_worker_opts_string = spark_worker_opts_list.inject('') {\n        arg, item -> \"${arg} -D${item}\"\n    }\n    def spark_worker_opts=\"export SPARK_WORKER_OPTS=\\\"${spark_worker_opts_string}\\\"\"\n    if (spark_config_name != '') {\n        spark_config_arg = \"--properties-file ${spark_config_name}\"\n        spark_config_env = \"\"\"\n        ${spark_worker_opts}\n        \"\"\"\n    } else {\n        spark_config_arg = \"\"\n        spark_config_env = \"\"\"\n        ${spark_worker_opts}\n        export SPARK_CONF_DIR=${spark_conf}\n        \"\"\"\n    }\n\n    def spark_env = create_spark_env(spark_work_dir, spark_config_env, task.ext.sparkLocation)\n    def lookup_ip_script = create_lookup_ip_script()\n    def check_session_id = create_check_session_id_script(spark_work_dir)\n    \"\"\"\n    echo \"Starting spark worker ${worker_id} - logging to ${spark_worker_log_file}\"\n    ${check_session_id}\n\n    rm -f ${spark_worker_log_file} || true\n    ${spark_env}\n    ${lookup_ip_script}\n\n    echo \"\\\n    ${task.ext.sparkLocation}/bin/spark-class org.apache.spark.deploy.worker.Worker \\\n    ${spark_master_uri} \\\n    -c ${worker_cores} \\\n    -m ${worker_mem_in_gb}G \\\n    -d ${spark_work_dir} \\\n    -h \\$SPARK_LOCAL_IP \\\n    ${spark_config_arg} \\\n    \"\n\n    ${task.ext.sparkLocation}/bin/spark-class org.apache.spark.deploy.worker.Worker \\\n    ${spark_master_uri} \\\n    -c ${worker_cores} \\\n    -m ${worker_mem_in_gb}G \\\n    -d ${spark_work_dir} \\\n    -h \\$SPARK_LOCAL_IP \\\n    ${spark_config_arg} \\\n    &> ${spark_worker_log_file} &\n    spid=\\$!\n    ${wait_to_terminate('spid', terminate_file_name, spark_worker_log_file)}\n    \"\"\"\n}",
        "nb_lignes_process": 77,
        "string_script": "    def terminate_file_name = get_terminate_file_name(spark_work_dir, terminate_name)\n    def spark_worker_log_file = get_spark_worker_log(spark_work_dir, worker_id)\n    def spark_config_name = get_spark_config_name(spark_conf, spark_work_dir)\n    def spark_config_env\n    def spark_config_arg\n    def spark_worker_opts_list = []\n    spark_worker_opts_list << \"spark.worker.cleanup.enabled=true\"\n    spark_worker_opts_list << \"spark.worker.cleanup.interval=30\"\n    spark_worker_opts_list << \"spark.worker.cleanup.appDataTtl=1\"\n    spark_worker_opts_list << \"spark.port.maxRetries=${params.max_connect_retries}\"\n    def spark_worker_opts_string = spark_worker_opts_list.inject('') {\n        arg, item -> \"${arg} -D${item}\"\n    }\n    def spark_worker_opts=\"export SPARK_WORKER_OPTS=\\\"${spark_worker_opts_string}\\\"\"\n    if (spark_config_name != '') {\n        spark_config_arg = \"--properties-file ${spark_config_name}\"\n        spark_config_env = \"\"\"\n        ${spark_worker_opts}\n        \"\"\"\n    } else {\n        spark_config_arg = \"\"\n        spark_config_env = \"\"\"\n        ${spark_worker_opts}\n        export SPARK_CONF_DIR=${spark_conf}\n        \"\"\"\n    }\n\n    def spark_env = create_spark_env(spark_work_dir, spark_config_env, task.ext.sparkLocation)\n    def lookup_ip_script = create_lookup_ip_script()\n    def check_session_id = create_check_session_id_script(spark_work_dir)\n    \"\"\"\n    echo \"Starting spark worker ${worker_id} - logging to ${spark_worker_log_file}\"\n    ${check_session_id}\n\n    rm -f ${spark_worker_log_file} || true\n    ${spark_env}\n    ${lookup_ip_script}\n\n    echo \"\\\n    ${task.ext.sparkLocation}/bin/spark-class org.apache.spark.deploy.worker.Worker \\\n    ${spark_master_uri} \\\n    -c ${worker_cores} \\\n    -m ${worker_mem_in_gb}G \\\n    -d ${spark_work_dir} \\\n    -h \\$SPARK_LOCAL_IP \\\n    ${spark_config_arg} \\\n    \"\n\n    ${task.ext.sparkLocation}/bin/spark-class org.apache.spark.deploy.worker.Worker \\\n    ${spark_master_uri} \\\n    -c ${worker_cores} \\\n    -m ${worker_mem_in_gb}G \\\n    -d ${spark_work_dir} \\\n    -h \\$SPARK_LOCAL_IP \\\n    ${spark_config_arg} \\\n    &> ${spark_worker_log_file} &\n    spid=\\$!\n    ${wait_to_terminate('spid', terminate_file_name, spark_worker_log_file)}\n    \"\"\"",
        "nb_lignes_script": 58,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "spark_master_uri",
            "worker_id",
            "spark_conf",
            "spark_work_dir",
            "worker_cores",
            "worker_mem_in_gb",
            "terminate_name"
        ],
        "nb_inputs": 7,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "JaneliaSciComp__nextflow-spark",
        "directive": [
            "container = \"${params.spark_container_repo}/${params.spark_container_name}:${params.spark_container_version}\"",
            "cpus { worker_cores }",
            "memory \"${worker_mem_in_gb+1} GB\""
        ],
        "when": "",
        "stub": ""
    },
    "wait_for_worker": {
        "name_process": "wait_for_worker",
        "string_process": "\nprocess wait_for_worker {\n    container = \"${params.spark_container_repo}/${params.spark_container_name}:${params.spark_container_version}\"\n    label 'small'\n\n    input:\n    val(spark_master_uri)\n    val(spark_work_dir)\n    val(terminate_name)\n    val(worker_id)\n\n    output:\n    tuple val(spark_master_uri),\n          val(spark_work_dir),\n          val(terminate_name),\n          val(worker_id)\n\n    script:\n    def terminate_file_name = get_terminate_file_name(spark_work_dir, terminate_name)\n    def spark_worker_log_file = get_spark_worker_log(spark_work_dir, worker_id)\n    def check_session_id = create_check_session_id_script(spark_work_dir)\n    \"\"\"\n    ${check_session_id}\n\n    while true; do\n\n        if [[ -e \"${spark_worker_log_file}\" ]]; then\n            found=`grep -o \"\\\\(Worker: Successfully registered with master ${spark_master_uri}\\\\)\" ${spark_worker_log_file} || true`\n\n            if [[ ! -z \\${found} ]]; then\n                echo \"\\${found}\"\n                break\n            fi\n        fi\n\n        if [[ -e \"${terminate_file_name}\" ]]; then\n            echo \"Terminate file ${terminate_file_name} found\"\n            exit 1\n        fi\n\n        if (( \\${SECONDS} > \\${MAX_WAIT_SECS} )); then\n            echo \"Spark worker ${worker_id} timed out after \\${SECONDS} seconds while waiting for master ${spark_master_uri}\"\n            cat ${spark_worker_log_file} >&2\n            exit 2\n        fi\n\n        sleep \\${SLEEP_SECS}\n        SECONDS=\\$(( \\${SECONDS} + \\${SLEEP_SECS} ))\n\n    done\n    \"\"\"\n}",
        "nb_lignes_process": 50,
        "string_script": "    def terminate_file_name = get_terminate_file_name(spark_work_dir, terminate_name)\n    def spark_worker_log_file = get_spark_worker_log(spark_work_dir, worker_id)\n    def check_session_id = create_check_session_id_script(spark_work_dir)\n    \"\"\"\n    ${check_session_id}\n\n    while true; do\n\n        if [[ -e \"${spark_worker_log_file}\" ]]; then\n            found=`grep -o \"\\\\(Worker: Successfully registered with master ${spark_master_uri}\\\\)\" ${spark_worker_log_file} || true`\n\n            if [[ ! -z \\${found} ]]; then\n                echo \"\\${found}\"\n                break\n            fi\n        fi\n\n        if [[ -e \"${terminate_file_name}\" ]]; then\n            echo \"Terminate file ${terminate_file_name} found\"\n            exit 1\n        fi\n\n        if (( \\${SECONDS} > \\${MAX_WAIT_SECS} )); then\n            echo \"Spark worker ${worker_id} timed out after \\${SECONDS} seconds while waiting for master ${spark_master_uri}\"\n            cat ${spark_worker_log_file} >&2\n            exit 2\n        fi\n\n        sleep \\${SLEEP_SECS}\n        SECONDS=\\$(( \\${SECONDS} + \\${SLEEP_SECS} ))\n\n    done\n    \"\"\"",
        "nb_lignes_script": 32,
        "language_script": "bash",
        "tools": [
            "BreakSeq"
        ],
        "tools_url": [
            "https://bio.tools/breakseq"
        ],
        "tools_dico": [
            {
                "name": "BreakSeq",
                "uri": "https://bio.tools/breakseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "Structural variation"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "Genomic structural variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "DNA structural variation"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Database of known human breakpoint junctions and software to search short reads against them.",
                "homepage": "http://sv.gersteinlab.org/breakseq/"
            }
        ],
        "inputs": [
            "spark_master_uri",
            "spark_work_dir",
            "terminate_name",
            "worker_id"
        ],
        "nb_inputs": 4,
        "outputs": [
            "worker_id"
        ],
        "nb_outputs": 1,
        "name_workflow": "JaneliaSciComp__nextflow-spark",
        "directive": [
            "container = \"${params.spark_container_repo}/${params.spark_container_name}:${params.spark_container_version}\"",
            "label 'small'"
        ],
        "when": "",
        "stub": ""
    },
    "spark_start_app": {
        "name_process": "spark_start_app",
        "string_process": "\nprocess spark_start_app {\n    container = \"${params.spark_container_repo}/${params.spark_container_name}:${params.spark_container_version}\"\n    cpus { driver_cores == 0 ? 1 : driver_cores }\n    memory { driver_memory.replace('k',\" KB\").replace('m',\" MB\").replace('g',\" GB\").replace('t',\" TB\") }\n\n    input:\n    val(spark_uri)\n    val(spark_conf)\n    val(spark_work_dir)\n    val(workers)\n    val(executor_cores_param)\n    val(mem_per_core_in_gb)\n    val(driver_cores)\n    val(driver_memory)\n    val(driver_stack_size)\n    val(driver_logconfig)\n    val(driver_deploy_mode)\n    val(app)\n    val(app_main)\n    val(app_args)\n    val(app_log)\n\n    output:\n    tuple val(spark_uri), val(spark_work_dir)\n    \n    script:\n                          \n    def submit_args_list = []\n    submit_args_list << \"--master\" << spark_uri\n    if (app_main != \"\") {\n        submit_args_list << \"--class ${app_main}\"\n    }\n    submit_args_list << \"--conf\"\n    def executor_cores = executor_cores_param as int\n    submit_args_list.add(\"spark.executor.cores=${executor_cores}\")\n    def parallelism = workers * executor_cores\n    if (parallelism > 0) {\n        submit_args_list << \"--conf\" << \"spark.files.openCostInBytes=0\"\n        submit_args_list << \"--conf\" << \"spark.default.parallelism=${parallelism}\"\n    }\n    def executor_memory = calc_executor_memory(executor_cores, mem_per_core_in_gb)\n    if (executor_memory > 0) {\n        submit_args_list << \"--executor-memory\" << \"${executor_memory}g\"\n    }\n    if (driver_cores > 0) {\n        submit_args_list << \"--conf\" << \"spark.driver.cores=${driver_cores}\"\n    }\n    if (driver_memory != '') {\n        submit_args_list << \"--driver-memory\" << driver_memory\n    }\n    def sparkDriverJavaOpts = []\n    if (driver_logconfig != null && driver_logconfig != '') {\n        submit_args_list << \"--conf\" << \"spark.executor.extraJavaOptions=-Dlog4j.configuration=file://${driver_logconfig}\"\n        sparkDriverJavaOpts << \"-Dlog4j.configuration=file://${driver_logconfig}\"\n    }\n    if (driver_stack_size != null && driver_stack_size != '') {\n        sparkDriverJavaOpts << \"-Xss${driver_stack_size}\"\n    }\n    if (sparkDriverJavaOpts.size() > 0) {\n        submit_args_list << \"--driver-java-options\"\n        submit_args_list << '\"' + sparkDriverJavaOpts.join(' ') + '\"'\n    }\n    submit_args_list << app << app_args\n    def submit_args = submit_args_list.join(' ')\n    def deploy_mode_arg = ''\n    def spark_config_name = get_spark_config_name(spark_conf, spark_work_dir)\n    if (driver_deploy_mode != null && driver_deploy_mode != '') {\n        deploy_mode_arg = \"--deploy-mode ${driver_deploy_mode}\"\n    }\n    def spark_config_env\n    def spark_config_arg\n    if (spark_config_name != '') {\n        spark_config_arg = \"--properties-file ${spark_config_name}\"\n        spark_config_env = \"\"\n    } else {\n        spark_config_arg = \"\"\n        spark_config_env = \"export SPARK_CONF_DIR=${spark_conf}\"\n    }\n    def spark_driver_log_file = get_spark_driver_log(spark_work_dir, app_log)\n    def spark_env = create_spark_env(spark_work_dir, spark_config_env, task.ext.sparkLocation)\n    def lookup_ip_script = create_lookup_ip_script()\n    def check_session_id = create_check_session_id_script(spark_work_dir)\n    \"\"\"\n    echo \"Starting the spark driver\"\n    ${check_session_id}\n\n    ${spark_env}\n\n    ${lookup_ip_script}\n\n    ${task.ext.sparkLocation}/bin/spark-class org.apache.spark.deploy.SparkSubmit \\\n    ${spark_config_arg} \\\n    ${deploy_mode_arg} \\\n    --conf spark.driver.host=\\${SPARK_LOCAL_IP} \\\n    --conf spark.driver.bindAddress=\\${SPARK_LOCAL_IP} \\\n    ${submit_args}\n    \"\"\"\n}",
        "nb_lignes_process": 97,
        "string_script": "    def submit_args_list = []\n    submit_args_list << \"--master\" << spark_uri\n    if (app_main != \"\") {\n        submit_args_list << \"--class ${app_main}\"\n    }\n    submit_args_list << \"--conf\"\n    def executor_cores = executor_cores_param as int\n    submit_args_list.add(\"spark.executor.cores=${executor_cores}\")\n    def parallelism = workers * executor_cores\n    if (parallelism > 0) {\n        submit_args_list << \"--conf\" << \"spark.files.openCostInBytes=0\"\n        submit_args_list << \"--conf\" << \"spark.default.parallelism=${parallelism}\"\n    }\n    def executor_memory = calc_executor_memory(executor_cores, mem_per_core_in_gb)\n    if (executor_memory > 0) {\n        submit_args_list << \"--executor-memory\" << \"${executor_memory}g\"\n    }\n    if (driver_cores > 0) {\n        submit_args_list << \"--conf\" << \"spark.driver.cores=${driver_cores}\"\n    }\n    if (driver_memory != '') {\n        submit_args_list << \"--driver-memory\" << driver_memory\n    }\n    def sparkDriverJavaOpts = []\n    if (driver_logconfig != null && driver_logconfig != '') {\n        submit_args_list << \"--conf\" << \"spark.executor.extraJavaOptions=-Dlog4j.configuration=file://${driver_logconfig}\"\n        sparkDriverJavaOpts << \"-Dlog4j.configuration=file://${driver_logconfig}\"\n    }\n    if (driver_stack_size != null && driver_stack_size != '') {\n        sparkDriverJavaOpts << \"-Xss${driver_stack_size}\"\n    }\n    if (sparkDriverJavaOpts.size() > 0) {\n        submit_args_list << \"--driver-java-options\"\n        submit_args_list << '\"' + sparkDriverJavaOpts.join(' ') + '\"'\n    }\n    submit_args_list << app << app_args\n    def submit_args = submit_args_list.join(' ')\n    def deploy_mode_arg = ''\n    def spark_config_name = get_spark_config_name(spark_conf, spark_work_dir)\n    if (driver_deploy_mode != null && driver_deploy_mode != '') {\n        deploy_mode_arg = \"--deploy-mode ${driver_deploy_mode}\"\n    }\n    def spark_config_env\n    def spark_config_arg\n    if (spark_config_name != '') {\n        spark_config_arg = \"--properties-file ${spark_config_name}\"\n        spark_config_env = \"\"\n    } else {\n        spark_config_arg = \"\"\n        spark_config_env = \"export SPARK_CONF_DIR=${spark_conf}\"\n    }\n    def spark_driver_log_file = get_spark_driver_log(spark_work_dir, app_log)\n    def spark_env = create_spark_env(spark_work_dir, spark_config_env, task.ext.sparkLocation)\n    def lookup_ip_script = create_lookup_ip_script()\n    def check_session_id = create_check_session_id_script(spark_work_dir)\n    \"\"\"\n    echo \"Starting the spark driver\"\n    ${check_session_id}\n\n    ${spark_env}\n\n    ${lookup_ip_script}\n\n    ${task.ext.sparkLocation}/bin/spark-class org.apache.spark.deploy.SparkSubmit \\\n    ${spark_config_arg} \\\n    ${deploy_mode_arg} \\\n    --conf spark.driver.host=\\${SPARK_LOCAL_IP} \\\n    --conf spark.driver.bindAddress=\\${SPARK_LOCAL_IP} \\\n    ${submit_args}\n    \"\"\"",
        "nb_lignes_script": 69,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "spark_uri",
            "spark_conf",
            "spark_work_dir",
            "workers",
            "executor_cores_param",
            "mem_per_core_in_gb",
            "driver_cores",
            "driver_memory",
            "driver_stack_size",
            "driver_logconfig",
            "driver_deploy_mode",
            "app",
            "app_main",
            "app_args",
            "app_log"
        ],
        "nb_inputs": 15,
        "outputs": [
            "spark_work_dir"
        ],
        "nb_outputs": 1,
        "name_workflow": "JaneliaSciComp__nextflow-spark",
        "directive": [
            "container = \"${params.spark_container_repo}/${params.spark_container_name}:${params.spark_container_version}\"",
            "cpus { driver_cores == 0 ? 1 : driver_cores }",
            "memory { driver_memory.replace('k',\" KB\").replace('m',\" MB\").replace('g',\" GB\").replace('t',\" TB\") }"
        ],
        "when": "",
        "stub": ""
    },
    "terminate_spark": {
        "name_process": "terminate_spark",
        "string_process": "\nprocess terminate_spark {\n    container = \"${params.spark_container_repo}/${params.spark_container_name}:${params.spark_container_version}\"\n    label 'small'\n\n    input:\n    val(spark_work_dir)\n    val(terminate_name)\n\n    output:\n    tuple val(terminate_file_name), val(spark_work_dir)\n\n    script:\n    terminate_file_name = get_terminate_file_name(spark_work_dir, terminate_name)\n    check_session_id = create_check_session_id_script(spark_work_dir)\n    \"\"\"\n    ${check_session_id}\n\n    cat > ${terminate_file_name} <<EOF\n    DONE\n    EOF\n    cat ${terminate_file_name}\n    \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "    terminate_file_name = get_terminate_file_name(spark_work_dir, terminate_name)\n    check_session_id = create_check_session_id_script(spark_work_dir)\n    \"\"\"\n    ${check_session_id}\n\n    cat > ${terminate_file_name} <<EOF\n    DONE\n    EOF\n    cat ${terminate_file_name}\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [
            "ProdoNet",
            "NeoFuse"
        ],
        "tools_url": [
            "https://bio.tools/prodonet",
            "https://bio.tools/NeoFuse"
        ],
        "tools_dico": [
            {
                "name": "ProdoNet",
                "uri": "https://bio.tools/prodonet",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0602",
                            "term": "Molecular interactions, pathways and networks"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3407",
                            "term": "Endocrinology and metabolism"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0204",
                            "term": "Gene regulation"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3407",
                            "term": "https://en.wikipedia.org/wiki/Endocrinology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3660",
                                    "term": "Metabolic network modelling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0533",
                                    "term": "Expression profile pathway mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3439",
                                    "term": "Pathway or network prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1781",
                                    "term": "Gene regulatory network analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3463",
                                    "term": "Expression correlation analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3660",
                                    "term": "http://edamontology.org/Metabolic%20pathway%20modelling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0533",
                                    "term": "Pathway mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3463",
                                    "term": "Co-expression analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A web-based application for the mapping of prokaryotic genes and the corresponding proteins to common gene regulatory and metabolic networks. Users input a list of genes from which shared operons, co-expressed genes and shared regulators are detected. Common metabolic pathways are then viewed on KEGG maps.",
                "homepage": "http://www.prodonet.tu-bs.de"
            },
            {
                "name": "NeoFuse",
                "uri": "https://bio.tools/NeoFuse",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2830",
                            "term": "Immunoproteins and antigens"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2640",
                            "term": "Oncology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "Gene transcripts"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2640",
                            "term": "Cancer biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2640",
                            "term": "https://en.wikipedia.org/wiki/Oncology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "mRNA features"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0310",
                                    "term": "Sequence assembly"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0252",
                                    "term": "Peptide immunogenicity prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3799",
                                    "term": "Quantification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0252",
                                    "term": "Immunogenicity prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0252",
                                    "term": "Antigenicity prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3799",
                                    "term": "Quantitation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Predicting fusion neoantigens from RNA sequencing data.\n\nThe Section for Bioinformatrics at the Biocenter of Innsbruck Medical University is commited to the generation, management, integration, and leveraging data from genomics studies.\n\nQuantification of the tumor immune contexture.\n\nZlatko Trajanoski awarded with ERC Advanced Grant.",
                "homepage": "https://icbi.i-med.ac.at/NeoFuse/"
            }
        ],
        "inputs": [
            "spark_work_dir",
            "terminate_name"
        ],
        "nb_inputs": 2,
        "outputs": [
            "spark_work_dir"
        ],
        "nb_outputs": 1,
        "name_workflow": "JaneliaSciComp__nextflow-spark",
        "directive": [
            "container = \"${params.spark_container_repo}/${params.spark_container_name}:${params.spark_container_version}\"",
            "label 'small'"
        ],
        "when": "",
        "stub": ""
    }
}