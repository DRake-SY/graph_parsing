{
    "preProcess": {
        "name_process": "preProcess",
        "string_process": "\nprocess preProcess {\n  input:\n  set val(name), file(reads) from raw_reads\n\n  output:\n  tuple name, file(outfiles) into read_files_fastqc, read_files_trimming, read_files_kraken\n\n  script:\n  if(params.name_split_on!=\"\"){\n    name = name.split(params.name_split_on)[0]\n    outfiles = [\"${name}_R1.fastq.gz\",\"${name}_R2.fastq.gz\"]\n    \"\"\"\n    mv ${reads[0]} ${name}_R1.fastq.gz\n    mv ${reads[1]} ${name}_R2.fastq.gz\n    \"\"\"\n  }else{\n    outfiles = reads\n    \"\"\"\n    \"\"\"\n  }\n}",
        "nb_lignes_process": 20,
        "string_script": "  if(params.name_split_on!=\"\"){\n    name = name.split(params.name_split_on)[0]\n    outfiles = [\"${name}_R1.fastq.gz\",\"${name}_R2.fastq.gz\"]\n    \"\"\"\n    mv ${reads[0]} ${name}_R1.fastq.gz\n    mv ${reads[1]} ${name}_R2.fastq.gz\n    \"\"\"\n  }else{\n    outfiles = reads\n    \"\"\"\n    \"\"\"\n  }",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "goname"
        ],
        "tools_url": [
            "https://bio.tools/goname"
        ],
        "tools_dico": [
            {
                "name": "goname",
                "uri": "https://bio.tools/goname",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0089",
                            "term": "Ontology and terminology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data retrieval"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data extraction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Retrieval"
                                }
                            ]
                        ],
                        "input": [],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2223",
                                "term": "Ontology metadata"
                            }
                        ]
                    }
                ],
                "description": "Find GO ontology terms by name.",
                "homepage": "http://emboss.open-bio.org/rel/rel6/apps/goname.html"
            }
        ],
        "inputs": [
            "raw_reads"
        ],
        "nb_inputs": 1,
        "outputs": [
            "read_files_fastqc",
            "read_files_trimming",
            "read_files_kraken"
        ],
        "nb_outputs": 3,
        "name_workflow": "wslh-bio__dryad",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "clean_reads": {
        "name_process": "clean_reads",
        "string_process": "\nprocess clean_reads {\n  tag \"$name\"\n  publishDir \"${params.outdir}/trimming\", mode: 'copy',pattern:\"*.trim.txt\"\n\n  input:\n  set val(name), file(reads) from read_files_trimming\n\n  output:\n  tuple name, file(\"${name}_clean{_1,_2}.fastq.gz\") into cleaned_reads_shovill, cleaned_reads_fastqc, cleaned_reads_mapping, cleaned_reads_kraken\n  file(\"${name}_clean{_1,_2}.fastq.gz\") into cleaned_reads_snp\n  file(\"${name}.phix.stats.txt\") into phix_cleanning_stats\n  file(\"${name}.adapters.stats.txt\") into adapter_cleanning_stats\n  tuple file(\"${name}.phix.stats.txt\"),file(\"${name}.adapters.stats.txt\"),file(\"${name}.trim.txt\") into multiqc_clean_reads\n  file(\"${name}.trim.txt\") into bbduk_files\n\n  script:\n  \"\"\"\n  bbduk.sh in1=${reads[0]} in2=${reads[1]} out1=${name}.trimmed_1.fastq.gz out2=${name}.trimmed_2.fastq.gz qtrim=${params.trimdirection} qtrim=${params.qualitytrimscore} minlength=${params.minlength} tbo tbe &> ${name}.out\n  repair.sh in1=${name}.trimmed_1.fastq.gz in2=${name}.trimmed_2.fastq.gz out1=${name}.paired_1.fastq.gz out2=${name}.paired_2.fastq.gz\n  bbduk.sh in1=${name}.paired_1.fastq.gz in2=${name}.paired_2.fastq.gz out1=${name}.rmadpt_1.fastq.gz out2=${name}.rmadpt_2.fastq.gz ref=/bbmap/resources/adapters.fa stats=${name}.adapters.stats.txt ktrim=r k=23 mink=11 hdist=1 tpe tbo\n  bbduk.sh in1=${name}.rmadpt_1.fastq.gz in2=${name}.rmadpt_2.fastq.gz out1=${name}_clean_1.fastq.gz out2=${name}_clean_2.fastq.gz outm=${name}.matched_phix.fq ref=/bbmap/resources/phix174_ill.ref.fa.gz k=31 hdist=1 stats=${name}.phix.stats.txt\n  grep -E 'Input:|QTrimmed:|Trimmed by overlap:|Total Removed:|Result:' ${name}.out > ${name}.trim.txt\n  \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "  \"\"\"\n  bbduk.sh in1=${reads[0]} in2=${reads[1]} out1=${name}.trimmed_1.fastq.gz out2=${name}.trimmed_2.fastq.gz qtrim=${params.trimdirection} qtrim=${params.qualitytrimscore} minlength=${params.minlength} tbo tbe &> ${name}.out\n  repair.sh in1=${name}.trimmed_1.fastq.gz in2=${name}.trimmed_2.fastq.gz out1=${name}.paired_1.fastq.gz out2=${name}.paired_2.fastq.gz\n  bbduk.sh in1=${name}.paired_1.fastq.gz in2=${name}.paired_2.fastq.gz out1=${name}.rmadpt_1.fastq.gz out2=${name}.rmadpt_2.fastq.gz ref=/bbmap/resources/adapters.fa stats=${name}.adapters.stats.txt ktrim=r k=23 mink=11 hdist=1 tpe tbo\n  bbduk.sh in1=${name}.rmadpt_1.fastq.gz in2=${name}.rmadpt_2.fastq.gz out1=${name}_clean_1.fastq.gz out2=${name}_clean_2.fastq.gz outm=${name}.matched_phix.fq ref=/bbmap/resources/phix174_ill.ref.fa.gz k=31 hdist=1 stats=${name}.phix.stats.txt\n  grep -E 'Input:|QTrimmed:|Trimmed by overlap:|Total Removed:|Result:' ${name}.out > ${name}.trim.txt\n  \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "totalVI"
        ],
        "tools_url": [
            "https://bio.tools/totalVI"
        ],
        "tools_dico": [
            {
                "name": "totalVI",
                "uri": "https://bio.tools/totalVI",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "Gene transcripts"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2229",
                            "term": "Cell biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0108",
                            "term": "Protein expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0166",
                            "term": "Protein structural motifs and surfaces"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "mRNA features"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0166",
                            "term": "Protein 3D motifs"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3435",
                                    "term": "Standardisation and normalisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3630",
                                    "term": "Protein quantification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression data analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3630",
                                    "term": "Protein quantitation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A Joint Model of RNA Expression and Surface Protein Abundance in Single Cells.",
                "homepage": "http://github.com/adamgayoso/totalVI_reproducibility"
            }
        ],
        "inputs": [
            "read_files_trimming"
        ],
        "nb_inputs": 1,
        "outputs": [
            "cleaned_reads_shovill",
            "cleaned_reads_fastqc",
            "cleaned_reads_mapping",
            "cleaned_reads_kraken",
            "cleaned_reads_snp",
            "phix_cleanning_stats",
            "adapter_cleanning_stats",
            "multiqc_clean_reads",
            "bbduk_files"
        ],
        "nb_outputs": 9,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "tag \"$name\"",
            "publishDir \"${params.outdir}/trimming\", mode: 'copy',pattern:\"*.trim.txt\""
        ],
        "when": "",
        "stub": ""
    },
    "bbduk_summary": {
        "name_process": "bbduk_summary",
        "string_process": "\nprocess bbduk_summary {\n  publishDir \"${params.outdir}/trimming\",mode:'copy'\n\n  input:\n  file(files) from bbduk_files.collect()\n\n  output:\n  file(\"bbduk_results.tsv\") into bbduk_tsv\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n\n  # function for summarizing bbduk output\n  def summarize_bbduk(file):\n      # get sample id from file name and set up data list\n      sample_id = os.path.basename(file).split(\".\")[0]\n      data = []\n      data.append(sample_id)\n      with open(file,\"r\") as inFile:\n          for i, line in enumerate(inFile):\n              # get total number of reads\n              if i == 0:\n                  num_reads = line.strip().split(\"\\\\t\")[1].replace(\" reads \",\"\")\n                  data.append(num_reads)\n              # get total number of reads removed\n              if i == 3:\n                  rm_reads = line.strip().split(\"\\\\t\")[1].replace(\"reads \",\"\")\n                  rm_reads = rm_reads.rstrip()\n                  data.append(rm_reads)\n      return data\n\n  # get all bbduk output files\n  files = glob.glob(\"*.trim.txt\")\n\n  # summarize bbduk output files\n  results = map(summarize_bbduk,files)\n\n  # convert results to data frame and write to tsv\n  df = DataFrame(results,columns=['Sample','Total Reads','Reads Removed'])\n  df.to_csv(f'bbduk_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"\n}",
        "nb_lignes_process": 46,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n\n  # function for summarizing bbduk output\n  def summarize_bbduk(file):\n      # get sample id from file name and set up data list\n      sample_id = os.path.basename(file).split(\".\")[0]\n      data = []\n      data.append(sample_id)\n      with open(file,\"r\") as inFile:\n          for i, line in enumerate(inFile):\n              # get total number of reads\n              if i == 0:\n                  num_reads = line.strip().split(\"\\\\t\")[1].replace(\" reads \",\"\")\n                  data.append(num_reads)\n              # get total number of reads removed\n              if i == 3:\n                  rm_reads = line.strip().split(\"\\\\t\")[1].replace(\"reads \",\"\")\n                  rm_reads = rm_reads.rstrip()\n                  data.append(rm_reads)\n      return data\n\n  # get all bbduk output files\n  files = glob.glob(\"*.trim.txt\")\n\n  # summarize bbduk output files\n  results = map(summarize_bbduk,files)\n\n  # convert results to data frame and write to tsv\n  df = DataFrame(results,columns=['Sample','Total Reads','Reads Removed'])\n  df.to_csv(f'bbduk_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"",
        "nb_lignes_script": 35,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "bbduk_files"
        ],
        "nb_inputs": 1,
        "outputs": [
            "bbduk_tsv"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "publishDir \"${params.outdir}/trimming\",mode:'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "fastqc": {
        "name_process": "fastqc",
        "string_process": "\nprocess fastqc {\n  tag \"$name\"\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/fastqc\", mode: 'copy',saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n  input:\n  set val(name), file(reads) from combined_reads\n\n  output:\n  file(\"*_fastqc.{zip,html}\") into fastqc_results, fastqc_multiqc\n\n  script:\n  \"\"\"\n  fastqc -q  ${reads}\n  \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "  \"\"\"\n  fastqc -q  ${reads}\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "FastQC"
        ],
        "tools_url": [
            "https://bio.tools/fastqc"
        ],
        "tools_dico": [
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            }
        ],
        "inputs": [
            "combined_reads"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fastqc_results",
            "fastqc_multiqc"
        ],
        "nb_outputs": 2,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "tag \"$name\"",
            "errorStrategy 'ignore'",
            "publishDir \"${params.outdir}/fastqc\", mode: 'copy',saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}"
        ],
        "when": "",
        "stub": ""
    },
    "fastqc_summary": {
        "name_process": "fastqc_summary",
        "string_process": "\nprocess fastqc_summary {\n  publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n  input:\n  file(fastqc) from fastqc_results.collect()\n\n  output:\n  file(\"fastqc_summary.txt\") into fastqc_summary\n\n  shell:\n  \"\"\"\n  zips=`ls *.zip`\n  for i in \\$zips; do\n      unzip -o \\$i &>/dev/null;\n  done\n  fq_folders=\\${zips}\n  for folder in \\$fq_folders; do\n    folder=\\${folder%.*}\n    cat \\$folder/summary.txt >> fastqc_summary.txt\n    ls .\n  done;\n  sed -i 's/.fastq.gz//g' fastqc_summary.txt\n  \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "  \"\"\"\n  zips=`ls *.zip`\n  for i in \\$zips; do\n      unzip -o \\$i &>/dev/null;\n  done\n  fq_folders=\\${zips}\n  for folder in \\$fq_folders; do\n    folder=\\${folder%.*}\n    cat \\$folder/summary.txt >> fastqc_summary.txt\n    ls .\n  done;\n  sed -i 's/.fastq.gz//g' fastqc_summary.txt\n  \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "NullSeq"
        ],
        "tools_url": [
            "https://bio.tools/nullseq"
        ],
        "tools_dico": [
            {
                "name": "NullSeq",
                "uri": "https://bio.tools/nullseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0364",
                                    "term": "Random sequence generation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Creates Random Coding Sequences with specified GC content and Amino Acid usage.",
                "homepage": "https://github.com/amarallab/NullSeq"
            }
        ],
        "inputs": [
            "fastqc_results"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fastqc_summary"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "publishDir \"${params.outdir}/fastqc\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "kraken": {
        "name_process": "kraken",
        "string_process": "\nprocess kraken {\n  tag \"$name\"\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/kraken\", mode: 'copy', pattern: \"*.kraken2.txt*\"\n\n  input:\n  set val(name), file(reads) from cleaned_reads_kraken\n\n  output:\n  tuple name, file(\"${name}.kraken2.txt\") into kraken_files, kraken_multiqc\n  file(\"Kraken2_DB.txt\") into kraken_version\n\n  script:\n  \"\"\"\n  kraken2 --db /kraken2-db/minikraken2_v1_8GB --threads ${task.cpus} --report ${name}.kraken2.txt --paired ${reads[0]} ${reads[1]}\n\n  ls /kraken2-db/ > Kraken2_DB.txt\n  \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "  \"\"\"\n  kraken2 --db /kraken2-db/minikraken2_v1_8GB --threads ${task.cpus} --report ${name}.kraken2.txt --paired ${reads[0]} ${reads[1]}\n\n  ls /kraken2-db/ > Kraken2_DB.txt\n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "kraken2"
        ],
        "tools_url": [
            "https://bio.tools/kraken2"
        ],
        "tools_dico": [
            {
                "name": "kraken2",
                "uri": "https://bio.tools/kraken2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3028",
                                "term": "Taxonomy"
                            }
                        ]
                    }
                ],
                "description": "Kraken 2 is the newest version of Kraken, a taxonomic classification system using exact k-mer matches to achieve high accuracy and fast classification speeds. This classifier matches each k-mer within a query sequence to the lowest common ancestor (LCA) of all genomes containing the given k-mer. The k-mer assignments inform the classification algorithm.",
                "homepage": "https://ccb.jhu.edu/software/kraken2/"
            }
        ],
        "inputs": [
            "cleaned_reads_kraken"
        ],
        "nb_inputs": 1,
        "outputs": [
            "kraken_files",
            "kraken_multiqc",
            "kraken_version"
        ],
        "nb_outputs": 3,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "tag \"$name\"",
            "errorStrategy 'ignore'",
            "publishDir \"${params.outdir}/kraken\", mode: 'copy', pattern: \"*.kraken2.txt*\""
        ],
        "when": "",
        "stub": ""
    },
    "kraken_summary": {
        "name_process": "kraken_summary",
        "string_process": "\nprocess kraken_summary {\n  tag \"$name\"\n  publishDir \"${params.outdir}/kraken\",mode:'copy'\n\n  input:\n  file(files) from kraken_files.collect()\n\n  output:\n  file(\"kraken_results.tsv\") into kraken_tsv\n  file(\"kraken_results.tsv\") into kraken_prokka\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n  \n  # function for summarizing kraken2 report files\n  def summarize_kraken(file):\n      # get sample id from file name\n      sample_id = os.path.basename(file).split('.')[0].replace('.kraken2.txt','')\n      data = []\n      # read kraken2 report file\n      with open(file,'r') as inFile:\n          for line in inFile:\n              line = line.strip()\n              sline = line.split('\\\\t')\n              # get unclassified reads result (denoted by 'unclassified') and append to data\n              if sline[5] == 'unclassified':\n                  data.append(sline)\n              # get species results (denoted by 'S') and append to data\n              if sline[3] == 'S':\n                  data.append(sline)\n      # convert data list to data frame\n      data_df = DataFrame(data, columns=['Percentage','Num_Covered','Num_Assigned','Rank','TaxID','Name'])\n      # remove left leading spaces from the Name column\n      data_df['Name'] = data_df['Name'].str.lstrip()\n      # sort data frame by percentages (largest to smallest)\n      data_df = data_df.sort_values(by=['Percentage'], ascending=False)\n      # make new data frame for unclassified reads only\n      unclass = data_df[data_df['Name']=='unclassified']\n      # exception for if no unclassified reads found\n      if unclass.empty:\n          # import pandas as pd\n          lst = [['0','NA','NA','NA','NA','NA']]\n          unclass = pd.DataFrame(lst, columns =['Percentage','Num_Covered','Num_Assigned','Rank','TaxID','Name'])\n      # subset data frame by species\n      species_df = data_df[data_df['Name']!='unclassified']\n      # get first two species matches (first two largest percentages) in data frame\n      species_df = species_df.head(2)\n      # check if species data frame has two rows\n      if len(species_df) == 0:\n          # add two empty rows to species data frame\n          species_df = species_df.append(pd.Series(), ignore_index=True)\n          species_df = species_df.append(pd.Series(), ignore_index=True)\n      if len(species_df) == 1:\n          # add one empty row to species data frame\n          species_df = species_df.append(pd.Series(), ignore_index=True)\n      # concatenate unclassified data frame and species data frame\n      df_concat = pd.concat([unclass,species_df])\n      # add sample name column to concatenated data frame\n      df_concat = df_concat.assign(Sample=sample_id)\n      # keep only Sample Percentage and Name columns in concatenated data frame\n      df_concat = df_concat[['Sample','Percentage','Name']]\n      # reset index of concatenated data frame using drop parameter to avoid old index added as column\n      df_concat = df_concat.reset_index(drop=True)\n      # add percentage sign to unclassified column\n      unclassified = df_concat.iloc[0]['Percentage'] + '%'\n      # convert to lists\n      # if primary species is nan, replace with NA\n      if str(df_concat.iloc[1]['Name']) == 'nan':\n          primary_species = 'NA'\n      # otherwise convert to (#%)\n      else:\n          primary_species = df_concat.iloc[1]['Name'] + ' (' + df_concat.iloc[1]['Percentage'] + '%)'\n      # repeat for secondary species\n      if str(df_concat.iloc[2]['Name']) == 'nan':\n          secondary_species = 'NA'\n      else:\n          secondary_species = df_concat.iloc[2]['Name'] + ' (' + df_concat.iloc[2]['Percentage'] + '%)'\n      # list of lists\n      combined = [[sample_id, unclassified, primary_species, secondary_species]]\n      # convert list of lists to data frame\n      combined_df = DataFrame(combined, columns=['Sample','Unclassified Reads (%)','Primary Species (%)','Secondary Species (%)'])\n      return combined_df\n  # get all kraken2 report files\n  files = glob.glob(\"*.kraken2.txt*\")\n  # summarize kraken2 report files\n  results = map(summarize_kraken, files)\n  # concatenate summary results and write to tsv\n  data_concat = pd.concat(results)\n  data_concat.to_csv(f'kraken_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"\n}",
        "nb_lignes_process": 95,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n  \n  # function for summarizing kraken2 report files\n  def summarize_kraken(file):\n      # get sample id from file name\n      sample_id = os.path.basename(file).split('.')[0].replace('.kraken2.txt','')\n      data = []\n      # read kraken2 report file\n      with open(file,'r') as inFile:\n          for line in inFile:\n              line = line.strip()\n              sline = line.split('\\\\t')\n              # get unclassified reads result (denoted by 'unclassified') and append to data\n              if sline[5] == 'unclassified':\n                  data.append(sline)\n              # get species results (denoted by 'S') and append to data\n              if sline[3] == 'S':\n                  data.append(sline)\n      # convert data list to data frame\n      data_df = DataFrame(data, columns=['Percentage','Num_Covered','Num_Assigned','Rank','TaxID','Name'])\n      # remove left leading spaces from the Name column\n      data_df['Name'] = data_df['Name'].str.lstrip()\n      # sort data frame by percentages (largest to smallest)\n      data_df = data_df.sort_values(by=['Percentage'], ascending=False)\n      # make new data frame for unclassified reads only\n      unclass = data_df[data_df['Name']=='unclassified']\n      # exception for if no unclassified reads found\n      if unclass.empty:\n          # import pandas as pd\n          lst = [['0','NA','NA','NA','NA','NA']]\n          unclass = pd.DataFrame(lst, columns =['Percentage','Num_Covered','Num_Assigned','Rank','TaxID','Name'])\n      # subset data frame by species\n      species_df = data_df[data_df['Name']!='unclassified']\n      # get first two species matches (first two largest percentages) in data frame\n      species_df = species_df.head(2)\n      # check if species data frame has two rows\n      if len(species_df) == 0:\n          # add two empty rows to species data frame\n          species_df = species_df.append(pd.Series(), ignore_index=True)\n          species_df = species_df.append(pd.Series(), ignore_index=True)\n      if len(species_df) == 1:\n          # add one empty row to species data frame\n          species_df = species_df.append(pd.Series(), ignore_index=True)\n      # concatenate unclassified data frame and species data frame\n      df_concat = pd.concat([unclass,species_df])\n      # add sample name column to concatenated data frame\n      df_concat = df_concat.assign(Sample=sample_id)\n      # keep only Sample Percentage and Name columns in concatenated data frame\n      df_concat = df_concat[['Sample','Percentage','Name']]\n      # reset index of concatenated data frame using drop parameter to avoid old index added as column\n      df_concat = df_concat.reset_index(drop=True)\n      # add percentage sign to unclassified column\n      unclassified = df_concat.iloc[0]['Percentage'] + '%'\n      # convert to lists\n      # if primary species is nan, replace with NA\n      if str(df_concat.iloc[1]['Name']) == 'nan':\n          primary_species = 'NA'\n      # otherwise convert to (#%)\n      else:\n          primary_species = df_concat.iloc[1]['Name'] + ' (' + df_concat.iloc[1]['Percentage'] + '%)'\n      # repeat for secondary species\n      if str(df_concat.iloc[2]['Name']) == 'nan':\n          secondary_species = 'NA'\n      else:\n          secondary_species = df_concat.iloc[2]['Name'] + ' (' + df_concat.iloc[2]['Percentage'] + '%)'\n      # list of lists\n      combined = [[sample_id, unclassified, primary_species, secondary_species]]\n      # convert list of lists to data frame\n      combined_df = DataFrame(combined, columns=['Sample','Unclassified Reads (%)','Primary Species (%)','Secondary Species (%)'])\n      return combined_df\n  # get all kraken2 report files\n  files = glob.glob(\"*.kraken2.txt*\")\n  # summarize kraken2 report files\n  results = map(summarize_kraken, files)\n  # concatenate summary results and write to tsv\n  data_concat = pd.concat(results)\n  data_concat.to_csv(f'kraken_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"",
        "nb_lignes_script": 82,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "kraken_files"
        ],
        "nb_inputs": 1,
        "outputs": [
            "kraken_tsv",
            "kraken_prokka"
        ],
        "nb_outputs": 2,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "tag \"$name\"",
            "publishDir \"${params.outdir}/kraken\",mode:'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "shovill": {
        "name_process": "shovill",
        "string_process": "\nprocess shovill {\n  tag \"$name\"\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/assembled\", mode: 'copy',pattern:\"*.fa\"\n  publishDir \"${params.outdir}/mapping/sams\", mode: 'copy',pattern:\"*.sam\"\n\n  input:\n  set val(name), file(reads) from cleaned_reads_shovill\n\n  output:\n  tuple name, file(\"${name}.contigs.fa\") into assembled_genomes_quality, assembled_genomes_prokka\n  tuple name, file(\"${name}.assembly.sam\") into assembly_sams\n\n  script:\n  \"\"\"\n  shovill --cpus ${task.cpus} --ram ${task.memory} --outdir ./output --R1 ${reads[0]} --R2 ${reads[1]} --force\n  mv ./output/contigs.fa ${name}.contigs.fa\n  bwa index ${name}.contigs.fa\n  bwa mem ${name}.contigs.fa ${reads[0]} ${reads[1]} > ${name}.assembly.sam\n  \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "  \"\"\"\n  shovill --cpus ${task.cpus} --ram ${task.memory} --outdir ./output --R1 ${reads[0]} --R2 ${reads[1]} --force\n  mv ./output/contigs.fa ${name}.contigs.fa\n  bwa index ${name}.contigs.fa\n  bwa mem ${name}.contigs.fa ${reads[0]} ${reads[1]} > ${name}.assembly.sam\n  \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "shovill",
            "BWA"
        ],
        "tools_url": [
            "https://bio.tools/shovill",
            "https://bio.tools/bwa"
        ],
        "tools_dico": [
            {
                "name": "shovill",
                "uri": "https://bio.tools/shovill",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            }
                        ]
                    }
                ],
                "description": "Shovill is a pipeline for assembly of bacterial isolate genomes from Illumina paired-end reads.  Shovill uses SPAdes at its core, but alters the steps before and after the primary assembly step to get similar results in less time. Shovill also supports other assemblers like SKESA, Velvet and Megahit, so you can take advantage of the pre- and post-processing the Shovill provides with those too.",
                "homepage": "https://github.com/tseemann/shovill"
            },
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            }
        ],
        "inputs": [
            "cleaned_reads_shovill"
        ],
        "nb_inputs": 1,
        "outputs": [
            "assembled_genomes_quality",
            "assembled_genomes_prokka",
            "assembly_sams"
        ],
        "nb_outputs": 3,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "tag \"$name\"",
            "errorStrategy 'ignore'",
            "publishDir \"${params.outdir}/assembled\", mode: 'copy',pattern:\"*.fa\"",
            "publishDir \"${params.outdir}/mapping/sams\", mode: 'copy',pattern:\"*.sam\""
        ],
        "when": "",
        "stub": ""
    },
    "quast": {
        "name_process": "quast",
        "string_process": "\nprocess quast {\n  tag \"$name\"\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/quast\",mode:'copy',pattern: \"${name}.quast.tsv\"\n\n  input:\n  set val(name), file(assembly) from assembled_genomes_quality\n\n  output:\n  file(\"${name}.quast.tsv\") into quast_files\n  file(\"${name}.report.quast.tsv\") into quast_multiqc\n\n  script:\n  \"\"\"\n  quast.py ${assembly} -o .\n  mv report.tsv ${name}.report.quast.tsv\n  mv transposed_report.tsv ${name}.quast.tsv\n  \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "  \"\"\"\n  quast.py ${assembly} -o .\n  mv report.tsv ${name}.report.quast.tsv\n  mv transposed_report.tsv ${name}.quast.tsv\n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "assembled_genomes_quality"
        ],
        "nb_inputs": 1,
        "outputs": [
            "quast_files",
            "quast_multiqc"
        ],
        "nb_outputs": 2,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "tag \"$name\"",
            "errorStrategy 'ignore'",
            "publishDir \"${params.outdir}/quast\",mode:'copy',pattern: \"${name}.quast.tsv\""
        ],
        "when": "",
        "stub": ""
    },
    "quast_summary": {
        "name_process": "quast_summary",
        "string_process": "\nprocess quast_summary {\n  publishDir \"${params.outdir}/quast\",mode:'copy'\n\n  input:\n  file(files) from quast_files.collect()\n\n  output:\n  file(\"quast_results.tsv\") into quast_tsv\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n\n  files = glob.glob(\"*.quast.tsv\")\n\n  dfs = []\n\n  for file in files:\n      sample_id = os.path.basename(file).split(\".\")[0]\n      df = pd.read_csv(file, sep='\\\\t')\n      df = df.iloc[:,[1,7,17]]\n      df = df.assign(Sample=sample_id)\n      df = df.rename(columns={'# contigs (>= 0 bp)':'Contigs','Total length (>= 0 bp)':'Assembly Length (bp)'})\n      df = df[['Sample', 'Contigs','Assembly Length (bp)', 'N50']]\n      dfs.append(df)\n\n  dfs_concat = pd.concat(dfs)\n  dfs_concat.to_csv(f'quast_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n\n  files = glob.glob(\"*.quast.tsv\")\n\n  dfs = []\n\n  for file in files:\n      sample_id = os.path.basename(file).split(\".\")[0]\n      df = pd.read_csv(file, sep='\\\\t')\n      df = df.iloc[:,[1,7,17]]\n      df = df.assign(Sample=sample_id)\n      df = df.rename(columns={'# contigs (>= 0 bp)':'Contigs','Total length (>= 0 bp)':'Assembly Length (bp)'})\n      df = df[['Sample', 'Contigs','Assembly Length (bp)', 'N50']]\n      dfs.append(df)\n\n  dfs_concat = pd.concat(dfs)\n  dfs_concat.to_csv(f'quast_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "quast_files"
        ],
        "nb_inputs": 1,
        "outputs": [
            "quast_tsv"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "publishDir \"${params.outdir}/quast\",mode:'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "prokka_setup": {
        "name_process": "prokka_setup",
        "string_process": "\nprocess prokka_setup {\n  tag \"$name\"\n\n  input:\n  file(kraken) from kraken_prokka\n  set val(name), file(input) from assembled_genomes_prokka\n\n  output:\n  tuple name, file(\"${name}.*.fa\") into prokka_input\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import pandas as pd\n  import shutil\n\n  genomeFile = '${input}'\n  sid = genomeFile.split('.')[0]\n  df = pd.read_csv('kraken_results.tsv', header=0, delimiter='\\\\t')\n  df = df[df['Sample'] == sid]\n  taxa = df.iloc[0]['Primary Species (%)']\n  taxa = taxa.split(' ')\n  taxa = taxa[0] + '_' + taxa[1]\n  shutil.copyfile(genomeFile, f'{sid}.{taxa}.fa')\n  \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import pandas as pd\n  import shutil\n\n  genomeFile = '${input}'\n  sid = genomeFile.split('.')[0]\n  df = pd.read_csv('kraken_results.tsv', header=0, delimiter='\\\\t')\n  df = df[df['Sample'] == sid]\n  taxa = df.iloc[0]['Primary Species (%)']\n  taxa = taxa.split(' ')\n  taxa = taxa[0] + '_' + taxa[1]\n  shutil.copyfile(genomeFile, f'{sid}.{taxa}.fa')\n  \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "kraken_prokka",
            "assembled_genomes_prokka"
        ],
        "nb_inputs": 2,
        "outputs": [
            "prokka_input"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "tag \"$name\""
        ],
        "when": "",
        "stub": ""
    },
    "prokka": {
        "name_process": "prokka",
        "string_process": "\nprocess prokka {\n  tag \"$name\"\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/annotated\",mode:'copy'\n\n  input:\n  set val(name), file(assembly) from prokka_input\n\n  output:\n  file(\"${name}.gff\") into annotated_genomes\n  file(\"${name}.prokka.stats.txt\") into prokka_multiqc\n\n  script:\n  \"\"\"\n  filename=${assembly}\n  handle=\\${filename%.*}\n  taxa=\\${handle##*.}\n  genus=\\${taxa%_*}\n  species=\\${taxa##*_}\n\n  prokka --cpu ${task.cpus} --force --compliant --prefix ${name} --genus \\$genus --species \\$species --strain ${name} --mincontiglen 500 --outdir . ${assembly} > ${name}.log\n  mv ${name}.txt ${name}.prokka.stats.txt\n  \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "  \"\"\"\n  filename=${assembly}\n  handle=\\${filename%.*}\n  taxa=\\${handle##*.}\n  genus=\\${taxa%_*}\n  species=\\${taxa##*_}\n\n  prokka --cpu ${task.cpus} --force --compliant --prefix ${name} --genus \\$genus --species \\$species --strain ${name} --mincontiglen 500 --outdir . ${assembly} > ${name}.log\n  mv ${name}.txt ${name}.prokka.stats.txt\n  \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [
            "Prokka"
        ],
        "tools_url": [
            "https://bio.tools/prokka"
        ],
        "tools_dico": [
            {
                "name": "Prokka",
                "uri": "https://bio.tools/prokka",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "Coding region prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0362",
                                    "term": "Genome annotation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "ORF prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "ORF finding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene finding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene calling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Software tool to annotate bacterial, archaeal and viral genomes quickly and produce standards-compliant output files.",
                "homepage": "https://github.com/tseemann/prokka"
            }
        ],
        "inputs": [
            "prokka_input"
        ],
        "nb_inputs": 1,
        "outputs": [
            "annotated_genomes",
            "prokka_multiqc"
        ],
        "nb_outputs": 2,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "tag \"$name\"",
            "errorStrategy 'ignore'",
            "publishDir \"${params.outdir}/annotated\",mode:'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "roary": {
        "name_process": "roary",
        "string_process": "\nprocess roary {\n  publishDir \"${params.outdir}\",mode:'copy'\n\n  numGenomes = 0\n  input:\n  file(genomes) from annotated_genomes.collect()\n\n  output:\n  file(\"core_gene_alignment.aln\") into core_aligned_genomes\n  file(\"core_genome_statistics.txt\") into core_aligned_stats\n\n  script:\n  if(params.roary_mafft == true){\n    mafft=\"-n\"\n  }else{mafft=\"\"}\n  \"\"\"\n  cpus=`grep -c ^processor /proc/cpuinfo`\n  roary -e ${mafft} -p \\$cpus ${genomes}\n  mv summary_statistics.txt core_genome_statistics.txt\n  \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "  if(params.roary_mafft == true){\n    mafft=\"-n\"\n  }else{mafft=\"\"}\n  \"\"\"\n  cpus=`grep -c ^processor /proc/cpuinfo`\n  roary -e ${mafft} -p \\$cpus ${genomes}\n  mv summary_statistics.txt core_genome_statistics.txt\n  \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [
            "Roary"
        ],
        "tools_url": [
            "https://bio.tools/roary"
        ],
        "tools_dico": [
            {
                "name": "Roary",
                "uri": "https://bio.tools/roary",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A high speed stand alone pan genome pipeline, which takes annotated assemblies in GFF3 format (produced by Prokka (Seemann, 2014)) and calculates the pan genome.",
                "homepage": "http://sanger-pathogens.github.io/Roary/"
            }
        ],
        "inputs": [
            "annotated_genomes"
        ],
        "nb_inputs": 1,
        "outputs": [
            "core_aligned_genomes",
            "core_aligned_stats"
        ],
        "nb_outputs": 2,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "publishDir \"${params.outdir}\",mode:'copy' numGenomes = 0"
        ],
        "when": "",
        "stub": ""
    },
    "cg_tree": {
        "name_process": "cg_tree",
        "string_process": "\nprocess cg_tree {\n  publishDir \"${params.outdir}\",mode:'copy'\n\n  input:\n  file(alignedGenomes) from core_aligned_genomes\n\n  output:\n  file(\"core_genome.tree\") optional true into cgtree\n\n  script:\n    \"\"\"\n    numGenomes=`grep -o '>' core_gene_alignment.aln | wc -l`\n    if [ \\$numGenomes -gt 3 ]\n    then\n      iqtree -nt AUTO -s core_gene_alignment.aln -m ${params.cg_tree_model} -bb 1000\n      mv core_gene_alignment.aln.contree core_genome.tree\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "    \"\"\"\n    numGenomes=`grep -o '>' core_gene_alignment.aln | wc -l`\n    if [ \\$numGenomes -gt 3 ]\n    then\n      iqtree -nt AUTO -s core_gene_alignment.aln -m ${params.cg_tree_model} -bb 1000\n      mv core_gene_alignment.aln.contree core_genome.tree\n    fi\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "core_aligned_genomes"
        ],
        "nb_inputs": 1,
        "outputs": [
            "cgtree"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "publishDir \"${params.outdir}\",mode:'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "cfsan": {
        "name_process": "cfsan",
        "string_process": " process cfsan {\n      publishDir \"${params.outdir}\", mode: 'copy'\n\n      input:\n      file(reads) from cleaned_reads_snp.collect()\n      file(reference) from snp_reference\n      file(config) from snp_config\n      output:\n      file(\"snp_distance_matrix.tsv\") into snp_mat\n      file(\"snpma.fasta\") into snp_alignment\n\n      script:\n      \"\"\"\n      #!/usr/bin/env python\n      import subprocess\n      import glob\n      import os,sys\n\n      fwd_reads = glob.glob(\"*_clean_1.fastq.gz\")\n      fwd_reads.sort()\n      rev_reads = glob.glob(\"*_clean_2.fastq.gz\")\n      rev_reads.sort()\n\n      if len(fwd_reads) != len(rev_reads):\n        sys.exit(\"Uneven number of forward and reverse reads.\")\n\n      os.mkdir(\"input_reads\")\n\n      c = 0\n      while c < len(fwd_reads):\n        name = os.path.basename(fwd_reads[c]).split('_clean_1')[0]\n        path = os.path.join(\"input_reads\",name)\n        os.mkdir(path)\n        os.rename(fwd_reads[c],os.path.join(path,fwd_reads[c]))\n        os.rename(rev_reads[c],os.path.join(path,rev_reads[c]))\n        c += 1\n\n      command = \"cfsan_snp_pipeline run ${reference} -o . -s input_reads\"\n      process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n      output, error = process.communicate()\n      print output\n      print error\n      \"\"\"\n    }",
        "nb_lignes_process": 42,
        "string_script": "      \"\"\"\n      #!/usr/bin/env python\n      import subprocess\n      import glob\n      import os,sys\n\n      fwd_reads = glob.glob(\"*_clean_1.fastq.gz\")\n      fwd_reads.sort()\n      rev_reads = glob.glob(\"*_clean_2.fastq.gz\")\n      rev_reads.sort()\n\n      if len(fwd_reads) != len(rev_reads):\n        sys.exit(\"Uneven number of forward and reverse reads.\")\n\n      os.mkdir(\"input_reads\")\n\n      c = 0\n      while c < len(fwd_reads):\n        name = os.path.basename(fwd_reads[c]).split('_clean_1')[0]\n        path = os.path.join(\"input_reads\",name)\n        os.mkdir(path)\n        os.rename(fwd_reads[c],os.path.join(path,fwd_reads[c]))\n        os.rename(rev_reads[c],os.path.join(path,rev_reads[c]))\n        c += 1\n\n      command = \"cfsan_snp_pipeline run ${reference} -o . -s input_reads\"\n      process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n      output, error = process.communicate()\n      print output\n      print error\n      \"\"\"",
        "nb_lignes_script": 30,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "cleaned_reads_snp",
            "snp_reference",
            "snp_config"
        ],
        "nb_inputs": 3,
        "outputs": [
            "snp_mat",
            "snp_alignment"
        ],
        "nb_outputs": 2,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "publishDir \"${params.outdir}\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "snp_tree": {
        "name_process": "snp_tree",
        "string_process": " process snp_tree {\n      publishDir \"${params.outdir}\", mode: 'copy'\n\n      input:\n      file(snp_fasta) from snp_alignment\n\n      output:\n      file(\"snp.tree\") optional true\n\n      script:\n        \"\"\"\n        numGenomes=`grep -o '>' snpma.fasta | wc -l`\n        if [ \\$numGenomes -gt 3 ]\n        then\n          iqtree -nt AUTO -s snpma.fasta -m ${params.cg_tree_model} -bb 1000\n          mv snpma.fasta.contree snp.tree\n        fi\n        \"\"\"\n    }",
        "nb_lignes_process": 17,
        "string_script": "        \"\"\"\n        numGenomes=`grep -o '>' snpma.fasta | wc -l`\n        if [ \\$numGenomes -gt 3 ]\n        then\n          iqtree -nt AUTO -s snpma.fasta -m ${params.cg_tree_model} -bb 1000\n          mv snpma.fasta.contree snp.tree\n        fi\n        \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "snp_alignment"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "publishDir \"${params.outdir}\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "bwa": {
        "name_process": "bwa",
        "string_process": " process bwa {\n      tag \"$name\"\n      errorStrategy 'ignore'\n      publishDir \"${params.outdir}/mapping/sams\", mode: 'copy',pattern:\"*.sam\"\n\n      input:\n      file(reference) from mapping_reference.first()\n      set val(name), file(reads) from cleaned_reads_mapping\n\n      output:\n      tuple name, file(\"${name}.reference.sam\") into reference_sams\n\n      script:\n      \"\"\"\n      bwa index ${reference}\n      bwa mem ${reference} ${reads[0]} ${reads[1]} > ${name}.reference.sam\n      \"\"\"\n    }",
        "nb_lignes_process": 16,
        "string_script": "      \"\"\"\n      bwa index ${reference}\n      bwa mem ${reference} ${reads[0]} ${reads[1]} > ${name}.reference.sam\n      \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "BWA"
        ],
        "tools_url": [
            "https://bio.tools/bwa"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            }
        ],
        "inputs": [
            "mapping_reference",
            "cleaned_reads_mapping"
        ],
        "nb_inputs": 2,
        "outputs": [
            "reference_sams"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "tag \"$name\"",
            "errorStrategy 'ignore'",
            "publishDir \"${params.outdir}/mapping/sams\", mode: 'copy',pattern:\"*.sam\""
        ],
        "when": "",
        "stub": ""
    },
    "samtools": {
        "name_process": "samtools",
        "string_process": "\nprocess samtools {\n  tag \"$name\"\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/mapping/bams\", mode: 'copy',pattern:\"*.sorted.bam*\"\n  publishDir \"${params.outdir}/mapping/depth\", mode: 'copy',pattern:\"*.depth.tsv\"\n  publishDir \"${params.outdir}/mapping/stats\", mode: 'copy',pattern:\"*.stats.txt\"\n\n  input:\n  set val(name), file(sam) from sam_files\n\n  output:\n  tuple name, file(\"*.depth.tsv\") into reference_depth_results,assembly_depth_results\n  tuple name, file(\"*.mapped.tsv\") into reference_mapped_results,assembly_mapped_results\n  file(\"*.stats.txt\")\n  file(\"*.sorted.bam*\")\n\n  shell:\n  \"\"\"\n  filename=${sam}\n  handle=\\${filename%.*}\n  type=\\${handle##*.}\n\n  samtools view -S -b ${name}.\\$type.sam > ${name}.\\$type.bam\n  samtools sort ${name}.\\$type.bam > ${name}.\\$type.sorted.bam\n  samtools index ${name}.\\$type.sorted.bam\n  samtools depth -a ${name}.\\$type.sorted.bam > ${name}.\\$type.depth.tsv\n  samtools stats ${name}.\\$type.sorted.bam > ${name}.\\$type.stats.txt\n  samtools view -c -F 260 ${name}.\\$type.sorted.bam > ${name}.\\$type.mapped.tsv\n  samtools view -c ${name}.\\$type.sorted.bam >> ${name}.\\$type.mapped.tsv\n  \"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "  \"\"\"\n  filename=${sam}\n  handle=\\${filename%.*}\n  type=\\${handle##*.}\n\n  samtools view -S -b ${name}.\\$type.sam > ${name}.\\$type.bam\n  samtools sort ${name}.\\$type.bam > ${name}.\\$type.sorted.bam\n  samtools index ${name}.\\$type.sorted.bam\n  samtools depth -a ${name}.\\$type.sorted.bam > ${name}.\\$type.depth.tsv\n  samtools stats ${name}.\\$type.sorted.bam > ${name}.\\$type.stats.txt\n  samtools view -c -F 260 ${name}.\\$type.sorted.bam > ${name}.\\$type.mapped.tsv\n  samtools view -c ${name}.\\$type.sorted.bam >> ${name}.\\$type.mapped.tsv\n  \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "sam_files"
        ],
        "nb_inputs": 1,
        "outputs": [
            "reference_depth_results",
            "assembly_depth_results",
            "reference_mapped_results",
            "assembly_mapped_results"
        ],
        "nb_outputs": 4,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "tag \"$name\"",
            "errorStrategy 'ignore'",
            "publishDir \"${params.outdir}/mapping/bams\", mode: 'copy',pattern:\"*.sorted.bam*\"",
            "publishDir \"${params.outdir}/mapping/depth\", mode: 'copy',pattern:\"*.depth.tsv\"",
            "publishDir \"${params.outdir}/mapping/stats\", mode: 'copy',pattern:\"*.stats.txt\""
        ],
        "when": "",
        "stub": ""
    },
    "assembly_coverage_stats": {
        "name_process": "assembly_coverage_stats",
        "string_process": "\nprocess assembly_coverage_stats {\n  publishDir \"${params.outdir}/mapping\", mode: 'copy'\n\n  input:\n  file(depth) from assembly_depth_results.collect()\n  file(mapped) from assembly_mapped_results.collect()\n\n  output:\n  file('coverage_stats.tsv') into assembly_mapping_tsv\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import glob\n  import os\n  from numpy import median\n  from numpy import average\n  \n  # function for summarizing samtools depth files\n  def summarize_depth(file):\n      # get sample id from file name and set up data list\n      sid = os.path.basename(file).split('.')[0]\n      data = []\n      # open samtools depth file and get depth\n      with open(file,'r') as inFile:\n          for line in inFile:\n              data.append(int(line.strip().split()[2]))\n      # get median and average depth\n      med = int(median(data))\n      avg = int(average(data))\n      # return sample id, median and average depth\n      result = f\"{sid}\\\\t{med}\\\\t{avg}\\\\n\"\n      return result\n  \n  # get all samtools depth files\n  files = glob.glob(\"*.assembly.depth.tsv*\")\n  \n  # summarize samtools depth files\n  results = map(summarize_depth,files)\n  \n  # write results to file\n  with open('coverage_stats.tsv', 'w') as outFile:\n      outFile.write(\"Sample\\\\tMedian Coverage (Mapped to Assembly)\\\\tMean Coverage (Mapped to Assembly)\\\\n\")\n      for result in results:\n          outFile.write(result)\n  \"\"\"\n}",
        "nb_lignes_process": 46,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n  import glob\n  import os\n  from numpy import median\n  from numpy import average\n  \n  # function for summarizing samtools depth files\n  def summarize_depth(file):\n      # get sample id from file name and set up data list\n      sid = os.path.basename(file).split('.')[0]\n      data = []\n      # open samtools depth file and get depth\n      with open(file,'r') as inFile:\n          for line in inFile:\n              data.append(int(line.strip().split()[2]))\n      # get median and average depth\n      med = int(median(data))\n      avg = int(average(data))\n      # return sample id, median and average depth\n      result = f\"{sid}\\\\t{med}\\\\t{avg}\\\\n\"\n      return result\n  \n  # get all samtools depth files\n  files = glob.glob(\"*.assembly.depth.tsv*\")\n  \n  # summarize samtools depth files\n  results = map(summarize_depth,files)\n  \n  # write results to file\n  with open('coverage_stats.tsv', 'w') as outFile:\n      outFile.write(\"Sample\\\\tMedian Coverage (Mapped to Assembly)\\\\tMean Coverage (Mapped to Assembly)\\\\n\")\n      for result in results:\n          outFile.write(result)\n  \"\"\"",
        "nb_lignes_script": 34,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "assembly_depth_results",
            "assembly_mapped_results"
        ],
        "nb_inputs": 2,
        "outputs": [
            "assembly_mapping_tsv"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "publishDir \"${params.outdir}/mapping\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "reference_mapping_stats": {
        "name_process": "reference_mapping_stats",
        "string_process": " process reference_mapping_stats {\n      publishDir \"${params.outdir}/mapping\", mode: 'copy'\n\n      input:\n      file(depth) from reference_depth_results.collect()\n      file(mapped) from reference_mapped_results.collect()\n\n      output:\n      file('mapping_results.tsv') into reference_mapping_tsv\n\n      script:\n      \"\"\"\n      #!/usr/bin/env python3\n      import pandas as pd\n      import os\n      import glob\n      from functools import reduce\n\n      depth_files = glob.glob(\"*.reference.depth.tsv\")\n      depth_dfs = []\n      cols = [\"Sample\",\"Base Pairs Mapped to Reference >1X (%)\",\"Base Pairs Mapped to Reference >40X (%)\"]\n      depth_dfs.append(cols)\n\n      for file in depth_files:\n          sampleID = os.path.basename(file).split(\".\")[0]\n          depth_df = pd.read_csv(file, sep=\"\\\\t\", header=None)\n          overForty = int((len(depth_df[(depth_df[2]>40)])/len(depth_df)) * 100)\n          overOne = int((len(depth_df[(depth_df[2]>1)])/len(depth_df)) * 100)\n          stats = [sampleID, overOne, overForty]\n          depth_dfs.append(stats)\n      depth_df = pd.DataFrame(depth_dfs[1:], columns=depth_dfs[0])\n\n      read_files = glob.glob(\"*.reference.mapped.tsv\")\n      read_dfs = []\n      cols = [\"Sample\",\"Reads Mapped to Reference (%)\"]\n      read_dfs.append(cols)\n      for file in read_files:\n          sampleID = os.path.basename(file).split(\".\")[0]\n          read_df = pd.read_csv(file, sep=\"\\\\t\", header=None)\n          mapped_reads = read_df.iloc[0][0]\n          all_reads = read_df.iloc[1][0]\n          percent_mapped = int((mapped_reads/all_reads) * 100)\n          stats = [sampleID,percent_mapped]\n          read_dfs.append(stats)\n      read_dfs = pd.DataFrame(read_dfs[1:], columns=read_dfs[0])\n\n      dfs = [depth_df, read_dfs]\n      merged = reduce(lambda  left,right: pd.merge(left,right,on=[\"Sample\"], how=\"left\"), dfs)\n      merged[['Reads Mapped to Reference (%)','Base Pairs Mapped to Reference >1X (%)','Base Pairs Mapped to Reference >40X (%)']] = merged[['Reads Mapped to Reference (%)','Base Pairs Mapped to Reference >1X (%)','Base Pairs Mapped to Reference >40X (%)']].astype(str) + '%'\n      merged.to_csv(\"mapping_results.tsv\",sep=\"\\\\t\", index=False, header=True, na_rep=\"NaN\")\n      \"\"\"\n    }",
        "nb_lignes_process": 50,
        "string_script": "      \"\"\"\n      #!/usr/bin/env python3\n      import pandas as pd\n      import os\n      import glob\n      from functools import reduce\n\n      depth_files = glob.glob(\"*.reference.depth.tsv\")\n      depth_dfs = []\n      cols = [\"Sample\",\"Base Pairs Mapped to Reference >1X (%)\",\"Base Pairs Mapped to Reference >40X (%)\"]\n      depth_dfs.append(cols)\n\n      for file in depth_files:\n          sampleID = os.path.basename(file).split(\".\")[0]\n          depth_df = pd.read_csv(file, sep=\"\\\\t\", header=None)\n          overForty = int((len(depth_df[(depth_df[2]>40)])/len(depth_df)) * 100)\n          overOne = int((len(depth_df[(depth_df[2]>1)])/len(depth_df)) * 100)\n          stats = [sampleID, overOne, overForty]\n          depth_dfs.append(stats)\n      depth_df = pd.DataFrame(depth_dfs[1:], columns=depth_dfs[0])\n\n      read_files = glob.glob(\"*.reference.mapped.tsv\")\n      read_dfs = []\n      cols = [\"Sample\",\"Reads Mapped to Reference (%)\"]\n      read_dfs.append(cols)\n      for file in read_files:\n          sampleID = os.path.basename(file).split(\".\")[0]\n          read_df = pd.read_csv(file, sep=\"\\\\t\", header=None)\n          mapped_reads = read_df.iloc[0][0]\n          all_reads = read_df.iloc[1][0]\n          percent_mapped = int((mapped_reads/all_reads) * 100)\n          stats = [sampleID,percent_mapped]\n          read_dfs.append(stats)\n      read_dfs = pd.DataFrame(read_dfs[1:], columns=read_dfs[0])\n\n      dfs = [depth_df, read_dfs]\n      merged = reduce(lambda  left,right: pd.merge(left,right,on=[\"Sample\"], how=\"left\"), dfs)\n      merged[['Reads Mapped to Reference (%)','Base Pairs Mapped to Reference >1X (%)','Base Pairs Mapped to Reference >40X (%)']] = merged[['Reads Mapped to Reference (%)','Base Pairs Mapped to Reference >1X (%)','Base Pairs Mapped to Reference >40X (%)']].astype(str) + '%'\n      merged.to_csv(\"mapping_results.tsv\",sep=\"\\\\t\", index=False, header=True, na_rep=\"NaN\")\n      \"\"\"",
        "nb_lignes_script": 39,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "reference_depth_results",
            "reference_mapped_results"
        ],
        "nb_inputs": 2,
        "outputs": [
            "reference_mapping_tsv"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "publishDir \"${params.outdir}/mapping\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "merge_results": {
        "name_process": "merge_results",
        "string_process": "\nprocess merge_results {\n  publishDir \"${params.outdir}/\", mode: 'copy'\n\n  input:\n  file(bbduk) from bbduk_tsv\n  file(quast) from quast_tsv\n  file(assembly) from assembly_mapping_tsv\n  file(kraken) from kraken_tsv\n  file(vkraken) from kraken_version.first()\n  file(reference) from reference_mapping_tsv.ifEmpty{ 'empty' }\n\n  output:\n  file('dryad_report.csv')\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n\n  import os\n  import glob\n  import pandas as pd\n  from functools import reduce\n\n  with open('Kraken2_DB.txt', 'r') as krakenFile:\n      krakenDB_version = krakenFile.readline().strip()\n\n\n  files = glob.glob('*.tsv')\n\n  dfs = []\n\n  for file in files:\n      df = pd.read_csv(file, header=0, delimiter='\\\\t')\n      dfs.append(df)\n\n  merged = reduce(lambda  left,right: pd.merge(left,right,on=['Sample'],\n                                              how='left'), dfs)\n  merged = merged.assign(krakenDB=krakenDB_version)\n\n  merged = merged.rename(columns={'Contigs':'Contigs (#)','krakenDB':'Kraken Database Verion'})\n\n  merged.to_csv('dryad_report.csv', index=False, sep=',', encoding='utf-8')\n  \"\"\"\n}",
        "nb_lignes_process": 43,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n\n  import os\n  import glob\n  import pandas as pd\n  from functools import reduce\n\n  with open('Kraken2_DB.txt', 'r') as krakenFile:\n      krakenDB_version = krakenFile.readline().strip()\n\n\n  files = glob.glob('*.tsv')\n\n  dfs = []\n\n  for file in files:\n      df = pd.read_csv(file, header=0, delimiter='\\\\t')\n      dfs.append(df)\n\n  merged = reduce(lambda  left,right: pd.merge(left,right,on=['Sample'],\n                                              how='left'), dfs)\n  merged = merged.assign(krakenDB=krakenDB_version)\n\n  merged = merged.rename(columns={'Contigs':'Contigs (#)','krakenDB':'Kraken Database Verion'})\n\n  merged.to_csv('dryad_report.csv', index=False, sep=',', encoding='utf-8')\n  \"\"\"",
        "nb_lignes_script": 27,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "bbduk_tsv",
            "quast_tsv",
            "assembly_mapping_tsv",
            "kraken_tsv",
            "kraken_version",
            "reference_mapping_tsv"
        ],
        "nb_inputs": 6,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "publishDir \"${params.outdir}/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "multiqc": {
        "name_process": "multiqc",
        "string_process": "\nprocess multiqc {\n  publishDir \"${params.outdir}\",mode:'copy'\n\n  input:\n  file(a) from multiqc_clean_reads.collect()\n  file(b) from fastqc_multiqc.collect()\n                                        \n  file(d) from kraken_multiqc.collect()\n  file(e) from quast_multiqc.collect()\n  file(f) from prokka_multiqc.collect()\n  file(g) from logo\n  file(config) from multiqc_config\n\n\n\n  output:\n  file(\"*.html\") into multiqc_output\n\n  script:\n  \"\"\"\n  multiqc -c ${config} .\n  \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "  \"\"\"\n  multiqc -c ${config} .\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "MultiQC"
        ],
        "tools_url": [
            "https://bio.tools/multiqc"
        ],
        "tools_dico": [
            {
                "name": "MultiQC",
                "uri": "https://bio.tools/multiqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0091",
                            "term": "Bioinformatics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2428",
                                    "term": "Validation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2048",
                                "term": "Report"
                            }
                        ]
                    }
                ],
                "description": "MultiQC aggregates results from multiple bioinformatics analyses across many samples into a single report. It searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.",
                "homepage": "http://multiqc.info/"
            }
        ],
        "inputs": [
            "multiqc_clean_reads",
            "fastqc_multiqc",
            "kraken_multiqc",
            "quast_multiqc",
            "prokka_multiqc",
            "logo",
            "multiqc_config"
        ],
        "nb_inputs": 7,
        "outputs": [
            "multiqc_output"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__dryad",
        "directive": [
            "publishDir \"${params.outdir}\",mode:'copy'"
        ],
        "when": "",
        "stub": ""
    }
}