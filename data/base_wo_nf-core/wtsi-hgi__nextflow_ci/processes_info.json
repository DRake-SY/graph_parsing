{
    "visualiseMetadata": {
        "name_process": "visualiseMetadata",
        "string_process": "process visualiseMetadata{\n\n\n  publishDir \"${params.outdir}/PDFs/\", mode: 'copy', overwrite: true\n  \n\n  when:\n  params.run_metadata_visualisation\n\n  input:\n  path(my_channel)\n\n  script:\n\n  \"\"\"\n  \n  python $workflow.projectDir/../bin/main_PDF_visualisation.py $my_channel\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 19,
        "string_script": "  \"\"\"\n  \n  python $workflow.projectDir/../bin/main_PDF_visualisation.py $my_channel\n\n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "my_channel"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "wtsi-hgi__nextflow_ci",
        "directive": [
            "publishDir \"${params.outdir}/PDFs/\", mode: 'copy', overwrite: true"
        ],
        "when": "params.run_metadata_visualisation",
        "stub": ""
    },
    "imeta_study": {
        "name_process": "imeta_study",
        "string_process": "process imeta_study {\n    tag \"${study_id}\"\n    publishDir \"${params.outdir}/imeta_study/study_id_${study_id}/\", mode: 'copy', pattern: \"samples.tsv\", overwrite: true\n    publishDir \"${params.outdir}/imeta_study/study_id_${study_id}/\", mode: 'copy', pattern: \"samples_noduplicates.tsv\", overwrite: true\n    publishDir \"${params.outdir}/\", mode: 'copy', pattern: \"samples.tsv\", saveAs: { filename -> \"${study_id}.$filename\" }, overwrite: true\n    publishDir \"${params.outdir}/\", mode: 'copy', pattern: \"samples_noduplicates.tsv\", saveAs: { filename -> \"${study_id}.$filename\" }, overwrite: true\n\n    when: \n    params.study_id_mode.run_imeta_study\n\n    input: \n    val(study_id)\n\n    output: \n    tuple val(study_id), path('samples.tsv'), emit: irods_samples_tsv\n    tuple val(study_id), path('samples_noduplicates.tsv'), emit: samples_noduplicates_tsv\n    env(WORK_DIR), emit: work_dir_to_remove\n\n    script:\n    \"\"\"\n    bash $workflow.projectDir/../bin/imeta_study.sh ${study_id}\n    awk '!a[\\$1]++' samples.tsv > samples_noduplicates.tsv \n\n    # Save work dir so that it can be removed onComplete of workflow, \n    # to ensure that this task Irods search is re-run on each run NF run, \n    # in case new sequencing samples are ready: \n    WORK_DIR=\\$PWD\n    \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "    \"\"\"\n    bash $workflow.projectDir/../bin/imeta_study.sh ${study_id}\n    awk '!a[\\$1]++' samples.tsv > samples_noduplicates.tsv \n\n    # Save work dir so that it can be removed onComplete of workflow, \n    # to ensure that this task Irods search is re-run on each run NF run, \n    # in case new sequencing samples are ready: \n    WORK_DIR=\\$PWD\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "study_id"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "wtsi-hgi__nextflow_ci",
        "directive": [
            "tag \"${study_id}\"",
            "publishDir \"${params.outdir}/imeta_study/study_id_${study_id}/\", mode: 'copy', pattern: \"samples.tsv\", overwrite: true",
            "publishDir \"${params.outdir}/imeta_study/study_id_${study_id}/\", mode: 'copy', pattern: \"samples_noduplicates.tsv\", overwrite: true",
            "publishDir \"${params.outdir}/\", mode: 'copy', pattern: \"samples.tsv\", saveAs: { filename -> \"${study_id}.$filename\" }, overwrite: true",
            "publishDir \"${params.outdir}/\", mode: 'copy', pattern: \"samples_noduplicates.tsv\", saveAs: { filename -> \"${study_id}.$filename\" }, overwrite: true"
        ],
        "when": "params.study_id_mode.run_imeta_study",
        "stub": ""
    },
    "baton_study": {
        "name_process": "baton_study",
        "string_process": "process baton_study {\n    tag \"${study_id}\"\n    publishDir \"${params.outdir}/samples/study_id_${study_id}/\", mode: 'copy', pattern: \"samples.tsv\", overwrite: true\n    publishDir \"${params.outdir}/samples/study_id_${study_id}/\", mode: 'copy', pattern: \"samples_noduplicates.tsv\", overwrite: true\n\n    input: \n    val(study_id)\n\n    output: \n    tuple val(study_id), path('samples.tsv'), emit: samples_tsv\n    tuple val(study_id), path('samples_noduplicates.tsv'), emit: samples_noduplicates_tsv\n\n    script:\n    \"\"\"\n    bash $workflow.projectDir/../bin/baton.sh ${study_id}\n    awk '!a[\\$1]++' samples.tsv > samples_noduplicates.tsv \n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    bash $workflow.projectDir/../bin/baton.sh ${study_id}\n    awk '!a[\\$1]++' samples.tsv > samples_noduplicates.tsv \n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "study_id"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "wtsi-hgi__nextflow_ci",
        "directive": [
            "tag \"${study_id}\"",
            "publishDir \"${params.outdir}/samples/study_id_${study_id}/\", mode: 'copy', pattern: \"samples.tsv\", overwrite: true",
            "publishDir \"${params.outdir}/samples/study_id_${study_id}/\", mode: 'copy', pattern: \"samples_noduplicates.tsv\", overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "gsheet_to_csv": {
        "name_process": "gsheet_to_csv",
        "string_process": "process gsheet_to_csv {\n    tag \"${gsheet}\"\n    publishDir \"${params.outdir}/\", mode: 'copy', pattern: \"${output_csv_name}\", overwrite: true\n    \n    when: \n    params.google_spreadsheet_mode.run_gsheet_to_csv\n\n    input: \n    val(gsheet)\n    path(creds_json)\n    val(output_csv_name)\n\n    output: \n    path(\"${output_csv_name}\"), emit: samples_csv\n    val(output_csv_name), emit: output_csv_name\n    env(WORK_DIR), emit: work_dir_to_remove\n    env(study_id), emit: study_id\n\n    script:\n    \"\"\"\n    python3 $workflow.projectDir/../bin/google_spreadsheet_to_csv.py \\\\\n       --creds_json ${creds_json} --gsheet ${gsheet} --output_csv_name ${output_csv_name}\n\n    # Save work dir so that it can be removed onComplete of workflow, \n    # to ensure that this task Irods search is re-run on each run NF run, \n    # in case new sequencing samples are ready: \n    WORK_DIR=\\$PWD\n    study_id=gsheet\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "    \"\"\"\n    python3 $workflow.projectDir/../bin/google_spreadsheet_to_csv.py \\\\\n       --creds_json ${creds_json} --gsheet ${gsheet} --output_csv_name ${output_csv_name}\n\n    # Save work dir so that it can be removed onComplete of workflow, \n    # to ensure that this task Irods search is re-run on each run NF run, \n    # in case new sequencing samples are ready: \n    WORK_DIR=\\$PWD\n    study_id=gsheet\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "gsheet",
            "creds_json",
            "output_csv_name"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "wtsi-hgi__nextflow_ci",
        "directive": [
            "tag \"${gsheet}\"",
            "publishDir \"${params.outdir}/\", mode: 'copy', pattern: \"${output_csv_name}\", overwrite: true"
        ],
        "when": "params.google_spreadsheet_mode.run_gsheet_to_csv",
        "stub": ""
    },
    "crams_to_fastq": {
        "name_process": "crams_to_fastq",
        "string_process": "process crams_to_fastq {\n    tag \"${sample}-${study_id}\"\n    publishDir \"${params.outdir}/crams_to_fastq/fastq/${study_id}/${sample}/\", mode: \"${params.copy_mode}\", overwrite: true, pattern: \"*.fastq.gz\"\n    publishDir \"${params.outdir}/crams_to_fastq/merged_crams/${study_id}/${sample}/\", mode: \"${params.copy_mode}\", overwrite: true, pattern: \"${study_id}.${sample}_merged.cram\"\n    \n    when: \n    params.run_crams_to_fastq\n    \n    input: \n    tuple val(study_id), val(sample), path(crams) \n\n    output: \n    tuple val(study_id), val(sample), path(\"*.fastq.gz\"), emit: study_sample_fastqs\n    tuple val(study_id), val(sample), path(\"${study_id}.${sample}_merged.cram\"), emit: study_sample_mergedcram\n    path('*.lostcause.tsv'), emit: lostcause optional true \n    path('*.numreads.tsv'), emit: numreads optional true \n    env(study_id), emit: study_id\n\n    script:\n    def cramfile = \"${study_id}.${sample}_merged.cram\"\n    \"\"\"\n    export REF_PATH=/lustre/scratch117/core/sciops_repository/cram_cache/%2s/%2s/%s:/lustre/scratch118/core/sciops_repository/cram_cache/%2s/%2s/%s:URL=http:://sf2-farm-srv1.internal.sanger.ac.uk::8000/%s\n    export PATH=/opt/conda/envs/nf-core-rnaseq-1.3/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n    \n    samtools merge -@ ${task.cpus} -f $cramfile ${crams}\n    f1=${study_id}.${sample}_1.fastq.gz\n    f2=${study_id}.${sample}_2.fastq.gz\n    f0=${study_id}.${sample}.fastq.gz\n    numreads=\\$(samtools view -c -F 0x900 $cramfile)\n    if (( \\$numreads >= ${params.crams_to_fastq_min_reads} )); then\n                              # -O {stdout} -u {no compression}\n                              # -N {always append /1 and /2 to the read name}\n                              # -F 0x900 (bit 1, 8, filter secondary and supplementary reads)\n      echo -e \"study_id\\\\tsample\\\\tnumreads\" > ${study_id}.${sample}.numreads.tsv\n      echo -e \"${study_id}\\\\t${sample}\\\\t\\${numreads}\" >> ${study_id}.${sample}.numreads.tsv\n      samtools collate    \\\\\n          -O -u           \\\\\n          -@ ${task.cpus} \\\\\n          $cramfile pfx-${sample} | \\\\\n      samtools fastq      \\\\\n          -N              \\\\\n          -F 0x900        \\\\\n          -@ ${task.cpus} \\\\\n          -1 \\$f1 -2 \\$f2 -0 \\$f0 \\\\\n          -\n      sleep 2\n      find . -name \\\"*.fastq.gz\\\" -type 'f' -size -160k -delete\n    else\n      echo -e \"study_id\\\\tsample\\\\tnumreads\" > ${study_id}.${sample}.lostcause.tsv\n      echo -e \"${study_id}\\\\t${sample}\\\\t\\${numreads}\" >> ${study_id}.${sample}.lostcause.tsv\n    fi\n\n    study_id=gsheet\n    \"\"\"\n}",
        "nb_lignes_process": 53,
        "string_script": "    def cramfile = \"${study_id}.${sample}_merged.cram\"\n    \"\"\"\n    export REF_PATH=/lustre/scratch117/core/sciops_repository/cram_cache/%2s/%2s/%s:/lustre/scratch118/core/sciops_repository/cram_cache/%2s/%2s/%s:URL=http:://sf2-farm-srv1.internal.sanger.ac.uk::8000/%s\n    export PATH=/opt/conda/envs/nf-core-rnaseq-1.3/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n    \n    samtools merge -@ ${task.cpus} -f $cramfile ${crams}\n    f1=${study_id}.${sample}_1.fastq.gz\n    f2=${study_id}.${sample}_2.fastq.gz\n    f0=${study_id}.${sample}.fastq.gz\n    numreads=\\$(samtools view -c -F 0x900 $cramfile)\n    if (( \\$numreads >= ${params.crams_to_fastq_min_reads} )); then\n                              # -O {stdout} -u {no compression}\n                              # -N {always append /1 and /2 to the read name}\n                              # -F 0x900 (bit 1, 8, filter secondary and supplementary reads)\n      echo -e \"study_id\\\\tsample\\\\tnumreads\" > ${study_id}.${sample}.numreads.tsv\n      echo -e \"${study_id}\\\\t${sample}\\\\t\\${numreads}\" >> ${study_id}.${sample}.numreads.tsv\n      samtools collate    \\\\\n          -O -u           \\\\\n          -@ ${task.cpus} \\\\\n          $cramfile pfx-${sample} | \\\\\n      samtools fastq      \\\\\n          -N              \\\\\n          -F 0x900        \\\\\n          -@ ${task.cpus} \\\\\n          -1 \\$f1 -2 \\$f2 -0 \\$f0 \\\\\n          -\n      sleep 2\n      find . -name \\\"*.fastq.gz\\\" -type 'f' -size -160k -delete\n    else\n      echo -e \"study_id\\\\tsample\\\\tnumreads\" > ${study_id}.${sample}.lostcause.tsv\n      echo -e \"${study_id}\\\\t${sample}\\\\t\\${numreads}\" >> ${study_id}.${sample}.lostcause.tsv\n    fi\n\n    study_id=gsheet\n    \"\"\"",
        "nb_lignes_script": 34,
        "language_script": "bash",
        "tools": [
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "study_id",
            "sample",
            "crams"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "wtsi-hgi__nextflow_ci",
        "directive": [
            "tag \"${sample}-${study_id}\"",
            "publishDir \"${params.outdir}/crams_to_fastq/fastq/${study_id}/${sample}/\", mode: \"${params.copy_mode}\", overwrite: true, pattern: \"*.fastq.gz\"",
            "publishDir \"${params.outdir}/crams_to_fastq/merged_crams/${study_id}/${sample}/\", mode: \"${params.copy_mode}\", overwrite: true, pattern: \"${study_id}.${sample}_merged.cram\""
        ],
        "when": "params.run_crams_to_fastq",
        "stub": ""
    },
    "imeta_study_cellranger": {
        "name_process": "imeta_study_cellranger",
        "string_process": "process imeta_study_cellranger {\n    tag \"${sample} ${run_id} ${study_id}\"\n    \n    publishDir \"${params.outdir}/multiple_cellranger/${study_id}/\", pattern: \"${sample}.mult.cellranger.irods.txt\", mode: \"copy\"\n    \n    when: \n    params.run_imeta_study_cellranger\n\n    input: \n    tuple val(study_id), val(sample), val(run_id)\n\n    output: \n    tuple val(study_id), val(sample), val(run_id), env(CELLRANGER_IRODS_OBJECT), env(WORK_DIR), emit: study_id_sample_cellranger_object\n    env(WORK_DIR), emit: work_dir_to_remove\n    tuple val(study_id), val(sample), val(run_id), path(\"${sample}.mult.cellranger.irods.txt\"), emit: study_id_sample_mutiple_cellranger optional true\n\n    script:\n    \"\"\"\n    bash $workflow.projectDir/../bin/imeta_study_cellranger.sh ${sample} ${run_id}\n    if [ -f cellranger.object.txt ] \n    then \n        echo file cellranger.object.txt found\n        if [[ \\$(wc -l <cellranger.object.txt) -ge 2 ]]\n        then\n          echo \\\"warning: more than one cellranger output found in Irods for sample ${sample}:\\\"\n          echo \\\"....     will only download last ONE (sort | tail -n 1) of the multiple cellranger outputs from Irods.\\\"\n          cat cellranger.object.txt | sort | tail -n 1 > ${sample}.mult.cellranger.irods.txt\n        else\n          echo \\\"One and only one cellranger output found in Irods for sample ${sample}\\\"\n        fi\n        CELLRANGER_IRODS_OBJECT=\\$(cat cellranger.object.txt | sort | tail -n 1)\n        WORK_DIR=dont_remove\n        rm cellranger.object.txt\n    else\n        echo not found file cellranger.object.txt\n        CELLRANGER_IRODS_OBJECT=cellranger_irods_not_found\n        WORK_DIR=\\$PWD\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "    \"\"\"\n    bash $workflow.projectDir/../bin/imeta_study_cellranger.sh ${sample} ${run_id}\n    if [ -f cellranger.object.txt ] \n    then \n        echo file cellranger.object.txt found\n        if [[ \\$(wc -l <cellranger.object.txt) -ge 2 ]]\n        then\n          echo \\\"warning: more than one cellranger output found in Irods for sample ${sample}:\\\"\n          echo \\\"....     will only download last ONE (sort | tail -n 1) of the multiple cellranger outputs from Irods.\\\"\n          cat cellranger.object.txt | sort | tail -n 1 > ${sample}.mult.cellranger.irods.txt\n        else\n          echo \\\"One and only one cellranger output found in Irods for sample ${sample}\\\"\n        fi\n        CELLRANGER_IRODS_OBJECT=\\$(cat cellranger.object.txt | sort | tail -n 1)\n        WORK_DIR=dont_remove\n        rm cellranger.object.txt\n    else\n        echo not found file cellranger.object.txt\n        CELLRANGER_IRODS_OBJECT=cellranger_irods_not_found\n        WORK_DIR=\\$PWD\n    fi\n    \"\"\"",
        "nb_lignes_script": 21,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "study_id",
            "sample",
            "run_id"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "wtsi-hgi__nextflow_ci",
        "directive": [
            "tag \"${sample} ${run_id} ${study_id}\"",
            "publishDir \"${params.outdir}/multiple_cellranger/${study_id}/\", pattern: \"${sample}.mult.cellranger.irods.txt\", mode: \"copy\""
        ],
        "when": "params.run_imeta_study_cellranger",
        "stub": ""
    },
    "imeta_samples_csv": {
        "name_process": "imeta_samples_csv",
        "string_process": "process imeta_samples_csv {\n    tag \"${input_csv}\"\n    publishDir \"${params.outdir}/imeta_study/study_id_${study_id}/\", mode: 'copy', pattern: \"samples.tsv\", overwrite: true\n    publishDir \"${params.outdir}/\", mode: 'copy', pattern: \"samples.tsv\", overwrite: true\n    publishDir \"${params.outdir}/imeta_study/study_id_${study_id}/\", mode: 'copy', pattern: \"samples_noduplicates.tsv\", overwrite: true\n    publishDir \"${params.outdir}/\", mode: 'copy', pattern: \"samples_noduplicates.tsv\", overwrite: true\n    \n    when: \n    params.run_imeta_samples\n\n    input: \n    path(input_csv)\n    val(irods_sample_column)\n\n    output: \n    tuple env(study_id), path('samples.tsv'), emit: irods_samples_tsv\n    env(WORK_DIR), emit: work_dir_to_remove\n    tuple env(study_id), path('samples_noduplicates.tsv'), emit: samples_noduplicates_tsv\n    env(study_id), emit: study_id\n\n    script:\n    \"\"\"\n    bash $workflow.projectDir/../bin/imeta_samples.sh $input_csv \\\"$irods_sample_column\\\"\n    awk '!a[\\$1]++' samples.tsv > samples_noduplicates.tsv \n\n    # Save work dir so that it can be removed onComplete of workflow, \n    # to ensure that this task Irods search is re-run on each run NF run, \n    # in case new sequencing samples are ready: \n    WORK_DIR=\\$PWD\n    study_id=tsv\n    \"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "    \"\"\"\n    bash $workflow.projectDir/../bin/imeta_samples.sh $input_csv \\\"$irods_sample_column\\\"\n    awk '!a[\\$1]++' samples.tsv > samples_noduplicates.tsv \n\n    # Save work dir so that it can be removed onComplete of workflow, \n    # to ensure that this task Irods search is re-run on each run NF run, \n    # in case new sequencing samples are ready: \n    WORK_DIR=\\$PWD\n    study_id=tsv\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "input_csv",
            "irods_sample_column"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "wtsi-hgi__nextflow_ci",
        "directive": [
            "tag \"${input_csv}\"",
            "publishDir \"${params.outdir}/imeta_study/study_id_${study_id}/\", mode: 'copy', pattern: \"samples.tsv\", overwrite: true",
            "publishDir \"${params.outdir}/\", mode: 'copy', pattern: \"samples.tsv\", overwrite: true",
            "publishDir \"${params.outdir}/imeta_study/study_id_${study_id}/\", mode: 'copy', pattern: \"samples_noduplicates.tsv\", overwrite: true",
            "publishDir \"${params.outdir}/\", mode: 'copy', pattern: \"samples_noduplicates.tsv\", overwrite: true"
        ],
        "when": "params.run_imeta_samples",
        "stub": ""
    }
}