{
    "FASTQC": {
        "name_process": "FASTQC",
        "string_process": "\nprocess FASTQC {\n    publishDir \"${params.outdir}/figures\", mode: \"copy\"\n    label \"high_computation\"\n\n    container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n\n    input:\n    path fastqs\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    fastqc -o . --threads $task.cpus $fastqs\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    fastqc -o . --threads $task.cpus $fastqs\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "FastQC"
        ],
        "tools_url": [
            "https://bio.tools/fastqc"
        ],
        "tools_dico": [
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            }
        ],
        "inputs": [
            "fastqs"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "publishDir \"${params.outdir}/figures\", mode: \"copy\"",
            "label \"high_computation\"",
            "container \"quay.io/biocontainers/fastqc:0.11.9--0\""
        ],
        "when": "",
        "stub": ""
    },
    "GUESS_MATCH_ORDER": {
        "name_process": "GUESS_MATCH_ORDER",
        "string_process": "\nprocess GUESS_MATCH_ORDER {\n    publishDir \"${params.outdir}/interm/guess_matching_order\", mode: \"copy\"\n    label \"process_low\"\n\n    conda (params.enable_conda ? \"conda-forge::bash='5.1'\" : null)\n    container \"nakor/bash:5.1.4\"\n    \n    input:\n    path index\n    path \"meta.csv\"                                                              \n\n    output:\n    path \"barcodes_meta.csv\"\n\n    script:\n    z = index[0].getExtension() == 'gz' ? 'z' : ''\n    fwd = index[0]\n    rev = index.size() == 2 ? index[1] : ''\n    \"\"\"\n    #!/usr/bin/env bash\n\n    # Remove carriage returns at the end of file\n    sed -e 's/\\\\r//g' meta.csv > metadata.csv\n\n    # Reverse complement if set\n    if [ \"$params.rc_rev_index\" == true ]; then\n        cat metadata.csv | rev | cut -d, -f1 | tr \"ATGC\" \"TACG\" > rev_idx_rc.csv\n        cat metadata.csv | rev | cut -d, -f2- | rev > samp_idx1.csv\n        paste -d, samp_idx1.csv rev_idx_rc.csv > metadata.csv\n    fi\n\n    if [ ${index.size()} -eq 2 ]; then\n        reversed=\\$([ $params.matching == reversed ] && echo true || echo false)\n\n        # if \"auto\", we check which matching order looks best\n        if [ \"$params.matching\" == \"auto\" ]; then\n            symbol=\\$(${z}cat $fwd | head -c 2)\n            \n            # extract the most frequent fwd/rev index reads pairs and compare it to the metadata \n            paste -d' ' \\\\\n                <( ${z}cat $fwd | grep \"^\\$symbol\" -A1 | grep -v \\$symbol | grep -v \"-\" ) \\\\\n                <( ${z}cat $rev | grep \"^\\$symbol\" -A1 | grep -v \\$symbol | grep -v \"-\" ) \\\\\n                | grep -v N \\\\\n                | sort | uniq -c | sort -rnk1 | head -n 20 \\\\\n                | sed 's/^[[:space:]]*//g' | sed 's/ /,/g' \\\\\n                | cut -d, -f2,3 > freqs.txt\n\n            awk -F, '{OFS=\",\"} {print \\$2,\\$1}' freqs.txt > freqs_rc.txt\n\n            # take the order that matches best\n            n1=\\$(comm -12 <(sort <(cut -d, -f2,3 metadata.csv)) <(sort freqs.txt) | wc -l)\n            n2=\\$(comm -12 <(sort <(cut -d, -f2,3 metadata.csv)) <(sort freqs_rc.txt) | wc -l)\n\n            reversed=\\$([ \\$n2 -ge \\$n1 ] && echo true || echo false)\n        fi\n\n        [ \\$reversed==true ] \\\\\n            && awk -F\",\" '{OFS=\",\"}{print \\$1,\\$3,\\$2}' metadata.csv > barcodes_meta.csv \\\\\n            || mv metadata.csv barcodes_meta.csv\n    else\n        awk -F\",\" '{OFS=\",\"}{print \\$1,\\$2,NaN}' metadata.csv > barcodes_meta.csv\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 63,
        "string_script": "    z = index[0].getExtension() == 'gz' ? 'z' : ''\n    fwd = index[0]\n    rev = index.size() == 2 ? index[1] : ''\n    \"\"\"\n    #!/usr/bin/env bash\n\n    # Remove carriage returns at the end of file\n    sed -e 's/\\\\r//g' meta.csv > metadata.csv\n\n    # Reverse complement if set\n    if [ \"$params.rc_rev_index\" == true ]; then\n        cat metadata.csv | rev | cut -d, -f1 | tr \"ATGC\" \"TACG\" > rev_idx_rc.csv\n        cat metadata.csv | rev | cut -d, -f2- | rev > samp_idx1.csv\n        paste -d, samp_idx1.csv rev_idx_rc.csv > metadata.csv\n    fi\n\n    if [ ${index.size()} -eq 2 ]; then\n        reversed=\\$([ $params.matching == reversed ] && echo true || echo false)\n\n        # if \"auto\", we check which matching order looks best\n        if [ \"$params.matching\" == \"auto\" ]; then\n            symbol=\\$(${z}cat $fwd | head -c 2)\n            \n            # extract the most frequent fwd/rev index reads pairs and compare it to the metadata \n            paste -d' ' \\\\\n                <( ${z}cat $fwd | grep \"^\\$symbol\" -A1 | grep -v \\$symbol | grep -v \"-\" ) \\\\\n                <( ${z}cat $rev | grep \"^\\$symbol\" -A1 | grep -v \\$symbol | grep -v \"-\" ) \\\\\n                | grep -v N \\\\\n                | sort | uniq -c | sort -rnk1 | head -n 20 \\\\\n                | sed 's/^[[:space:]]*//g' | sed 's/ /,/g' \\\\\n                | cut -d, -f2,3 > freqs.txt\n\n            awk -F, '{OFS=\",\"} {print \\$2,\\$1}' freqs.txt > freqs_rc.txt\n\n            # take the order that matches best\n            n1=\\$(comm -12 <(sort <(cut -d, -f2,3 metadata.csv)) <(sort freqs.txt) | wc -l)\n            n2=\\$(comm -12 <(sort <(cut -d, -f2,3 metadata.csv)) <(sort freqs_rc.txt) | wc -l)\n\n            reversed=\\$([ \\$n2 -ge \\$n1 ] && echo true || echo false)\n        fi\n\n        [ \\$reversed==true ] \\\\\n            && awk -F\",\" '{OFS=\",\"}{print \\$1,\\$3,\\$2}' metadata.csv > barcodes_meta.csv \\\\\n            || mv metadata.csv barcodes_meta.csv\n    else\n        awk -F\",\" '{OFS=\",\"}{print \\$1,\\$2,NaN}' metadata.csv > barcodes_meta.csv\n    fi\n    \"\"\"",
        "nb_lignes_script": 47,
        "language_script": "bash",
        "tools": [
            "denvax"
        ],
        "tools_url": [
            "https://bio.tools/denvax"
        ],
        "tools_dico": [
            {
                "name": "denvax",
                "uri": "https://bio.tools/denvax",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3379",
                            "term": "Preclinical and clinical studies"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatric medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "https://en.wikipedia.org/wiki/Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0560",
                                    "term": "DNA vaccine design"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Serostatus testing and dengue vaccine cost-benefit thresholds | R package for manuscript \"Serostatus Testing & Dengue Vaccine Cost Benefit Thresholds\"",
                "homepage": "https://cran.r-project.org/web/packages/denvax/index.html"
            }
        ],
        "inputs": [
            "index"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "publishDir \"${params.outdir}/interm/guess_matching_order\", mode: \"copy\"",
            "label \"process_low\"",
            "conda (params.enable_conda ? \"conda-forge::bash='5.1'\" : null)",
            "container \"nakor/bash:5.1.4\""
        ],
        "when": "",
        "stub": ""
    },
    "TO_H5": {
        "name_process": "TO_H5",
        "string_process": "\nprocess TO_H5 {\n    tag \"$split\"\n    label \"process_medium\"\n\n    container \"nakor/metaflowmics-python:0.0.1\"\n    conda (params.enable_conda ? \"conda-forge::biopython conda-forge::h5py conda-forge::numpy\" : null)\n\n    input:\n    tuple val(split), file(index)\n\n    output:\n    tuple val(split), path(\"*.h5\")\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import argparse\n\n    from Bio.SeqIO.QualityIO import FastqGeneralIterator\n    import numpy as np\n    import h5py\n\n    NUCLS = list('ACGTN')\n    MAPPING_BASE5 = str.maketrans(''.join(NUCLS), '01234')\n    RC_TRANS = str.maketrans('ACGT', 'TGCA')\n\n    def seq2str(seq):\n        if hasattr(seq, 'seq'):\n            return str(seq.seq)\n        return seq\n\n    def seq2intlist(bc):\n        bc_vec = np.fromiter(seq2str(bc).translate(MAPPING_BASE5), dtype='uint8')\n        return bc_vec\n\n    def seq2idx(bc):\n        bc_idx = int(seq2str(bc).translate(MAPPING_BASE5), 5)\n        return bc_idx\n\n    def get_lengths(filename):\n        handle = open(filename)\n        rid_len = len(next(handle).strip().split()[0])\n        bc_len = len(next(handle).strip())\n\n        return (rid_len, bc_len)\n\n    def fastqs_to_h5(fastqs, n_reads, bc_len=None, rid_len=None, split=0, rc=False):\n        parsers = [FastqGeneralIterator(open(fastq)) for fastq in fastqs]\n\n        data = [\n            {'rid': np.empty(n_reads, dtype='S{}'.format(rid_len+10)),\n             'index': np.zeros(n_reads, dtype='uint32'),\n             'seq': np.zeros((n_reads, bc_len), dtype='uint8'),\n             'qual': np.zeros((n_reads, bc_len), dtype='uint8')}\n            for _ in fastqs]\n\n        for k, parser in enumerate(parsers):\n            for i, (rid, seq, qual) in enumerate(parser):\n                if rc:\n                    seq = seq.translate(RC_TRANS)\n                data[k]['rid'][i] = rid.split()[0]\n                data[k]['index'][i] = seq2idx(seq)\n                data[k]['seq'][i] = seq2intlist(seq)\n                data[k]['qual'][i] = [ord(score)-33 for score in qual]\n\n                if k == 1:\n                    (rid_fwd, rid_rev) = data[0]['rid'][i], data[1]['rid'][i]\n                    if rid_fwd != rid_rev:\n                        print('Error: {} != {}: forward and reverse read IDs do not match.'\n                              .format(rid_fwd, rid_rev))\n                        exit(42)\n\n        h5_handle = h5py.File('data_{}.h5'.format(split), 'w')\n\n        options = ['fwd', 'rev'][:len(fastqs)]\n        for k, orient in enumerate(options):\n            for key, val in data[k].items():\n                h5_handle.create_dataset('{}/{}'.format(key, orient), data=val)\n        h5_handle.close()\n    \n    n_reads = sum(1 for _ in open(\"${index[0]}\")) // 4\n    (rid_len, bc_len) = get_lengths(\"${index[0]}\")\n    \n    fastqs_to_h5(\n        [\"${index.join('\", \"')}\"], \n        n_reads, \n        bc_len=bc_len, rid_len=rid_len, \n        split=$split, rc=${params.rc_rev_index.toString().capitalize()})\n    \"\"\"\n}",
        "nb_lignes_process": 90,
        "string_script": "    \"\"\"\n    #!/usr/bin/env python\n\n    import argparse\n\n    from Bio.SeqIO.QualityIO import FastqGeneralIterator\n    import numpy as np\n    import h5py\n\n    NUCLS = list('ACGTN')\n    MAPPING_BASE5 = str.maketrans(''.join(NUCLS), '01234')\n    RC_TRANS = str.maketrans('ACGT', 'TGCA')\n\n    def seq2str(seq):\n        if hasattr(seq, 'seq'):\n            return str(seq.seq)\n        return seq\n\n    def seq2intlist(bc):\n        bc_vec = np.fromiter(seq2str(bc).translate(MAPPING_BASE5), dtype='uint8')\n        return bc_vec\n\n    def seq2idx(bc):\n        bc_idx = int(seq2str(bc).translate(MAPPING_BASE5), 5)\n        return bc_idx\n\n    def get_lengths(filename):\n        handle = open(filename)\n        rid_len = len(next(handle).strip().split()[0])\n        bc_len = len(next(handle).strip())\n\n        return (rid_len, bc_len)\n\n    def fastqs_to_h5(fastqs, n_reads, bc_len=None, rid_len=None, split=0, rc=False):\n        parsers = [FastqGeneralIterator(open(fastq)) for fastq in fastqs]\n\n        data = [\n            {'rid': np.empty(n_reads, dtype='S{}'.format(rid_len+10)),\n             'index': np.zeros(n_reads, dtype='uint32'),\n             'seq': np.zeros((n_reads, bc_len), dtype='uint8'),\n             'qual': np.zeros((n_reads, bc_len), dtype='uint8')}\n            for _ in fastqs]\n\n        for k, parser in enumerate(parsers):\n            for i, (rid, seq, qual) in enumerate(parser):\n                if rc:\n                    seq = seq.translate(RC_TRANS)\n                data[k]['rid'][i] = rid.split()[0]\n                data[k]['index'][i] = seq2idx(seq)\n                data[k]['seq'][i] = seq2intlist(seq)\n                data[k]['qual'][i] = [ord(score)-33 for score in qual]\n\n                if k == 1:\n                    (rid_fwd, rid_rev) = data[0]['rid'][i], data[1]['rid'][i]\n                    if rid_fwd != rid_rev:\n                        print('Error: {} != {}: forward and reverse read IDs do not match.'\n                              .format(rid_fwd, rid_rev))\n                        exit(42)\n\n        h5_handle = h5py.File('data_{}.h5'.format(split), 'w')\n\n        options = ['fwd', 'rev'][:len(fastqs)]\n        for k, orient in enumerate(options):\n            for key, val in data[k].items():\n                h5_handle.create_dataset('{}/{}'.format(key, orient), data=val)\n        h5_handle.close()\n    \n    n_reads = sum(1 for _ in open(\"${index[0]}\")) // 4\n    (rid_len, bc_len) = get_lengths(\"${index[0]}\")\n    \n    fastqs_to_h5(\n        [\"${index.join('\", \"')}\"], \n        n_reads, \n        bc_len=bc_len, rid_len=rid_len, \n        split=$split, rc=${params.rc_rev_index.toString().capitalize()})\n    \"\"\"",
        "nb_lignes_script": 75,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "split",
            "index"
        ],
        "nb_inputs": 2,
        "outputs": [
            "split"
        ],
        "nb_outputs": 1,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$split\"",
            "label \"process_medium\"",
            "container \"nakor/metaflowmics-python:0.0.1\"",
            "conda (params.enable_conda ? \"conda-forge::biopython conda-forge::h5py conda-forge::numpy\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "ERROR_MODEL": {
        "name_process": "ERROR_MODEL",
        "string_process": "\nprocess ERROR_MODEL {\n    publishDir \"${params.outdir}/interm/error_model\", mode: \"copy\"\n    label \"process_high\"\n\n    container \"nakor/metaflowmics-python:0.0.1\"\n    conda (params.enable_conda ? \"conda-forge::pandas conda-forge::h5py conda-forge::scikit-learn conda-forge::bokeh\" : null)\n\n    input:\n    path h5\n    path meta_file\n\n    output:\n    path \"*.h5\", emit: h5\n    path \"*.html\", emit: html\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import argparse\n    from glob import glob\n\n    import pandas as pd\n    import numpy as np\n    import h5py\n    from sklearn.isotonic import IsotonicRegression\n\n    from bokeh.io import output_file, save\n    from bokeh.plotting import figure\n    from bokeh.layouts import gridplot\n\n    NUCLS = list('ACGTN')\n    MAPPING_BASE5 = str.maketrans(''.join(NUCLS), '01234')\n\n\n    def seq2str(seq):\n        if hasattr(seq, 'seq'):\n            return str(seq.seq)\n        return seq\n\n    def seq2intlist(bc):\n        bc_vec = np.fromiter(seq2str(bc).translate(MAPPING_BASE5), dtype='uint8')\n        return bc_vec\n\n    def get_h5_keys():\n        h5_file = glob(\"*.h5\")[0]\n        handle = h5py.File(h5_file, 'r')\n\n        return list(handle.get('seq').keys())\n\n    def load_h5():\n        orientations = get_h5_keys()\n\n        indexes = {orient: [] for orient in orientations}\n        sequences = {orient: [] for orient in orientations}\n        qualities = {orient: [] for orient in orientations}\n\n        for h5 in sorted(glob(\"*.h5\")):\n            handle = h5py.File(h5, 'r')\n            for orient in orientations:\n                indexes[orient].append(handle.get('index/{}'.format(orient))[:])\n                sequences[orient].append(handle.get('seq/{}'.format(orient))[:])\n                qualities[orient].append(handle.get('qual/{}'.format(orient))[:])\n            handle.close()\n\n        fastq_data = {\n            orient: {'index': np.concatenate(indexes[orient]),\n                     'seq': np.vstack(sequences[orient]),\n                     'qual': np.vstack(qualities[orient])}\n            for orient in orientations\n        }\n        return fastq_data\n\n    def idx2seq(nb):\n        repr_b5 = \"{:08}\".format(int(np.base_repr(nb, 5)))\n        nucl = repr_b5.translate(str.maketrans(\"01234\", ''.join(NUCLS)))\n        return nucl\n\n\n    def compute_transition_matrix(fastq_data, barcodes, max_dist=$params.max_mismatches, \n                                  n_bases_max=int($params.n_bases), eps=1e-6):\n        '''\n        - Barcodes in a numpy array of nucleotide index (base 5)\n        -\n        '''\n\n        # barcodes_index = np.array([int(''.join(map(str, bc)), 5) for bc in barcodes])\n        bc_len = len(barcodes[0])\n        transition_matrix = np.zeros((4, 5, 40)) # Transitions (A,C,G,T) to (A,C,G,T,N) and quality_scores\n        generator = zip(fastq_data['index'], fastq_data['seq'], fastq_data['qual'])\n        n_bases = 0\n\n        mem = {}\n        while n_bases < n_bases_max:\n\n            try:\n                (code, intlist, qual) = next(generator)\n            except StopIteration:\n                print('Warning: not enough bases for the error model ({:,}/{:,})'\n                      .format(n_bases, n_bases_max))\n\n            if n_bases % 10*bc_len == 0:\n                print(\"{:,}/{:,}\".format(n_bases, n_bases_max), end='\\\\r')\n\n            if code in mem:\n                matching_bc = mem[code]\n            else:\n                matching_bc = barcodes[np.sum(barcodes != intlist[None, :], axis=1) <= max_dist]\n                mem[code] = matching_bc\n\n            if len(matching_bc) == 0:\n                continue\n\n            for bc in matching_bc:\n                tup, counts = np.unique(np.vstack((bc, intlist, qual-1)),\n                                        return_counts=True, axis=1)\n                transition_matrix[tup[0, :], tup[1, :], tup[2, :]] += counts\n                n_bases += bc_len\n\n        sums = transition_matrix.sum(axis=0)\n        sums[sums==0] = eps\n\n        transition_matrix /= sums\n\n        return transition_matrix\n    \n    def regression_model(freqs, deg=2, same=False, method='isotonic'):\n        '''\n        - qual: all measured quality scores when a transition was observed\n        - proportion of transitions for a given quality\n        '''\n\n        observed_transitions = (~np.isnan(freqs)) & (freqs>0)\n\n        x = np.arange(1, 41)[observed_transitions]\n        y = -np.log10(freqs[observed_transitions])\n\n        if len(x) == 0:\n            return np.zeros(40)\n\n        if method == 'polynomial':\n            z = np.polyfit(x, y, 3)\n            polynom = np.poly1d(z)\n            y_interp = 10**-polynom(np.arange(1, 41))\n        elif method == 'isotonic':\n            ir = IsotonicRegression(y_min=0, out_of_bounds='clip', increasing=not same)\n            ir.fit(x, y)\n            y_interp = 10**-ir.predict(np.arange(1, 41))\n        else:\n            print('Unknown method: {}. Aborting.'.format(method))\n            exit(1)\n        return y_interp\n\n    def compute_error_models(idx, barcodes, orient='fwd'):\n\n        barcodes_int = np.array(\n            [list(seq.translate(MAPPING_BASE5)) for seq in barcodes]\n        ).astype('uint8')\n\n        transitions = compute_transition_matrix(idx, barcodes_int)\n\n        transitions_interp = np.array(\n            [[regression_model(err_prob, same=(i==j))\n              for i, err_prob in enumerate(transition_from_orig)]\n             for j, transition_from_orig in enumerate(transitions)])\n\n        display_models_bokeh(transitions, transitions_interp, orient=orient)\n        print('Error model computed')\n\n        return transitions_interp\n\n    def display_models_bokeh(trans_matrix, trans_matrix_interp, orient='fwd'):\n        data = []\n\n        for matrix in [trans_matrix, trans_matrix_interp]:\n            # For each matrix, stores non-zero probabilities with transitions and quality\n            nz_info = np.nonzero(matrix)\n            data.append(np.vstack((matrix[nz_info], *nz_info)).T)\n\n        labels = np.array(['y1']*len(data[0]) + ['y_hat']*len(data[1]))\n\n        data = np.vstack(data)\n\n        transitions = [\"{}->{}\".format(NUCLS[i], NUCLS[j]) for i, j in data[:, [1, 2]].astype(int)]\n\n        data = pd.DataFrame({'transition': transitions,\n                             'quality': 1 + data[:, 3].astype(int),\n                             'P_err': data[:, 0],\n                             'type': labels})\n\n        data.set_index(['transition', 'type'], inplace=True)\n\n        plots = []\n        tools = ['hover', 'box_zoom', 'reset']\n\n        observed_transitions = set(pd.MultiIndex.get_level_values(data.index,'transition'))\n        for i, nucl_i in enumerate(NUCLS[:-1]):\n            for j, nucl_j in enumerate(NUCLS):\n                transition_label = \"{}->{}\".format(nucl_i, nucl_j)\n                if transition_label not in observed_transitions:\n                    plots.append(None)\n                    continue\n\n                data_s = data.loc[transition_label].sort_values(by='quality')\n\n                p = figure(title=\"{}->{}\".format(nucl_i, nucl_j), tooltips=[], tools=tools,\n                           x_range=(0, 40), y_range=(-0.1, 1.1))\n                p.circle(x='quality', y='P_err', source=data_s.loc[['y1']], alpha=0.7, size=8)\n                p.line(x='quality', y='P_err', source=data_s.loc[['y_hat']], color='red',\n                       line_dash=\"dashed\", line_width=2)\n                plots.append(p)\n\n        grid = gridplot(plots, ncols=5, plot_width=300, plot_height=300)\n        output_file(\"error_model_{}.html\".format(orient))\n        save(grid)\n\n    # main script\n    barcodes = (pd.read_csv(\"$meta_file\", dtype=str, names=[\"sample_name\", \"fwd\", \"rev\"])\n                .dropna(axis=1, how='all')\n                .drop('sample_name', axis=1))\n\n    fastq_data = load_h5()\n\n    transition_h5 = h5py.File('transition_probs.h5', 'w')\n    for (orient, val) in fastq_data.items():\n        error_trans = compute_error_models(val, barcodes[orient].unique(), orient=orient)\n        transition_h5.create_dataset(orient, data=error_trans, dtype='f4')\n    transition_h5.close()\n    \"\"\"\n    \n}",
        "nb_lignes_process": 230,
        "string_script": "    \"\"\"\n    #!/usr/bin/env python\n\n    import argparse\n    from glob import glob\n\n    import pandas as pd\n    import numpy as np\n    import h5py\n    from sklearn.isotonic import IsotonicRegression\n\n    from bokeh.io import output_file, save\n    from bokeh.plotting import figure\n    from bokeh.layouts import gridplot\n\n    NUCLS = list('ACGTN')\n    MAPPING_BASE5 = str.maketrans(''.join(NUCLS), '01234')\n\n\n    def seq2str(seq):\n        if hasattr(seq, 'seq'):\n            return str(seq.seq)\n        return seq\n\n    def seq2intlist(bc):\n        bc_vec = np.fromiter(seq2str(bc).translate(MAPPING_BASE5), dtype='uint8')\n        return bc_vec\n\n    def get_h5_keys():\n        h5_file = glob(\"*.h5\")[0]\n        handle = h5py.File(h5_file, 'r')\n\n        return list(handle.get('seq').keys())\n\n    def load_h5():\n        orientations = get_h5_keys()\n\n        indexes = {orient: [] for orient in orientations}\n        sequences = {orient: [] for orient in orientations}\n        qualities = {orient: [] for orient in orientations}\n\n        for h5 in sorted(glob(\"*.h5\")):\n            handle = h5py.File(h5, 'r')\n            for orient in orientations:\n                indexes[orient].append(handle.get('index/{}'.format(orient))[:])\n                sequences[orient].append(handle.get('seq/{}'.format(orient))[:])\n                qualities[orient].append(handle.get('qual/{}'.format(orient))[:])\n            handle.close()\n\n        fastq_data = {\n            orient: {'index': np.concatenate(indexes[orient]),\n                     'seq': np.vstack(sequences[orient]),\n                     'qual': np.vstack(qualities[orient])}\n            for orient in orientations\n        }\n        return fastq_data\n\n    def idx2seq(nb):\n        repr_b5 = \"{:08}\".format(int(np.base_repr(nb, 5)))\n        nucl = repr_b5.translate(str.maketrans(\"01234\", ''.join(NUCLS)))\n        return nucl\n\n\n    def compute_transition_matrix(fastq_data, barcodes, max_dist=$params.max_mismatches, \n                                  n_bases_max=int($params.n_bases), eps=1e-6):\n        '''\n        - Barcodes in a numpy array of nucleotide index (base 5)\n        -\n        '''\n\n        # barcodes_index = np.array([int(''.join(map(str, bc)), 5) for bc in barcodes])\n        bc_len = len(barcodes[0])\n        transition_matrix = np.zeros((4, 5, 40)) # Transitions (A,C,G,T) to (A,C,G,T,N) and quality_scores\n        generator = zip(fastq_data['index'], fastq_data['seq'], fastq_data['qual'])\n        n_bases = 0\n\n        mem = {}\n        while n_bases < n_bases_max:\n\n            try:\n                (code, intlist, qual) = next(generator)\n            except StopIteration:\n                print('Warning: not enough bases for the error model ({:,}/{:,})'\n                      .format(n_bases, n_bases_max))\n\n            if n_bases % 10*bc_len == 0:\n                print(\"{:,}/{:,}\".format(n_bases, n_bases_max), end='\\\\r')\n\n            if code in mem:\n                matching_bc = mem[code]\n            else:\n                matching_bc = barcodes[np.sum(barcodes != intlist[None, :], axis=1) <= max_dist]\n                mem[code] = matching_bc\n\n            if len(matching_bc) == 0:\n                continue\n\n            for bc in matching_bc:\n                tup, counts = np.unique(np.vstack((bc, intlist, qual-1)),\n                                        return_counts=True, axis=1)\n                transition_matrix[tup[0, :], tup[1, :], tup[2, :]] += counts\n                n_bases += bc_len\n\n        sums = transition_matrix.sum(axis=0)\n        sums[sums==0] = eps\n\n        transition_matrix /= sums\n\n        return transition_matrix\n    \n    def regression_model(freqs, deg=2, same=False, method='isotonic'):\n        '''\n        - qual: all measured quality scores when a transition was observed\n        - proportion of transitions for a given quality\n        '''\n\n        observed_transitions = (~np.isnan(freqs)) & (freqs>0)\n\n        x = np.arange(1, 41)[observed_transitions]\n        y = -np.log10(freqs[observed_transitions])\n\n        if len(x) == 0:\n            return np.zeros(40)\n\n        if method == 'polynomial':\n            z = np.polyfit(x, y, 3)\n            polynom = np.poly1d(z)\n            y_interp = 10**-polynom(np.arange(1, 41))\n        elif method == 'isotonic':\n            ir = IsotonicRegression(y_min=0, out_of_bounds='clip', increasing=not same)\n            ir.fit(x, y)\n            y_interp = 10**-ir.predict(np.arange(1, 41))\n        else:\n            print('Unknown method: {}. Aborting.'.format(method))\n            exit(1)\n        return y_interp\n\n    def compute_error_models(idx, barcodes, orient='fwd'):\n\n        barcodes_int = np.array(\n            [list(seq.translate(MAPPING_BASE5)) for seq in barcodes]\n        ).astype('uint8')\n\n        transitions = compute_transition_matrix(idx, barcodes_int)\n\n        transitions_interp = np.array(\n            [[regression_model(err_prob, same=(i==j))\n              for i, err_prob in enumerate(transition_from_orig)]\n             for j, transition_from_orig in enumerate(transitions)])\n\n        display_models_bokeh(transitions, transitions_interp, orient=orient)\n        print('Error model computed')\n\n        return transitions_interp\n\n    def display_models_bokeh(trans_matrix, trans_matrix_interp, orient='fwd'):\n        data = []\n\n        for matrix in [trans_matrix, trans_matrix_interp]:\n            # For each matrix, stores non-zero probabilities with transitions and quality\n            nz_info = np.nonzero(matrix)\n            data.append(np.vstack((matrix[nz_info], *nz_info)).T)\n\n        labels = np.array(['y1']*len(data[0]) + ['y_hat']*len(data[1]))\n\n        data = np.vstack(data)\n\n        transitions = [\"{}->{}\".format(NUCLS[i], NUCLS[j]) for i, j in data[:, [1, 2]].astype(int)]\n\n        data = pd.DataFrame({'transition': transitions,\n                             'quality': 1 + data[:, 3].astype(int),\n                             'P_err': data[:, 0],\n                             'type': labels})\n\n        data.set_index(['transition', 'type'], inplace=True)\n\n        plots = []\n        tools = ['hover', 'box_zoom', 'reset']\n\n        observed_transitions = set(pd.MultiIndex.get_level_values(data.index,'transition'))\n        for i, nucl_i in enumerate(NUCLS[:-1]):\n            for j, nucl_j in enumerate(NUCLS):\n                transition_label = \"{}->{}\".format(nucl_i, nucl_j)\n                if transition_label not in observed_transitions:\n                    plots.append(None)\n                    continue\n\n                data_s = data.loc[transition_label].sort_values(by='quality')\n\n                p = figure(title=\"{}->{}\".format(nucl_i, nucl_j), tooltips=[], tools=tools,\n                           x_range=(0, 40), y_range=(-0.1, 1.1))\n                p.circle(x='quality', y='P_err', source=data_s.loc[['y1']], alpha=0.7, size=8)\n                p.line(x='quality', y='P_err', source=data_s.loc[['y_hat']], color='red',\n                       line_dash=\"dashed\", line_width=2)\n                plots.append(p)\n\n        grid = gridplot(plots, ncols=5, plot_width=300, plot_height=300)\n        output_file(\"error_model_{}.html\".format(orient))\n        save(grid)\n\n    # main script\n    barcodes = (pd.read_csv(\"$meta_file\", dtype=str, names=[\"sample_name\", \"fwd\", \"rev\"])\n                .dropna(axis=1, how='all')\n                .drop('sample_name', axis=1))\n\n    fastq_data = load_h5()\n\n    transition_h5 = h5py.File('transition_probs.h5', 'w')\n    for (orient, val) in fastq_data.items():\n        error_trans = compute_error_models(val, barcodes[orient].unique(), orient=orient)\n        transition_h5.create_dataset(orient, data=error_trans, dtype='f4')\n    transition_h5.close()\n    \"\"\"",
        "nb_lignes_script": 212,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "h5",
            "meta_file"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "publishDir \"${params.outdir}/interm/error_model\", mode: \"copy\"",
            "label \"process_high\"",
            "container \"nakor/metaflowmics-python:0.0.1\"",
            "conda (params.enable_conda ? \"conda-forge::pandas conda-forge::h5py conda-forge::scikit-learn conda-forge::bokeh\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MAP_INDEX_TO_SAMPLE": {
        "name_process": "MAP_INDEX_TO_SAMPLE",
        "string_process": "\nprocess MAP_INDEX_TO_SAMPLE {\n    publishDir \"${params.outdir}/interm/sample_index_mapping\", mode: \"copy\"\n    tag \"$split\"\n    label \"process_high\"\n    stageInMode \"copy\"\n\n    container \"nakor/metaflowmics-python:0.0.1\"\n    conda (params.enable_conda ? \"conda-forge::pandas conda-forge::h5py\" : null)\n\n    input:\n    tuple val(split), path(h5)\n    path error_model\n    path meta_file\n\n    output:\n    tuple val(split), path(\"demux*.tsv\"), emit: tsv\n    path \"sample_counts*.csv\", emit: counts\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    import numpy as np\n    import h5py\n\n\n    NUCLS = list('ACGTN')\n    MAPPING_BASE5 = str.maketrans(''.join(NUCLS), '01234')\n\n    def seq2str(seq):\n        if hasattr(seq, 'seq'):\n            return str(seq.seq)\n        return seq\n\n    def seq2intlist(bc):\n        bc_vec = np.fromiter(seq2str(bc).translate(MAPPING_BASE5), dtype='uint8')\n        return bc_vec\n\n    def idx2seq(nb):\n        repr_b5 = \"{:08}\".format(int(np.base_repr(nb, 5)))\n        nucl = repr_b5.translate(MAPPING_BASE5)\n        return nucl\n\n    def from_h5(filename, field=None):\n        handle = h5py.File(filename, 'r')\n        keys = handle.keys()\n\n        if field is not None:\n            keys = [\"{}/{}\".format(field, key) \n                    for key in handle.get(field).keys()]\n\n        data = {key.split('/')[-1]: np.array(handle.get(key)[:]) for key in keys}\n\n        handle.close()\n\n        return data\n\n    def calc_probs(sequences, qualities, error_model, barcodes, max_mismatches=2):\n        '''\n        data: index read data (seq index, codes and quals)\n        '''\n\n        print('Loading data')\n        transition_probs = from_h5(error_model)\n        orientations = transition_probs.keys()\n\n        print('Computing likelihood')\n\n        bc_sequences = {orient: np.array([seq2intlist(bc) for bc in barcodes[orient]])\n                        for orient in orientations}\n\n        transitions_all = [transition_probs[orient][\n            bc_sequences[orient][:, None],\n            sequences[orient],\n            qualities[orient]-1].prod(axis=2) for orient in orientations]\n\n        if len(transitions_all) == 1:\n            assignments = transitions_all[0].argmax(axis=0)\n        else:\n            assignments = np.multiply(*transitions_all).argmax(axis=0)\n\n        diffs = sum([bc_sequences[orient][assignments, :] != sequences[orient]\n                     for orient in orientations]).sum(axis=1)\n\n        assignments = barcodes.iloc[assignments].reset_index()\n        assignments.insert(1, 'mismatches', diffs)\n        assignments.loc[diffs > max_mismatches, 'sample_name'] = np.nan\n\n        return assignments.values\n\n    def format_result(assignments, rids, seq):\n\n        trans = str.maketrans('01234', 'ACGTN')\n        index_string = np.array(\n            [[''.join(x) for x in np.core.defchararray.translate(codes.astype('U1'), trans)]\n            for codes in seq.values()])\n\n        result = np.vstack([rids[None, :], index_string, assignments[:, [0, 1]].T]).T\n\n        return pd.DataFrame(result)\n    \n    # main script\n\n    barcodes = pd.read_csv(\"$meta_file\", dtype=str, names=[\"sample_name\", \"fwd\", \"rev\"]).dropna(how='all', axis=1)\n    barcodes.sample_name = barcodes.sample_name.str.replace(\"[^a-zA-Z0-9_.]\", \"_\", regex=True)\n    barcodes.set_index('sample_name', inplace=True)\n\n    read_ids = from_h5(\"$h5\", field='rid')['fwd'].astype(str)\n    sequences = from_h5(\"$h5\", field='seq')\n    quals = from_h5(\"$h5\", field='qual')\n\n    assignments = calc_probs(sequences, quals, \"$error_model\", barcodes, max_mismatches=$params.max_mismatches)\n    result = format_result(assignments, read_ids, sequences)\n\n    result.to_csv(\"demux_info_${split}.tsv\", sep='\\\\t', header=False, index=False)\n\n    sample_sizes = result[result.columns[-2]].astype(str).value_counts()\n\n    (\n        sample_sizes[~sample_sizes.index.isnull()]\n        .to_csv(\"sample_counts_${split}.csv\", header=False)\n    )\n    \n    \n    \"\"\"\n    \n}",
        "nb_lignes_process": 127,
        "string_script": "    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    import numpy as np\n    import h5py\n\n\n    NUCLS = list('ACGTN')\n    MAPPING_BASE5 = str.maketrans(''.join(NUCLS), '01234')\n\n    def seq2str(seq):\n        if hasattr(seq, 'seq'):\n            return str(seq.seq)\n        return seq\n\n    def seq2intlist(bc):\n        bc_vec = np.fromiter(seq2str(bc).translate(MAPPING_BASE5), dtype='uint8')\n        return bc_vec\n\n    def idx2seq(nb):\n        repr_b5 = \"{:08}\".format(int(np.base_repr(nb, 5)))\n        nucl = repr_b5.translate(MAPPING_BASE5)\n        return nucl\n\n    def from_h5(filename, field=None):\n        handle = h5py.File(filename, 'r')\n        keys = handle.keys()\n\n        if field is not None:\n            keys = [\"{}/{}\".format(field, key) \n                    for key in handle.get(field).keys()]\n\n        data = {key.split('/')[-1]: np.array(handle.get(key)[:]) for key in keys}\n\n        handle.close()\n\n        return data\n\n    def calc_probs(sequences, qualities, error_model, barcodes, max_mismatches=2):\n        '''\n        data: index read data (seq index, codes and quals)\n        '''\n\n        print('Loading data')\n        transition_probs = from_h5(error_model)\n        orientations = transition_probs.keys()\n\n        print('Computing likelihood')\n\n        bc_sequences = {orient: np.array([seq2intlist(bc) for bc in barcodes[orient]])\n                        for orient in orientations}\n\n        transitions_all = [transition_probs[orient][\n            bc_sequences[orient][:, None],\n            sequences[orient],\n            qualities[orient]-1].prod(axis=2) for orient in orientations]\n\n        if len(transitions_all) == 1:\n            assignments = transitions_all[0].argmax(axis=0)\n        else:\n            assignments = np.multiply(*transitions_all).argmax(axis=0)\n\n        diffs = sum([bc_sequences[orient][assignments, :] != sequences[orient]\n                     for orient in orientations]).sum(axis=1)\n\n        assignments = barcodes.iloc[assignments].reset_index()\n        assignments.insert(1, 'mismatches', diffs)\n        assignments.loc[diffs > max_mismatches, 'sample_name'] = np.nan\n\n        return assignments.values\n\n    def format_result(assignments, rids, seq):\n\n        trans = str.maketrans('01234', 'ACGTN')\n        index_string = np.array(\n            [[''.join(x) for x in np.core.defchararray.translate(codes.astype('U1'), trans)]\n            for codes in seq.values()])\n\n        result = np.vstack([rids[None, :], index_string, assignments[:, [0, 1]].T]).T\n\n        return pd.DataFrame(result)\n    \n    # main script\n\n    barcodes = pd.read_csv(\"$meta_file\", dtype=str, names=[\"sample_name\", \"fwd\", \"rev\"]).dropna(how='all', axis=1)\n    barcodes.sample_name = barcodes.sample_name.str.replace(\"[^a-zA-Z0-9_.]\", \"_\", regex=True)\n    barcodes.set_index('sample_name', inplace=True)\n\n    read_ids = from_h5(\"$h5\", field='rid')['fwd'].astype(str)\n    sequences = from_h5(\"$h5\", field='seq')\n    quals = from_h5(\"$h5\", field='qual')\n\n    assignments = calc_probs(sequences, quals, \"$error_model\", barcodes, max_mismatches=$params.max_mismatches)\n    result = format_result(assignments, read_ids, sequences)\n\n    result.to_csv(\"demux_info_${split}.tsv\", sep='\\\\t', header=False, index=False)\n\n    sample_sizes = result[result.columns[-2]].astype(str).value_counts()\n\n    (\n        sample_sizes[~sample_sizes.index.isnull()]\n        .to_csv(\"sample_counts_${split}.csv\", header=False)\n    )\n    \n    \n    \"\"\"",
        "nb_lignes_script": 106,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "split",
            "h5",
            "error_model",
            "meta_file"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "publishDir \"${params.outdir}/interm/sample_index_mapping\", mode: \"copy\"",
            "tag \"$split\"",
            "label \"process_high\"",
            "stageInMode \"copy\"",
            "container \"nakor/metaflowmics-python:0.0.1\"",
            "conda (params.enable_conda ? \"conda-forge::pandas conda-forge::h5py\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "SAMPLE_SIZE_DISTRIBUTION": {
        "name_process": "SAMPLE_SIZE_DISTRIBUTION",
        "string_process": "\nprocess SAMPLE_SIZE_DISTRIBUTION {\n    publishDir \"${params.outdir}/figures\", mode: \"copy\"\n    label \"process_low\"\n\n    container \"nakor/metaflowmics-python:0.0.1\"\n    conda (params.enable_conda ? \"conda-forge::pandas conda-forge::bokeh\" : null)\n\n    input:\n    path counts\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    from math import log10\n    from pathlib import Path\n\n    import numpy as np\n    import pandas as pd\n\n    from bokeh.plotting import figure\n    from bokeh.io import save, output_file\n    from bokeh.models import tickers\n\n\n    def count_samples(folder='.'):\n        '''\n        Count the number of read pair/sample in [folder]\n        '''\n\n        files = Path(folder).glob(\"sample_counts*.csv\")\n        summaries = pd.concat([pd.read_csv(summary, index_col=0, dtype=str, header=None)\n                               for summary in files],\n                              axis=1, sort=True)\n\n        return summaries.fillna(0).astype(int).sum(axis=1)\n\n    def plot_bokeh(counts):\n\n        hist, edges = np.histogram(np.log10(counts), bins=max(5, len(counts)//10), density=False)\n        hist_df = pd.DataFrame({'count': hist,\n                                \"left\": edges[:-1],\n                                \"right\": edges[1:]})\n        hist_df[\"interval\"] = [\"{:,} - {:,}\".format(int(10**left), int(10**right))\n                               for left, right in zip(hist_df[\"left\"], hist_df[\"right\"])]\n\n        x_min = int(min(edges))\n        x_max = max(4, 1+int(max(edges)))\n\n        p = figure(plot_height=800, plot_width=800,\n                   x_range=[x_min, x_max], tools='hover,box_zoom',\n                   tooltips=[('Size range', '@interval'),\n                             ('#Samples in interval', str(\"@count\"))],\n                    title=\"Sample size distribution\",\n                    x_axis_label=\"Sample read count\",\n                    y_axis_label=\"Occurrences\")\n\n        p.quad(bottom=0, top=\"count\", left=\"left\", \n               right=\"right\", source=hist_df, fill_color=\"SteelBlue\", \n               line_color=\"black\", fill_alpha=0.7,\n               hover_fill_alpha=1.0, hover_fill_color=\"Tan\")\n\n        ticks = list(range(x_min, x_max))\n        minor_ticks = np.log10([i*10**j for i in range(1, 10) for j in ticks])\n\n        p.xaxis.ticker = tickers.FixedTicker(ticks=ticks, minor_ticks=minor_ticks)\n        p.xaxis.major_label_overrides = {tick: \"{:,}\".format(int(10**tick)) for tick in ticks}\n        p.yaxis.minor_tick_line_color = None\n\n        p.axis.major_label_text_font_size = \"12pt\"\n        p.axis.axis_label_text_font_size = \"14pt\"\n        p.title.text_font_size = \"18pt\"\n\n        output_file('sample_sizes.html')\n        save(p)\n\n    # main script\n    counts = count_samples()\n    counts.to_csv('sample_sizes.csv', header=False)\n    plot_bokeh(counts)\n    \"\"\"\n    \n}",
        "nb_lignes_process": 85,
        "string_script": "    \"\"\"\n    #!/usr/bin/env python\n\n    from math import log10\n    from pathlib import Path\n\n    import numpy as np\n    import pandas as pd\n\n    from bokeh.plotting import figure\n    from bokeh.io import save, output_file\n    from bokeh.models import tickers\n\n\n    def count_samples(folder='.'):\n        '''\n        Count the number of read pair/sample in [folder]\n        '''\n\n        files = Path(folder).glob(\"sample_counts*.csv\")\n        summaries = pd.concat([pd.read_csv(summary, index_col=0, dtype=str, header=None)\n                               for summary in files],\n                              axis=1, sort=True)\n\n        return summaries.fillna(0).astype(int).sum(axis=1)\n\n    def plot_bokeh(counts):\n\n        hist, edges = np.histogram(np.log10(counts), bins=max(5, len(counts)//10), density=False)\n        hist_df = pd.DataFrame({'count': hist,\n                                \"left\": edges[:-1],\n                                \"right\": edges[1:]})\n        hist_df[\"interval\"] = [\"{:,} - {:,}\".format(int(10**left), int(10**right))\n                               for left, right in zip(hist_df[\"left\"], hist_df[\"right\"])]\n\n        x_min = int(min(edges))\n        x_max = max(4, 1+int(max(edges)))\n\n        p = figure(plot_height=800, plot_width=800,\n                   x_range=[x_min, x_max], tools='hover,box_zoom',\n                   tooltips=[('Size range', '@interval'),\n                             ('#Samples in interval', str(\"@count\"))],\n                    title=\"Sample size distribution\",\n                    x_axis_label=\"Sample read count\",\n                    y_axis_label=\"Occurrences\")\n\n        p.quad(bottom=0, top=\"count\", left=\"left\", \n               right=\"right\", source=hist_df, fill_color=\"SteelBlue\", \n               line_color=\"black\", fill_alpha=0.7,\n               hover_fill_alpha=1.0, hover_fill_color=\"Tan\")\n\n        ticks = list(range(x_min, x_max))\n        minor_ticks = np.log10([i*10**j for i in range(1, 10) for j in ticks])\n\n        p.xaxis.ticker = tickers.FixedTicker(ticks=ticks, minor_ticks=minor_ticks)\n        p.xaxis.major_label_overrides = {tick: \"{:,}\".format(int(10**tick)) for tick in ticks}\n        p.yaxis.minor_tick_line_color = None\n\n        p.axis.major_label_text_font_size = \"12pt\"\n        p.axis.axis_label_text_font_size = \"14pt\"\n        p.title.text_font_size = \"18pt\"\n\n        output_file('sample_sizes.html')\n        save(p)\n\n    # main script\n    counts = count_samples()\n    counts.to_csv('sample_sizes.csv', header=False)\n    plot_bokeh(counts)\n    \"\"\"",
        "nb_lignes_script": 69,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "counts"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "publishDir \"${params.outdir}/figures\", mode: \"copy\"",
            "label \"process_low\"",
            "container \"nakor/metaflowmics-python:0.0.1\"",
            "conda (params.enable_conda ? \"conda-forge::pandas conda-forge::bokeh\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "WRITE_SAMPLES_TO_FASTQ": {
        "name_process": "WRITE_SAMPLES_TO_FASTQ",
        "string_process": "\nprocess WRITE_SAMPLES_TO_FASTQ {\n    tag \"$split\"\n    label \"process_low\"\n\n    container \"nakor/metaflowmics-python:0.0.1\"\n    conda (params.enable_conda ? \"conda-forge::biopython conda-forge::pandas\" : null)\n\n    input:\n    tuple val(split), path(fastq), path(mapping)\n\n    output:\n    path \"*.fastq\"\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    from Bio.SeqIO.QualityIO import FastqGeneralIterator\n    import pandas as pd\n\n    demux_info = pd.read_csv(\"$mapping\", header=None, sep=\"\\\\t\", dtype=str, index_col=0)\n    \n    if demux_info[2].count() == 0:\n        demux_info.drop(columns=[2], inplace=True)\n        demux_info.columns = [\"fwd\", \"sample_name\", \"mismatches\"]\n    else:\n        demux_info.columns = [\"fwd\", \"rev\", \"sample_name\", \"mismatches\"]\n\n    read_orient = [\"fwd\", \"rev\"][:${fastq.size()}]\n    \n    print(\"Preparing handles.\")\n    handles = {}\n    for sample in demux_info[\"sample_name\"].unique():\n        if not pd.isnull(sample):\n            for i, orient in enumerate(read_orient, 1):\n                handles[sample+orient] = open(f\"{sample}_R{i}.fastq\", \"w\")\n\n    parsers = [FastqGeneralIterator(open(fastq, \"r\")) for fastq in \"$fastq\".split()]\n\n    print(\"Starting demultiplexing\")\n    for seq_nb, sequences in enumerate(zip(*parsers)):\n        ids = [seq[0].split()[0] for seq in sequences]\n\n        if len(ids) > 1:\n            if ids[0] != ids[1]:\n                print(\"Sequence #{}: {} (fwd) and {} (rev) do not match. The forward and reverse read files seem to be out of order\"\n                      .format(seq_nb, *ids))\n                exit(42)\n\n        sample_assignment = demux_info.loc[ids[0], \"sample_name\"]\n\n        if pd.isnull(sample_assignment):\n            continue\n\n        for orient, seq in zip(read_orient, sequences):\n            handles[sample_assignment + orient].write(\"@{}\\\\n{}\\\\n+\\\\n{}\\\\n\".format(*seq))\n\n    for sample in demux_info[\"sample_name\"].unique():\n        if not pd.isnull(sample):\n            for orient in read_orient:\n                handles[sample + orient].close()\n\n    print(\"Demultiplexing finished.\")\n    \"\"\"\n    \n}",
        "nb_lignes_process": 65,
        "string_script": "    \"\"\"\n    #!/usr/bin/env python\n\n    from Bio.SeqIO.QualityIO import FastqGeneralIterator\n    import pandas as pd\n\n    demux_info = pd.read_csv(\"$mapping\", header=None, sep=\"\\\\t\", dtype=str, index_col=0)\n    \n    if demux_info[2].count() == 0:\n        demux_info.drop(columns=[2], inplace=True)\n        demux_info.columns = [\"fwd\", \"sample_name\", \"mismatches\"]\n    else:\n        demux_info.columns = [\"fwd\", \"rev\", \"sample_name\", \"mismatches\"]\n\n    read_orient = [\"fwd\", \"rev\"][:${fastq.size()}]\n    \n    print(\"Preparing handles.\")\n    handles = {}\n    for sample in demux_info[\"sample_name\"].unique():\n        if not pd.isnull(sample):\n            for i, orient in enumerate(read_orient, 1):\n                handles[sample+orient] = open(f\"{sample}_R{i}.fastq\", \"w\")\n\n    parsers = [FastqGeneralIterator(open(fastq, \"r\")) for fastq in \"$fastq\".split()]\n\n    print(\"Starting demultiplexing\")\n    for seq_nb, sequences in enumerate(zip(*parsers)):\n        ids = [seq[0].split()[0] for seq in sequences]\n\n        if len(ids) > 1:\n            if ids[0] != ids[1]:\n                print(\"Sequence #{}: {} (fwd) and {} (rev) do not match. The forward and reverse read files seem to be out of order\"\n                      .format(seq_nb, *ids))\n                exit(42)\n\n        sample_assignment = demux_info.loc[ids[0], \"sample_name\"]\n\n        if pd.isnull(sample_assignment):\n            continue\n\n        for orient, seq in zip(read_orient, sequences):\n            handles[sample_assignment + orient].write(\"@{}\\\\n{}\\\\n+\\\\n{}\\\\n\".format(*seq))\n\n    for sample in demux_info[\"sample_name\"].unique():\n        if not pd.isnull(sample):\n            for orient in read_orient:\n                handles[sample + orient].close()\n\n    print(\"Demultiplexing finished.\")\n    \"\"\"",
        "nb_lignes_script": 49,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "split",
            "fastq",
            "mapping"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$split\"",
            "label \"process_low\"",
            "container \"nakor/metaflowmics-python:0.0.1\"",
            "conda (params.enable_conda ? \"conda-forge::biopython conda-forge::pandas\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "GZIP": {
        "name_process": "GZIP",
        "string_process": "\nprocess GZIP {\n    publishDir \"${params.outdir}/fastqs\", mode: \"copy\"\n    label \"high_computation\"\n    conda (params.enable_conda ? \"conda-forge::bash='5.1'\" : null)\n    container \"nakor/bash:5.1.4\"\n\n    input:\n\tpath f\n\n    output:\n    path \"*.gz\"\n\t\n    script:\n    \"\"\"\n    ls *.fastq | xargs -P $task.cpus -I % bash -c \"gzip -c % > %.gz\"\n    \"\"\"\t\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    ls *.fastq | xargs -P $task.cpus -I % bash -c \"gzip -c % > %.gz\"\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "publishDir \"${params.outdir}/fastqs\", mode: \"copy\"",
            "label \"high_computation\"",
            "conda (params.enable_conda ? \"conda-forge::bash='5.1'\" : null)",
            "container \"nakor/bash:5.1.4\""
        ],
        "when": "",
        "stub": ""
    },
    "COUNT_KMERS": {
        "name_process": "COUNT_KMERS",
        "string_process": "\nprocess COUNT_KMERS {\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::numpy\" : null)\n\n    input:\n    path fasta\n\n    output:\n    path \"ref*.npy\", emit: kmers\n    path \"freqs*.npy\", emit: freqs\n\n    script:\n    if (params.feature == 'nucl') {\n        alphabet = \"ACGT\"\n        base = 2\n    } else {\n        alphabet = \"ARNDCQEGHILKMFPSTWYV\"\n        base = 5\n    }\n    \"\"\"\n    #!/usr/bin/env python\n\n    from itertools import product\n    import numpy as np\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n\n    alphabet = \"$alphabet\"\n    N = len(alphabet)\n    k = $params.k\n\n    kmer_idx = {''.join(x): i for i, x in enumerate(product(alphabet, repeat=k))}\n\n    n_db = sum(1 for l in open(\"$fasta\") if l.startswith('>'))\n\n    ref = np.zeros((n_db, N**k), dtype=np.uint16)\n\n    with open(\"$fasta\") as r:\n        for i, (_, s) in enumerate(SimpleFastaParser(r)):\n            # remove potential gaps\n            s = s.replace('-', '')\n            # compute kmer counts\n            kmer_indices = [kmer_idx[s[i:i+k]] for i in range(len(s)-k+1) if s[i:i+k] in kmer_idx]\n            occurrences = np.bincount(kmer_indices, minlength=N**k)\n\n            ref[i] = occurrences\n\n            if i % 1000 == 0:\n                print(f\"{i:,}/{n_db:,} sequences processed\")\n\n    kmer_usage = np.argsort(-(ref > 0).sum(axis=0))\n\n    np.save(f\"freqs_{k}mers.npy\", kmer_usage)\n    np.save(f\"ref_{k}mers.npy\", ref)\n    \"\"\"\n}",
        "nb_lignes_process": 60,
        "string_script": "    if (params.feature == 'nucl') {\n        alphabet = \"ACGT\"\n        base = 2\n    } else {\n        alphabet = \"ARNDCQEGHILKMFPSTWYV\"\n        base = 5\n    }\n    \"\"\"\n    #!/usr/bin/env python\n\n    from itertools import product\n    import numpy as np\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n\n    alphabet = \"$alphabet\"\n    N = len(alphabet)\n    k = $params.k\n\n    kmer_idx = {''.join(x): i for i, x in enumerate(product(alphabet, repeat=k))}\n\n    n_db = sum(1 for l in open(\"$fasta\") if l.startswith('>'))\n\n    ref = np.zeros((n_db, N**k), dtype=np.uint16)\n\n    with open(\"$fasta\") as r:\n        for i, (_, s) in enumerate(SimpleFastaParser(r)):\n            # remove potential gaps\n            s = s.replace('-', '')\n            # compute kmer counts\n            kmer_indices = [kmer_idx[s[i:i+k]] for i in range(len(s)-k+1) if s[i:i+k] in kmer_idx]\n            occurrences = np.bincount(kmer_indices, minlength=N**k)\n\n            ref[i] = occurrences\n\n            if i % 1000 == 0:\n                print(f\"{i:,}/{n_db:,} sequences processed\")\n\n    kmer_usage = np.argsort(-(ref > 0).sum(axis=0))\n\n    np.save(f\"freqs_{k}mers.npy\", kmer_usage)\n    np.save(f\"ref_{k}mers.npy\", ref)\n    \"\"\"",
        "nb_lignes_script": 41,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fasta"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"nakor/metaflowmics-python:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::numpy\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "UPDATE_MSA_WITH_REF": {
        "name_process": "UPDATE_MSA_WITH_REF",
        "string_process": "\nprocess UPDATE_MSA_WITH_REF {\n    label \"process_low\"\n    publishDir params.outdir, mode: params.publish_dir_mode\n\n    container \"nakor/metaflowmics-python:0.0.1\"\n    conda (params.enable_conda ? \"conda-forge::biopython conda-forge:pandas\" : null)\n\n    input:\n    path aln\n    path ref\n\n    output:\n    path \"*.updated.afa\", emit: afa\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n\n    def map_on_ref(ref, query):\n        query_updated = []\n        pos = 0\n        for aa in ref:\n           if aa != '-':\n               query_updated.append(query[pos])\n               pos += 1\n           else:\n               query_updated.append('-')\n               \n        return ''.join(query_updated)\n\n    refs = {}\n    with open(\"$ref\") as reader:\n        for (title, seq) in SimpleFastaParser(reader):\n            lineage = title.split()[1]\n            tax = lineage.split(\"$params.sep\")[$params.field]\n            refs[tax] = seq\n\n    with open(\"$aln\") as reader:\n        for (title, seq) in SimpleFastaParser(reader):\n            lineage = title.split()[1]\n            tax = lineage.split(\"$params.sep\")[$params.field]\n            seq_updated = map_on_ref(refs[tax], seq)\n      \n            with open(f\"{tax}.updated.afa\", \"a\") as writer:\n                writer.write(f\">{title}\\\\n{seq_updated}\\\\n\")    \n    \"\"\"    \n}",
        "nb_lignes_process": 48,
        "string_script": "    \"\"\"\n    #!/usr/bin/env python\n\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n\n    def map_on_ref(ref, query):\n        query_updated = []\n        pos = 0\n        for aa in ref:\n           if aa != '-':\n               query_updated.append(query[pos])\n               pos += 1\n           else:\n               query_updated.append('-')\n               \n        return ''.join(query_updated)\n\n    refs = {}\n    with open(\"$ref\") as reader:\n        for (title, seq) in SimpleFastaParser(reader):\n            lineage = title.split()[1]\n            tax = lineage.split(\"$params.sep\")[$params.field]\n            refs[tax] = seq\n\n    with open(\"$aln\") as reader:\n        for (title, seq) in SimpleFastaParser(reader):\n            lineage = title.split()[1]\n            tax = lineage.split(\"$params.sep\")[$params.field]\n            seq_updated = map_on_ref(refs[tax], seq)\n      \n            with open(f\"{tax}.updated.afa\", \"a\") as writer:\n                writer.write(f\">{title}\\\\n{seq_updated}\\\\n\")    \n    \"\"\"",
        "nb_lignes_script": 32,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "aln",
            "ref"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_low\"",
            "publishDir params.outdir, mode: params.publish_dir_mode",
            "container \"nakor/metaflowmics-python:0.0.1\"",
            "conda (params.enable_conda ? \"conda-forge::biopython conda-forge:pandas\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_REMOVE_LINEAGE": {
        "name_process": "MOTHUR_REMOVE_LINEAGE",
        "string_process": "\nprocess MOTHUR_REMOVE_LINEAGE {\n    tag \"$otu_id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.44.1--hf0cea05_2\"\n    conda (params.enable_conda ? \"bioconda::mothur:1.44.1\" : null)\n\n    input:\n    tuple val(otu_id), file(list), file(constax), file(shared), file(count_table)\n\n    output:\n    tuple val(otu_id), path(\"${outprefix}.cons.taxonomy\"), emit: constaxonomy\n    tuple val(otu_id), path(\"${outprefix}.list\"), emit: list\n    tuple val(otu_id), path(\"${outprefix}.shared\"), emit: shared\n    tuple val(otu_id), path(\"${outprefix}.count_table\"), emit: count_table    \n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    outprefix = options.suffix ? \"${options.suffix}.${otu_id}\" : \"${procname}.${otu_id}\"\n    \"\"\"\n    mothur \"#remove.lineage(constaxonomy=$constax, list=$list, shared=$shared, taxon='$params.taxa_to_filter');list.seqs(list=current);get.seqs(accnos=current,count=$count)\"\n    # rename outputs\n    mv *.pick.list ${outprefix}.list\n    mv *.pick.cons.taxonomy ${outprefix}.cons.taxonomy\n    mv *.pick.shared ${outprefix}.shared\n    mv *.pick.count_table ${outprefix}.count_table\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d'=' -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 36,
        "string_script": "    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    outprefix = options.suffix ? \"${options.suffix}.${otu_id}\" : \"${procname}.${otu_id}\"\n    \"\"\"\n    mothur \"#remove.lineage(constaxonomy=$constax, list=$list, shared=$shared, taxon='$params.taxa_to_filter');list.seqs(list=current);get.seqs(accnos=current,count=$count)\"\n    # rename outputs\n    mv *.pick.list ${outprefix}.list\n    mv *.pick.cons.taxonomy ${outprefix}.cons.taxonomy\n    mv *.pick.shared ${outprefix}.shared\n    mv *.pick.count_table ${outprefix}.count_table\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d'=' -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "otu_id",
            "list",
            "constax",
            "shared",
            "count_table"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$otu_id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.44.1--hf0cea05_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur:1.44.1\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "DADA2_CHIMERA": {
        "name_process": "DADA2_CHIMERA",
        "string_process": "\nprocess DADA2_CHIMERA {\n    tag \"$meta.id\"\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:[\"id\"]) }\n\n    container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2\" : null)\n\n    input:\n    tuple val(meta), path(contigs)\n\n    output:\n    tuple val(meta), path(\"*.RDS\"), emit: rds    \n    path(\"*.csv\"), emit: summary\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n\n    derep <- readRDS(\"$contigs\")\n    chimera <- isBimeraDenovo(derep, verbose=T)\n\n    kept <- (1:length(chimera))[!chimera]\n\n    # Remove uniques sequences identified as chimeric\n    derep[[\"uniques\"]] <- derep[[\"uniques\"]][kept]\n\n    # Find new maximum read length (in case we removed the longest)\n    max_len <- max(sapply(names(derep[[\"uniques\"]]), nchar))\n    # Subset the qualities\n    derep[[\"quals\"]] <- derep[[\"quals\"]][kept, 1:max_len]\n\n    # Subset the read -> unique mapping\n    derep[[\"map\"]] <- derep[[\"map\"]][which(derep[[\"map\"]] %in% kept)]\n\n    # Write counts\n    counts <- getUniques(derep)\n    data <- sprintf(\"chimera,,${meta.id},%s,%s\",sum(counts),sum(counts>0))\n    write(data, \"summary.csv\")\n\n    saveRDS(derep, \"${meta.id}.nochim.RDS\")\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"\n}",
        "nb_lignes_process": 52,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n\n    derep <- readRDS(\"$contigs\")\n    chimera <- isBimeraDenovo(derep, verbose=T)\n\n    kept <- (1:length(chimera))[!chimera]\n\n    # Remove uniques sequences identified as chimeric\n    derep[[\"uniques\"]] <- derep[[\"uniques\"]][kept]\n\n    # Find new maximum read length (in case we removed the longest)\n    max_len <- max(sapply(names(derep[[\"uniques\"]]), nchar))\n    # Subset the qualities\n    derep[[\"quals\"]] <- derep[[\"quals\"]][kept, 1:max_len]\n\n    # Subset the read -> unique mapping\n    derep[[\"map\"]] <- derep[[\"map\"]][which(derep[[\"map\"]] %in% kept)]\n\n    # Write counts\n    counts <- getUniques(derep)\n    data <- sprintf(\"chimera,,${meta.id},%s,%s\",sum(counts),sum(counts>0))\n    write(data, \"summary.csv\")\n\n    saveRDS(derep, \"${meta.id}.nochim.RDS\")\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"",
        "nb_lignes_script": 30,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "contigs"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_low\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:[\"id\"]) }",
            "container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"",
            "conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "PEAR": {
        "name_process": "PEAR",
        "string_process": "\nprocess PEAR {\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/pear:0.9.6--h36cd882_7\"\n    conda (params.enable_conda ? \"bioconda::pear=0.9.6\" : null)\n\n    input:\n    tuple val(meta), path(fastq)\n\n    output:\n    tuple val(meta_upd), path(\"*.assembled.fastq\"), emit: assembled\n    tuple val(meta_upd), path(\"*.unassembled.*.fastq\"), optional: true, emit: unassembled\n    tuple val(meta_upd), path(\"*.discarded.fastq\"), optional: true, emit: discarded\n    path \"*.version.txt\", emit: version\n\n    script:\n\tmeta_upd = meta + [paired_end: false]\n\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env bash\n\n    pear -f ${fastq[0]} -r ${fastq[1]} -o ${meta.id}.fastq \\\\\n      -j $task.cpus \\\\\n      -n $params.min_contig_length -m $params.max_contig_length \\\\\n      -v $params.min_overlap \\\\\n      -q $params.quality_threshold\n\n    pear | grep -i \"PEAR v[0-9]\" | cut -d' ' -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "\tmeta_upd = meta + [paired_end: false]\n\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env bash\n\n    pear -f ${fastq[0]} -r ${fastq[1]} -o ${meta.id}.fastq \\\\\n      -j $task.cpus \\\\\n      -n $params.min_contig_length -m $params.max_contig_length \\\\\n      -v $params.min_overlap \\\\\n      -q $params.quality_threshold\n\n    pear | grep -i \"PEAR v[0-9]\" | cut -d' ' -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "denvax",
            "PEAR"
        ],
        "tools_url": [
            "https://bio.tools/denvax",
            "https://bio.tools/pear"
        ],
        "tools_dico": [
            {
                "name": "denvax",
                "uri": "https://bio.tools/denvax",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3379",
                            "term": "Preclinical and clinical studies"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatric medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "https://en.wikipedia.org/wiki/Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0560",
                                    "term": "DNA vaccine design"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Serostatus testing and dengue vaccine cost-benefit thresholds | R package for manuscript \"Serostatus Testing & Dengue Vaccine Cost Benefit Thresholds\"",
                "homepage": "https://cran.r-project.org/web/packages/denvax/index.html"
            },
            {
                "name": "PEAR",
                "uri": "https://bio.tools/pear",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence merging"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence splicing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Paired-end read merger. PEAR evaluates all possible paired-end read overlaps without requiring the target fragment size as input. In addition, it implements a statistical test for minimizing false-positive results.",
                "homepage": "http://sco.h-its.org/exelixis/web/software/pear/"
            }
        ],
        "inputs": [
            "meta",
            "fastq"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_medium\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/pear:0.9.6--h36cd882_7\"",
            "conda (params.enable_conda ? \"bioconda::pear=0.9.6\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "HMMER_HMMALIGN": {
        "name_process": "HMMER_HMMALIGN",
        "string_process": "\nprocess HMMER_HMMALIGN {\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    conda (params.enable_conda ? \"bioconda::hmmer=3.3.2\" : null)\n    container \"quay.io/biocontainers/hmmer:3.3.2--h1b792b2_1\"\n\n    input:\n    tuple val(meta), path(fasta)\n    tuple val(db_meta), path(hmm)\n\n    output:\n    tuple val(meta_upd), path(\"*.afa\"), emit: afa\n    path \"*.version.txt\", emit: version\n\n    script:\n\tmeta_upd = meta + db_meta\n    def software = getSoftwareName(task.process)\n    def prefix   = fasta.getBaseName()\n    def fastacmd = fasta.getExtension() == 'gz' ? \"gunzip -c $fasta\" : \"cat $fasta\"\n    \"\"\"\n    $fastacmd |\n        sed 's/[[:space:]]/|/g' |\n        hmmalign --outformat Pfam $options.args $hmm - |\n        egrep -v \"^#|^\\$|^//\\$\" |\n        awk '{print \">\"\\$1\"\\\\n\"\\$2}' |\n        sed 's/|/ /g' > ${prefix}.afa\n\n    echo \\$(hmmalign -h | grep -o '^# HMMER [0-9.]*') | sed 's/^# HMMER *//' > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "\tmeta_upd = meta + db_meta\n    def software = getSoftwareName(task.process)\n    def prefix   = fasta.getBaseName()\n    def fastacmd = fasta.getExtension() == 'gz' ? \"gunzip -c $fasta\" : \"cat $fasta\"\n    \"\"\"\n    $fastacmd |\n        sed 's/[[:space:]]/|/g' |\n        hmmalign --outformat Pfam $options.args $hmm - |\n        egrep -v \"^#|^\\$|^//\\$\" |\n        awk '{print \">\"\\$1\"\\\\n\"\\$2}' |\n        sed 's/|/ /g' > ${prefix}.afa\n\n    echo \\$(hmmalign -h | grep -o '^# HMMER [0-9.]*') | sed 's/^# HMMER *//' > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta",
            "db_meta",
            "hmm"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_medium\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "conda (params.enable_conda ? \"bioconda::hmmer=3.3.2\" : null)",
            "container \"quay.io/biocontainers/hmmer:3.3.2--h1b792b2_1\""
        ],
        "when": "",
        "stub": ""
    },
    "DADA2_DADA": {
        "name_process": "DADA2_DADA",
        "string_process": "\nprocess DADA2_DADA {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:[\"id\"]) }\n\n    container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-stringr\" : null)\n\n    input:\n    tuple val(meta), path(reads), path(errors)\n\n    output:\n    tuple val(meta), path(\"*.denoised.RDS\"), emit: denoised\n    tuple val(meta), path(\"*.derep.RDS\"), emit: derep, optional: true\n    path \"summary.csv\", optional: true, emit: summary\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def suffix = meta.paired_end ? \"_${meta.orient}\" : \"\"\n\tdef pool = params.pool == \"pseudo\" ? \"'pseudo'\" : params.pool\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    library(stringr)\n\n    derep_files <- sort(list.files(\".\", pattern=\".[^(errors)].RDS\"))\n    sample_names <- gsub(\"${suffix}\\\\\\\\.[a-z]+.RDS\", \"\", derep_files)\n    derep <- lapply(derep_files, readRDS)\n    names(derep) <- sample_names\n\n    err <- readRDS(\"$errors\")\n\n    denoised <- dada(derep, err=err, multithread=TRUE, pool=$pool)\n\n    if (length(sample_names) == 1) {\n        denoised <- list($meta.id=denoised)\n    }\n\n    sapply(names(denoised), function(x) saveRDS(denoised[[x]], sprintf(\"%s${suffix}.denoised.RDS\", x)))\n\n    # Write counts\n    if (\"$meta.orient\" == \"R1\") {\n        counts <- lapply(denoised, getUniques)\n        abund <- sapply(counts, sum)\n        richness <- sapply(counts, function(x) sum(x>0))\n        data <- sprintf(\"denoising,,%s,%s,%s\",sample_names,abund,richness)\n        write(data, \"summary.csv\")\n    }\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"\n}",
        "nb_lignes_process": 57,
        "string_script": "    def software = getSoftwareName(task.process)\n    def suffix = meta.paired_end ? \"_${meta.orient}\" : \"\"\n\tdef pool = params.pool == \"pseudo\" ? \"'pseudo'\" : params.pool\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    library(stringr)\n\n    derep_files <- sort(list.files(\".\", pattern=\".[^(errors)].RDS\"))\n    sample_names <- gsub(\"${suffix}\\\\\\\\.[a-z]+.RDS\", \"\", derep_files)\n    derep <- lapply(derep_files, readRDS)\n    names(derep) <- sample_names\n\n    err <- readRDS(\"$errors\")\n\n    denoised <- dada(derep, err=err, multithread=TRUE, pool=$pool)\n\n    if (length(sample_names) == 1) {\n        denoised <- list($meta.id=denoised)\n    }\n\n    sapply(names(denoised), function(x) saveRDS(denoised[[x]], sprintf(\"%s${suffix}.denoised.RDS\", x)))\n\n    # Write counts\n    if (\"$meta.orient\" == \"R1\") {\n        counts <- lapply(denoised, getUniques)\n        abund <- sapply(counts, sum)\n        richness <- sapply(counts, function(x) sum(x>0))\n        data <- sprintf(\"denoising,,%s,%s,%s\",sample_names,abund,richness)\n        write(data, \"summary.csv\")\n    }\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"",
        "nb_lignes_script": 34,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads",
            "errors"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_medium\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:[\"id\"]) }",
            "container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"",
            "conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-stringr\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_SCREEN_SEQS": {
        "name_process": "MOTHUR_SCREEN_SEQS",
        "string_process": "\nprocess MOTHUR_SCREEN_SEQS {\n    tag \"$meta.id\"\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), file(fasta), file(count)\n\n    output:\n    tuple val(meta), path(\"${outprefix}.fasta\"), emit: fasta\n    tuple val(meta), path(\"${outprefix}.count_table\"), emit: count_table\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    screen.seqs(fasta=$fasta, count=$count, minlength=${params.min_aln_len});\n    screen.seqs(fasta=current, count=current, optimize=start-end, criteria=${params.criteria});\n    summary.seqs(fasta=current)\"\n\n    mv *.good.good.${fasta.getExtension()} ${outprefix}.fasta\n\n    # if it exists\n    if [ -f *.good.good.count_table ]; then\n        mv *.good.good.count_table ${outprefix}.count_table\n    elif [ -f *.good.count_table ]; then\n        mv *.good.count_table ${outprefix}.count_table\n    else\n        cp $count ${outprefix}.count_table\n    fi\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 43,
        "string_script": "    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    screen.seqs(fasta=$fasta, count=$count, minlength=${params.min_aln_len});\n    screen.seqs(fasta=current, count=current, optimize=start-end, criteria=${params.criteria});\n    summary.seqs(fasta=current)\"\n\n    mv *.good.good.${fasta.getExtension()} ${outprefix}.fasta\n\n    # if it exists\n    if [ -f *.good.good.count_table ]; then\n        mv *.good.good.count_table ${outprefix}.count_table\n    elif [ -f *.good.count_table ]; then\n        mv *.good.count_table ${outprefix}.count_table\n    else\n        cp $count ${outprefix}.count_table\n    fi\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "fasta",
            "count"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_low\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_CLASSIFY_OTUS": {
        "name_process": "MOTHUR_CLASSIFY_OTUS",
        "string_process": "\nprocess MOTHUR_CLASSIFY_OTUS {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), file(list), file(count), file(tax)\n\n    output:\n    tuple val(meta), path(\"*.cons.taxonomy\"), emit: taxonomy\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    list.seqs(list=$list); get.seqs(taxonomy=$tax, accnos=current);\n    classify.otu(taxonomy=current, list=current, count=$count, probs=f)\"\n\n    # rename output\n    mv *.cons.taxonomy ${outprefix}.cons.taxonomy\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    list.seqs(list=$list); get.seqs(taxonomy=$tax, accnos=current);\n    classify.otu(taxonomy=current, list=current, count=$count, probs=f)\"\n\n    # rename output\n    mv *.cons.taxonomy ${outprefix}.cons.taxonomy\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "list",
            "count",
            "tax"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_medium\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "CDHIT": {
        "name_process": "CDHIT",
        "string_process": "\nprocess CDHIT {\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/cd-hit:4.8.1--h2e03b76_5\"\n    conda (params.enable_conda ? \"bioconda::cd-hit=4.8.1\" : null)\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.clstr\"), emit: cluster\n    tuple val(meta), path(\"*.repr.fa\"), emit: repr\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env bash\n    \n    cd-hit \\\\\n      -T $task.cpus \\\\\n      -M ${task.memory.getMega()} \\\\\n      -i $fasta \\\\\n      -o cdhit.repr.fa \\\\\n      -c $params.identity\n\n    cd-hit | grep -i \"CD-HIT version\" | cut -d' ' -f4 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env bash\n    \n    cd-hit \\\\\n      -T $task.cpus \\\\\n      -M ${task.memory.getMega()} \\\\\n      -i $fasta \\\\\n      -o cdhit.repr.fa \\\\\n      -c $params.identity\n\n    cd-hit | grep -i \"CD-HIT version\" | cut -d' ' -f4 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "denvax",
            "cd-hit"
        ],
        "tools_url": [
            "https://bio.tools/denvax",
            "https://bio.tools/cd-hit"
        ],
        "tools_dico": [
            {
                "name": "denvax",
                "uri": "https://bio.tools/denvax",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3379",
                            "term": "Preclinical and clinical studies"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatric medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "https://en.wikipedia.org/wiki/Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0560",
                                    "term": "DNA vaccine design"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Serostatus testing and dengue vaccine cost-benefit thresholds | R package for manuscript \"Serostatus Testing & Dengue Vaccine Cost Benefit Thresholds\"",
                "homepage": "https://cran.r-project.org/web/packages/denvax/index.html"
            },
            {
                "name": "cd-hit",
                "uri": "https://bio.tools/cd-hit",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0291",
                                    "term": "Sequence clustering"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0291",
                                    "term": "Sequence cluster construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0291",
                                    "term": "Sequence cluster generation"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0006",
                                "term": "Data"
                            },
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            }
                        ]
                    }
                ],
                "description": "Cluster a nucleotide dataset into representative sequences.",
                "homepage": "https://github.com/weizhongli/cdhit"
            }
        ],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/cd-hit:4.8.1--h2e03b76_5\"",
            "conda (params.enable_conda ? \"bioconda::cd-hit=4.8.1\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_CLUSTER": {
        "name_process": "MOTHUR_CLUSTER",
        "string_process": "\nprocess MOTHUR_CLUSTER {\n    tag \"${otu_id}\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), path(fasta), path(count), path(tax)\n    each otu_id\n\n    output:\n    tuple val(meta_upd), path(\"*.list\"), emit: list\n    tuple val(meta_upd), path(\"*.shared\"), emit: shared\n    tuple val(meta_upd), path(fasta), emit:fasta\n    tuple val(meta_upd), path(count), emit:count_table\n    tuple val(meta_upd), path(tax), emit:taxonomy\n    path \"*.version.txt\", emit: version\n\n    script:\n    meta_upd = meta + [id: \"${otu_id}\", otu_id: otu_id]\n    def ext = [\"rep.fasta\", \"cons.taxonomy\", \"shared\", \"list\", \"database\"]\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta_upd.id}\"\n    \"\"\"\n    mothur \"#\n    cluster(count=$count, fasta=$fasta, method=${otu_id == 100 ? \"unique\" : \"dgc\"}, cutoff=${(100-meta_upd.otu_id) / 100});\n    make.shared(count=current, list=current)\"\n\n    mv *.list ${outprefix}.list\n    mv *.shared ${outprefix}.shared\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 40,
        "string_script": "    meta_upd = meta + [id: \"${otu_id}\", otu_id: otu_id]\n    def ext = [\"rep.fasta\", \"cons.taxonomy\", \"shared\", \"list\", \"database\"]\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta_upd.id}\"\n    \"\"\"\n    mothur \"#\n    cluster(count=$count, fasta=$fasta, method=${otu_id == 100 ? \"unique\" : \"dgc\"}, cutoff=${(100-meta_upd.otu_id) / 100});\n    make.shared(count=current, list=current)\"\n\n    mv *.list ${outprefix}.list\n    mv *.shared ${outprefix}.shared\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "fasta",
            "count",
            "tax",
            "otu_id"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"${otu_id}\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "VSEARCH_SINTAX": {
        "name_process": "VSEARCH_SINTAX",
        "string_process": "\nprocess VSEARCH_SINTAX{\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta) }\n\n    container \"quay.io/biocontainers/vsearch:2.17.0--h95f258a_1\"\n    conda (params.enable_conda ? \"bioconda::vsearch=2.17.0\" : null)\n\n    input:\n    tuple val(meta), path(fasta)\n    path database\n\n    output:\n    tuple val(meta), path(\"annotations_*.tsv\"), emit: taxonomy\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env bash\n\n    vsearch \\\\\n        --threads $task.cpus \\\\\n        --db $database \\\\\n        --sintax $fasta \\\\\n        --sintax_cutoff $params.tax_confidence \\\\\n        --tabbedout annotations_sintax-${meta.id}.tsv\n\n    echo \\$(vsearch --version 2>&1) | grep \"RAM\" | sed \"s/vsearch v//\" | sed \"s/, .*//\" > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env bash\n\n    vsearch \\\\\n        --threads $task.cpus \\\\\n        --db $database \\\\\n        --sintax $fasta \\\\\n        --sintax_cutoff $params.tax_confidence \\\\\n        --tabbedout annotations_sintax-${meta.id}.tsv\n\n    echo \\$(vsearch --version 2>&1) | grep \"RAM\" | sed \"s/vsearch v//\" | sed \"s/, .*//\" > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "denvax",
            "VSEARCH"
        ],
        "tools_url": [
            "https://bio.tools/denvax",
            "https://bio.tools/vsearch"
        ],
        "tools_dico": [
            {
                "name": "denvax",
                "uri": "https://bio.tools/denvax",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3379",
                            "term": "Preclinical and clinical studies"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatric medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "https://en.wikipedia.org/wiki/Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0560",
                                    "term": "DNA vaccine design"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Serostatus testing and dengue vaccine cost-benefit thresholds | R package for manuscript \"Serostatus Testing & Dengue Vaccine Cost Benefit Thresholds\"",
                "homepage": "https://cran.r-project.org/web/packages/denvax/index.html"
            },
            {
                "name": "VSEARCH",
                "uri": "https://bio.tools/vsearch",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2520",
                                    "term": "DNA mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0450",
                                    "term": "Chimera detection"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0450",
                                    "term": "Chimeric sequence detection"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2977",
                                "term": "Nucleic acid sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_0865",
                                "term": "Sequence similarity score"
                            }
                        ]
                    }
                ],
                "description": "High-throughput search and clustering sequence analysis tool. It supports de novo and reference based chimera detection, clustering, full-length and prefix dereplication, reverse complementation, masking, all-vs-all pairwise global alignment, exact and global alignment searching, shuffling, subsampling and sorting. It also supports FASTQ file analysis, filtering and conversion.",
                "homepage": "https://github.com/torognes/vsearch"
            }
        ],
        "inputs": [
            "meta",
            "fasta",
            "database"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta) }",
            "container \"quay.io/biocontainers/vsearch:2.17.0--h95f258a_1\"",
            "conda (params.enable_conda ? \"bioconda::vsearch=2.17.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "FASTTREE": {
        "name_process": "FASTTREE",
        "string_process": "\nprocess FASTTREE {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    conda (params.enable_conda ? \"bioconda::fasttree\" : null)\n    container \"quay.io/biocontainers/fasttree:2.1.8--h779adbc_6\"\n\n    input:\n    tuple val(meta), path(repfasta)\n\n    output:\n    tuple val(meta), path(\"*.nwk\"), emit: nwk\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\t\n    \"\"\"\n    FastTree -nt $repfasta > fasttree.${meta.id}.nwk\n\n    fasttree 2>&1 >/dev/null | head -1 | sed 's/.*version \\\\([0-9\\\\.]*\\\\).*/\\\\1/g' \\\\\n    > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\t\n    \"\"\"\n    FastTree -nt $repfasta > fasttree.${meta.id}.nwk\n\n    fasttree 2>&1 >/dev/null | head -1 | sed 's/.*version \\\\([0-9\\\\.]*\\\\).*/\\\\1/g' \\\\\n    > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "FastTree",
            "NullSeq"
        ],
        "tools_url": [
            "https://bio.tools/fasttree",
            "https://bio.tools/nullseq"
        ],
        "tools_dico": [
            {
                "name": "FastTree",
                "uri": "https://bio.tools/fasttree",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0547",
                                    "term": "Phylogenetic inference (maximum likelihood and Bayesian methods)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0540",
                                    "term": "Phylogenetic inference (from molecular sequences)"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0547",
                                    "term": "Phylogenetic tree construction (maximum likelihood and Bayesian methods)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0547",
                                    "term": "Phylogenetic tree generation (maximum likelihood and Bayesian methods)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0540",
                                    "term": "Phylogenetic tree construction (from molecular sequences)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0540",
                                    "term": "Phylogenetic tree generation (from molecular sequences)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Infers approximately-maximum-likelihood phylogenetic trees from alignments of nucleotide or protein sequences.",
                "homepage": "http://www.microbesonline.org/fasttree/"
            },
            {
                "name": "NullSeq",
                "uri": "https://bio.tools/nullseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0364",
                                    "term": "Random sequence generation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Creates Random Coding Sequences with specified GC content and Amino Acid usage.",
                "homepage": "https://github.com/amarallab/NullSeq"
            }
        ],
        "inputs": [
            "meta",
            "repfasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "conda (params.enable_conda ? \"bioconda::fasttree\" : null)",
            "container \"quay.io/biocontainers/fasttree:2.1.8--h779adbc_6\""
        ],
        "when": "",
        "stub": ""
    },
    "SUMMARIZE_TABLE": {
        "name_process": "SUMMARIZE_TABLE",
        "string_process": "\nprocess SUMMARIZE_TABLE {\n    tag \"$step\"\n    label \"process_low\"\n\n    container \"nakor/metaflowmics-r:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::r-data.table\" : null)\n\n    input:\n    tuple val(meta), val(step), path(table)\n\n    output:\n    tuple val(meta), file(\"summary.csv\")\n\n    script:\n    drop = \"NULL\"\n    sep = \"auto\"\n    rownames = 1\n    taxa_are_rows = \"T\"\n\totu_id = meta.otu_id ?: \"\"\n    \n    if (table.getExtension() == \"count_table\") {\n        drop = \"'total'\"\n        sep = \"\\\\t\"\n        rownames = 1\n        taxa_are_rows = \"T\"\n    }\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(data.table)\n\n    table <- data.frame(\n        fread(\"$table\", drop=$drop, sep=\"$sep\"),\n        row.names=$rownames, check.names=F\n    )\n\n    if (!$taxa_are_rows) {\n        table <- t(table)\n    }\n    \n    summary <- cbind(\n        rep(\"$step\", ncol(table)),\n        rep(\"$otu_id\", ncol(table)),\n        colnames(table),\n        colSums(table),\n        colSums(table > 0)\n    )\n\n    write.table(summary, \"summary.csv\", quote=F, row.names=F, col.names=F, sep=\",\")    \n    \"\"\"\n    \n}",
        "nb_lignes_process": 51,
        "string_script": "    drop = \"NULL\"\n    sep = \"auto\"\n    rownames = 1\n    taxa_are_rows = \"T\"\n\totu_id = meta.otu_id ?: \"\"\n    \n    if (table.getExtension() == \"count_table\") {\n        drop = \"'total'\"\n        sep = \"\\\\t\"\n        rownames = 1\n        taxa_are_rows = \"T\"\n    }\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(data.table)\n\n    table <- data.frame(\n        fread(\"$table\", drop=$drop, sep=\"$sep\"),\n        row.names=$rownames, check.names=F\n    )\n\n    if (!$taxa_are_rows) {\n        table <- t(table)\n    }\n    \n    summary <- cbind(\n        rep(\"$step\", ncol(table)),\n        rep(\"$otu_id\", ncol(table)),\n        colnames(table),\n        colSums(table),\n        colSums(table > 0)\n    )\n\n    write.table(summary, \"summary.csv\", quote=F, row.names=F, col.names=F, sep=\",\")    \n    \"\"\"",
        "nb_lignes_script": 35,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "step",
            "table"
        ],
        "nb_inputs": 3,
        "outputs": [
            "meta"
        ],
        "nb_outputs": 1,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$step\"",
            "label \"process_low\"",
            "container \"nakor/metaflowmics-r:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::r-data.table\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "READ_TRACKING": {
        "name_process": "READ_TRACKING",
        "string_process": "\nprocess READ_TRACKING {\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        meta:meta) }\n\n    container \"nakor/metaflowmics-r:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::r-dplyr'=1.0.9' conda-forge::r-tidyr'=1.2' conda-forge::r-stringr\" : null)\n\n    input:\n    path counts\n\n    output:\n    file(\"summary-per-sample-per-step*.csv\")\n\n    script:\n\tdef outprefix = \"summary-per-sample-per-step\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(stringr)\n    library(dplyr)\n    library(tidyr)\n\n    cols <- c(\"step\", \"otu_id\", \"sample\", \"total\", \"nuniq\")\n    data <- read.csv(\"$counts\", header=F, col.names=cols)\n\n    # Order the step according to total count and uniques\n    col_order <- data %>%\n        mutate(nuniq=as.numeric(nuniq)) %>%\n        replace_na(list(nuniq=Inf)) %>%\n        group_by(step) %>% summarise(m1=sum(total), m2=sum(nuniq)) %>%\n        arrange(desc(m1), desc(m2)) %>%\n        pull(step) %>% as.character\n\n    # Reshape the table into wide format\n    summary <- data %>%\n      mutate(\n        step=factor(step, col_order),\n        label=ifelse(is.na(nuniq), total, sprintf(\"%s (%s uniques)\", total, nuniq))\n      ) %>% \n      select(step, sample, label, otu_id) %>%\n      arrange(step)\n\n    # Split summary per clustering ids\n    before_clustering <- summary %>% filter(is.na(otu_id))\n    after_clustering <- summary %>% filter(!is.na(otu_id))\n    thresholds <- after_clustering %>% pull(otu_id) %>% unique\n\n    if (length(thresholds) > 0) {\n        for (id in thresholds) {\n            summary_i <- before_clustering %>%\n              bind_rows(after_clustering %>% filter(otu_id==id)) %>%\n              select(-otu_id) %>%\n              pivot_wider(names_from=step, values_from=label, values_fill=\"0\")\n            write.table(summary_i, sprintf(\"${outprefix}.%s.csv\", id), quote=F, row.names=F, sep=\",\")\n        }\n    } else { # reads were all filtered before clustering\n        summary <- summary %>% \n            select(-otu_id) %>% \n            pivot_wider(names_from=step, values_from=label, values_fill=\"0\")\n        write.table(summary, \"${outprefix}.csv\", quote=F, row.names=F, sep=\",\")\n    }\n    \"\"\"\n}",
        "nb_lignes_process": 65,
        "string_script": "\tdef outprefix = \"summary-per-sample-per-step\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(stringr)\n    library(dplyr)\n    library(tidyr)\n\n    cols <- c(\"step\", \"otu_id\", \"sample\", \"total\", \"nuniq\")\n    data <- read.csv(\"$counts\", header=F, col.names=cols)\n\n    # Order the step according to total count and uniques\n    col_order <- data %>%\n        mutate(nuniq=as.numeric(nuniq)) %>%\n        replace_na(list(nuniq=Inf)) %>%\n        group_by(step) %>% summarise(m1=sum(total), m2=sum(nuniq)) %>%\n        arrange(desc(m1), desc(m2)) %>%\n        pull(step) %>% as.character\n\n    # Reshape the table into wide format\n    summary <- data %>%\n      mutate(\n        step=factor(step, col_order),\n        label=ifelse(is.na(nuniq), total, sprintf(\"%s (%s uniques)\", total, nuniq))\n      ) %>% \n      select(step, sample, label, otu_id) %>%\n      arrange(step)\n\n    # Split summary per clustering ids\n    before_clustering <- summary %>% filter(is.na(otu_id))\n    after_clustering <- summary %>% filter(!is.na(otu_id))\n    thresholds <- after_clustering %>% pull(otu_id) %>% unique\n\n    if (length(thresholds) > 0) {\n        for (id in thresholds) {\n            summary_i <- before_clustering %>%\n              bind_rows(after_clustering %>% filter(otu_id==id)) %>%\n              select(-otu_id) %>%\n              pivot_wider(names_from=step, values_from=label, values_fill=\"0\")\n            write.table(summary_i, sprintf(\"${outprefix}.%s.csv\", id), quote=F, row.names=F, sep=\",\")\n        }\n    } else { # reads were all filtered before clustering\n        summary <- summary %>% \n            select(-otu_id) %>% \n            pivot_wider(names_from=step, values_from=label, values_fill=\"0\")\n        write.table(summary, \"${outprefix}.csv\", quote=F, row.names=F, sep=\",\")\n    }\n    \"\"\"",
        "nb_lignes_script": 47,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "counts"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_low\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, meta:meta) }",
            "container \"nakor/metaflowmics-r:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::r-dplyr'=1.0.9' conda-forge::r-tidyr'=1.2' conda-forge::r-stringr\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "GET_SUBSAMPLING_THRESHOLD": {
        "name_process": "GET_SUBSAMPLING_THRESHOLD",
        "string_process": "\nprocess GET_SUBSAMPLING_THRESHOLD {\n    label \"process_low\"\n\ttag \"$params.subsampling_quantile\"\n\n    container \"nakor/metaflowmics-r:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::r-data.table\" : null)\n\n    input:\n    path count\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(data.table)\n\n    table <- data.frame(fread(\"$count\", drop=c(2)), row.names=1, check.names=F)\n    sample_sizes <- colSums(table)\n    threshold <- floor(quantile(sample_sizes, $params.subsampling_quantile, names=F))\n\n    if (threshold < $params.min_subsampling) {\n        threshold <- ifelse($params.min_subsampling < max(sample_sizes), \n                            $params.min_subsampling,\n                            threshold)\n    }\n\n    cat(threshold)\n    \"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(data.table)\n\n    table <- data.frame(fread(\"$count\", drop=c(2)), row.names=1, check.names=F)\n    sample_sizes <- colSums(table)\n    threshold <- floor(quantile(sample_sizes, $params.subsampling_quantile, names=F))\n\n    if (threshold < $params.min_subsampling) {\n        threshold <- ifelse($params.min_subsampling < max(sample_sizes), \n                            $params.min_subsampling,\n                            threshold)\n    }\n\n    cat(threshold)\n    \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "count"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_low\"",
            "tag \"$params.subsampling_quantile\"",
            "container \"nakor/metaflowmics-r:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::r-data.table\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "CONVERT_TO_MOTHUR_FORMAT": {
        "name_process": "CONVERT_TO_MOTHUR_FORMAT",
        "string_process": "\nprocess CONVERT_TO_MOTHUR_FORMAT {\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }\t\n\n    container \"nakor/metaflowmics-r:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::r-data.table\" : null)\n\n    input:\n    tuple val(meta), path(\"abundance.tsv\"), path(\"taxonomy.csv\")\n\n    output:\n    tuple val(meta), path(\"*.shared\"), emit: shared\n    tuple val(meta), path(\"*.taxonomy\"), emit: taxonomy\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(data.table)\n\n    abund <- data.frame(fread(\"abundance.tsv\"), row.names=1, check.names=F)\n\n    if ($params.taxa_are_rows) {\n        abund <- t(abund)\n    }\n    \n    shared <- cbind(\n        rep(1-$meta.otu_id/100, nrow(abund)),\n        rownames(abund),\n        rep(ncol(abund), nrow(abund)),\n        abund\n    )\n    colnames(shared) <- c(\"label\", \"Group\", \"numOtus\", colnames(abund))\n    write.table(shared, \"OTUs.${meta.id}.shared\", quote=F, sep=\"\\\\t\", row.names=F)\n\n    tax <- data.frame(fread(\"taxonomy.csv\"), row.names=1, check.names=F)\n    rownames(tax) <- gsub(\";.*\", \"\", rownames(tax))\n    rank_names <- colnames(tax)\n\n    tax <- cbind(\n        rownames(tax), \n        colSums(abund), \n        apply(tax, 1, function(x) paste(x, collapse=\";\"))\n    )\n    colnames(tax) <- c(\"OTU\", \"Size\", \"Taxonomy\")\n    tax[, \"Taxonomy\"] <- gsub(\"[a-z]__\", \"\", tax[, \"Taxonomy\"])\n\n    write.table(tax, \"OTUs.${meta.id}.taxonomy\", quote=F, sep=\"\\\\t\", row.names=F)\n    \"\"\"\n}",
        "nb_lignes_process": 51,
        "string_script": "    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(data.table)\n\n    abund <- data.frame(fread(\"abundance.tsv\"), row.names=1, check.names=F)\n\n    if ($params.taxa_are_rows) {\n        abund <- t(abund)\n    }\n    \n    shared <- cbind(\n        rep(1-$meta.otu_id/100, nrow(abund)),\n        rownames(abund),\n        rep(ncol(abund), nrow(abund)),\n        abund\n    )\n    colnames(shared) <- c(\"label\", \"Group\", \"numOtus\", colnames(abund))\n    write.table(shared, \"OTUs.${meta.id}.shared\", quote=F, sep=\"\\\\t\", row.names=F)\n\n    tax <- data.frame(fread(\"taxonomy.csv\"), row.names=1, check.names=F)\n    rownames(tax) <- gsub(\";.*\", \"\", rownames(tax))\n    rank_names <- colnames(tax)\n\n    tax <- cbind(\n        rownames(tax), \n        colSums(abund), \n        apply(tax, 1, function(x) paste(x, collapse=\";\"))\n    )\n    colnames(tax) <- c(\"OTU\", \"Size\", \"Taxonomy\")\n    tax[, \"Taxonomy\"] <- gsub(\"[a-z]__\", \"\", tax[, \"Taxonomy\"])\n\n    write.table(tax, \"OTUs.${meta.id}.taxonomy\", quote=F, sep=\"\\\\t\", row.names=F)\n    \"\"\"",
        "nb_lignes_script": 33,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_low\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"nakor/metaflowmics-r:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::r-data.table\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_GET_SEQS": {
        "name_process": "MOTHUR_GET_SEQS",
        "string_process": "\nprocess MOTHUR_GET_SEQS {\n    tag \"$meta.id\"\n    label \"process_low\"\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), file(ref), file(filt)\n\n    output:\n    tuple val(meta), path(\"*.pick.${filt.getExtension()}\")\n\n    script:\n    def ref_ext = ref.getExtension()\n    def arg = filt.getExtension().replaceAll(\"_table\", \"\")\n    \"\"\"\n    mothur \"#list.seqs(${ref_ext}=$ref);get.seqs(accnos=current,$arg=$filt)\"\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "    def ref_ext = ref.getExtension()\n    def arg = filt.getExtension().replaceAll(\"_table\", \"\")\n    \"\"\"\n    mothur \"#list.seqs(${ref_ext}=$ref);get.seqs(accnos=current,$arg=$filt)\"\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "ref",
            "filt"
        ],
        "nb_inputs": 3,
        "outputs": [
            "meta"
        ],
        "nb_outputs": 1,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_low\"",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "HOLOVIEWS_SCATTER": {
        "name_process": "HOLOVIEWS_SCATTER",
        "string_process": "\nprocess HOLOVIEWS_SCATTER {\n    tag \"$meta.id\"\n    label \"process_low\"\n    label \"plot\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        meta:meta) }\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::datatable pandas'>=1' holoviews\" : null)\n\n    input:\n    tuple val(meta), file(mg)\n\n    output:\n    tuple val(meta), file(\"scatter*.html\")\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    import holoviews as hv\n\n    hv.extension(\"bokeh\")\n\n    scatter_opt = dict(\n        size=8, alpha=0.8, \n        tools=[\"hover\"], \n        axiswise=True\n    )\n\n    data = pd.read_csv(\"${mg}\")\n    numeric_data = data.groupby(\"OTU\")[\"count\"].agg(prevalence=lambda x: sum(x>0), abundance=sum)\n    metadata = data.groupby(\"OTU\").first().drop(columns=[\"Group\", \"count\", \"relabund\"])\n    data = pd.concat([numeric_data, metadata], axis=1)\n\n    y_cols = [\"prevalence\"] + metadata.columns.tolist()\n\n    for rank in [\"Phylum\", \"Class\"]:\n        data_r = data[data[rank].notnull()]\n\n        if rank == \"Class\" and \"Proteobacteria\" in set(data_r.Phylum):\n            data_r = data_r[data_r.Phylum == \"Proteobacteria\"]\n    \n        names = sorted(data_r[rank].unique())\n\n        scatters = [hv.Scatter(\n            data_r[data_r[rank] == name], \"abundance\", y_cols, label=name\n        ).opts(**scatter_opt) for name in names]\n\n        scatters = hv.Layout(scatters).cols(5)\n        hv.save(scatters, f\"scatterplot-{rank}_${meta.id}.html\")\n    \"\"\"\n}",
        "nb_lignes_process": 56,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    import holoviews as hv\n\n    hv.extension(\"bokeh\")\n\n    scatter_opt = dict(\n        size=8, alpha=0.8, \n        tools=[\"hover\"], \n        axiswise=True\n    )\n\n    data = pd.read_csv(\"${mg}\")\n    numeric_data = data.groupby(\"OTU\")[\"count\"].agg(prevalence=lambda x: sum(x>0), abundance=sum)\n    metadata = data.groupby(\"OTU\").first().drop(columns=[\"Group\", \"count\", \"relabund\"])\n    data = pd.concat([numeric_data, metadata], axis=1)\n\n    y_cols = [\"prevalence\"] + metadata.columns.tolist()\n\n    for rank in [\"Phylum\", \"Class\"]:\n        data_r = data[data[rank].notnull()]\n\n        if rank == \"Class\" and \"Proteobacteria\" in set(data_r.Phylum):\n            data_r = data_r[data_r.Phylum == \"Proteobacteria\"]\n    \n        names = sorted(data_r[rank].unique())\n\n        scatters = [hv.Scatter(\n            data_r[data_r[rank] == name], \"abundance\", y_cols, label=name\n        ).opts(**scatter_opt) for name in names]\n\n        scatters = hv.Layout(scatters).cols(5)\n        hv.save(scatters, f\"scatterplot-{rank}_${meta.id}.html\")\n    \"\"\"",
        "nb_lignes_script": 36,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "mg"
        ],
        "nb_inputs": 2,
        "outputs": [
            "meta"
        ],
        "nb_outputs": 1,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_low\"",
            "label \"plot\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, meta:meta) }",
            "container \"nakor/metaflowmics-python:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::datatable pandas'>=1' holoviews\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "EMBOSS_TRANSEQ": {
        "name_process": "EMBOSS_TRANSEQ",
        "string_process": "\nprocess EMBOSS_TRANSEQ {\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n\t\tpattern: \"*.single.faa\",\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"nakor/metaflowmics-biotools:0.0.1\"\n    conda (params.enable_conda ? \"bioconda::emboss=6.6.0\" : null)\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.faa\"), emit: faa                           \n    tuple val(meta), path(\"*.single.faa\"), emit: single\n    tuple val(meta), path(\"*.multiple.faa\"), emit: multiple\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    outprefix = fasta.getSimpleName()\n    \"\"\"\n    #!/usr/bin/env bash\n\n    ## 1) Translate the sequence\n    ## 2) Replace title suffix pattern from \"_{frame}\" to \" {frame}\"\n    ## 3) Reformat to two-line fasta and remove short sequences\n\n    transeq -auto -trim \\\\\n      -outseq stdout \\\\\n      -sequence $fasta \\\\\n      -table $params.table \\\\\n      -frame $params.frames \\\\\n    | sed -E '/^>/s/([^ ]+)_([1-6])( .*)?/\\\\1\\\\3 frame=\\\\2/g' \\\\\n    | seqtk seq -l0 \\\\\n    > ${outprefix}.faa\n\n    ## 4) Discard sequences with stop codons in the middle of the sequence\n    grep -B1 -v '[>*]' --no-group-separator ${outprefix}.faa > ${outprefix}.nostop.faa\n\n    ## 5) Find the sequence with single or multiple possible ORFs\n    grep '^>' ${outprefix}.nostop.faa | cut -d' ' -f1 | uniq -c > freqs.txt\n    awk '\\$1==1' freqs.txt | cut -d'>' -f2 > single_frame.txt\n    awk '\\$1>1' freqs.txt | cut -d'>' -f2 > multiple_frames.txt\n\n    ## 6) Subset the translated sequences with those ids\n    seqtk subseq ${outprefix}.nostop.faa single_frame.txt > ${outprefix}.single.faa\n    seqtk subseq ${outprefix}.nostop.faa multiple_frames.txt > ${outprefix}.multiple.faa\n\n    transeq -h 2>&1 | grep -i version | cut -d':' -f3 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 53,
        "string_script": "    def software = getSoftwareName(task.process)\n    outprefix = fasta.getSimpleName()\n    \"\"\"\n    #!/usr/bin/env bash\n\n    ## 1) Translate the sequence\n    ## 2) Replace title suffix pattern from \"_{frame}\" to \" {frame}\"\n    ## 3) Reformat to two-line fasta and remove short sequences\n\n    transeq -auto -trim \\\\\n      -outseq stdout \\\\\n      -sequence $fasta \\\\\n      -table $params.table \\\\\n      -frame $params.frames \\\\\n    | sed -E '/^>/s/([^ ]+)_([1-6])( .*)?/\\\\1\\\\3 frame=\\\\2/g' \\\\\n    | seqtk seq -l0 \\\\\n    > ${outprefix}.faa\n\n    ## 4) Discard sequences with stop codons in the middle of the sequence\n    grep -B1 -v '[>*]' --no-group-separator ${outprefix}.faa > ${outprefix}.nostop.faa\n\n    ## 5) Find the sequence with single or multiple possible ORFs\n    grep '^>' ${outprefix}.nostop.faa | cut -d' ' -f1 | uniq -c > freqs.txt\n    awk '\\$1==1' freqs.txt | cut -d'>' -f2 > single_frame.txt\n    awk '\\$1>1' freqs.txt | cut -d'>' -f2 > multiple_frames.txt\n\n    ## 6) Subset the translated sequences with those ids\n    seqtk subseq ${outprefix}.nostop.faa single_frame.txt > ${outprefix}.single.faa\n    seqtk subseq ${outprefix}.nostop.faa multiple_frames.txt > ${outprefix}.multiple.faa\n\n    transeq -h 2>&1 | grep -i version | cut -d':' -f3 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 31,
        "language_script": "bash",
        "tools": [
            "denvax",
            "transeq",
            "seqtk"
        ],
        "tools_url": [
            "https://bio.tools/denvax",
            "https://bio.tools/transeq",
            "https://bio.tools/seqtk"
        ],
        "tools_dico": [
            {
                "name": "denvax",
                "uri": "https://bio.tools/denvax",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3379",
                            "term": "Preclinical and clinical studies"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatric medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "https://en.wikipedia.org/wiki/Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0560",
                                    "term": "DNA vaccine design"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Serostatus testing and dengue vaccine cost-benefit thresholds | R package for manuscript \"Serostatus Testing & Dengue Vaccine Cost Benefit Thresholds\"",
                "homepage": "https://cran.r-project.org/web/packages/denvax/index.html"
            },
            {
                "name": "transeq",
                "uri": "https://bio.tools/transeq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0371",
                                    "term": "DNA translation"
                                }
                            ],
                            []
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0849",
                                "term": "Sequence record"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2886",
                                "term": "Protein sequence record"
                            }
                        ]
                    }
                ],
                "description": "Translate nucleic acid sequences.",
                "homepage": "http://emboss.open-bio.org/rel/rel6/apps/transeq.html"
            },
            {
                "name": "seqtk",
                "uri": "https://bio.tools/seqtk",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Biological databases"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Data management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Information systems"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Databases and information systems"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Data handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2121",
                                    "term": "Sequence file editing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Utility operation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File processing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Report handling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A tool for processing sequences in the FASTA or FASTQ format. It parses both FASTA and FASTQ files which can also be optionally compressed by gzip.",
                "homepage": "https://github.com/lh3/seqtk"
            }
        ],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_medium\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , pattern: \"*.single.faa\" , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"nakor/metaflowmics-biotools:0.0.1\"",
            "conda (params.enable_conda ? \"bioconda::emboss=6.6.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_MAKE_SHARED": {
        "name_process": "MOTHUR_MAKE_SHARED",
        "string_process": "\nprocess MOTHUR_MAKE_SHARED {\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), file(list), file(count)\n\n    output:\n    tuple val(meta), path(\"*.shared\"), emit: shared\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#make.shared(count=$count, list=$list)\"\n\n    # rename output\n    mv *.shared ${outprefix}.shared\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#make.shared(count=$count, list=$list)\"\n\n    # rename output\n    mv *.shared ${outprefix}.shared\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "list",
            "count"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_medium\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_GET_OTU_REP": {
        "name_process": "MOTHUR_GET_OTU_REP",
        "string_process": "\nprocess MOTHUR_GET_OTU_REP {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), file(list), file(fasta), file(count)\n\n    output:\n    tuple val(meta), path(\"*.rep.fasta\"), emit: fasta\n    tuple val(meta), path(\"*.rep.count_table\"), emit: count_table\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#get.oturep(fasta=$fasta, list=$list, count=$count, method=abundance, rename=t)\"\n\n    mv *.rep.fasta ${outprefix}.rep.fasta\n    mv *.rep.count_table ${outprefix}.rep.count_table\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#get.oturep(fasta=$fasta, list=$list, count=$count, method=abundance, rename=t)\"\n\n    mv *.rep.fasta ${outprefix}.rep.fasta\n    mv *.rep.count_table ${outprefix}.rep.count_table\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "list",
            "fasta",
            "count"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_ALIGN_SEQS": {
        "name_process": "MOTHUR_ALIGN_SEQS",
        "string_process": "\nprocess MOTHUR_ALIGN_SEQS {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), path(fasta)\n    path db_aln\n\n    output:\n    tuple val(meta), path(\"*.fasta\"), emit: fasta\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    align.seqs(fasta=$fasta, reference=$db_aln);\n    filter.seqs(fasta=current, vertical=T);\"\n\n    mv *.filter.fasta ${outprefix}.fasta\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    align.seqs(fasta=$fasta, reference=$db_aln);\n    filter.seqs(fasta=current, vertical=T);\"\n\n    mv *.filter.fasta ${outprefix}.fasta\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "fasta",
            "db_aln"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "DADA2_MAKESEQUENCETABLE": {
        "name_process": "DADA2_MAKESEQUENCETABLE",
        "string_process": "\nprocess DADA2_MAKESEQUENCETABLE {\n    tag \"\"\n    label \"process_high\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:[\"id\"]) }\n\n    container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2\" : null)\n\n    input:\n    path merged_rds\n\n    output:\n    tuple val(100), path(\"dada2_ESVs.100.fasta\"), emit: fasta\n    tuple val(100), path(\"dada2_ESVs.100.csv\"), emit: abundance\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(dada2)\n\n    merged <- readRDS(\"${merged_rds}\")\n    merged.accepted <- lapply(merged, function(df) df[df\\$accept,])\n    esvTable <- makeSequenceTable(merged.accepted)\n\n    print(\"Removing empty samples\")\n    not_empty_samples <- rowSums(esvTable)>0\n    esvTable <- esvTable[not_empty_samples, ]\n\n    write.csv(esvTable,\"dada2_ESVs.100.csv\")\n\n    esv_ids <- sprintf(\"esv_%s\", c(1:dim(esvTable)[2]))\n    uniquesToFasta(esvTable, \"dada2_ESVs.100.fasta\", ids=esv_ids)\n    colnames(esvTable) <- esv_ids\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"\n}",
        "nb_lignes_process": 43,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(dada2)\n\n    merged <- readRDS(\"${merged_rds}\")\n    merged.accepted <- lapply(merged, function(df) df[df\\$accept,])\n    esvTable <- makeSequenceTable(merged.accepted)\n\n    print(\"Removing empty samples\")\n    not_empty_samples <- rowSums(esvTable)>0\n    esvTable <- esvTable[not_empty_samples, ]\n\n    write.csv(esvTable,\"dada2_ESVs.100.csv\")\n\n    esv_ids <- sprintf(\"esv_%s\", c(1:dim(esvTable)[2]))\n    uniquesToFasta(esvTable, \"dada2_ESVs.100.fasta\", ids=esv_ids)\n    colnames(esvTable) <- esv_ids\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"",
        "nb_lignes_script": 20,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "merged_rds"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:[\"id\"]) }",
            "container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"",
            "conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_SYNC": {
        "name_process": "MOTHUR_SYNC",
        "string_process": "\nprocess MOTHUR_SYNC {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), file(shared), file(list), file(fasta), file(count), file(tax)\n\n    output:\n    tuple val(meta), path(\"*.fasta\"), emit: fasta\n    tuple val(meta), path(\"*.count_table\"), emit: count_table\n    tuple val(meta), path(\"*.taxonomy\"), emit: taxonomy    \n    tuple val(meta), path(\"*.list\"), emit: list\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def outprefix = \"OTUs.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    list.otus(shared=$shared);\n    get.otus(accnos=current, list=$list);\n    list.seqs(list=current);\n    get.seqs(accnos=current, fasta=$fasta);    \n    get.seqs(accnos=current, count=$count);    \n    get.seqs(accnos=current, taxonomy=$tax)\"\n \n    mv *.pick.list ${outprefix}.list\n    mv *.pick.fasta ${outprefix}.fasta\n    mv *.pick.count_table ${outprefix}.count_table\n    mv *.pick.taxonomy ${outprefix}.taxonomy\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 41,
        "string_script": "    def software = getSoftwareName(task.process)\n    def outprefix = \"OTUs.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    list.otus(shared=$shared);\n    get.otus(accnos=current, list=$list);\n    list.seqs(list=current);\n    get.seqs(accnos=current, fasta=$fasta);    \n    get.seqs(accnos=current, count=$count);    \n    get.seqs(accnos=current, taxonomy=$tax)\"\n \n    mv *.pick.list ${outprefix}.list\n    mv *.pick.fasta ${outprefix}.fasta\n    mv *.pick.count_table ${outprefix}.count_table\n    mv *.pick.taxonomy ${outprefix}.taxonomy\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "shared",
            "list",
            "fasta",
            "count",
            "tax"
        ],
        "nb_inputs": 6,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "TRANSLATE": {
        "name_process": "TRANSLATE",
        "string_process": "\nprocess TRANSLATE {\n    tag \"$meta\"\n    label \"process_low\"\n    publishDir params.outdir, mode: params.publish_dir_mode\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::bipython conda-forge::pandas\" : null)\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*_main.faa\"), emit: main\n    tuple val(meta), path(\"*_mult.faa\"), emit: mult\n    tuple val(meta), path(\"*_other.faa\"), emit: other\n                                        \n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n    from Bio.Seq import Seq\n    from Bio.Data import CodonTable\n\n    def translate(seq, table):\n        proteins = []\n\n        for frame in range(0, 3):\n            nucl_length = len(seq) - frame\n            prot_length = nucl_length // 3\n            dna = seq[frame:3*prot_length+frame]\n            protein = seq.translate(table=table)\n            if '*' not in protein:\n                proteins.append((frame, protein))\n                continue\n\n            protein = dna.reverse_complement().translate(table=table)\n            if '*' not in protein:\n                protein.description = \"{protein.description} rc\"\n                proteins.append((frame, protein))\n        return proteins\n\n    with open(\"$fasta\", \"r\") as reader, \\\\\n         open(\"${fasta.getBaseName()}_main.faa\", \"w\") as writer, \\\\\n         open(\"${fasta.getBaseName()}_mult.faa\", \"w\") as writer_mult, \\\\\n         open(\"${fasta.getBaseName()}_other.faa\", \"w\") as writer_other:\n        for (title, seq) in SimpleFastaParser(reader):\n\n            if any(x not in \"ACGT\" for x in seq):\n                writer_other.write(f\">{title} | unknown_nucleotide \\\\n{seq}\\\\n\")\n                continue\n\n            lineage = dict(zip(\n                [\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"subfamily\", \"genus\", \"species\"],\n                title.split(';')\n            ))\n\n            seq = Seq(seq)\n\n            proteins = translate(seq, $params.table)\n\n            if len(proteins) == 0:\n                proteins = [prot for table in CodonTable.ambiguous_generic_by_id.keys()\n                            for prot in translate(seq, table)]\n                handle = writer_other\n            elif len(proteins) == 1:\n                handle = writer\n            elif len(proteins) > 1:\n                handle = writer_mult\n            \n            for (frame, protein) in proteins:\n                handle.write(f\">{title}|frame={frame};table={$params.table}\\\\n{protein}\\\\n\")\n    \"\"\"\n}",
        "nb_lignes_process": 74,
        "string_script": "    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n    from Bio.Seq import Seq\n    from Bio.Data import CodonTable\n\n    def translate(seq, table):\n        proteins = []\n\n        for frame in range(0, 3):\n            nucl_length = len(seq) - frame\n            prot_length = nucl_length // 3\n            dna = seq[frame:3*prot_length+frame]\n            protein = seq.translate(table=table)\n            if '*' not in protein:\n                proteins.append((frame, protein))\n                continue\n\n            protein = dna.reverse_complement().translate(table=table)\n            if '*' not in protein:\n                protein.description = \"{protein.description} rc\"\n                proteins.append((frame, protein))\n        return proteins\n\n    with open(\"$fasta\", \"r\") as reader, \\\\\n         open(\"${fasta.getBaseName()}_main.faa\", \"w\") as writer, \\\\\n         open(\"${fasta.getBaseName()}_mult.faa\", \"w\") as writer_mult, \\\\\n         open(\"${fasta.getBaseName()}_other.faa\", \"w\") as writer_other:\n        for (title, seq) in SimpleFastaParser(reader):\n\n            if any(x not in \"ACGT\" for x in seq):\n                writer_other.write(f\">{title} | unknown_nucleotide \\\\n{seq}\\\\n\")\n                continue\n\n            lineage = dict(zip(\n                [\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"subfamily\", \"genus\", \"species\"],\n                title.split(';')\n            ))\n\n            seq = Seq(seq)\n\n            proteins = translate(seq, $params.table)\n\n            if len(proteins) == 0:\n                proteins = [prot for table in CodonTable.ambiguous_generic_by_id.keys()\n                            for prot in translate(seq, table)]\n                handle = writer_other\n            elif len(proteins) == 1:\n                handle = writer\n            elif len(proteins) > 1:\n                handle = writer_mult\n            \n            for (frame, protein) in proteins:\n                handle.write(f\">{title}|frame={frame};table={$params.table}\\\\n{protein}\\\\n\")\n    \"\"\"",
        "nb_lignes_script": 56,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta\"",
            "label \"process_low\"",
            "publishDir params.outdir, mode: params.publish_dir_mode",
            "container \"nakor/metaflowmics-python:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::bipython conda-forge::pandas\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "LULU": {
        "name_process": "LULU",
        "string_process": "\nprocess LULU {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        pattern: \"*.{csv,shared,fasta}\",\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta) }\n\n    container \"nakor/metaflowmics-r:0.0.2\"\n                                   \n\n    input:\n    tuple val(meta), path(matchlist), path(abundance), path(fasta)\n\n    output:\n    tuple val(meta), path(\"abundance_table-lulu-*.{csv,shared}\"), emit: abundance\n    tuple val(meta), path(\"*.fasta\"), emit: fasta\n    tuple val(meta), path(\"mapping_discarded*.txt\"), emit: discarded, optional: true\n    path \"*.summary\", emit: summary\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def ext = abundance.getExtension()\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(lulu)\n    library(data.table)\n    library(seqinr)\n\n    fast_table_load <- function(filename, drop=c(), row.names=1) {\n        data <- data.frame(\n            fread(filename, nThread=$task.cpus, drop=drop, header=T),\n            row.names=row.names, check.names=F\n        )\n        return(data)\n    }\n\n    if(\"$ext\"=='shared') {\n        otutab <- t(fast_table_load(\"$abundance\", drop=c(1, 3), row.names=1))\n    } else {\n        otutab <- fast_table_load(\"$abundance\", row.names=1)        \n    }\n\n    matchList <- read.table(\"$matchlist\", header=FALSE, col.names=c(\"OTU1\", \"OTU2\", \"pctIdentity\"),\n                            as.is=TRUE, check.names=F, stringsAsFactors=FALSE)\n    fasta <- read.fasta(\"$fasta\", seqtype=\"DNA\", forceDNAtolower=F)\n    names(fasta) <- gsub('[\\\\t;].*', '', names(fasta))\n\n    res <- lulu(as.data.frame(otutab), matchList, \n                minimum_ratio_type=\"$params.lulu_min_ratio_type\",\n                minimum_ratio=$params.lulu_min_ratio,\n                minimum_match=$params.lulu_min_match,\n                minimum_relative_cooccurence=$params.lulu_min_rel_cooccurence)\n    otutab <- res\\$curated_table\n    fasta <- fasta[res\\$curated_otus]\n\n    if (\"$ext\"=='shared') {\n        shared <- cbind(\n            rep(1-${meta.otu_id}/100, ncol(otutab)),\n            colnames(otutab),\n            rep(nrow(otutab), ncol(otutab)),\n            t(otutab)\n        )\n        colnames(shared) <- c('label', 'Group', 'numOtus', rownames(otutab))\n        write.table(shared, \"abundance_table-lulu-${meta.id}.shared\", quote=F, sep='\\\\t', row.names=F)\n    } else {\n        abund <- cbind(\n            rownames(otutab),\n            otutab\n        )\n        colnames(abund) <- c(\"OTU\", colnames(otutab))\n        write.table(abund, \"abundance_table-lulu-${meta.id}.csv\", quote=F, sep=',', row.names=F)\n    }\n\n    write.table(res\\$otu_map[res\\$discarded_otus,], \"mapping_discarded-${meta.id}.txt\", quote=F)\n    write.fasta(fasta, names(fasta), \"sequences-${meta.id}.fasta\")\n\n    # Write counts\n    summary <- cbind(\n        rep(\"LULU\", ncol(otutab)),\n        rep(\"$meta.id\", ncol(otutab)),\n        colnames(otutab),\n        colSums(otutab),\n        colSums(otutab > 0)\n    )\n    write.table(summary, \"lulu_${meta.id}.summary\", quote=F, sep=',', row.names=F, col.names=F)    \n\n    writeLines(paste0(packageVersion('lulu')), \"${software}.version.txt\")\n    \"\"\"\n}",
        "nb_lignes_process": 93,
        "string_script": "    def software = getSoftwareName(task.process)\n    def ext = abundance.getExtension()\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(lulu)\n    library(data.table)\n    library(seqinr)\n\n    fast_table_load <- function(filename, drop=c(), row.names=1) {\n        data <- data.frame(\n            fread(filename, nThread=$task.cpus, drop=drop, header=T),\n            row.names=row.names, check.names=F\n        )\n        return(data)\n    }\n\n    if(\"$ext\"=='shared') {\n        otutab <- t(fast_table_load(\"$abundance\", drop=c(1, 3), row.names=1))\n    } else {\n        otutab <- fast_table_load(\"$abundance\", row.names=1)        \n    }\n\n    matchList <- read.table(\"$matchlist\", header=FALSE, col.names=c(\"OTU1\", \"OTU2\", \"pctIdentity\"),\n                            as.is=TRUE, check.names=F, stringsAsFactors=FALSE)\n    fasta <- read.fasta(\"$fasta\", seqtype=\"DNA\", forceDNAtolower=F)\n    names(fasta) <- gsub('[\\\\t;].*', '', names(fasta))\n\n    res <- lulu(as.data.frame(otutab), matchList, \n                minimum_ratio_type=\"$params.lulu_min_ratio_type\",\n                minimum_ratio=$params.lulu_min_ratio,\n                minimum_match=$params.lulu_min_match,\n                minimum_relative_cooccurence=$params.lulu_min_rel_cooccurence)\n    otutab <- res\\$curated_table\n    fasta <- fasta[res\\$curated_otus]\n\n    if (\"$ext\"=='shared') {\n        shared <- cbind(\n            rep(1-${meta.otu_id}/100, ncol(otutab)),\n            colnames(otutab),\n            rep(nrow(otutab), ncol(otutab)),\n            t(otutab)\n        )\n        colnames(shared) <- c('label', 'Group', 'numOtus', rownames(otutab))\n        write.table(shared, \"abundance_table-lulu-${meta.id}.shared\", quote=F, sep='\\\\t', row.names=F)\n    } else {\n        abund <- cbind(\n            rownames(otutab),\n            otutab\n        )\n        colnames(abund) <- c(\"OTU\", colnames(otutab))\n        write.table(abund, \"abundance_table-lulu-${meta.id}.csv\", quote=F, sep=',', row.names=F)\n    }\n\n    write.table(res\\$otu_map[res\\$discarded_otus,], \"mapping_discarded-${meta.id}.txt\", quote=F)\n    write.fasta(fasta, names(fasta), \"sequences-${meta.id}.fasta\")\n\n    # Write counts\n    summary <- cbind(\n        rep(\"LULU\", ncol(otutab)),\n        rep(\"$meta.id\", ncol(otutab)),\n        colnames(otutab),\n        colSums(otutab),\n        colSums(otutab > 0)\n    )\n    write.table(summary, \"lulu_${meta.id}.summary\", quote=F, sep=',', row.names=F, col.names=F)    \n\n    writeLines(paste0(packageVersion('lulu')), \"${software}.version.txt\")\n    \"\"\"",
        "nb_lignes_script": 68,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "matchlist",
            "abundance",
            "fasta"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , pattern: \"*.{csv,shared,fasta}\" , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta) }",
            "container \"nakor/metaflowmics-r:0.0.2\""
        ],
        "when": "",
        "stub": ""
    },
    "FILTER_FASTA": {
        "name_process": "FILTER_FASTA",
        "string_process": "\nprocess FILTER_FASTA {\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::biopython conda-forge::pandas\" : null)\n\n    input:\n    tuple val(meta), path(fasta), path(hits)\n\n    output:\n    path \"*.fasta\", emit: fasta\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n\n    hits = pd.read_csv(\"$hits\", sep=\"\\\\t\")\n    hits[\"contig\"] = hits.qseqid.str.split(\"|\").str[0]\n\n    kept = set(hits.groupby(\"contig\").pident.agg(lambda x: x.idxmax()))\n\n    with open(\"$fasta\", \"r\") as reader, \\\\\n         open(\"${fasta.getBaseName()}_filt.fasta\", \"w\") as writer:\n        for i, (title, seq) in enumerate(SimpleFastaParser(reader)):\n            if title in kept:\n                writer.write(f'{title}\\\\n{seq}\\\\n')\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n\n    hits = pd.read_csv(\"$hits\", sep=\"\\\\t\")\n    hits[\"contig\"] = hits.qseqid.str.split(\"|\").str[0]\n\n    kept = set(hits.groupby(\"contig\").pident.agg(lambda x: x.idxmax()))\n\n    with open(\"$fasta\", \"r\") as reader, \\\\\n         open(\"${fasta.getBaseName()}_filt.fasta\", \"w\") as writer:\n        for i, (title, seq) in enumerate(SimpleFastaParser(reader)):\n            if title in kept:\n                writer.write(f'{title}\\\\n{seq}\\\\n')\n    \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta",
            "hits"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_low\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"nakor/metaflowmics-python:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::biopython conda-forge::pandas\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "COMPUTE_MSA_REPRESENTATIVE": {
        "name_process": "COMPUTE_MSA_REPRESENTATIVE",
        "string_process": "\nprocess COMPUTE_MSA_REPRESENTATIVE {\n    label \"process_low\"\n    publishDir params.outdir, mode: params.publish_dir_mode\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::biopython conda-forge:pandas\" : null)\n\n    input:\n    path fasta\n\n    output:\n    path \"*.repr.fa\", emit: repr\n\n    script:\n    def prefix = fasta.getSimpleName()\n\tdef skip_gap = params.skip_gap ? \"if letter == '-': continue\" :\"\"\n    \"\"\"\n    #!/usr/bin/env python\n\n    from collections import defaultdict\n    import re\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n    import pandas as pd\n\n    aln_len = len(next(SimpleFastaParser(open(\"$fasta\")))[1])\n\n    freqs = [defaultdict(lambda: 0) for _ in range(aln_len)]\n\n    with open(\"$fasta\", \"r\") as reader:\n        for (title, seq) in SimpleFastaParser(reader):\n            for (i, letter) in enumerate(seq):\n\t            $skip_gap\n\t            freqs[i][letter] += 1\n\n    freqs = pd.DataFrame(freqs).fillna(0).astype(int)\n\n    consensus = ''.join(pd.DataFrame(freqs).idxmax(axis=1))\n    cons_lineage = title.split()[1]\n\n    with open(\"${prefix}.repr.fa\", \"w\") as writer:\n        writer.write(f\">repr|${prefix} {cons_lineage}\\\\n{consensus}\\\\n\")\n    \"\"\"\n}",
        "nb_lignes_process": 42,
        "string_script": "    def prefix = fasta.getSimpleName()\n\tdef skip_gap = params.skip_gap ? \"if letter == '-': continue\" :\"\"\n    \"\"\"\n    #!/usr/bin/env python\n\n    from collections import defaultdict\n    import re\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n    import pandas as pd\n\n    aln_len = len(next(SimpleFastaParser(open(\"$fasta\")))[1])\n\n    freqs = [defaultdict(lambda: 0) for _ in range(aln_len)]\n\n    with open(\"$fasta\", \"r\") as reader:\n        for (title, seq) in SimpleFastaParser(reader):\n            for (i, letter) in enumerate(seq):\n\t            $skip_gap\n\t            freqs[i][letter] += 1\n\n    freqs = pd.DataFrame(freqs).fillna(0).astype(int)\n\n    consensus = ''.join(pd.DataFrame(freqs).idxmax(axis=1))\n    cons_lineage = title.split()[1]\n\n    with open(\"${prefix}.repr.fa\", \"w\") as writer:\n        writer.write(f\">repr|${prefix} {cons_lineage}\\\\n{consensus}\\\\n\")\n    \"\"\"",
        "nb_lignes_script": 27,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fasta"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_low\"",
            "publishDir params.outdir, mode: params.publish_dir_mode",
            "container \"nakor/metaflowmics-python:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::biopython conda-forge:pandas\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "HMMER_HMMBUILD": {
        "name_process": "HMMER_HMMBUILD",
        "string_process": "\nprocess HMMER_HMMBUILD {\n\ttag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::hmmer=3.3.2\" : null)\n    container \"quay.io/biocontainers/hmmer:3.3.2--h1b792b2_1\"\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"hmmdb\"), emit: hmm\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    hmmbuild $options.args --cpu $task.cpus hmmdb $fasta\n\n    echo \\$(hmmbuild -h | grep -o '^# HMMER [0-9.]*') | sed 's/^# HMMER *//' > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    hmmbuild $options.args --cpu $task.cpus hmmdb $fasta\n\n    echo \\$(hmmbuild -h | grep -o '^# HMMER [0-9.]*') | sed 's/^# HMMER *//' > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::hmmer=3.3.2\" : null)",
            "container \"quay.io/biocontainers/hmmer:3.3.2--h1b792b2_1\""
        ],
        "when": "",
        "stub": ""
    },
    "SEQTK_SUBSEQ": {
        "name_process": "SEQTK_SUBSEQ",
        "string_process": "\nprocess SEQTK_SUBSEQ {\n    tag \"$meta\"\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/seqtk:1.3--h5bf99c6_3\"\n    conda (params.enable_conda ? \"bioconda::seqtk=1.3\" : null)\n\n    input:\n    tuple val(meta), path(fasta), path(ids)\n\n    output:\n    tuple val(meta), path(\"*.fasta\"), emit: fasta\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    outprefix = fasta.getBaseName()\n    \"\"\"\n    #!/usr/bin/env bash\n\n    for f in $ids; do\n        prefix=${outprefix}_\\$(basename \\$f .txt)\n        seqtk subseq $fasta \\$f > \\${prefix}.fasta\n    done\n\n    seqtk -h 2>&1 | grep -i version | cut -d':' -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "    def software = getSoftwareName(task.process)\n    outprefix = fasta.getBaseName()\n    \"\"\"\n    #!/usr/bin/env bash\n\n    for f in $ids; do\n        prefix=${outprefix}_\\$(basename \\$f .txt)\n        seqtk subseq $fasta \\$f > \\${prefix}.fasta\n    done\n\n    seqtk -h 2>&1 | grep -i version | cut -d':' -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "denvax",
            "seqtk"
        ],
        "tools_url": [
            "https://bio.tools/denvax",
            "https://bio.tools/seqtk"
        ],
        "tools_dico": [
            {
                "name": "denvax",
                "uri": "https://bio.tools/denvax",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3379",
                            "term": "Preclinical and clinical studies"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatric medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "https://en.wikipedia.org/wiki/Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0560",
                                    "term": "DNA vaccine design"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Serostatus testing and dengue vaccine cost-benefit thresholds | R package for manuscript \"Serostatus Testing & Dengue Vaccine Cost Benefit Thresholds\"",
                "homepage": "https://cran.r-project.org/web/packages/denvax/index.html"
            },
            {
                "name": "seqtk",
                "uri": "https://bio.tools/seqtk",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Biological databases"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Data management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Information systems"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Databases and information systems"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Data handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2121",
                                    "term": "Sequence file editing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Utility operation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File processing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Report handling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A tool for processing sequences in the FASTA or FASTQ format. It parses both FASTA and FASTQ files which can also be optionally compressed by gzip.",
                "homepage": "https://github.com/lh3/seqtk"
            }
        ],
        "inputs": [
            "meta",
            "fasta",
            "ids"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta\"",
            "label \"process_low\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/seqtk:1.3--h5bf99c6_3\"",
            "conda (params.enable_conda ? \"bioconda::seqtk=1.3\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_MAKE_DATABASE": {
        "name_process": "MOTHUR_MAKE_DATABASE",
        "string_process": "\nprocess MOTHUR_MAKE_DATABASE {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }    \n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), file(shared), file(constax), file(repfasta), file(repcount)\n\n    output:\n    tuple val(meta), path(\"*.database\"), emit: database\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"OTUs.${meta.id}\"\n    \"\"\"\n    mothur \"#create.database(shared=$shared,repfasta=$repfasta,constaxonomy=$constax,count=$repcount)\"\n    mv *.database ${outprefix}.database\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"OTUs.${meta.id}\"\n    \"\"\"\n    mothur \"#create.database(shared=$shared,repfasta=$repfasta,constaxonomy=$constax,count=$repcount)\"\n    mv *.database ${outprefix}.database\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "shared",
            "constax",
            "repfasta",
            "repcount"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_CLASSIFY_SEQS": {
        "name_process": "MOTHUR_CLASSIFY_SEQS",
        "string_process": "\nprocess MOTHUR_CLASSIFY_SEQS {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }        \n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), path(fasta), path(count)\n    path(db_aln)\n    path(db_tax)\n\n    output:\n    tuple val(meta), path(\"*.taxonomy\"), emit: taxonomy\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#classify.seqs(fasta=$fasta, count=$count, template=$db_aln, taxonomy=$db_tax, cutoff=$params.classification_consensus)\"\n    # rename output\n    mv *.wang.taxonomy ${outprefix}.taxonomy\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#classify.seqs(fasta=$fasta, count=$count, template=$db_aln, taxonomy=$db_tax, cutoff=$params.classification_consensus)\"\n    # rename output\n    mv *.wang.taxonomy ${outprefix}.taxonomy\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "fasta",
            "count",
            "db_aln",
            "db_tax"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "SYNC_SEQIDS": {
        "name_process": "SYNC_SEQIDS",
        "string_process": "\nprocess SYNC_SEQIDS {\n\ttag \"$meta.id\"\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::biopython conda-forge::pandas\" : null)\n\n    input:\n    tuple val(meta), path(ref_fa)\n    path files\n\n    output:\n    tuple val(meta), path(\"*.sync.{fna,fasta}\"), optional: true, emit: fna\n    tuple val(meta), path(\"*.sync.faa\"), optional: true, emit: faa\n    tuple val(meta), path(\"*.sync.count_table\"), optional: true, emit: count_table\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    from pathlib import Path\n    from Bio import SeqIO\n    import pandas as pd\n\n    seq_info = {\n      seq.id: seq.description.split(\"frame=\")[-1]\n      for seq in SeqIO.parse(\"$ref_fa\", \"fasta\")\n    }\n\n    for f in [\"${files.join('\",\"')}\"]:\n        fpath = Path(f)\n        if fpath.suffix == \".count_table\":\n            table = pd.read_csv(fpath, index_col=0, sep='\\\\t').loc[seq_info.keys()]\n            table.to_csv(f\"{fpath.stem}.sync.count_table\", sep=\"\\\\t\")\n        else: # we assume it is a fasta formatted file\n            seq_data = {seq.id: seq for seq in SeqIO.parse(fpath, \"fasta\")}\n\n            with open(f\"{fpath.stem}.sync{fpath.suffix}\", \"w\") as w:\n                for (seq_id, frame) in seq_info.items():\n                    seq = seq_data[seq_id]\n                    if frame.isdigit() and int(frame) > 3:\n                        seq = seq.reverse_complement(id=True, description=True)\n                    w.write(seq.format(\"fasta-2line\"))\n    \"\"\"\n}",
        "nb_lignes_process": 48,
        "string_script": "    \"\"\"\n    #!/usr/bin/env python\n\n    from pathlib import Path\n    from Bio import SeqIO\n    import pandas as pd\n\n    seq_info = {\n      seq.id: seq.description.split(\"frame=\")[-1]\n      for seq in SeqIO.parse(\"$ref_fa\", \"fasta\")\n    }\n\n    for f in [\"${files.join('\",\"')}\"]:\n        fpath = Path(f)\n        if fpath.suffix == \".count_table\":\n            table = pd.read_csv(fpath, index_col=0, sep='\\\\t').loc[seq_info.keys()]\n            table.to_csv(f\"{fpath.stem}.sync.count_table\", sep=\"\\\\t\")\n        else: # we assume it is a fasta formatted file\n            seq_data = {seq.id: seq for seq in SeqIO.parse(fpath, \"fasta\")}\n\n            with open(f\"{fpath.stem}.sync{fpath.suffix}\", \"w\") as w:\n                for (seq_id, frame) in seq_info.items():\n                    seq = seq_data[seq_id]\n                    if frame.isdigit() and int(frame) > 3:\n                        seq = seq.reverse_complement(id=True, description=True)\n                    w.write(seq.format(\"fasta-2line\"))\n    \"\"\"",
        "nb_lignes_script": 26,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "ref_fa",
            "files"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_low\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"nakor/metaflowmics-python:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::biopython conda-forge::pandas\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "VSEARCH_CLUSTER": {
        "name_process": "VSEARCH_CLUSTER",
        "string_process": "\nprocess VSEARCH_CLUSTER {\n    tag \"$otu_id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta) }\n\n    container \"quay.io/biocontainers/vsearch:2.17.0--h95f258a_1\"\n    conda (params.enable_conda ? \"bioconda::vsearch=2.17.0\" : null)\n\n    input:\n    tuple val(meta), path(fasta)\n    each otu_id\n\n    output:\n    tuple val(meta_upd), path(\"vsearch_OTUs-*.tsv\"), emit: tsv\n    tuple val(meta_upd), path(\"vsearch_OTUs-*.fasta\"), emit: fasta\n                                        \n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n                                    \n\tmeta_upd = meta.clone()\n\tmeta_upd[\"id\"] = \"${otu_id}\"\n\tmeta_upd[\"otu_id\"] = otu_id\t\n    \"\"\"\n    #!/usr/bin/env bash\n    vsearch $options.args \\\\\n        --threads $task.cpus \\\\\n        --sizein \\\\\n        --sizeout \\\\\n        --fasta_width 0 \\\\\n        --id ${otu_id/100} \\\\\n        --strand plus \\\\\n        --cluster_size ${fasta} \\\\\n        --uc clusters${otu_id}.uc \\\\\n        --relabel OTU${otu_id}_ \\\\\n        --centroids vsearch_OTUs-${otu_id}.fasta \\\\\n        --otutabout vsearch_OTUs-${otu_id}.tsv\n\n    echo \\$(vsearch --version 2>&1) | grep \"RAM\" | sed \"s/vsearch v//\" | sed \"s/, .*//\" > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 45,
        "string_script": "    def software = getSoftwareName(task.process)\n                                    \n\tmeta_upd = meta.clone()\n\tmeta_upd[\"id\"] = \"${otu_id}\"\n\tmeta_upd[\"otu_id\"] = otu_id\t\n    \"\"\"\n    #!/usr/bin/env bash\n    vsearch $options.args \\\\\n        --threads $task.cpus \\\\\n        --sizein \\\\\n        --sizeout \\\\\n        --fasta_width 0 \\\\\n        --id ${otu_id/100} \\\\\n        --strand plus \\\\\n        --cluster_size ${fasta} \\\\\n        --uc clusters${otu_id}.uc \\\\\n        --relabel OTU${otu_id}_ \\\\\n        --centroids vsearch_OTUs-${otu_id}.fasta \\\\\n        --otutabout vsearch_OTUs-${otu_id}.tsv\n\n    echo \\$(vsearch --version 2>&1) | grep \"RAM\" | sed \"s/vsearch v//\" | sed \"s/, .*//\" > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 21,
        "language_script": "bash",
        "tools": [
            "denvax",
            "VSEARCH"
        ],
        "tools_url": [
            "https://bio.tools/denvax",
            "https://bio.tools/vsearch"
        ],
        "tools_dico": [
            {
                "name": "denvax",
                "uri": "https://bio.tools/denvax",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3379",
                            "term": "Preclinical and clinical studies"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatric medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "https://en.wikipedia.org/wiki/Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0560",
                                    "term": "DNA vaccine design"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Serostatus testing and dengue vaccine cost-benefit thresholds | R package for manuscript \"Serostatus Testing & Dengue Vaccine Cost Benefit Thresholds\"",
                "homepage": "https://cran.r-project.org/web/packages/denvax/index.html"
            },
            {
                "name": "VSEARCH",
                "uri": "https://bio.tools/vsearch",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2520",
                                    "term": "DNA mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0450",
                                    "term": "Chimera detection"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0450",
                                    "term": "Chimeric sequence detection"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2977",
                                "term": "Nucleic acid sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_0865",
                                "term": "Sequence similarity score"
                            }
                        ]
                    }
                ],
                "description": "High-throughput search and clustering sequence analysis tool. It supports de novo and reference based chimera detection, clustering, full-length and prefix dereplication, reverse complementation, masking, all-vs-all pairwise global alignment, exact and global alignment searching, shuffling, subsampling and sorting. It also supports FASTQ file analysis, filtering and conversion.",
                "homepage": "https://github.com/torognes/vsearch"
            }
        ],
        "inputs": [
            "meta",
            "fasta",
            "otu_id"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$otu_id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta) }",
            "container \"quay.io/biocontainers/vsearch:2.17.0--h95f258a_1\"",
            "conda (params.enable_conda ? \"bioconda::vsearch=2.17.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "VSEARCH_CHIMERA": {
        "name_process": "VSEARCH_CHIMERA",
        "string_process": "\nprocess VSEARCH_CHIMERA {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:[\"id\"]) }\n\n    container \"quay.io/biocontainers/vsearch:2.17.0--h95f258a_1\"\n    conda (params.enable_conda ? \"bioconda::vsearch=2.17.0\" : null)\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*-nochimera.fasta\"), emit: fasta\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env bash\n\n    vsearch $options.args \\\\\n        --threads $task.cpus \\\\\n        --uchime3_denovo $fasta \\\\\n        --sizein \\\\\n        --sizeout \\\\\n        --fasta_width 0 \\\\\n        --nonchimeras ${meta.id}-nochimera.fasta\n\n    echo \\$(vsearch --version 2>&1) | grep \"RAM\" | sed \"s/vsearch v//\" | sed \"s/, .*//\" > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env bash\n\n    vsearch $options.args \\\\\n        --threads $task.cpus \\\\\n        --uchime3_denovo $fasta \\\\\n        --sizein \\\\\n        --sizeout \\\\\n        --fasta_width 0 \\\\\n        --nonchimeras ${meta.id}-nochimera.fasta\n\n    echo \\$(vsearch --version 2>&1) | grep \"RAM\" | sed \"s/vsearch v//\" | sed \"s/, .*//\" > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "denvax",
            "VSEARCH"
        ],
        "tools_url": [
            "https://bio.tools/denvax",
            "https://bio.tools/vsearch"
        ],
        "tools_dico": [
            {
                "name": "denvax",
                "uri": "https://bio.tools/denvax",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3379",
                            "term": "Preclinical and clinical studies"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatric medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "https://en.wikipedia.org/wiki/Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0560",
                                    "term": "DNA vaccine design"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Serostatus testing and dengue vaccine cost-benefit thresholds | R package for manuscript \"Serostatus Testing & Dengue Vaccine Cost Benefit Thresholds\"",
                "homepage": "https://cran.r-project.org/web/packages/denvax/index.html"
            },
            {
                "name": "VSEARCH",
                "uri": "https://bio.tools/vsearch",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2520",
                                    "term": "DNA mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0450",
                                    "term": "Chimera detection"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0450",
                                    "term": "Chimeric sequence detection"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2977",
                                "term": "Nucleic acid sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_0865",
                                "term": "Sequence similarity score"
                            }
                        ]
                    }
                ],
                "description": "High-throughput search and clustering sequence analysis tool. It supports de novo and reference based chimera detection, clustering, full-length and prefix dereplication, reverse complementation, masking, all-vs-all pairwise global alignment, exact and global alignment searching, shuffling, subsampling and sorting. It also supports FASTQ file analysis, filtering and conversion.",
                "homepage": "https://github.com/torognes/vsearch"
            }
        ],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:[\"id\"]) }",
            "container \"quay.io/biocontainers/vsearch:2.17.0--h95f258a_1\"",
            "conda (params.enable_conda ? \"bioconda::vsearch=2.17.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "COIL": {
        "name_process": "COIL",
        "string_process": "\nprocess COIL {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta) }\n\n    container \"nakor/coil:0.0.1\"\n                                   \n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"*.fasta\"), emit: afasta\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def ext = abundance.getExtension()\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(seqinr)\n    library(coil)\n\n    seq = read.fasta(\"$fasta\", as.string=TRUE)\n\n    #parse the data in the header line by splitting the name on the | character\n\n    #step 1: build the coi5p object\n    dat = coi5p(seq, name=\"example_sequence_1\")\n\n    #step 2: frame the sequence\n    dat = frame(dat)\n\n    #step 3: by default censored translation is performed - see vignette for details\n    dat = translate(dat)\n\n    ##step 3a: if taxonomy is known, but the translation table is not, a helper function\n    #can be used to look up the proper translation table.\n    which_trans_table(\"Scyliorhinidae\")\n\n    #step 3a: the proper translation table can be passed to the translation function\n    dat = translate(dat, trans_table = 2)\n\n    writeLines(paste0(packageVersion('coil')), \"${software}.version.txt\")\n    \"\"\"\n}",
        "nb_lignes_process": 50,
        "string_script": "    def software = getSoftwareName(task.process)\n    def ext = abundance.getExtension()\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(seqinr)\n    library(coil)\n\n    seq = read.fasta(\"$fasta\", as.string=TRUE)\n\n    #parse the data in the header line by splitting the name on the | character\n\n    #step 1: build the coi5p object\n    dat = coi5p(seq, name=\"example_sequence_1\")\n\n    #step 2: frame the sequence\n    dat = frame(dat)\n\n    #step 3: by default censored translation is performed - see vignette for details\n    dat = translate(dat)\n\n    ##step 3a: if taxonomy is known, but the translation table is not, a helper function\n    #can be used to look up the proper translation table.\n    which_trans_table(\"Scyliorhinidae\")\n\n    #step 3a: the proper translation table can be passed to the translation function\n    dat = translate(dat, trans_table = 2)\n\n    writeLines(paste0(packageVersion('coil')), \"${software}.version.txt\")\n    \"\"\"",
        "nb_lignes_script": 29,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta) }",
            "container \"nakor/coil:0.0.1\""
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_DIST_SEQS": {
        "name_process": "MOTHUR_DIST_SEQS",
        "string_process": "\nprocess MOTHUR_DIST_SEQS {\n    tag \"$meta.id\"\n    label \"process_high\"\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), file(fasta)\n\n    output:\n    tuple val(meta), path(\"$outprefix*.dist\"), emit: dist\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def cutoff = params.cutoff ?: 1\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    outprefix = \"${procname}.${meta.id}\"    \n    \"\"\"\n    # Manually rename sequences\n    # sed \"/^>/s/.*\\\\(Otu[0-9]*\\\\)\\\\(.*\\\\)/>\\\\1\\\\t\\\\1\\\\2/\" $fasta > renamed.fasta\n\n    mothur \"#filter.seqs(fasta=$fasta, trump=.); dist.seqs(fasta=current, cutoff=$cutoff)\"\n\n    if [ \"${params.format.toLowerCase()}\" == \"vsearch\" ]; then\n        awk '{OFS=\"\\\\t\"}{print \\$1,\\$2,100*(1-\\$3)}' *.dist > ${outprefix}.dist\n    else\n        mv *.dist ${outprefix}.dist\n    fi\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def software = getSoftwareName(task.process)\n    def cutoff = params.cutoff ?: 1\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    outprefix = \"${procname}.${meta.id}\"    \n    \"\"\"\n    # Manually rename sequences\n    # sed \"/^>/s/.*\\\\(Otu[0-9]*\\\\)\\\\(.*\\\\)/>\\\\1\\\\t\\\\1\\\\2/\" $fasta > renamed.fasta\n\n    mothur \"#filter.seqs(fasta=$fasta, trump=.); dist.seqs(fasta=current, cutoff=$cutoff)\"\n\n    if [ \"${params.format.toLowerCase()}\" == \"vsearch\" ]; then\n        awk '{OFS=\"\\\\t\"}{print \\$1,\\$2,100*(1-\\$3)}' *.dist > ${outprefix}.dist\n    else\n        mv *.dist ${outprefix}.dist\n    fi\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_REMOVE_RARE": {
        "name_process": "MOTHUR_REMOVE_RARE",
        "string_process": "\nprocess MOTHUR_REMOVE_RARE {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), file(list), file(count)\n\n    output:\n    tuple val(meta), path(\"${outprefix}.list\"), emit: list\n    tuple val(meta), path(\"${outprefix}.shared\"), emit: shared\n    tuple val(meta), path(\"${outprefix}.count_table\"), emit: count_table\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    remove.rare(count=$count, list=$list, nseqs=${params.min_abundance});\n    make.shared(count=current, list=current);\n    count.seqs(count=current, compress=f)\"\n\n    # rename outputs\n    mv *.pick.shared ${outprefix}.shared\n    mv *.pick.list ${outprefix}.list\n    mv *.pick.full.count_table ${outprefix}.count_table\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    remove.rare(count=$count, list=$list, nseqs=${params.min_abundance});\n    make.shared(count=current, list=current);\n    count.seqs(count=current, compress=f)\"\n\n    # rename outputs\n    mv *.pick.shared ${outprefix}.shared\n    mv *.pick.list ${outprefix}.list\n    mv *.pick.full.count_table ${outprefix}.count_table\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "list",
            "count"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_medium\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "HOLOVIEWS_PREPARE": {
        "name_process": "HOLOVIEWS_PREPARE",
        "string_process": "\nprocess HOLOVIEWS_PREPARE {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    label \"plot\"\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::datatable pandas'>=1'\" : null)\n\n    input:\n    tuple val(meta), file(shared), file(tax)\n\n    output:\n    tuple val(meta), file(\"metagenome.csv\")\n\n    script:\n    def software = getSoftwareName(task.process)\n    def max_mem = task.memory ? task.memory.getBytes() : \"None\"\n    \"\"\"\n    #!/usr/bin/env python\n\n    import datatable as dt\n    import pandas as pd\n\n    # Load taxonomy\n    ranks = [\"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\", \"Species\"]\n    otu_meta = dt.fread(\"$tax\", columns=dict(Size=None)).to_pandas()\n    otu_meta = otu_meta.set_index(\"OTU\").Taxonomy.str.rstrip(\";\").str.split(\";\", expand=True)\n\n    if len(otu_meta.columns) > len(ranks):\n        ranks = [\"Domain\"] + ranks\n    otu_meta.columns = ranks[:otu_meta.shape[1]]\n\n    # Load count data\n    abund = dt.fread(\"$shared\", columns=dict(label=None, numOtus=None), \n                     memory_limit=$max_mem)\n    abund = abund.to_pandas().melt(id_vars=\"Group\", var_name=\"OTU\", value_name=\"count\")\n\n    # binary columns are set to bool by fread --> convert to int\n    abund = abund.astype(dict(count=int))[abund[\"count\"]>0]\n\n    # compute relative abundance\n    abund[\"relabund\"] = abund.groupby(\"Group\")[\"count\"].transform(lambda x: x/sum(x) if any(x>0) else 0)\n    \n    # Merge\n    data = abund.merge(otu_meta.reset_index(), on=\"OTU\", how=\"left\")\n    \n    data.to_csv(\"metagenome.csv\", index=False)\n    \"\"\"\n}",
        "nb_lignes_process": 48,
        "string_script": "    def software = getSoftwareName(task.process)\n    def max_mem = task.memory ? task.memory.getBytes() : \"None\"\n    \"\"\"\n    #!/usr/bin/env python\n\n    import datatable as dt\n    import pandas as pd\n\n    # Load taxonomy\n    ranks = [\"Kingdom\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Genus\", \"Species\"]\n    otu_meta = dt.fread(\"$tax\", columns=dict(Size=None)).to_pandas()\n    otu_meta = otu_meta.set_index(\"OTU\").Taxonomy.str.rstrip(\";\").str.split(\";\", expand=True)\n\n    if len(otu_meta.columns) > len(ranks):\n        ranks = [\"Domain\"] + ranks\n    otu_meta.columns = ranks[:otu_meta.shape[1]]\n\n    # Load count data\n    abund = dt.fread(\"$shared\", columns=dict(label=None, numOtus=None), \n                     memory_limit=$max_mem)\n    abund = abund.to_pandas().melt(id_vars=\"Group\", var_name=\"OTU\", value_name=\"count\")\n\n    # binary columns are set to bool by fread --> convert to int\n    abund = abund.astype(dict(count=int))[abund[\"count\"]>0]\n\n    # compute relative abundance\n    abund[\"relabund\"] = abund.groupby(\"Group\")[\"count\"].transform(lambda x: x/sum(x) if any(x>0) else 0)\n    \n    # Merge\n    data = abund.merge(otu_meta.reset_index(), on=\"OTU\", how=\"left\")\n    \n    data.to_csv(\"metagenome.csv\", index=False)\n    \"\"\"",
        "nb_lignes_script": 32,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "shared",
            "tax"
        ],
        "nb_inputs": 3,
        "outputs": [
            "meta"
        ],
        "nb_outputs": 1,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_medium\"",
            "label \"plot\"",
            "container \"nakor/metaflowmics-python:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::datatable pandas'>=1'\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "VSEARCH_USEARCH_GLOBAL": {
        "name_process": "VSEARCH_USEARCH_GLOBAL",
        "string_process": "\nprocess VSEARCH_USEARCH_GLOBAL {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta) }\n\n    container \"quay.io/biocontainers/vsearch:2.17.0--h95f258a_1\"\n    conda (params.enable_conda ? \"bioconda::vsearch=2.17.0\" : null)\n\n    input:\n    tuple val(meta), path(fasta)\n\n    output:\n    tuple val(meta), path(\"matchlist-*.tsv\"), emit: tsv\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env bash\n\n    vsearch $options.args \\\\\n        --usearch_global $fasta \\\\\n        --threads $task.cpus \\\\\n        --id ${params.lulu_min_match/100} \\\\\n        --db $fasta --self \\\\\n        --iddef 1 \\\\\n        --userout matchlist-${meta.id}.tsv \\\\\n        -userfields query+target+id \\\\\n        --maxaccepts 0 \\\\\n        --query_cov .9 \\\\\n        --maxhits 10\n\n    echo \\$(vsearch --version 2>&1) | grep \"RAM\" | sed \"s/vsearch v//\" | sed \"s/, .*//\" > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env bash\n\n    vsearch $options.args \\\\\n        --usearch_global $fasta \\\\\n        --threads $task.cpus \\\\\n        --id ${params.lulu_min_match/100} \\\\\n        --db $fasta --self \\\\\n        --iddef 1 \\\\\n        --userout matchlist-${meta.id}.tsv \\\\\n        -userfields query+target+id \\\\\n        --maxaccepts 0 \\\\\n        --query_cov .9 \\\\\n        --maxhits 10\n\n    echo \\$(vsearch --version 2>&1) | grep \"RAM\" | sed \"s/vsearch v//\" | sed \"s/, .*//\" > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [
            "denvax",
            "VSEARCH"
        ],
        "tools_url": [
            "https://bio.tools/denvax",
            "https://bio.tools/vsearch"
        ],
        "tools_dico": [
            {
                "name": "denvax",
                "uri": "https://bio.tools/denvax",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3379",
                            "term": "Preclinical and clinical studies"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatric medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "https://en.wikipedia.org/wiki/Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0560",
                                    "term": "DNA vaccine design"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Serostatus testing and dengue vaccine cost-benefit thresholds | R package for manuscript \"Serostatus Testing & Dengue Vaccine Cost Benefit Thresholds\"",
                "homepage": "https://cran.r-project.org/web/packages/denvax/index.html"
            },
            {
                "name": "VSEARCH",
                "uri": "https://bio.tools/vsearch",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2520",
                                    "term": "DNA mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0450",
                                    "term": "Chimera detection"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0450",
                                    "term": "Chimeric sequence detection"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2977",
                                "term": "Nucleic acid sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_0865",
                                "term": "Sequence similarity score"
                            }
                        ]
                    }
                ],
                "description": "High-throughput search and clustering sequence analysis tool. It supports de novo and reference based chimera detection, clustering, full-length and prefix dereplication, reverse complementation, masking, all-vs-all pairwise global alignment, exact and global alignment searching, shuffling, subsampling and sorting. It also supports FASTQ file analysis, filtering and conversion.",
                "homepage": "https://github.com/torognes/vsearch"
            }
        ],
        "inputs": [
            "meta",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta) }",
            "container \"quay.io/biocontainers/vsearch:2.17.0--h95f258a_1\"",
            "conda (params.enable_conda ? \"bioconda::vsearch=2.17.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "HOLOVIEWS_BARS": {
        "name_process": "HOLOVIEWS_BARS",
        "string_process": "\nprocess HOLOVIEWS_BARS {\n    tag \"$meta.id\"\n    label \"process_low\"\n    label \"plot\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        meta:meta) }\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::datatable pandas'>=1' holoviews\" : null)\n\n    input:\n    tuple val(meta), file(mg)\n\n    output:\n    tuple val(meta), file(\"barplot*.html\")\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    import holoviews as hv\n\n    hv.extension(\"bokeh\")\n\n    data = pd.read_csv(\"${mg}\").drop(columns=[\"OTU\"])\n\n    n_samples = data.Group.nunique()\n    \n    legend_opts = dict(\n        label_text_font_size=\"12px\",\n        glyph_height=12,\n        spacing=0,\n        label_height=12\n     )\n\n    plot_opts = dict(\n        width=800, height=max(20*n_samples, 300),\n        stacked=True, tools=[\"hover\"], \n        invert_axes=True,\n        legend_position=\"right\",\n        legend_offset=(30, 0),\n        legend_opts=legend_opts\n    )\n\n    filler = f\"Others(<{${params.min_abund*100}:g}%)\"\n\n    def combine(group, filler=filler):\n        out = pd.Series(dict(\n            (col, vals.sum()) if col in {\"relabund\", \"count\"} \n            else (col, vals.iloc[0])\n            for (col, vals) in group.iteritems()\n        ))\n\n        if out[\"relabund\"] < $params.min_abund:\n            tax_cols = group.drop(columns=[\"Group\", \"relabund\", \"count\"]).columns\n            out[tax_cols] = filler\n\n        return out\n\n    for rank in [\"Phylum\", \"Class\"]:\n\n        tax_cols = data.loc[:, \"Kingdom\":rank].columns.tolist()\n\n        df = (data.loc[:, \"Group\":rank]\n            .groupby([\"Group\", rank]).apply(combine)\n            .reset_index(drop=True))\n\n        if rank == \"Class\" and \"Proteobacteria\" in set(data.Phylum):\n            df = df[df.Phylum == \"Proteobacteria\"]\n            df.relabund = df.groupby(\"Group\").relabund.transform(lambda x: x/sum(x))\n\n        sorted_taxa = df.groupby(rank).relabund.sum().sort_values().index[::-1]\n        sorted_groups = sorted(df.Group.unique())[::-1]\n\n        if filler in sorted_taxa:\n            sorted_taxa = sorted_taxa.drop(filler).append(pd.Index([filler]))\n\n        bars = (\n            hv.Bars(df.reset_index(), kdims=[\"Group\", rank],\n                    vdims=[\"relabund\", \"count\"] + tax_cols[:-1])\n            .redim.values(**{rank: sorted_taxa, \"Group\": sorted_groups})\n            .opts(**plot_opts)\n        )\n        hv.save(bars, f\"barplot-{rank}_${meta.id}.html\")\n\n    \"\"\"\n}",
        "nb_lignes_process": 90,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    import holoviews as hv\n\n    hv.extension(\"bokeh\")\n\n    data = pd.read_csv(\"${mg}\").drop(columns=[\"OTU\"])\n\n    n_samples = data.Group.nunique()\n    \n    legend_opts = dict(\n        label_text_font_size=\"12px\",\n        glyph_height=12,\n        spacing=0,\n        label_height=12\n     )\n\n    plot_opts = dict(\n        width=800, height=max(20*n_samples, 300),\n        stacked=True, tools=[\"hover\"], \n        invert_axes=True,\n        legend_position=\"right\",\n        legend_offset=(30, 0),\n        legend_opts=legend_opts\n    )\n\n    filler = f\"Others(<{${params.min_abund*100}:g}%)\"\n\n    def combine(group, filler=filler):\n        out = pd.Series(dict(\n            (col, vals.sum()) if col in {\"relabund\", \"count\"} \n            else (col, vals.iloc[0])\n            for (col, vals) in group.iteritems()\n        ))\n\n        if out[\"relabund\"] < $params.min_abund:\n            tax_cols = group.drop(columns=[\"Group\", \"relabund\", \"count\"]).columns\n            out[tax_cols] = filler\n\n        return out\n\n    for rank in [\"Phylum\", \"Class\"]:\n\n        tax_cols = data.loc[:, \"Kingdom\":rank].columns.tolist()\n\n        df = (data.loc[:, \"Group\":rank]\n            .groupby([\"Group\", rank]).apply(combine)\n            .reset_index(drop=True))\n\n        if rank == \"Class\" and \"Proteobacteria\" in set(data.Phylum):\n            df = df[df.Phylum == \"Proteobacteria\"]\n            df.relabund = df.groupby(\"Group\").relabund.transform(lambda x: x/sum(x))\n\n        sorted_taxa = df.groupby(rank).relabund.sum().sort_values().index[::-1]\n        sorted_groups = sorted(df.Group.unique())[::-1]\n\n        if filler in sorted_taxa:\n            sorted_taxa = sorted_taxa.drop(filler).append(pd.Index([filler]))\n\n        bars = (\n            hv.Bars(df.reset_index(), kdims=[\"Group\", rank],\n                    vdims=[\"relabund\", \"count\"] + tax_cols[:-1])\n            .redim.values(**{rank: sorted_taxa, \"Group\": sorted_groups})\n            .opts(**plot_opts)\n        )\n        hv.save(bars, f\"barplot-{rank}_${meta.id}.html\")\n\n    \"\"\"",
        "nb_lignes_script": 70,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "mg"
        ],
        "nb_inputs": 2,
        "outputs": [
            "meta"
        ],
        "nb_outputs": 1,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_low\"",
            "label \"plot\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, meta:meta) }",
            "container \"nakor/metaflowmics-python:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::datatable pandas'>=1' holoviews\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_SUMMARY_SINGLE": {
        "name_process": "MOTHUR_SUMMARY_SINGLE",
        "string_process": "\nprocess MOTHUR_SUMMARY_SINGLE {\n    tag \"$meta.id\"\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        pattern: \"*.summary\",\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), val(step), file(shared)\n\n    output:\n    tuple val(meta), path(\"*.{csv,summary}\"), emit: summary\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def ext = shared.getBaseName()\n    \"\"\"\n    mothur \"#summary.single(shared=$shared, calc=$params.calc)\"\n\n    # summary\n    if [ \"$params.calc\" = \"nseqs-sobs\" ]; then\n        cut -f2- *.groups.summary | tail -n+2 \\\\\n        | awk '{OFS=\",\"}{print \"$step\",\"$meta.otu_id\",\\$1,int(\\$2),int(\\$3)}' \\\\\n        | tr \"\\\\t\" \",\" \\\\\n        > ${ext}.csv\n        rm -f *.groups.summary\n    else\n        mv *.summary alpha-diversity.${meta.otu_id}.summary\n    fi\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 39,
        "string_script": "    def software = getSoftwareName(task.process)\n    def ext = shared.getBaseName()\n    \"\"\"\n    mothur \"#summary.single(shared=$shared, calc=$params.calc)\"\n\n    # summary\n    if [ \"$params.calc\" = \"nseqs-sobs\" ]; then\n        cut -f2- *.groups.summary | tail -n+2 \\\\\n        | awk '{OFS=\",\"}{print \"$step\",\"$meta.otu_id\",\\$1,int(\\$2),int(\\$3)}' \\\\\n        | tr \"\\\\t\" \",\" \\\\\n        > ${ext}.csv\n        rm -f *.groups.summary\n    else\n        mv *.summary alpha-diversity.${meta.otu_id}.summary\n    fi\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "step",
            "shared"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_low\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , pattern: \"*.summary\" , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_CONSENSUS": {
        "name_process": "MOTHUR_CONSENSUS",
        "string_process": "\nprocess MOTHUR_CONSENSUS {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), file(shared), file(list), file(fasta), file(count), file(tax)\n\n    output:\n    tuple val(meta), path(\"*.rep.fasta\"), emit: repfasta\n    tuple val(meta), path(\"*.rep.count_table\"), emit: repcount_table\n    tuple val(meta), path(\"*.cons.taxonomy\"), emit: constaxonomy\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    outprefix = \"OTUs.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    classify.otu(taxonomy=$tax, list=$list, count=$count, probs=f);\n    get.oturep(fasta=$fasta, list=current, count=current, method=abundance, rename=t)\"\n\n    mv *.rep.fasta ${outprefix}.rep.fasta\n    mv *.rep.count_table ${outprefix}.rep.count_table\n    mv *.cons.taxonomy ${outprefix}.cons.taxonomy\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 35,
        "string_script": "    def software = getSoftwareName(task.process)\n    outprefix = \"OTUs.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    classify.otu(taxonomy=$tax, list=$list, count=$count, probs=f);\n    get.oturep(fasta=$fasta, list=current, count=current, method=abundance, rename=t)\"\n\n    mv *.rep.fasta ${outprefix}.rep.fasta\n    mv *.rep.count_table ${outprefix}.rep.count_table\n    mv *.cons.taxonomy ${outprefix}.cons.taxonomy\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "shared",
            "list",
            "fasta",
            "count",
            "tax"
        ],
        "nb_inputs": 6,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "CLEAN_TAXONOMY": {
        "name_process": "CLEAN_TAXONOMY",
        "string_process": "\nprocess CLEAN_TAXONOMY {\n    label \"process_low\"\n    publishDir \"${params.outdir}/clean_taxonomy\", mode: params.publish_dir_mode\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::biopython\" : null)\n\n    input:\n    path fasta\n\n    output:\n    path \"*.with_tax.fasta\"\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n\timport re\n\timport pandas as pd\n\tfrom Bio.SeqIO.FastaIO import SimpleFastaParser\n\n\tRANKS = [\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\"]\n\n\tdef format_taxon_name(taxon):\n\t    taxon_clean = re.sub(\"[^A-Za-z0-9_-]\", \"_\", taxon.strip())\n\t    taxon_clean = re.sub(\"__+\", \"_\", taxon_clean)\n\t    return taxon_clean\n\n\tdef fill_missing_ranks(lineage):\n\t\tmissing = [i for i,v in enumerate(lineage) if not v][::-1]\n\t\twhile missing:\n\t\t\tidx = missing.pop()\n\t\t\tprev = lineage[idx-1] # assume the highest level is always known\n\t\t\tprev_rank = RANKS[idx-1][0]\n\t\t\tparts = prev.split(\"__\")\n\n\t\t\tnew_name = f\"{prev_rank}__{parts[-1]}\"\n\t\t\tif len(parts) > 1:\n\t\t\t\tprefix = \"__\".join(parts[:-1])\n\t\t\t\tnew_name = f\"{prefix}__{new_name}\"\n\t\t\tlineage[idx] = new_name\n\t\treturn lineage\n\n\tcriteria = {}\n\tuniq_seq = {}\n\n\twith open(\"$fasta\", \"r\") as reader:\n\t    for i, (title, seq) in enumerate(SimpleFastaParser(reader)):\n\t        info = title.strip().split()\n\t        lineage = \" \".join(info[1:])\n\t        lineage = lineage.split(\"$params.sep\")\n\t        lineage = [format_taxon_name(taxon) for taxon in lineage]\n\n\t        n_missing = sum(1 for x in lineage if not x)\n\t        # check if duplicated\n\t        if seq in uniq_seq and n_missing >= criteria[seq]:\n\t            continue\n\t        criteria[seq] = n_missing\n\n\t        lineage = \"$params.sep\".join(\n              fill_missing_ranks(lineage)\n            )\n\t        uniq_seq[seq] = f'{info[0]} {lineage}'\n\n\twith open(\"${fasta.getBaseName()}.with_tax.fasta\", \"w\") as writer:\n\t    for (seq, header) in uniq_seq.items():\n\t        writer.write(f'>{header}\\\\n{seq}\\\\n')\n    \"\"\"\n}",
        "nb_lignes_process": 68,
        "string_script": "    \"\"\"\n    #!/usr/bin/env python\n\n\timport re\n\timport pandas as pd\n\tfrom Bio.SeqIO.FastaIO import SimpleFastaParser\n\n\tRANKS = [\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\"]\n\n\tdef format_taxon_name(taxon):\n\t    taxon_clean = re.sub(\"[^A-Za-z0-9_-]\", \"_\", taxon.strip())\n\t    taxon_clean = re.sub(\"__+\", \"_\", taxon_clean)\n\t    return taxon_clean\n\n\tdef fill_missing_ranks(lineage):\n\t\tmissing = [i for i,v in enumerate(lineage) if not v][::-1]\n\t\twhile missing:\n\t\t\tidx = missing.pop()\n\t\t\tprev = lineage[idx-1] # assume the highest level is always known\n\t\t\tprev_rank = RANKS[idx-1][0]\n\t\t\tparts = prev.split(\"__\")\n\n\t\t\tnew_name = f\"{prev_rank}__{parts[-1]}\"\n\t\t\tif len(parts) > 1:\n\t\t\t\tprefix = \"__\".join(parts[:-1])\n\t\t\t\tnew_name = f\"{prefix}__{new_name}\"\n\t\t\tlineage[idx] = new_name\n\t\treturn lineage\n\n\tcriteria = {}\n\tuniq_seq = {}\n\n\twith open(\"$fasta\", \"r\") as reader:\n\t    for i, (title, seq) in enumerate(SimpleFastaParser(reader)):\n\t        info = title.strip().split()\n\t        lineage = \" \".join(info[1:])\n\t        lineage = lineage.split(\"$params.sep\")\n\t        lineage = [format_taxon_name(taxon) for taxon in lineage]\n\n\t        n_missing = sum(1 for x in lineage if not x)\n\t        # check if duplicated\n\t        if seq in uniq_seq and n_missing >= criteria[seq]:\n\t            continue\n\t        criteria[seq] = n_missing\n\n\t        lineage = \"$params.sep\".join(\n              fill_missing_ranks(lineage)\n            )\n\t        uniq_seq[seq] = f'{info[0]} {lineage}'\n\n\twith open(\"${fasta.getBaseName()}.with_tax.fasta\", \"w\") as writer:\n\t    for (seq, header) in uniq_seq.items():\n\t        writer.write(f'>{header}\\\\n{seq}\\\\n')\n    \"\"\"",
        "nb_lignes_script": 53,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fasta"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_low\"",
            "publishDir \"${params.outdir}/clean_taxonomy\", mode: params.publish_dir_mode",
            "container \"nakor/metaflowmics-python:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::biopython\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_SUMMARY_SHARED": {
        "name_process": "MOTHUR_SUMMARY_SHARED",
        "string_process": "\nprocess MOTHUR_SUMMARY_SHARED {\n    tag \"$meta.id\"\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), file(shared)\n\n    output:\n    path \"*.summary\", emit: summary\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def ext = shared.getBaseName()\n    \"\"\"\n    mothur \"#summary.shared(shared=$shared, calc=${params.calc}, distance=lt)\"\n    mv *.summary beta-diversity.${meta.otu_id}.summary\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "    def software = getSoftwareName(task.process)\n    def ext = shared.getBaseName()\n    \"\"\"\n    mothur \"#summary.shared(shared=$shared, calc=${params.calc}, distance=lt)\"\n    mv *.summary beta-diversity.${meta.otu_id}.summary\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "shared"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_low\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "EMBOSS_TRANALIGN": {
        "name_process": "EMBOSS_TRANALIGN",
        "string_process": "\nprocess EMBOSS_TRANALIGN {\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"nakor/metaflowmics-biotools:0.0.1\"\n    conda (params.enable_conda ? \"bioconda::emboss=6.6.0\" : null)\n\n    input:\n    tuple val(meta), path(afa), path(fna)\n\n    output:\n    tuple val(meta), path(\"*.codons.afa\"), emit: fna\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix = fna.getSimpleName()\n    \"\"\"\n    #!/usr/bin/env bash\n\n    tranalign \\\\\n      -asequence $fna \\\\\n      -bsequence $afa \\\\\n      -outseq stdout \\\\\n      -table 5 |\n        seqtk seq -l0 > rev.afa\n\n    [ -s rev.afa ] && echo \"tranalign was successfull\" || (echo \"Something went wrong. Are sequences in the same order?\" && exit 1)\n\n    aln_len=\\$(grep -v '>' rev.afa | wc -L)\n\n    awk -v L=\\$aln_len '{if (NR % 2 == 0 && length(\\$1)<1707) \\$1=\\$1\"-\"; print}' rev.afa \\\\\n        > ${prefix}.codons.afa\n\n    tranalign -h 2>&1 | grep -i version | cut -d':' -f3 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 39,
        "string_script": "    def software = getSoftwareName(task.process)\n    def prefix = fna.getSimpleName()\n    \"\"\"\n    #!/usr/bin/env bash\n\n    tranalign \\\\\n      -asequence $fna \\\\\n      -bsequence $afa \\\\\n      -outseq stdout \\\\\n      -table 5 |\n        seqtk seq -l0 > rev.afa\n\n    [ -s rev.afa ] && echo \"tranalign was successfull\" || (echo \"Something went wrong. Are sequences in the same order?\" && exit 1)\n\n    aln_len=\\$(grep -v '>' rev.afa | wc -L)\n\n    awk -v L=\\$aln_len '{if (NR % 2 == 0 && length(\\$1)<1707) \\$1=\\$1\"-\"; print}' rev.afa \\\\\n        > ${prefix}.codons.afa\n\n    tranalign -h 2>&1 | grep -i version | cut -d':' -f3 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 20,
        "language_script": "bash",
        "tools": [
            "denvax",
            "tranalign",
            "seqtk"
        ],
        "tools_url": [
            "https://bio.tools/denvax",
            "https://bio.tools/tranalign",
            "https://bio.tools/seqtk"
        ],
        "tools_dico": [
            {
                "name": "denvax",
                "uri": "https://bio.tools/denvax",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3379",
                            "term": "Preclinical and clinical studies"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatric medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "https://en.wikipedia.org/wiki/Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0560",
                                    "term": "DNA vaccine design"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Serostatus testing and dengue vaccine cost-benefit thresholds | R package for manuscript \"Serostatus Testing & Dengue Vaccine Cost Benefit Thresholds\"",
                "homepage": "https://cran.r-project.org/web/packages/denvax/index.html"
            },
            {
                "name": "tranalign",
                "uri": "https://bio.tools/tranalign",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0097",
                            "term": "Nucleic acid structure analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0097",
                            "term": "Nucleic acid structure"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0371",
                                    "term": "DNA translation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0260",
                                    "term": "Sequence alignment conversion"
                                }
                            ],
                            []
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_1384",
                                "term": "Protein sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_0849",
                                "term": "Sequence record"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2887",
                                "term": "Nucleic acid sequence record"
                            }
                        ]
                    }
                ],
                "description": "Generate an alignment of nucleic coding regions from aligned proteins.",
                "homepage": "http://emboss.open-bio.org/rel/rel6/apps/tranalign.html"
            },
            {
                "name": "seqtk",
                "uri": "https://bio.tools/seqtk",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Biological databases"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Data management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Information systems"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Databases and information systems"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Data handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2121",
                                    "term": "Sequence file editing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Utility operation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File processing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Report handling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A tool for processing sequences in the FASTA or FASTQ format. It parses both FASTA and FASTQ files which can also be optionally compressed by gzip.",
                "homepage": "https://github.com/lh3/seqtk"
            }
        ],
        "inputs": [
            "meta",
            "afa",
            "fna"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_medium\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"nakor/metaflowmics-biotools:0.0.1\"",
            "conda (params.enable_conda ? \"bioconda::emboss=6.6.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "BUILD_ASV_TABLE": {
        "name_process": "BUILD_ASV_TABLE",
        "string_process": "\nprocess BUILD_ASV_TABLE {\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        meta:meta) }\n\n    container \"nakor/metaflowmics-r:0.0.1\"    \n    conda (params.enable_conda ? \"conda-forge::r-stringr bioconda::bioconductor-dada2=1.18 conda-forge::r-seqinr\" : null)\n\n    input:\n    path(rds)\n\n    output:\n    path \"ASVs_duplicates_to_cluster.fasta\", optional: true, emit: fasta_dup\n    path \"ASVs-100.fasta\", emit: fasta    \n    path \"ASVs-100.{count_table,tsv}\", emit: count_table\n                                          \n\n    script:\n                                                   \n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(dada2)\n    library(seqinr)\n\n    # Collect denoised reads\n    denoised <- list.files(path=\".\", pattern=\"*.denoised.RDS\")\n\n    sample_names <- unname(sapply(denoised, function(x) gsub(\".denoised.RDS\", \"\", x)))\n    merged <- lapply(denoised, function (x) readRDS(x))\n    names(merged) <- sample_names\n    \n    # Retrieve merged object\n    asv_table <- makeSequenceTable(merged)\n    asv_ids <- sprintf(\"asv_%s\", 1:dim(asv_table)[2])\n\n    # Write ASV sequences\n    uniquesToFasta(asv_table, \"ASVs-100.fasta\", ids=asv_ids)\n\n    # Format count table\n    count_table <- cbind(\n        asv_ids, \n        colSums(asv_table),\n        t(asv_table[rowSums(asv_table)>0,])\n    )\n    colnames(count_table) <- c(\"Representative_Sequence\", \"total\", sample_names)\n\n    if (\"${params.format.toLowerCase()}\" == \"mothur\") {\n        # Write abundances\n        write.table(count_table, file=\"ASVs-100.count_table\", quote=F, sep=\"\\\\t\",\n                    row.names=F, col.names=T)\n    } else {\n        # Write abundances\n        write.table(count_table[, -c(2)],\"ASVs-100.tsv\", quote=F, row.names=F, sep=\"\\\\t\")\n\n        # Write duplicated fasta sequences with header formatted for VSEARCH\n        list.fasta <- list()\n        i = 1\n        for(seq in colnames(asv_table)) {\n            for(sample in rownames(asv_table)) {\n                abd = asv_table[sample, seq]\n                if(abd > 0) {\n                    seq_id = sprintf(\"ASV_%s;sample=%s;size=%s\", i, sample, abd)\n                    list.fasta[seq_id] = seq\n                }\n            }\n            i <- i+1\n        }\n        write.fasta(list.fasta, names=names(list.fasta), \n                    file.out='ASVs_duplicates_to_cluster.fasta')\n    }\n    \"\"\"\n}",
        "nb_lignes_process": 73,
        "string_script": "    \"\"\"\n    #!/usr/bin/env Rscript\n    library(dada2)\n    library(seqinr)\n\n    # Collect denoised reads\n    denoised <- list.files(path=\".\", pattern=\"*.denoised.RDS\")\n\n    sample_names <- unname(sapply(denoised, function(x) gsub(\".denoised.RDS\", \"\", x)))\n    merged <- lapply(denoised, function (x) readRDS(x))\n    names(merged) <- sample_names\n    \n    # Retrieve merged object\n    asv_table <- makeSequenceTable(merged)\n    asv_ids <- sprintf(\"asv_%s\", 1:dim(asv_table)[2])\n\n    # Write ASV sequences\n    uniquesToFasta(asv_table, \"ASVs-100.fasta\", ids=asv_ids)\n\n    # Format count table\n    count_table <- cbind(\n        asv_ids, \n        colSums(asv_table),\n        t(asv_table[rowSums(asv_table)>0,])\n    )\n    colnames(count_table) <- c(\"Representative_Sequence\", \"total\", sample_names)\n\n    if (\"${params.format.toLowerCase()}\" == \"mothur\") {\n        # Write abundances\n        write.table(count_table, file=\"ASVs-100.count_table\", quote=F, sep=\"\\\\t\",\n                    row.names=F, col.names=T)\n    } else {\n        # Write abundances\n        write.table(count_table[, -c(2)],\"ASVs-100.tsv\", quote=F, row.names=F, sep=\"\\\\t\")\n\n        # Write duplicated fasta sequences with header formatted for VSEARCH\n        list.fasta <- list()\n        i = 1\n        for(seq in colnames(asv_table)) {\n            for(sample in rownames(asv_table)) {\n                abd = asv_table[sample, seq]\n                if(abd > 0) {\n                    seq_id = sprintf(\"ASV_%s;sample=%s;size=%s\", i, sample, abd)\n                    list.fasta[seq_id] = seq\n                }\n            }\n            i <- i+1\n        }\n        write.fasta(list.fasta, names=names(list.fasta), \n                    file.out='ASVs_duplicates_to_cluster.fasta')\n    }\n    \"\"\"",
        "nb_lignes_script": 51,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "rds"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_medium\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, meta:meta) }",
            "container \"nakor/metaflowmics-r:0.0.1\"",
            "conda (params.enable_conda ? \"conda-forge::r-stringr bioconda::bioconductor-dada2=1.18 conda-forge::r-seqinr\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "BACKTRANSLATE": {
        "name_process": "BACKTRANSLATE",
        "string_process": "\nprocess BACKTRANSLATE {\n    tag \"$meta.id\"\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::bipython conda-forge::pandas\" : null)\n\n    input:\n    tuple val(meta), path(afa), path(fna)\n\n    output:\n    tuple val(meta), path(\"*.codons.afa\"), emit: fna\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    from Bio import SeqIO, AlignIO\n    from Bio.SeqRecord import SeqRecord\n    from Bio.Seq import Seq\n    from Bio.Data import CodonTable\n    from Bio.Align import MultipleSeqAlignment\n    from Bio.codonalign import build\n\n    msa = {aln.id: aln for aln in AlignIO.read(\"$afa\", \"fasta\")}\n    msa_clean = []\n\n    dna_all = []\n    for dna in SeqIO.parse(\"$fna\", \"fasta\"):\n\n        if dna.id not in msa:\n            continue\n    \n        prot = msa[dna.id]\n        dna.description = prot.description\n\n        frame = int(dna.description.split()[-1])\n        offset = (frame-1) % 3\n        aa_len = (len(dna)-offset) // 3\n        dna.seq = dna.seq[offset:3*aa_len+offset]\n\n        if frame > 3:\n           dna.seq = dna.seq.reverse_complement()\n\n        prot_seq = prot.seq.ungap('-').ungap('.').upper()\n        \n        if len(prot_seq) > len(dna):\n            # a partial codon was translated\n            prot_seq = prot_seq.tomutable()\n            prot_seq.pop()\n            prot_seq = prot_seq.toseq()\n            \n        prot.seq = prot_seq\n        msa_clean.append(prot)\n        dna_all.append(dna)\n\n    msa_clean = MultipleSeqAlignment(msa_clean)\n    codon_aln = build(msa_clean, dna_all, codon_table=CodonTable.generic_by_id[5])\n\n    with open(\"${meta.id}.codons.afa\", \"w\") as writer:\n        for seq in codon_aln:\n            ref = msa[seq.id]\n            codons = seq.seq\n            seq_str = ''.join(\n                '...' if ref.seq[i] == '.'\n                else codons.get_codon(i).lower() if ref.seq[i].islower()\n                else codons.get_codon(i)\n                for i in range(len(ref))\n            )\n            writer.write(f\">{ref.description}\\\\n{seq_str}\\\\n\")\n    \"\"\"\n}",
        "nb_lignes_process": 75,
        "string_script": "    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    from Bio import SeqIO, AlignIO\n    from Bio.SeqRecord import SeqRecord\n    from Bio.Seq import Seq\n    from Bio.Data import CodonTable\n    from Bio.Align import MultipleSeqAlignment\n    from Bio.codonalign import build\n\n    msa = {aln.id: aln for aln in AlignIO.read(\"$afa\", \"fasta\")}\n    msa_clean = []\n\n    dna_all = []\n    for dna in SeqIO.parse(\"$fna\", \"fasta\"):\n\n        if dna.id not in msa:\n            continue\n    \n        prot = msa[dna.id]\n        dna.description = prot.description\n\n        frame = int(dna.description.split()[-1])\n        offset = (frame-1) % 3\n        aa_len = (len(dna)-offset) // 3\n        dna.seq = dna.seq[offset:3*aa_len+offset]\n\n        if frame > 3:\n           dna.seq = dna.seq.reverse_complement()\n\n        prot_seq = prot.seq.ungap('-').ungap('.').upper()\n        \n        if len(prot_seq) > len(dna):\n            # a partial codon was translated\n            prot_seq = prot_seq.tomutable()\n            prot_seq.pop()\n            prot_seq = prot_seq.toseq()\n            \n        prot.seq = prot_seq\n        msa_clean.append(prot)\n        dna_all.append(dna)\n\n    msa_clean = MultipleSeqAlignment(msa_clean)\n    codon_aln = build(msa_clean, dna_all, codon_table=CodonTable.generic_by_id[5])\n\n    with open(\"${meta.id}.codons.afa\", \"w\") as writer:\n        for seq in codon_aln:\n            ref = msa[seq.id]\n            codons = seq.seq\n            seq_str = ''.join(\n                '...' if ref.seq[i] == '.'\n                else codons.get_codon(i).lower() if ref.seq[i].islower()\n                else codons.get_codon(i)\n                for i in range(len(ref))\n            )\n            writer.write(f\">{ref.description}\\\\n{seq_str}\\\\n\")\n    \"\"\"",
        "nb_lignes_script": 57,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "afa",
            "fna"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_low\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"nakor/metaflowmics-python:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::bipython conda-forge::pandas\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "DADA2_ASSIGN_TAXONOMY": {
        "name_process": "DADA2_ASSIGN_TAXONOMY",
        "string_process": "\nprocess DADA2_ASSIGN_TAXONOMY {\n    tag \"${meta.id}\"\n    label \"process_high\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:[\"id\"]) }\n\n    container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2\" : null)\n\n    input:\n    tuple val(meta), path(fasta)\n    path db\n\n    output:\n    tuple val(meta), path(\"taxonomy*.csv\"), emit: taxonomy\n    tuple val(meta), path(\"confidence*.csv\"), emit: confidence\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(dada2)\n    library(ShortRead)\n\n    sr <- readFasta(\"$fasta\")\n    seq_ids <- as.character(id(sr))\n    seq <- as.character(sread(sr))\n    assignments <- assignTaxonomy(\n        setNames(seq, seq_ids), \"$db\",\n        minBoot=$params.tax_confidence,\n        tryRC=TRUE,\n        outputBootstraps=TRUE\n    )\n    taxa = cbind(seq_ids, assignments\\$tax)\n    write.table(taxa, \"taxonomy_${meta.id}.csv\", quote=F, row.names=F, sep=\",\")\n\n    scores <- cbind(seq_ids, assignments\\$boot)\n    write.table(scores, \"confidence_${meta.id}.csv\", quote=F, row.names=F, sep=\",\")\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"\n}",
        "nb_lignes_process": 46,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(dada2)\n    library(ShortRead)\n\n    sr <- readFasta(\"$fasta\")\n    seq_ids <- as.character(id(sr))\n    seq <- as.character(sread(sr))\n    assignments <- assignTaxonomy(\n        setNames(seq, seq_ids), \"$db\",\n        minBoot=$params.tax_confidence,\n        tryRC=TRUE,\n        outputBootstraps=TRUE\n    )\n    taxa = cbind(seq_ids, assignments\\$tax)\n    write.table(taxa, \"taxonomy_${meta.id}.csv\", quote=F, row.names=F, sep=\",\")\n\n    scores <- cbind(seq_ids, assignments\\$boot)\n    write.table(scores, \"confidence_${meta.id}.csv\", quote=F, row.names=F, sep=\",\")\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "fasta",
            "db"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"${meta.id}\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:[\"id\"]) }",
            "container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"",
            "conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "HOLOVIEWS_CLUSTERMAP": {
        "name_process": "HOLOVIEWS_CLUSTERMAP",
        "string_process": "\nprocess HOLOVIEWS_CLUSTERMAP {\n    tag \"$meta.id\"\n    label \"process_low\"\n    label \"plot\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        meta:meta) }\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::datatable pandas'>=1' scipy holoviews\" : null)\n\n    input:\n    tuple val(meta), file(mg)\n\n    output:\n    tuple val(meta), file(\"clustermap*.html\")\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    from scipy.cluster.hierarchy import linkage, leaves_list\n    import holoviews as hv\n\n    hv.extension(\"bokeh\")\n\n    data = pd.read_csv(\"${mg}\")\n    \n    # Filter OTUs\n    scores = data.groupby(\"OTU\").relabund.agg(max_abd=max, sd=\"std\")\n    otus = scores.index[(scores.max_abd > $params.min_abund) & (scores.sd > 0)]\n    data = data[data.OTU.isin(otus)].set_index([\"Group\", \"OTU\"])\n\n    # Compute z_scores\n    zscores = data.relabund.unstack(fill_value=0)\n    zscores = (zscores - zscores.mean()) / zscores.std()\n\n    # Cluster rows and columns\n    group_order = sorted(zscores.index)\n    feature_order = sorted(zscores.columns)\n\n    try:\n        sample_links = leaves_list(linkage(\n            zscores, method=\"average\", metric=\"braycurtis\"\n        ))\n        group_order = zscores.index[sample_links]\n    except ValueError:\n        print(\"Something went wrong with the sample clustering\")\n\n    try:\n        feature_links = leaves_list(linkage(\n            zscores.T, method=\"average\", metric=\"braycurtis\"\n        ))\n        feature_order = zscores.columns[feature_links]\n    except ValueError:\n        print(\"Something went wrong with the OTU clustering\")\n    \n    # Reformat data\n    data = data.merge(\n        zscores.stack().rename(\"z_score\"), \n        left_index=True, right_index=True, how=\"outer\"\n    ).reset_index()\n\n    # Plot\n    hm_opt = dict(\n        tools=[\"hover\"],\n        height=max(10*data.Group.nunique(), 300),\n        width=max(10*data.OTU.nunique(), 300),\n        xrotation=90\n    )\n\n    kdims = [\"OTU\", \"Group\"]\n    vdims = [\"z_score\"] + data.columns.drop([\"z_score\"]+kdims).tolist()\n\n    hm = hv.HeatMap(\n        data=data, kdims=kdims, vdims=vdims\n    ).opts(**hm_opt).redim.values(Group=group_order, OTU=feature_order)\n\n    hv.save(hm, f\"clustermap_${meta.id}.html\")\n    \"\"\"\n}",
        "nb_lignes_process": 83,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env python\n\n    import pandas as pd\n    from scipy.cluster.hierarchy import linkage, leaves_list\n    import holoviews as hv\n\n    hv.extension(\"bokeh\")\n\n    data = pd.read_csv(\"${mg}\")\n    \n    # Filter OTUs\n    scores = data.groupby(\"OTU\").relabund.agg(max_abd=max, sd=\"std\")\n    otus = scores.index[(scores.max_abd > $params.min_abund) & (scores.sd > 0)]\n    data = data[data.OTU.isin(otus)].set_index([\"Group\", \"OTU\"])\n\n    # Compute z_scores\n    zscores = data.relabund.unstack(fill_value=0)\n    zscores = (zscores - zscores.mean()) / zscores.std()\n\n    # Cluster rows and columns\n    group_order = sorted(zscores.index)\n    feature_order = sorted(zscores.columns)\n\n    try:\n        sample_links = leaves_list(linkage(\n            zscores, method=\"average\", metric=\"braycurtis\"\n        ))\n        group_order = zscores.index[sample_links]\n    except ValueError:\n        print(\"Something went wrong with the sample clustering\")\n\n    try:\n        feature_links = leaves_list(linkage(\n            zscores.T, method=\"average\", metric=\"braycurtis\"\n        ))\n        feature_order = zscores.columns[feature_links]\n    except ValueError:\n        print(\"Something went wrong with the OTU clustering\")\n    \n    # Reformat data\n    data = data.merge(\n        zscores.stack().rename(\"z_score\"), \n        left_index=True, right_index=True, how=\"outer\"\n    ).reset_index()\n\n    # Plot\n    hm_opt = dict(\n        tools=[\"hover\"],\n        height=max(10*data.Group.nunique(), 300),\n        width=max(10*data.OTU.nunique(), 300),\n        xrotation=90\n    )\n\n    kdims = [\"OTU\", \"Group\"]\n    vdims = [\"z_score\"] + data.columns.drop([\"z_score\"]+kdims).tolist()\n\n    hm = hv.HeatMap(\n        data=data, kdims=kdims, vdims=vdims\n    ).opts(**hm_opt).redim.values(Group=group_order, OTU=feature_order)\n\n    hv.save(hm, f\"clustermap_${meta.id}.html\")\n    \"\"\"",
        "nb_lignes_script": 63,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "mg"
        ],
        "nb_inputs": 2,
        "outputs": [
            "meta"
        ],
        "nb_outputs": 1,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_low\"",
            "label \"plot\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, meta:meta) }",
            "container \"nakor/metaflowmics-python:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::datatable pandas'>=1' scipy holoviews\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "RDP_CLASSIFY": {
        "name_process": "RDP_CLASSIFY",
        "string_process": "\nprocess RDP_CLASSIFY {\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    conda (params.enable_conda ? \"bioconda::rdptools=2.0.3\" : null)\n    container \"quay.io/biocontainers/rdptools:2.0.3--hdfd78af_1\"\n\n    input:\n    tuple val(meta), path(fasta)\n    path rdp_db_files\n\n    output:\n    tuple val(meta), path(\"*.tsv\"), emit: tsv\n    tuple val(meta), path(\"*.taxonomy\"), emit: taxonomy\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix   = fasta.getBaseName()\n    \"\"\"\n    classifier -Xmx${task.memory.getGiga()}g classify -t rRNAClassifier.properties -o ${prefix}.tsv $fasta -c $params.rdp_confidence\n\tawk -F'\\\\t' '{printf \\$1\"\"FS; for(i=6;i<=NF;i=i+3) printf \\$i\";\"; print \"\"}' ${prefix}.tsv > ${prefix}.taxonomy\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "    def software = getSoftwareName(task.process)\n    def prefix   = fasta.getBaseName()\n    \"\"\"\n    classifier -Xmx${task.memory.getGiga()}g classify -t rRNAClassifier.properties -o ${prefix}.tsv $fasta -c $params.rdp_confidence\n\tawk -F'\\\\t' '{printf \\$1\"\"FS; for(i=6;i<=NF;i=i+3) printf \\$i\";\"; print \"\"}' ${prefix}.tsv > ${prefix}.taxonomy\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "16S classifier"
        ],
        "tools_url": [
            "https://bio.tools/16s_classifier"
        ],
        "tools_dico": [
            {
                "name": "16S classifier",
                "uri": "https://bio.tools/16s_classifier",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3474",
                            "term": "Machine learning"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0659",
                            "term": "Functional, regulatory and non-coding RNA"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Random Forest based tool which is developed to carry out fast, efficient and accurate taxonomic classification of 16S rRNA sequences. It has the unique ability to classify small Hypervariable Regions of 16S rRNA.",
                "homepage": "http://metabiosys.iiserb.ac.in/16Sclassifier/application.php"
            }
        ],
        "inputs": [
            "meta",
            "fasta",
            "rdp_db_files"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "conda (params.enable_conda ? \"bioconda::rdptools=2.0.3\" : null)",
            "container \"quay.io/biocontainers/rdptools:2.0.3--hdfd78af_1\""
        ],
        "when": "",
        "stub": ""
    },
    "MUSCLE": {
        "name_process": "MUSCLE",
        "string_process": "\nprocess MUSCLE {\n    tag \"${fasta.getBaseName()}\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"nakor/muscle:5.0.1278\"\n                                                                    \n                                                                  \n\n    input:\n    path fasta\n\n    output:\n    path \"*.afa\", emit: afa\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def arg = fasta.getExtension() == \"afa\" ? \"-profile\" : \"\"\n    def prefix = fasta.getBaseName()\n    \"\"\"\n    #!/usr/bin/env bash\n\n    n=\\$(grep -c \"^>\" $fasta)\n\n    if [ \"$arg\" == \"\" ] && [ \\$n -ge $params.max_seq ] ; then\n        opt=\"-maxiters 1 -diags -sv -distance1 kbit20_3\"\n    else\n        opt=\"\"\n    fi\n\n    # Run muscle and convert to 2-line fasta\n    cat $fasta | muscle $arg \\$opt -in - \\\\\n        | awk '/^>/ {printf(\"\\\\n%s\\\\n\",\\$0);next; } { printf(\"%s\",\\$0);}  END {printf(\"\\\\n\");}' \\\\\n        | tail -n+2 > ${prefix}.afa\n\n    echo \\$(muscle -version 2>&1) | cut -d\" \" -f2  > ${software}.version.txt\n\n    [ -s ${prefix}.afa ] && echo \"muscle was successfull\" || (echo \"Something went wrong. Did muscle segfault?\" && exit 1)\n    \"\"\"\n}",
        "nb_lignes_process": 43,
        "string_script": "    def software = getSoftwareName(task.process)\n    def arg = fasta.getExtension() == \"afa\" ? \"-profile\" : \"\"\n    def prefix = fasta.getBaseName()\n    \"\"\"\n    #!/usr/bin/env bash\n\n    n=\\$(grep -c \"^>\" $fasta)\n\n    if [ \"$arg\" == \"\" ] && [ \\$n -ge $params.max_seq ] ; then\n        opt=\"-maxiters 1 -diags -sv -distance1 kbit20_3\"\n    else\n        opt=\"\"\n    fi\n\n    # Run muscle and convert to 2-line fasta\n    cat $fasta | muscle $arg \\$opt -in - \\\\\n        | awk '/^>/ {printf(\"\\\\n%s\\\\n\",\\$0);next; } { printf(\"%s\",\\$0);}  END {printf(\"\\\\n\");}' \\\\\n        | tail -n+2 > ${prefix}.afa\n\n    echo \\$(muscle -version 2>&1) | cut -d\" \" -f2  > ${software}.version.txt\n\n    [ -s ${prefix}.afa ] && echo \"muscle was successfull\" || (echo \"Something went wrong. Did muscle segfault?\" && exit 1)\n    \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [
            "denvax",
            "MUSCLE",
            "NextSV"
        ],
        "tools_url": [
            "https://bio.tools/denvax",
            "https://bio.tools/muscle",
            "https://bio.tools/nextsv"
        ],
        "tools_dico": [
            {
                "name": "denvax",
                "uri": "https://bio.tools/denvax",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3379",
                            "term": "Preclinical and clinical studies"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatric medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "https://en.wikipedia.org/wiki/Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0560",
                                    "term": "DNA vaccine design"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Serostatus testing and dengue vaccine cost-benefit thresholds | R package for manuscript \"Serostatus Testing & Dengue Vaccine Cost Benefit Thresholds\"",
                "homepage": "https://cran.r-project.org/web/packages/denvax/index.html"
            },
            {
                "name": "MUSCLE",
                "uri": "https://bio.tools/muscle",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0492",
                                    "term": "Multiple sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0492",
                                    "term": "Multiple alignment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3495",
                                "term": "RNA sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            }
                        ]
                    }
                ],
                "description": "This tool performs multiple sequence alignments of nucleotide or amino acid sequences.",
                "homepage": "https://www.drive5.com/muscle/"
            },
            {
                "name": "NextSV",
                "uri": "https://bio.tools/nextsv",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "Structural variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "Genomic structural variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "DNA structural variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3228",
                                    "term": "Structural variation detection"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3228",
                                    "term": "Structural variation discovery"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A meta SV caller and a computational pipeline to perform SV calling from low coverage long-read sequencing data. It integrates three aligners and three SV callers and generates two integrated call sets (sensitive/stringent) for different analysis purpose.",
                "homepage": "http://github.com/Nextomics/NextSV"
            }
        ],
        "inputs": [
            "fasta"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"${fasta.getBaseName()}\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"nakor/muscle:5.0.1278\""
        ],
        "when": "",
        "stub": ""
    },
    "DADA2_FILTERANDTRIM": {
        "name_process": "DADA2_FILTERANDTRIM",
        "string_process": "\nprocess DADA2_FILTERANDTRIM {\n    tag \"$meta.id\"\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:[\"id\"]) }\n\n    container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2\" : null)\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.fastq.gz\"), emit: fastq, optional: true\n    path(\"*.csv\"), emit: summary\n    path \"*.png\", emit: png, optional: true\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def rmphix = params.keep_phix ? \"TRUE\" : \"FALSE\"\n    def n = meta.paired_end ? 2 : 1\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    library(ggplot2)\n\n    filt_params <- data.frame(\n        minLen=c($params.min_read_len),\n        truncLen=c($params.trunc_len),\n        truncQ=c($params.trunc_quality),\n        maxEE=c($params.max_expected_error)\n    )[1:$n,]\n\n    if (\"$meta.paired_end\" == \"true\") {\n        io <- list(fwd=\"${reads[0]}\", filt=\"${meta.id}_R1.trimmed.fastq.gz\",\n                   rev=\"${reads[1]}\", filt.rev=\"${meta.id}_R2.trimmed.fastq.gz\")\n    } else {\n        io <- list(fwd=\"${reads[0]}\", filt=\"${meta.id}.trimmed.fastq.gz\")\n    }\n\n    params <- c(io, filt_params, list(rm.phix=${rmphix}))\n\n    read_count <- do.call(filterAndTrim, params)[, \"reads.out\"]\n    write(sprintf(\"qc,,$meta.id,%s,\", read_count), \"summary.csv\")\n\n    # Plot if we kept all reads\n    if (file.exists(io[[\"filt\"]])) {\n        fig <- plotQualityProfile(io)\n        ggsave(\"quality-profile_${meta.id}.png\", plot=fig, type=\"cairo-png\" )\n    }\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"\n}",
        "nb_lignes_process": 58,
        "string_script": "    def software = getSoftwareName(task.process)\n    def rmphix = params.keep_phix ? \"TRUE\" : \"FALSE\"\n    def n = meta.paired_end ? 2 : 1\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    library(ggplot2)\n\n    filt_params <- data.frame(\n        minLen=c($params.min_read_len),\n        truncLen=c($params.trunc_len),\n        truncQ=c($params.trunc_quality),\n        maxEE=c($params.max_expected_error)\n    )[1:$n,]\n\n    if (\"$meta.paired_end\" == \"true\") {\n        io <- list(fwd=\"${reads[0]}\", filt=\"${meta.id}_R1.trimmed.fastq.gz\",\n                   rev=\"${reads[1]}\", filt.rev=\"${meta.id}_R2.trimmed.fastq.gz\")\n    } else {\n        io <- list(fwd=\"${reads[0]}\", filt=\"${meta.id}.trimmed.fastq.gz\")\n    }\n\n    params <- c(io, filt_params, list(rm.phix=${rmphix}))\n\n    read_count <- do.call(filterAndTrim, params)[, \"reads.out\"]\n    write(sprintf(\"qc,,$meta.id,%s,\", read_count), \"summary.csv\")\n\n    # Plot if we kept all reads\n    if (file.exists(io[[\"filt\"]])) {\n        fig <- plotQualityProfile(io)\n        ggsave(\"quality-profile_${meta.id}.png\", plot=fig, type=\"cairo-png\" )\n    }\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"",
        "nb_lignes_script": 35,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_low\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:[\"id\"]) }",
            "container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"",
            "conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "ITSXPRESS": {
        "name_process": "ITSXPRESS",
        "string_process": "\nprocess ITSXPRESS {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bbmap=38.69 bioconda::itsxpress=1.8.0\" : null)\n    container \"quay.io/biocontainers/itsxpress:1.8.0--pyhdfd78af_2\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta_upd), path(\"*.fastq.gz\"), emit: fastq\n    path \"*.log\", emit: log\n    path \"*.version.txt\", emit: version\n\n    script:\n\tmeta_upd = meta.clone()\n\tmeta_upd[\"paired\"] = false\t\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    def rev_arg = params.paired_end ? \"--fastq2 ${reads[1]}\" : \"--single_end\"\n    \"\"\"\n    itsxpress --taxa All --region $params.locus --threads $task.cpus \\\\\n        --outfile ${meta.id}-${params.locus}.fastq.gz --log ITSxpress_${meta.id}.log \\\\\n        --fastq ${reads[0]} $rev_arg\n\n    pip show itsxpress | grep ^Version | sed 's/.*: //' > \"${software}.version.txt\"\n    \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "\tmeta_upd = meta.clone()\n\tmeta_upd[\"paired\"] = false\t\n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    def rev_arg = params.paired_end ? \"--fastq2 ${reads[1]}\" : \"--single_end\"\n    \"\"\"\n    itsxpress --taxa All --region $params.locus --threads $task.cpus \\\\\n        --outfile ${meta.id}-${params.locus}.fastq.gz --log ITSxpress_${meta.id}.log \\\\\n        --fastq ${reads[0]} $rev_arg\n\n    pip show itsxpress | grep ^Version | sed 's/.*: //' > \"${software}.version.txt\"\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "ITSxpress",
            "PPIP"
        ],
        "tools_url": [
            "https://bio.tools/itsxpress",
            "https://bio.tools/ppip"
        ],
        "tools_dico": [
            {
                "name": "ITSxpress",
                "uri": "https://bio.tools/itsxpress",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Sequence trimming"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Trimming"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2975",
                                "term": "Nucleic acid sequence (raw)"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2975",
                                "term": "Nucleic acid sequence (raw)"
                            }
                        ]
                    }
                ],
                "description": "ITSxpress: Software to rapidly trim the Internally transcribed spacer (ITS) region of FASTQ files",
                "homepage": "https://github.com/USDA-ARS-GBRU/itsxpress"
            },
            {
                "name": "PPIP",
                "uri": "https://bio.tools/ppip",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3644",
                                    "term": "de Novo sequencing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3631",
                                    "term": "Peptide identification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3631",
                                    "term": "Peptide-spectrum-matching"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0943",
                                "term": "Mass spectrum"
                            },
                            {
                                "uri": "http://edamontology.org/data_2603",
                                "term": "Expression data"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2974",
                                "term": "Protein sequence (raw)"
                            }
                        ]
                    }
                ],
                "description": "An automated software for identification of bioactive endogenous peptides",
                "homepage": "https://github.com/Shawn-Xu/PPIP"
            }
        ],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? \"bioconda::bbmap=38.69 bioconda::itsxpress=1.8.0\" : null)",
            "container \"quay.io/biocontainers/itsxpress:1.8.0--pyhdfd78af_2\""
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_CHIMERA": {
        "name_process": "MOTHUR_CHIMERA",
        "string_process": "\nprocess MOTHUR_CHIMERA {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), path(fasta), path(count)\n\n    output:\n    tuple val(meta), path(\"${outprefix}.fasta\"), emit: fasta\n    tuple val(meta), path(\"${outprefix}.count_table\"), emit: count_table\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#chimera.${params.chimera_tool}(fasta=$fasta, count=$count, dereplicate=t)\"\n\n    # rename outputs\n\tif compgen -G \"*.${params.chimera_tool}*.fasta\" > /dev/null; then\n        mv *.${params.chimera_tool}*.fasta ${outprefix}.fasta\n\telse\n\t\tcp $fasta ${outprefix}.fasta\n\tfi\n\n    mv *.${params.chimera_tool}*.count_table ${outprefix}.count_table\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#chimera.${params.chimera_tool}(fasta=$fasta, count=$count, dereplicate=t)\"\n\n    # rename outputs\n\tif compgen -G \"*.${params.chimera_tool}*.fasta\" > /dev/null; then\n        mv *.${params.chimera_tool}*.fasta ${outprefix}.fasta\n\telse\n\t\tcp $fasta ${outprefix}.fasta\n\tfi\n\n    mv *.${params.chimera_tool}*.count_table ${outprefix}.count_table\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [
            "Mothur",
            "NullSeq"
        ],
        "tools_url": [
            "https://bio.tools/Mothur",
            "https://bio.tools/nullseq"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            },
            {
                "name": "NullSeq",
                "uri": "https://bio.tools/nullseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0364",
                                    "term": "Random sequence generation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Creates Random Coding Sequences with specified GC content and Amino Acid usage.",
                "homepage": "https://github.com/amarallab/NullSeq"
            }
        ],
        "inputs": [
            "meta",
            "fasta",
            "count"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "DADA2_DEREPFASTQ": {
        "name_process": "DADA2_DEREPFASTQ",
        "string_process": "\nprocess DADA2_DEREPFASTQ {\n    tag \"$meta.id\"\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        pattern: \"*.RDS\",\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:[\"id\"]) }\n\n    container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-stringr\" : null)\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.RDS\"), emit: rds\n    tuple val(meta), path(\"*.fasta\"), emit: fasta\n    path \"summary.csv\", emit: summary\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    library(stringr)\n\n    for (read in c(\"${reads.join('\", \"')}\")) {\n        orient <- ifelse(str_detect(read, \"_R2.*.fastq.*\"), \"_R2\",\n                    ifelse(str_detect(read, \"_R1.*.fastq.*\"), \"_R1\", \"\"))\n        derep <- derepFastq(read)\n        saveRDS(derep, sprintf(\"${meta.id}%s.derep.RDS\", orient))\n        uniquesToFasta(derep, sprintf(\"${meta.id}%s.derep.fasta\", orient))\n    }\n\n    # Write counts\n    counts <- getUniques(derep)\n    data <- sprintf(\"Dereplication,,${meta.id},%s,%s\",sum(counts),sum(counts>0))\n    write(data, \"summary.csv\")\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"\n}",
        "nb_lignes_process": 45,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    library(stringr)\n\n    for (read in c(\"${reads.join('\", \"')}\")) {\n        orient <- ifelse(str_detect(read, \"_R2.*.fastq.*\"), \"_R2\",\n                    ifelse(str_detect(read, \"_R1.*.fastq.*\"), \"_R1\", \"\"))\n        derep <- derepFastq(read)\n        saveRDS(derep, sprintf(\"${meta.id}%s.derep.RDS\", orient))\n        uniquesToFasta(derep, sprintf(\"${meta.id}%s.derep.fasta\", orient))\n    }\n\n    # Write counts\n    counts <- getUniques(derep)\n    data <- sprintf(\"Dereplication,,${meta.id},%s,%s\",sum(counts),sum(counts>0))\n    write(data, \"summary.csv\")\n\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"",
        "nb_lignes_script": 21,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_low\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , pattern: \"*.RDS\" , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:[\"id\"]) }",
            "container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"",
            "conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-stringr\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_SUBSAMPLE": {
        "name_process": "MOTHUR_SUBSAMPLE",
        "string_process": "\nprocess MOTHUR_SUBSAMPLE {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), file(list), file(count)\n    each level\n\n    output:\n    tuple val(meta), path(\"*.shared\"), emit: shared\n    tuple val(meta), path(\"*.list\"), emit: list\n    tuple val(meta), path(\"*.count_table\"), emit: count_table\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    sub.sample(list=$list, count=$count, size=$level, persample=true);\n    make.shared(list=current, count=current)\"\n\n    # rename outputs\n    mv *.subsample.count_table ${outprefix}.count_table\n    mv *.shared ${outprefix}.shared\n\n    [ -f *subsample*.list ] && mv *subsample*.list ${outprefix}.list || cp $list ${outprefix}.list\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 39,
        "string_script": "    def software = getSoftwareName(task.process)\n    def procname = \"${task.process.tokenize(':')[-1].toLowerCase()}\"\n    def outprefix = \"${procname}.${meta.id}\"\n    \"\"\"\n    mothur \"#\n    sub.sample(list=$list, count=$count, size=$level, persample=true);\n    make.shared(list=current, count=current)\"\n\n    # rename outputs\n    mv *.subsample.count_table ${outprefix}.count_table\n    mv *.shared ${outprefix}.shared\n\n    [ -f *subsample*.list ] && mv *subsample*.list ${outprefix}.list || cp $list ${outprefix}.list\n\n    # print version\n    mothur -v | tail -n+2 | head -1 | cut -d\"=\" -f2 > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "list",
            "count",
            "level"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_medium\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "CUTADAPT": {
        "name_process": "CUTADAPT",
        "string_process": "\nprocess CUTADAPT {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }\n\n    conda (params.enable_conda ? 'bioconda::cutadapt=3.4' : null)\n    container 'quay.io/biocontainers/cutadapt:4.0--py37h8902056_0'\n\n    input:\n    tuple val(meta), path(reads)\n    path barcodes\n\n    output:\n    tuple val(meta), path('*.fastq.gz'), emit: reads\n    path '*.log', emit: log\n    path '*.version.txt', emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def trimmed  = params.single_end ?\n        \"-o {name}.fastq.gz\" :\n        \"-o {name}_R1.fastq.gz -p {name}_R2.fastq.gz\"\n\n    demux_opt = \"\"\n    if (params.paired_end && params.linked_bc) {\n        demux_opt += \" -a file:${barcodes[0]}\"\n    } else {\n        if (params.forward_bc.matches('5|3')) {\n            demux_opt += params.forward_bc == '5' ?\n                      \" -g file:${barcodes[0]}\" :\n                      \" -a file:${barcodes[0]}\"\n        }\n        if (params.reverse_bc.matches('5|3')) {\n            demux_opt += params.reverse_bc == '5' ?\n                \" -G file:${barcodes[1]}\" :\n                \" -A file:${barcodes[1]}\"\n        }\n    }\n\n    demux_opt += params.single_end ? \" --rc\" : \" --pair-adapters\"\n\n    \"\"\"\n    cutadapt --discard-untrimmed $demux_opt \\\\\n        --cores $task.cpus \\\\\n        -e $params.max_error_rate \\\\\n        $trimmed \\\\\n        $reads \\\\\n        > cutadapt.log\n\n    echo \\$(cutadapt --version) > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 53,
        "string_script": "    def software = getSoftwareName(task.process)\n    def trimmed  = params.single_end ?\n        \"-o {name}.fastq.gz\" :\n        \"-o {name}_R1.fastq.gz -p {name}_R2.fastq.gz\"\n\n    demux_opt = \"\"\n    if (params.paired_end && params.linked_bc) {\n        demux_opt += \" -a file:${barcodes[0]}\"\n    } else {\n        if (params.forward_bc.matches('5|3')) {\n            demux_opt += params.forward_bc == '5' ?\n                      \" -g file:${barcodes[0]}\" :\n                      \" -a file:${barcodes[0]}\"\n        }\n        if (params.reverse_bc.matches('5|3')) {\n            demux_opt += params.reverse_bc == '5' ?\n                \" -G file:${barcodes[1]}\" :\n                \" -A file:${barcodes[1]}\"\n        }\n    }\n\n    demux_opt += params.single_end ? \" --rc\" : \" --pair-adapters\"\n\n    \"\"\"\n    cutadapt --discard-untrimmed $demux_opt \\\\\n        --cores $task.cpus \\\\\n        -e $params.max_error_rate \\\\\n        $trimmed \\\\\n        $reads \\\\\n        > cutadapt.log\n\n    echo \\$(cutadapt --version) > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 32,
        "language_script": "bash",
        "tools": [
            "Cutadapt"
        ],
        "tools_url": [
            "https://bio.tools/cutadapt"
        ],
        "tools_dico": [
            {
                "name": "Cutadapt",
                "uri": "https://bio.tools/cutadapt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Sequence trimming"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Trimming"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3495",
                                "term": "RNA sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3495",
                                "term": "RNA sequence"
                            }
                        ]
                    }
                ],
                "description": "Find and remove adapter sequences, primers, poly-A tails and other types of unwanted sequence from your high-throughput sequencing reads.",
                "homepage": "https://pypi.python.org/pypi/cutadapt"
            }
        ],
        "inputs": [
            "meta",
            "reads",
            "barcodes"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_medium\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "conda (params.enable_conda ? 'bioconda::cutadapt=3.4' : null)",
            "container 'quay.io/biocontainers/cutadapt:4.0--py37h8902056_0'"
        ],
        "when": "",
        "stub": ""
    },
    "CUTADAPT_JAMP": {
        "name_process": "CUTADAPT_JAMP",
        "string_process": "\nprocess CUTADAPT_JAMP {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }\n\n    conda (params.enable_conda ? 'bioconda::cutadapt=3.4' : null)\n    container 'quay.io/biocontainers/cutadapt:4.0--py37h8902056_0'\n\n    input:\n    tuple val(meta), path(reads)\n    path barcodes\n\n    output:\n    tuple val(meta), path('trimmed/*.fastq.gz'), emit: reads\n    path '*.log', emit: log\n    path '*.version.txt', emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n\tdef adapter1 = \"GGWACWGGWTGAACWGTWTAYCCYCC\"\n\tdef adapter2 = \"TANACYTCNGGRTGNCCRAARAAYCA\"\n    \"\"\"\n\tmkdir demux trimmed\n\n\t# remove index first\n    cutadapt --discard-untrimmed --pair-adapters \\\\\n\t    -g file:$barcodes \\\\\n        -G file:$barcodes \\\\\n        --cores $task.cpus \\\\\n        -e $params.max_error_rate \\\\\n\t    -o demux/{name}_R1.fastq.gz -p demux/{name}_R2.fastq.gz \\\\\n        $reads \\\\\n        > cutadapt_demux.log\n\n\t# remove adapters\n\tfor f in \\$(ls demux/*_R1.fastq.gz); do\n        s=\\$(basename \\${f/_R1.fastq.gz//}); \n        cutadapt --discard-untrimmed --pair-adapters \\\\\n\t        --cores $task.cpus \\\\\n            -g ^$adapter1 -G ^$adapter2 \\\\\n            -g ^$adapter2 -G ^$adapter1 \\\\\n            -o trimmed/\\${s}_R1.fastq.gz -p trimmed/\\${s}_R2.fastq.gz \\\\\n            demux/\\${s}_R*.fastq.gz; >> cutadapt_trimming.log\n\tdone\n\n    echo \\$(cutadapt --version) > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 49,
        "string_script": "    def software = getSoftwareName(task.process)\n\tdef adapter1 = \"GGWACWGGWTGAACWGTWTAYCCYCC\"\n\tdef adapter2 = \"TANACYTCNGGRTGNCCRAARAAYCA\"\n    \"\"\"\n\tmkdir demux trimmed\n\n\t# remove index first\n    cutadapt --discard-untrimmed --pair-adapters \\\\\n\t    -g file:$barcodes \\\\\n        -G file:$barcodes \\\\\n        --cores $task.cpus \\\\\n        -e $params.max_error_rate \\\\\n\t    -o demux/{name}_R1.fastq.gz -p demux/{name}_R2.fastq.gz \\\\\n        $reads \\\\\n        > cutadapt_demux.log\n\n\t# remove adapters\n\tfor f in \\$(ls demux/*_R1.fastq.gz); do\n        s=\\$(basename \\${f/_R1.fastq.gz//}); \n        cutadapt --discard-untrimmed --pair-adapters \\\\\n\t        --cores $task.cpus \\\\\n            -g ^$adapter1 -G ^$adapter2 \\\\\n            -g ^$adapter2 -G ^$adapter1 \\\\\n            -o trimmed/\\${s}_R1.fastq.gz -p trimmed/\\${s}_R2.fastq.gz \\\\\n            demux/\\${s}_R*.fastq.gz; >> cutadapt_trimming.log\n\tdone\n\n    echo \\$(cutadapt --version) > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 28,
        "language_script": "bash",
        "tools": [
            "Cutadapt"
        ],
        "tools_url": [
            "https://bio.tools/cutadapt"
        ],
        "tools_dico": [
            {
                "name": "Cutadapt",
                "uri": "https://bio.tools/cutadapt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Sequence trimming"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Trimming"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3495",
                                "term": "RNA sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3495",
                                "term": "RNA sequence"
                            }
                        ]
                    }
                ],
                "description": "Find and remove adapter sequences, primers, poly-A tails and other types of unwanted sequence from your high-throughput sequencing reads.",
                "homepage": "https://pypi.python.org/pypi/cutadapt"
            }
        ],
        "inputs": [
            "meta",
            "reads",
            "barcodes"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_medium\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "conda (params.enable_conda ? 'bioconda::cutadapt=3.4' : null)",
            "container 'quay.io/biocontainers/cutadapt:4.0--py37h8902056_0'"
        ],
        "when": "",
        "stub": ""
    },
    "SPLIT_FASTA": {
        "name_process": "SPLIT_FASTA",
        "string_process": "\nprocess SPLIT_FASTA {\n    label \"process_low\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::biopython\" : null)\n\n    input:\n    path fasta\n\n    output:\n    path \"*.main.faa\", emit: main\n    path \"*.others.faa\", optional: true, emit: others\n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    import re\n    from collections import Counter, defaultdict\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n\n    sequences = defaultdict(lambda: [])\n    \n    with open(\"$fasta\", \"r\") as reader:\n        for (title, seq) in SimpleFastaParser(reader):\n            lineage = title.split()[1]\n            tax = lineage.split(\"$params.sep\")[$params.field]\n            sequences[tax].append(f\">{title}\\\\n{seq}\")\n\n    for (taxa, entries) in sequences.items():        \n        seq_str = \"\\\\n\".join(entries) + \"\\\\n\"\n\n        if len(entries) < $params.min_group_size:\n            fname = f\"{taxa}.others.faa\"\n        else:\n            fname = f\"{taxa}.main.faa\"\n        \n        with open(fname, \"w\") as writer:\n            writer.write(seq_str)\n    \"\"\"\n}",
        "nb_lignes_process": 44,
        "string_script": "    \"\"\"\n    #!/usr/bin/env python\n\n    import re\n    from collections import Counter, defaultdict\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n\n    sequences = defaultdict(lambda: [])\n    \n    with open(\"$fasta\", \"r\") as reader:\n        for (title, seq) in SimpleFastaParser(reader):\n            lineage = title.split()[1]\n            tax = lineage.split(\"$params.sep\")[$params.field]\n            sequences[tax].append(f\">{title}\\\\n{seq}\")\n\n    for (taxa, entries) in sequences.items():        \n        seq_str = \"\\\\n\".join(entries) + \"\\\\n\"\n\n        if len(entries) < $params.min_group_size:\n            fname = f\"{taxa}.others.faa\"\n        else:\n            fname = f\"{taxa}.main.faa\"\n        \n        with open(fname, \"w\") as writer:\n            writer.write(seq_str)\n    \"\"\"",
        "nb_lignes_script": 25,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fasta"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_low\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"nakor/metaflowmics-python:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::biopython\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "DADA2_LEARNERRORS": {
        "name_process": "DADA2_LEARNERRORS",
        "string_process": "\nprocess DADA2_LEARNERRORS {\n    tag \"$meta.id\"\n    label \"process_medium\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:['id']) }\n\n    container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2\" : null)\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.RDS\"), emit: rds, optional: true\n    path \"*.png\", emit: png, optional: true\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def suffix = meta.paired_end ? \"_${meta.orient}\" : \"\"\n    def outprefix = \"${meta.id}${suffix}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    library(stringr)\n    library(ggplot2)\n    \n    files <- sort(list.files(\".\", pattern=\"${suffix}.*.RDS\"))\n    sample_names <- gsub(\"${suffix}.*.RDS\", \"\", files)\n    reads <- lapply(files, readRDS)\n    names(reads) <- sample_names\n\n    tryCatch(\n        expr={ errors <- learnErrors(reads, multithread=TRUE, randomize=TRUE, nbases=1e7)\n               saveRDS(errors, \"${outprefix}.errors.RDS\") \n               fig <- plotErrors(errors, nominalQ=TRUE)\n               ggsave(\"profile_${outprefix}.png\", plot=fig, type=\"cairo-png\") },\n        error=function(e) { print(e) }            \n    )\n\n    writeLines(paste0(packageVersion('dada2')), \"${software}.version.txt\")    \n    \"\"\"\n}",
        "nb_lignes_process": 46,
        "string_script": "    def software = getSoftwareName(task.process)\n    def suffix = meta.paired_end ? \"_${meta.orient}\" : \"\"\n    def outprefix = \"${meta.id}${suffix}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    library(stringr)\n    library(ggplot2)\n    \n    files <- sort(list.files(\".\", pattern=\"${suffix}.*.RDS\"))\n    sample_names <- gsub(\"${suffix}.*.RDS\", \"\", files)\n    reads <- lapply(files, readRDS)\n    names(reads) <- sample_names\n\n    tryCatch(\n        expr={ errors <- learnErrors(reads, multithread=TRUE, randomize=TRUE, nbases=1e7)\n               saveRDS(errors, \"${outprefix}.errors.RDS\") \n               fig <- plotErrors(errors, nominalQ=TRUE)\n               ggsave(\"profile_${outprefix}.png\", plot=fig, type=\"cairo-png\") },\n        error=function(e) { print(e) }            \n    )\n\n    writeLines(paste0(packageVersion('dada2')), \"${software}.version.txt\")    \n    \"\"\"",
        "nb_lignes_script": 24,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_medium\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "container \"quay.io/biocontainers/bioconductor-dada2:1.22.0--r41h399db7b_0\"",
            "conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "DOWNLOAD_IBOL": {
        "name_process": "DOWNLOAD_IBOL",
        "string_process": "\nprocess DOWNLOAD_IBOL {\n    tag \"v$version\"\n    label \"process_low\"\n    publishDir \"${params.outdir}/download\", mode: params.publish_dir_mode\n    \n    conda (params.enable_conda ? \"conda-forge::bash=5.1\" : null)\n    container \"nakor/bash:5.1.4\"\n\n    input:\n    each version\n    \n    output:\n    tuple val(meta), path(\"iBOL_COI*.fna\"), emit: fna\n    path \"iBOL_COI*.tsv\", emit: tsv\n\n    script:\n    meta = [id: version]\n    url = \"https://v3.boldsystems.org/data/datarelease/NewPackages\"\n    name = \"iBOL_phase_${version}_COI.tsv\"\n    prefix = \"iBOL_COI_$version\"\n    \n    tax_cols = (9..15).findAll{it!=17}.collect{\"\\$$it\"}.join('\";\"')\n    \"\"\"\n    wget -c $url/${name}.zip && unzip ${name}.zip && rm -f ${name}.zip\n    \n    grep -v WITHDRAWN $name |\n      awk -F'\\\\t' 'NR==1 ||\n                   ((\\$31 != \"\") &&\n                   (\\$9 != \"Proteobacteria\") && \n                   (\\$31 ~/^[ACGT]*\\$/) &&\n                   (length(\\$31) > $params.min_length))' \\\\\n       > ${prefix}.tsv \\\\\n    && rm -f $name\n\n    tail -n+2 ${prefix}.tsv |\n      awk -F'\\\\t' '{\n        if (\\$36==\"\") \\$36=\\$28;\n        print \">\"\\$36\" Metazoa;\"$tax_cols\"\\\\n\"\\$31\n      }' > ${prefix}.fna\n    \"\"\"\n}",
        "nb_lignes_process": 40,
        "string_script": "    meta = [id: version]\n    url = \"https://v3.boldsystems.org/data/datarelease/NewPackages\"\n    name = \"iBOL_phase_${version}_COI.tsv\"\n    prefix = \"iBOL_COI_$version\"\n    \n    tax_cols = (9..15).findAll{it!=17}.collect{\"\\$$it\"}.join('\";\"')\n    \"\"\"\n    wget -c $url/${name}.zip && unzip ${name}.zip && rm -f ${name}.zip\n    \n    grep -v WITHDRAWN $name |\n      awk -F'\\\\t' 'NR==1 ||\n                   ((\\$31 != \"\") &&\n                   (\\$9 != \"Proteobacteria\") && \n                   (\\$31 ~/^[ACGT]*\\$/) &&\n                   (length(\\$31) > $params.min_length))' \\\\\n       > ${prefix}.tsv \\\\\n    && rm -f $name\n\n    tail -n+2 ${prefix}.tsv |\n      awk -F'\\\\t' '{\n        if (\\$36==\"\") \\$36=\\$28;\n        print \">\"\\$36\" Metazoa;\"$tax_cols\"\\\\n\"\\$31\n      }' > ${prefix}.fna\n    \"\"\"",
        "nb_lignes_script": 23,
        "language_script": "bash",
        "tools": [
            "meta",
            "CURLS",
            "goname"
        ],
        "tools_url": [
            "https://bio.tools/meta",
            "https://bio.tools/CURLS",
            "https://bio.tools/goname"
        ],
        "tools_dico": [
            {
                "name": "meta",
                "uri": "https://bio.tools/meta",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0160",
                            "term": "Sequence sites, features and motifs"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Nucleic acid sequence analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Sequence analysis (nucleic acid)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "meta is a program to produce and to align the TF-maps of two gene promoter regions. meta is very useful to characterize promoter regions from orthologous genes, or from co-regulated genes in microarrays, as it reduces the signal/noise ratio in a very significant manner, still detecting the real functional sites.",
                "homepage": "http://big.crg.cat/services/meta"
            },
            {
                "name": "CURLS",
                "uri": "https://bio.tools/CURLS",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3335",
                            "term": "Cardiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3421",
                            "term": "Surgery"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "Pathology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3335",
                            "term": "Cardiovascular medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3421",
                            "term": "https://en.wikipedia.org/wiki/Surgery"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "Disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "https://en.wikipedia.org/wiki/Pathology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "towards a wider use of basic echo applications in Africa.\n\nBACKGROUND:Point-of-care ultrasound is increasingly being used as a diagnostic tool in resource-limited settings. The majority of existing ultrasound protocols have been developed and implemented in high-resource settings. In sub-Saharan Africa (SSA), patients with heart failure of various etiologies commonly present late in the disease process, with a similar syndrome of dyspnea, edema and cardiomegaly on chest X-ray. The causes of heart failure in SSA differ from those in high-resource settings. Point-of-care ultrasound has the potential to identify the underlying etiology of heart failure, and lead to targeted therapy.\n\n||| HOMEPAGE MISSING!.\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'ultrasound', 'Cardiac ultrasound resource-limited settings', 'high-resource', 'cardiomegaly SSA'",
                "homepage": "https://www.ncbi.nlm.nih.gov/pubmed/?term=31883027"
            },
            {
                "name": "goname",
                "uri": "https://bio.tools/goname",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0089",
                            "term": "Ontology and terminology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data retrieval"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data extraction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Retrieval"
                                }
                            ]
                        ],
                        "input": [],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2223",
                                "term": "Ontology metadata"
                            }
                        ]
                    }
                ],
                "description": "Find GO ontology terms by name.",
                "homepage": "http://emboss.open-bio.org/rel/rel6/apps/goname.html"
            }
        ],
        "inputs": [
            "version"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"v$version\"",
            "label \"process_low\"",
            "publishDir \"${params.outdir}/download\", mode: params.publish_dir_mode",
            "conda (params.enable_conda ? \"conda-forge::bash=5.1\" : null)",
            "container \"nakor/bash:5.1.4\""
        ],
        "when": "",
        "stub": ""
    },
    "DOWNLOAD_IBOL_2": {
        "name_process": "DOWNLOAD_IBOL_2",
        "string_process": "\nprocess DOWNLOAD_IBOL_2 {\n    tag \"$taxon\"\n    label \"process_low\"\n    publishDir \"${params.outdir}/download\", mode: params.publish_dir_mode\n    \n    conda (params.enable_conda ? \"conda-forge::bash=5.1\" : null)\n    container \"nakor/bash:5.1.4\"\n\n    input:\n    each taxon\n    \n    output:\n    tuple val(meta), path(\"iBOL_COI*.filt.fna\"), emit: fna\n    path \"iBOL_COI*.filt.tsv\", emit: tsv\n    path \"iBOL_COI*.raw.tsv\", emit: raw\n\n    script:\n    meta = [id: taxon]\n    url = \"http://v3.boldsystems.org/index.php/API_Public\"\n    query = \"combined?format=tsv&taxon=$taxon\"\n    prefix = \"iBOL_COI_$taxon\"\n    tax_cols = (9..21).step(2).findAll{it!=17}.collect{\"\\$$it\"}.join('\";\"')\n\n    ext = file(\"$params.external/${taxon}.tsv\")\n    dl_cmd = ext.exists() ?\n        \"cat $ext\" :\n        \"wget -qO- \\\"$url/$query\\\"\"\n    \"\"\"\n    $dl_cmd > ${prefix}.raw.tsv\n\n\tcat ${prefix}.raw.tsv |\n        sed -r 's/\\\\r//g' |\n        tr -d '\\\\xa0' |\n        grep -v \"SUPPRESSED\" |\n        sed 's/\\\\t /\\\\t/g' |\n        sed '/^>/!s/-//g' |\n        awk -F'\\\\t' 'NR==1 ||\n        (\\$13 != \"\") &&\n        (\\$45 ~/^[ACGT-]*\\$/) &&\n\t\t(length(\\$45) > $params.min_length)' \\\\\n    > ${prefix}.filt.tsv\n\n    tail -n+2 ${prefix}.filt.tsv |\n      awk -F'\\\\t' '{\n        if (\\$44 ~/ */) \\$44=\\$42;\n        print \">\"\\$44\" Metazoa;\"$tax_cols\"\\\\n\"\\$45\n      }' \\\\\n    > ${prefix}.filt.fna\n    \"\"\"\n}",
        "nb_lignes_process": 49,
        "string_script": "    meta = [id: taxon]\n    url = \"http://v3.boldsystems.org/index.php/API_Public\"\n    query = \"combined?format=tsv&taxon=$taxon\"\n    prefix = \"iBOL_COI_$taxon\"\n    tax_cols = (9..21).step(2).findAll{it!=17}.collect{\"\\$$it\"}.join('\";\"')\n\n    ext = file(\"$params.external/${taxon}.tsv\")\n    dl_cmd = ext.exists() ?\n        \"cat $ext\" :\n        \"wget -qO- \\\"$url/$query\\\"\"\n    \"\"\"\n    $dl_cmd > ${prefix}.raw.tsv\n\n\tcat ${prefix}.raw.tsv |\n        sed -r 's/\\\\r//g' |\n        tr -d '\\\\xa0' |\n        grep -v \"SUPPRESSED\" |\n        sed 's/\\\\t /\\\\t/g' |\n        sed '/^>/!s/-//g' |\n        awk -F'\\\\t' 'NR==1 ||\n        (\\$13 != \"\") &&\n        (\\$45 ~/^[ACGT-]*\\$/) &&\n\t\t(length(\\$45) > $params.min_length)' \\\\\n    > ${prefix}.filt.tsv\n\n    tail -n+2 ${prefix}.filt.tsv |\n      awk -F'\\\\t' '{\n        if (\\$44 ~/ */) \\$44=\\$42;\n        print \">\"\\$44\" Metazoa;\"$tax_cols\"\\\\n\"\\$45\n      }' \\\\\n    > ${prefix}.filt.fna\n    \"\"\"",
        "nb_lignes_script": 31,
        "language_script": "bash",
        "tools": [
            "meta",
            "CURLS",
            "QueryOr",
            "drtext"
        ],
        "tools_url": [
            "https://bio.tools/meta",
            "https://bio.tools/CURLS",
            "https://bio.tools/queryor",
            "https://bio.tools/drtext"
        ],
        "tools_dico": [
            {
                "name": "meta",
                "uri": "https://bio.tools/meta",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0160",
                            "term": "Sequence sites, features and motifs"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Nucleic acid sequence analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Sequence analysis (nucleic acid)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "meta is a program to produce and to align the TF-maps of two gene promoter regions. meta is very useful to characterize promoter regions from orthologous genes, or from co-regulated genes in microarrays, as it reduces the signal/noise ratio in a very significant manner, still detecting the real functional sites.",
                "homepage": "http://big.crg.cat/services/meta"
            },
            {
                "name": "CURLS",
                "uri": "https://bio.tools/CURLS",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3335",
                            "term": "Cardiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3421",
                            "term": "Surgery"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "Pathology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3335",
                            "term": "Cardiovascular medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3421",
                            "term": "https://en.wikipedia.org/wiki/Surgery"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "Disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "https://en.wikipedia.org/wiki/Pathology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "towards a wider use of basic echo applications in Africa.\n\nBACKGROUND:Point-of-care ultrasound is increasingly being used as a diagnostic tool in resource-limited settings. The majority of existing ultrasound protocols have been developed and implemented in high-resource settings. In sub-Saharan Africa (SSA), patients with heart failure of various etiologies commonly present late in the disease process, with a similar syndrome of dyspnea, edema and cardiomegaly on chest X-ray. The causes of heart failure in SSA differ from those in high-resource settings. Point-of-care ultrasound has the potential to identify the underlying etiology of heart failure, and lead to targeted therapy.\n\n||| HOMEPAGE MISSING!.\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'ultrasound', 'Cardiac ultrasound resource-limited settings', 'high-resource', 'cardiomegaly SSA'",
                "homepage": "https://www.ncbi.nlm.nih.gov/pubmed/?term=31883027"
            },
            {
                "name": "QueryOr",
                "uri": "https://bio.tools/queryor",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3226",
                                    "term": "Variant prioritisation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A web-based platform to manage and retrieve data from human exome sequencing projects.",
                "homepage": "http://genomes.cribi.unipd.it/cgi-bin/queryor/login.pl"
            },
            {
                "name": "drtext",
                "uri": "https://bio.tools/drtext",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Biological databases"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Data management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Information systems"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Databases and information systems"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Query and retrieval"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Database retrieval"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0957",
                                "term": "Database metadata"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0957",
                                "term": "Database metadata"
                            }
                        ]
                    }
                ],
                "description": "Get data resource entries complete text.",
                "homepage": "http://emboss.open-bio.org/rel/rel6/apps/drtext.html"
            }
        ],
        "inputs": [
            "taxon"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$taxon\"",
            "label \"process_low\"",
            "publishDir \"${params.outdir}/download\", mode: params.publish_dir_mode",
            "conda (params.enable_conda ? \"conda-forge::bash=5.1\" : null)",
            "container \"nakor/bash:5.1.4\""
        ],
        "when": "",
        "stub": ""
    },
    "DOWNLOAD_UNITE": {
        "name_process": "DOWNLOAD_UNITE",
        "string_process": "\nprocess DOWNLOAD_UNITE {\n    tag \"$params.db_release\"\n    label \"process_low\"\n\n    conda (params.enable_conda ? \"conda-forge::bash=5.1\" : null)\n    container \"nakor/bash:5.1.4\"\n\n    output:\n    path \"*.fasta\", emit: fasta\n\n    script:\n    def root_url = \"https://files.plutof.ut.ee/public/orig\"\n\n    if (params.db_release == 'fungi') {\n        url_base = \"${root_url}/1E/66\"\n        file = \"1E662B6EB320312A61E7E3218327F34C7DB09CFF8E4686A89EF47886822DA6AB.gz\"\n        \"\"\"\n        wget -qO- $url_base/$file | tar xz\n        iconv -f utf-8 -t ascii sh_general_release*/*.fasta \\\\\n            > unite_fungi.fasta\n        rm -rf sh_general_release*\n        \"\"\"\n    } else {\n        url_base = \"${root_url}/BF/49\"\n        file = \"BF49FBF4B47314A1CC5238B280FC58BFB8CEBD44A8D45F4A2BF5B8A466715693.gz\"\n        \"\"\"\n        wget -qO- $url_base/$file \\\\\n            | gunzip \\\\\n            | iconv -f utf-8 -t ascii \\\\\n            > unite_all_eukaryotes.fasta\n        rm -f $file\n        \"\"\"\n    }\n}",
        "nb_lignes_process": 33,
        "string_script": "    def root_url = \"https://files.plutof.ut.ee/public/orig\"\n\n    if (params.db_release == 'fungi') {\n        url_base = \"${root_url}/1E/66\"\n        file = \"1E662B6EB320312A61E7E3218327F34C7DB09CFF8E4686A89EF47886822DA6AB.gz\"\n        \"\"\"\n        wget -qO- $url_base/$file | tar xz\n        iconv -f utf-8 -t ascii sh_general_release*/*.fasta \\\\\n            > unite_fungi.fasta\n        rm -rf sh_general_release*\n        \"\"\"\n    } else {\n        url_base = \"${root_url}/BF/49\"\n        file = \"BF49FBF4B47314A1CC5238B280FC58BFB8CEBD44A8D45F4A2BF5B8A466715693.gz\"\n        \"\"\"\n        wget -qO- $url_base/$file \\\\\n            | gunzip \\\\\n            | iconv -f utf-8 -t ascii \\\\\n            > unite_all_eukaryotes.fasta\n        rm -f $file\n        \"\"\"\n    }",
        "nb_lignes_script": 21,
        "language_script": "bash",
        "tools": [
            "mrcfile",
            "iConvert"
        ],
        "tools_url": [
            "https://bio.tools/mrcfile",
            "https://bio.tools/iconvert"
        ],
        "tools_dico": [
            {
                "name": "mrcfile",
                "uri": "https://bio.tools/mrcfile",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_1317",
                            "term": "Structural biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0611",
                            "term": "Electron microscopy"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "mrcfile is a Python implementation of the MRC2014 file format, which is used in structural biology to store image and volume data.\n\nIt allows MRC files to be created and opened easily using a very simple API, which exposes the file\u2019s header and data as numpy arrays. The code runs in Python 2 and 3 and is fully unit-tested.\n\nThis library aims to allow users and developers to read and write standard-compliant MRC files in Python as easily as possible, and with no dependencies on any compiled libraries except numpy. You can use it interactively to inspect files, correct headers and so on, or in scripts and larger software packages to provide basic MRC file I/O functions.",
                "homepage": "https://mrcfile.readthedocs.io/en/latest/readme.html"
            },
            {
                "name": "iConvert",
                "uri": "https://bio.tools/iconvert",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2885",
                            "term": "DNA polymorphism"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3169",
                            "term": "ChIP-seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype and phenotype"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Biological databases"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3169",
                            "term": "Chip-sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3169",
                            "term": "Chip Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3169",
                            "term": "ChIP-sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3169",
                            "term": "Chip sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype and phenotype resources"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype-phenotype analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype-phenotype"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Data management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Information systems"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Databases and information systems"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Allows to convert genotype allele formats (any format for any SNP chip contained in the SNPchiMp tool).",
                "homepage": "http://bioinformatics.tecnoparco.org/SNPchimp/index.php/links"
            }
        ],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$params.db_release\"",
            "label \"process_low\"",
            "conda (params.enable_conda ? \"conda-forge::bash=5.1\" : null)",
            "container \"nakor/bash:5.1.4\""
        ],
        "when": "",
        "stub": ""
    },
    "DOWNLOAD_SILVA_FOR_MOTHUR": {
        "name_process": "DOWNLOAD_SILVA_FOR_MOTHUR",
        "string_process": "\nprocess DOWNLOAD_SILVA_FOR_MOTHUR {\n    tag \"$db_name\"\n    label \"process_low\"\n\n    conda (params.enable_conda ? \"conda-forge::bash=5.1\" : null)\n    container \"nakor/bash:5.1.4\"\n\n    output:\n    path \"*.tax\", emit: tax\n    path \"*.align\", emit: aln\n\n    script:\n    def url_base = \"https://mothur.s3.us-east-2.amazonaws.com/wiki\"\n\tdb_name = \"silva.${params.silva_release}_v${params.silva_version}\"\n    \"\"\"\n    wget -qO- ${url_base}/${db_name}.tgz | tar xz\n    \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "    def url_base = \"https://mothur.s3.us-east-2.amazonaws.com/wiki\"\n\tdb_name = \"silva.${params.silva_release}_v${params.silva_version}\"\n    \"\"\"\n    wget -qO- ${url_base}/${db_name}.tgz | tar xz\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$db_name\"",
            "label \"process_low\"",
            "conda (params.enable_conda ? \"conda-forge::bash=5.1\" : null)",
            "container \"nakor/bash:5.1.4\""
        ],
        "when": "",
        "stub": ""
    },
    "DOWNLOAD_RDP_FOR_DADA2": {
        "name_process": "DOWNLOAD_RDP_FOR_DADA2",
        "string_process": "\nprocess DOWNLOAD_RDP_FOR_DADA2 {\n    tag \"$params.db_release\"\n    label \"process_low\"\n\n    conda (params.enable_conda ? \"conda-forge::bash=5.1\" : null)\n    container \"nakor/bash:5.1.4\"\n\n    output:\n    path \"*.fa\", emit: fasta\n\n    script:\n    url_base = \"https://zenodo.org/record/4587955/files\"\n    file = \"silva_nr99_v138.1_wSpecies_train_set.fa.gz?download=1\"\n    \"\"\"\n    wget -qO- \"$url_base/$file\" | gunzip \\\\\n        > silva_nr99_v138.1_wSpecies_train_set.fa\n    \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "    url_base = \"https://zenodo.org/record/4587955/files\"\n    file = \"silva_nr99_v138.1_wSpecies_train_set.fa.gz?download=1\"\n    \"\"\"\n    wget -qO- \"$url_base/$file\" | gunzip \\\\\n        > silva_nr99_v138.1_wSpecies_train_set.fa\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "mrcfile"
        ],
        "tools_url": [
            "https://bio.tools/mrcfile"
        ],
        "tools_dico": [
            {
                "name": "mrcfile",
                "uri": "https://bio.tools/mrcfile",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_1317",
                            "term": "Structural biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0611",
                            "term": "Electron microscopy"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "mrcfile is a Python implementation of the MRC2014 file format, which is used in structural biology to store image and volume data.\n\nIt allows MRC files to be created and opened easily using a very simple API, which exposes the file\u2019s header and data as numpy arrays. The code runs in Python 2 and 3 and is fully unit-tested.\n\nThis library aims to allow users and developers to read and write standard-compliant MRC files in Python as easily as possible, and with no dependencies on any compiled libraries except numpy. You can use it interactively to inspect files, correct headers and so on, or in scripts and larger software packages to provide basic MRC file I/O functions.",
                "homepage": "https://mrcfile.readthedocs.io/en/latest/readme.html"
            }
        ],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$params.db_release\"",
            "label \"process_low\"",
            "conda (params.enable_conda ? \"conda-forge::bash=5.1\" : null)",
            "container \"nakor/bash:5.1.4\""
        ],
        "when": "",
        "stub": ""
    },
    "DADA2_MERGEPAIRS": {
        "name_process": "DADA2_MERGEPAIRS",
        "string_process": "\nprocess DADA2_MERGEPAIRS {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process),\n                                        meta:meta, publish_by_meta:[\"id\"]) }\n\n    container \"nakor/metaflowmics-r:0.0.2\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2 conda-forge::r-stringr conda-forge::r-seqinr conda-forge::r-dplyr conda-forge::r-tidyr\" : null)\n\n    input:\n    path(derep)\n    path(denoised)\n\n    output:\n    path \"*.RDS\", emit: rds\n    tuple val(meta), path(\"ASVs.100.{count_table,tsv}\"), emit: count_table\n    tuple val(meta), path(\"ASVs.100.fasta\"), emit: fasta\n    tuple val(meta), path(\"ASVs_duplicates_to_cluster.fasta\"), optional: true, emit: fasta_dup\n    path \"*_summary.tsv\", emit: merge_summary, optional: true\n    path \"*.version.txt\", emit: version\n\n    script:\n    meta = [id: \"100\"]\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    library(stringr)\n    library(reshape2)\n    library(dplyr)\n    library(tidyr)\n    library(seqinr)\n\n    paired <- length(list.files(path=\".\", pattern=\"_R2.denoised.RDS\")) > 1\n\n    load_rds_dada2 <- function(type, orient=\"\") {\n        ext <- sprintf(\"%s.%s.RDS\", orient, type)\n        files <- list.files(path=\".\", pattern=ext)\n        obj <- lapply(files, readRDS)\n        names(obj) <- gsub(ext, \"\", files)\n        return(obj)\n    }\n\n    ## Merge if paired_end (or merging was not done previously)\n    if (paired) {\n        merged <- mergePairs(\n            derepF=load_rds_dada2(\"derep\", \"_R1\"),\n            dadaF=load_rds_dada2(\"denoised\", \"_R1\"),\n            derepR=load_rds_dada2(\"derep\", \"_R2\"),\n            dadaR=load_rds_dada2(\"denoised\", \"_R2\"),\n            minOverlap=${params.min_overlap},\n            maxMismatch=${params.max_mismatch},\n            returnReject=TRUE\n        )\n        saveRDS(merged, \"merged.RDS\")\n\n        ## Mismatch summary\n        mismatches <- lapply(merged, function(df) table(df\\$nmismatch))\n        summary <- reshape2::melt(mismatches, varnames=\"nmismatch\") %>% \n          pivot_wider(values_from=value, names_from=L1) %>% \n          arrange(nmismatch)\n        write.table(summary, \"mismatch_summary.tsv\", sep=\"\\\\t\", quote=F, row.names=F)\n\n        ## Removes the reads not matching the merging criteria\n        merged <- lapply(merged, function(df) df[df\\$accept,])\n    } else { ## not paired\n        merged <- load_rds_dada2(\"denoised\", \"\")\n        saveRDS(merged, \"merged.RDS\")\n    }\n\n    ## Make the ASV table\n    asv_table <- makeSequenceTable(merged)\n    asv_table <- asv_table[rowSums(asv_table)>0, ]\n    sample_names <- rownames(asv_table)\n\n    ## Write ASV sequences\n    asv_ids <- sprintf(\"ASV_%s\", c(1:dim(asv_table)[2]))\n    uniquesToFasta(asv_table, \"ASVs.100.fasta\", ids=asv_ids)\n\n    ## Compute count table\n    count_table <- t(asv_table)\n    rownames(count_table) <- asv_ids\n\n    count_table <- cbind(asv_ids, rowSums(count_table), count_table)\n    colnames(count_table) <- c(\"Representative_Sequence\", \"total\", sample_names)\n\n    if (\"${params.format.toLowerCase()}\" == \"mothur\") {\n        write.table(count_table, file=\"ASVs.100.count_table\", row.names=F, col.names=T, quote=F, sep=\"\\\\t\")\n    } else {\n        write.table(count_table[, -c(2)], \"ASVs.100.tsv\", quote=F, row.names=F, sep=\"\\\\t\")\n\n        # Write duplicated fasta sequences with header formatted for VSEARCH\n        list.fasta <- list()\n        i = 1\n        for(seq in colnames(asv_table)) {\n            for(sample in rownames(asv_table)) {\n                abd = asv_table[sample, seq]\n                if(abd > 0) {\n                    seq_id = sprintf(\"ASV_%s;sample=%s;size=%s\", i, sample, abd)\n                    list.fasta[seq_id] = seq\n                }\n            }\n            i <- i+1\n        }\n        write.fasta(list.fasta, names=names(list.fasta), \n                    file.out='ASVs_duplicates_to_cluster.fasta')        \n    }\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"\n}",
        "nb_lignes_process": 113,
        "string_script": "    meta = [id: \"100\"]\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(dada2)\n    library(stringr)\n    library(reshape2)\n    library(dplyr)\n    library(tidyr)\n    library(seqinr)\n\n    paired <- length(list.files(path=\".\", pattern=\"_R2.denoised.RDS\")) > 1\n\n    load_rds_dada2 <- function(type, orient=\"\") {\n        ext <- sprintf(\"%s.%s.RDS\", orient, type)\n        files <- list.files(path=\".\", pattern=ext)\n        obj <- lapply(files, readRDS)\n        names(obj) <- gsub(ext, \"\", files)\n        return(obj)\n    }\n\n    ## Merge if paired_end (or merging was not done previously)\n    if (paired) {\n        merged <- mergePairs(\n            derepF=load_rds_dada2(\"derep\", \"_R1\"),\n            dadaF=load_rds_dada2(\"denoised\", \"_R1\"),\n            derepR=load_rds_dada2(\"derep\", \"_R2\"),\n            dadaR=load_rds_dada2(\"denoised\", \"_R2\"),\n            minOverlap=${params.min_overlap},\n            maxMismatch=${params.max_mismatch},\n            returnReject=TRUE\n        )\n        saveRDS(merged, \"merged.RDS\")\n\n        ## Mismatch summary\n        mismatches <- lapply(merged, function(df) table(df\\$nmismatch))\n        summary <- reshape2::melt(mismatches, varnames=\"nmismatch\") %>% \n          pivot_wider(values_from=value, names_from=L1) %>% \n          arrange(nmismatch)\n        write.table(summary, \"mismatch_summary.tsv\", sep=\"\\\\t\", quote=F, row.names=F)\n\n        ## Removes the reads not matching the merging criteria\n        merged <- lapply(merged, function(df) df[df\\$accept,])\n    } else { ## not paired\n        merged <- load_rds_dada2(\"denoised\", \"\")\n        saveRDS(merged, \"merged.RDS\")\n    }\n\n    ## Make the ASV table\n    asv_table <- makeSequenceTable(merged)\n    asv_table <- asv_table[rowSums(asv_table)>0, ]\n    sample_names <- rownames(asv_table)\n\n    ## Write ASV sequences\n    asv_ids <- sprintf(\"ASV_%s\", c(1:dim(asv_table)[2]))\n    uniquesToFasta(asv_table, \"ASVs.100.fasta\", ids=asv_ids)\n\n    ## Compute count table\n    count_table <- t(asv_table)\n    rownames(count_table) <- asv_ids\n\n    count_table <- cbind(asv_ids, rowSums(count_table), count_table)\n    colnames(count_table) <- c(\"Representative_Sequence\", \"total\", sample_names)\n\n    if (\"${params.format.toLowerCase()}\" == \"mothur\") {\n        write.table(count_table, file=\"ASVs.100.count_table\", row.names=F, col.names=T, quote=F, sep=\"\\\\t\")\n    } else {\n        write.table(count_table[, -c(2)], \"ASVs.100.tsv\", quote=F, row.names=F, sep=\"\\\\t\")\n\n        # Write duplicated fasta sequences with header formatted for VSEARCH\n        list.fasta <- list()\n        i = 1\n        for(seq in colnames(asv_table)) {\n            for(sample in rownames(asv_table)) {\n                abd = asv_table[sample, seq]\n                if(abd > 0) {\n                    seq_id = sprintf(\"ASV_%s;sample=%s;size=%s\", i, sample, abd)\n                    list.fasta[seq_id] = seq\n                }\n            }\n            i <- i+1\n        }\n        write.fasta(list.fasta, names=names(list.fasta), \n                    file.out='ASVs_duplicates_to_cluster.fasta')        \n    }\n    writeLines(paste0(packageVersion(\"dada2\")), \"${software}.version.txt\")\n    \"\"\"",
        "nb_lignes_script": 87,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "derep",
            "denoised"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:[\"id\"]) }",
            "container \"nakor/metaflowmics-r:0.0.2\"",
            "conda (params.enable_conda ? \"bioconda::bioconductor-dada2=1.22 conda-forge::r-ggplot2 conda-forge::r-stringr conda-forge::r-seqinr conda-forge::r-dplyr conda-forge::r-tidyr\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "MOTHUR_GET_OTUS": {
        "name_process": "MOTHUR_GET_OTUS",
        "string_process": "\nprocess MOTHUR_GET_OTUS {\n    tag \"$meta.id\"\n    label \"process_low\"\n\n    container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"\n    conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)\n\n    input:\n    tuple val(meta), file(ref), file(filt)\n\n    output:\n    tuple val(meta), path(\"*.pick.${filt.getExtension()}\")\n\n    script:\n    def ref_ext = ref.getExtension()\n    def arg = filt.getExtension()\n    \"\"\"\n    mothur \"#list.otus(${ref_ext}=$ref);get.otus(accnos=current,$arg=$filt)\"\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "    def ref_ext = ref.getExtension()\n    def arg = filt.getExtension()\n    \"\"\"\n    mothur \"#list.otus(${ref_ext}=$ref);get.otus(accnos=current,$arg=$filt)\"\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "Mothur"
        ],
        "tools_url": [
            "https://bio.tools/Mothur"
        ],
        "tools_dico": [
            {
                "name": "Mothur",
                "uri": "https://bio.tools/Mothur",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffolding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer design"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3216",
                                    "term": "Scaffold generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "PCR primer prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0308",
                                    "term": "Primer design"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> LOW CONFIDENCE! | > COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/indices_diversity (MOTHUR.ORG/wiki) | Using Mothur to Determine Bacterial Community Composition and Structure in 16S Ribosomal RNA Datasets | Welcome to the website for the mothur project, initiated by Dr | We will be offering an R workshop December 18-20, 2019 | Welcome to the website for the mothur project, initiated by Dr",
                "homepage": "https://www.mothur.org"
            }
        ],
        "inputs": [
            "meta",
            "ref",
            "filt"
        ],
        "nb_inputs": 3,
        "outputs": [
            "meta"
        ],
        "nb_outputs": 1,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_low\"",
            "container \"quay.io/biocontainers/mothur:1.47.0--hb64bf22_2\"",
            "conda (params.enable_conda ? \"bioconda::mothur=1.47.0\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "KMER_FILTER": {
        "name_process": "KMER_FILTER",
        "string_process": "\nprocess KMER_FILTER {\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process)) }\n\n    container \"nakor/metaflowmics-python:0.0.2\"\n    conda (params.enable_conda ? \"conda-forge::numpy\" : null)\n\n    input:\n    tuple val(meta), path(query)\n    path db\n\n    output:\n    tuple val(meta), path(\"*.single.fasta\")\n\n    script:\n    prefix = \"${query.getSimpleName()}.single\"\n\n    if (params.feature == 'nucl') {\n        alphabet = \"ACGT\"\n        base = 2\n    } else {\n        alphabet = \"ARNDCQEGHILKMFPSTWYV\"\n        base = 5\n    }\n    \"\"\"\n    #!/usr/bin/env python\n\n    from itertools import product, groupby\n    import numpy as np\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n\n\n    alphabet = \"$alphabet\"\n    N = len(alphabet)\n    k = $params.k\n\n    kmer_usage = set(np.load(\"$db\")[:$params.n_sub])\n    kmer_db = {''.join(x) for i, x in enumerate(product(alphabet, repeat=k)) if i in kmer_usage}\n\n    with open(\"$query\") as r, open(\"${prefix}.fasta\", \"w\") as w:\n        for i, (_, entries) in enumerate(\n            groupby(SimpleFastaParser(r), lambda x: x[0].split()[0])\n        ):\n            best_score = 0\n            selected = ()\n\n            for (t, s) in entries:\n                # remove potential gaps\n                s = s.replace('-', '')\n                # compute kmer counts\n                score = sum(1 for i in range(len(s)-k+1) if s[i:i+k] in kmer_db)\n\n                if score > best_score:\n                    selected = (t.split('|')[0], s)\n                    best_score = score\n\n            w.write('>{}\\\\n{}\\\\n'.format(*selected))\n\n            if i % 1000 == 0:\n                print(f\"{i:,} sequences processed\")\n    \"\"\"\n}",
        "nb_lignes_process": 64,
        "string_script": "    prefix = \"${query.getSimpleName()}.single\"\n\n    if (params.feature == 'nucl') {\n        alphabet = \"ACGT\"\n        base = 2\n    } else {\n        alphabet = \"ARNDCQEGHILKMFPSTWYV\"\n        base = 5\n    }\n    \"\"\"\n    #!/usr/bin/env python\n\n    from itertools import product, groupby\n    import numpy as np\n    from Bio.SeqIO.FastaIO import SimpleFastaParser\n\n\n    alphabet = \"$alphabet\"\n    N = len(alphabet)\n    k = $params.k\n\n    kmer_usage = set(np.load(\"$db\")[:$params.n_sub])\n    kmer_db = {''.join(x) for i, x in enumerate(product(alphabet, repeat=k)) if i in kmer_usage}\n\n    with open(\"$query\") as r, open(\"${prefix}.fasta\", \"w\") as w:\n        for i, (_, entries) in enumerate(\n            groupby(SimpleFastaParser(r), lambda x: x[0].split()[0])\n        ):\n            best_score = 0\n            selected = ()\n\n            for (t, s) in entries:\n                # remove potential gaps\n                s = s.replace('-', '')\n                # compute kmer counts\n                score = sum(1 for i in range(len(s)-k+1) if s[i:i+k] in kmer_db)\n\n                if score > best_score:\n                    selected = (t.split('|')[0], s)\n                    best_score = score\n\n            w.write('>{}\\\\n{}\\\\n'.format(*selected))\n\n            if i % 1000 == 0:\n                print(f\"{i:,} sequences processed\")\n    \"\"\"",
        "nb_lignes_script": 45,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "query",
            "db"
        ],
        "nb_inputs": 3,
        "outputs": [
            "meta"
        ],
        "nb_outputs": 1,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process)) }",
            "container \"nakor/metaflowmics-python:0.0.2\"",
            "conda (params.enable_conda ? \"conda-forge::numpy\" : null)"
        ],
        "when": "",
        "stub": ""
    },
    "PHYLOSEQ_UNIFRAC": {
        "name_process": "PHYLOSEQ_UNIFRAC",
        "string_process": "\nprocess PHYLOSEQ_UNIFRAC {\n    tag \"$meta.id\"\n    label \"process_high\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options,\n                                        publish_dir:getSoftwareName(task.process))}\n\n    container \"nakor/metaflowmics-r:0.0.2\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-phyloseq=1.34.0 conda-forge::r-data.table\" : null)\n\n    input:\n    tuple val(meta), path(shared), path(tree)\n\n    output:\n    tuple val(meta), path(\"unifrac*.csv\"), emit: csv\n    path \"*.version.txt\", emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(data.table)\n    library(phyloseq)\n\n    abund <- data.frame(\n        fread(\"$shared\", drop=c(\"label\", \"numOtus\"), sep=\"\\\\t\"),\n        row.names=1, check.names=F\n    )\n\n    tree <- read_tree(\"$tree\")\n\n    ps <- phyloseq(\n        otu_table(as.matrix(abund), taxa_are_rows=FALSE),\n        phy_tree(tree)\n    )\n\n    dists <- UniFrac(ps, weighted=(\"$params.unifrac\"==\"weighted\"), parallel=TRUE)\n\n    write.csv(as.matrix(dists), \"unifrac_${params.unifrac}_${meta.id}.csv\")\n\n    writeLines(paste0(packageVersion(\"phyloseq\")), \"${software}.version.txt\")\n    \"\"\"\n}",
        "nb_lignes_process": 44,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(data.table)\n    library(phyloseq)\n\n    abund <- data.frame(\n        fread(\"$shared\", drop=c(\"label\", \"numOtus\"), sep=\"\\\\t\"),\n        row.names=1, check.names=F\n    )\n\n    tree <- read_tree(\"$tree\")\n\n    ps <- phyloseq(\n        otu_table(as.matrix(abund), taxa_are_rows=FALSE),\n        phy_tree(tree)\n    )\n\n    dists <- UniFrac(ps, weighted=(\"$params.unifrac\"==\"weighted\"), parallel=TRUE)\n\n    write.csv(as.matrix(dists), \"unifrac_${params.unifrac}_${meta.id}.csv\")\n\n    writeLines(paste0(packageVersion(\"phyloseq\")), \"${software}.version.txt\")\n    \"\"\"",
        "nb_lignes_script": 24,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "shared",
            "tree"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hawaiidatascience__metaflowmics",
        "directive": [
            "tag \"$meta.id\"",
            "label \"process_high\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process))}",
            "container \"nakor/metaflowmics-r:0.0.2\"",
            "conda (params.enable_conda ? \"bioconda::bioconductor-phyloseq=1.34.0 conda-forge::r-data.table\" : null)"
        ],
        "when": "",
        "stub": ""
    }
}