{
    "SC__TEMPLATE__PROCESS1": {
        "name_process": "SC__TEMPLATE__PROCESS1",
        "string_process": "\nprocess SC__TEMPLATE__PROCESS1 {\n\n    container params.sc.template.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink'\n    label 'compute_resources__default'\n\n    input:\n        tuple val(sampleId), path(f)\n\n    output:\n        tuple val(sampleId), path(\"${sampleId}.SC__TEMPLATE__PROCESS1.h5ad\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.template)\n\t\tprocessParams = sampleParams.local\n                                                                                                   \n        \"\"\"\n        ${binDir}process1.py \\\n            --input ${f} \\\n            --n_workers ${task.cpus} \\\n            --memory_limit ${task.memory.toGiga()} \\\n            --output ${sampleId}.SC__TEMPLATE__PROCESS1.h5ad\n        \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.template)\n\t\tprocessParams = sampleParams.local\n                                                                                                   \n        \"\"\"\n        ${binDir}process1.py \\\n            --input ${f} \\\n            --n_workers ${task.cpus} \\\n            --memory_limit ${task.memory.toGiga()} \\\n            --output ${sampleId}.SC__TEMPLATE__PROCESS1.h5ad\n        \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.template.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink'",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "AUCELL": {
        "name_process": "AUCELL",
        "string_process": "\nprocess AUCELL {\n\n    cache 'deep'\n    container toolParams.container\n    publishDir \"${toolParams.scenicoutdir}/${sampleId}/aucell/${\"numRuns\" in toolParams && toolParams.numRuns > 1 ? \"run_\" + runId : \"\"}\", mode: 'link', overwrite: true\n    label 'compute_resources__scenic_aucell'\n\n    input:\n        tuple \\\n           val(sampleId), \\\n           path(filteredLoom), \\\n           path(regulons), \\\n           val(runId)\n        val type\n\n    output:\n        tuple \\\n           val(sampleId), \\\n           path(filteredLoom), \\\n           path(\"${outputFileName}\"), \\\n           val(runId)\n\n    script:\n        if(toolParams.numRuns > 2 && task.maxForks > 1 && task.executor == \"local\")\n            throw new Exception(\"Running multi-runs SCENIC is quite computationally extensive. Please submit it as a job instead.\")\n\n        outputFileName = \"numRuns\" in toolParams && toolParams.numRuns > 1 ? \n            sampleId + \"__run_\" + runId +\"__auc_\" + type + \".loom\": \n            sampleId + \"__auc_\" + type + \".loom\"\n        seed = \"numRuns\" in toolParams && toolParams.numRuns > 1 ? \n            (params.global.seed + runId) : \n            params.global.seed\n        \"\"\"\n        pyscenic aucell \\\n            $filteredLoom \\\n            $regulons \\\n            -o \"${outputFileName}\" \\\n            --cell_id_attribute ${toolParams.cell_id_attribute} \\\n            --gene_attribute ${toolParams.gene_attribute} \\\n            --seed ${seed} \\\n            --num_workers ${task.cpus}\n        \"\"\"\n\n}",
        "nb_lignes_process": 43,
        "string_script": "        if(toolParams.numRuns > 2 && task.maxForks > 1 && task.executor == \"local\")\n            throw new Exception(\"Running multi-runs SCENIC is quite computationally extensive. Please submit it as a job instead.\")\n\n        outputFileName = \"numRuns\" in toolParams && toolParams.numRuns > 1 ? \n            sampleId + \"__run_\" + runId +\"__auc_\" + type + \".loom\": \n            sampleId + \"__auc_\" + type + \".loom\"\n        seed = \"numRuns\" in toolParams && toolParams.numRuns > 1 ? \n            (params.global.seed + runId) : \n            params.global.seed\n        \"\"\"\n        pyscenic aucell \\\n            $filteredLoom \\\n            $regulons \\\n            -o \"${outputFileName}\" \\\n            --cell_id_attribute ${toolParams.cell_id_attribute} \\\n            --gene_attribute ${toolParams.gene_attribute} \\\n            --seed ${seed} \\\n            --num_workers ${task.cpus}\n        \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [
            "Seed"
        ],
        "tools_url": [
            "https://bio.tools/seed-eco"
        ],
        "tools_dico": [
            {
                "name": "Seed",
                "uri": "https://bio.tools/seed-eco",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0610",
                            "term": "Ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3365",
                            "term": "Data architecture, analysis and design"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An R/Shiny package for visualizing ecological data. It provides a visual interface for generating a wide variety of plots, including histograms, scatterplots, bar plots, stacked bar plots, PCoA plots, cluster dendrograms, and heatmaps.",
                "homepage": "https://github.com/danlbek/Seed"
            }
        ],
        "inputs": [
            "sampleId",
            "filteredLoom",
            "regulons",
            "runId",
            "type"
        ],
        "nb_inputs": 5,
        "outputs": [
            "sampleId",
            "filteredLoom",
            "runId"
        ],
        "nb_outputs": 3,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "container toolParams.container",
            "publishDir \"${toolParams.scenicoutdir}/${sampleId}/aucell/${\"numRuns\" in toolParams && toolParams.numRuns > 1 ? \"run_\" + runId : \"\"}\", mode: 'link', overwrite: true",
            "label 'compute_resources__scenic_aucell'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__GENERATE_REPORT": {
        "name_process": "SC__SCANPY__GENERATE_REPORT",
        "string_process": "\nprocess SC__SCANPY__GENERATE_REPORT {\n\n  \tcontainer params.sc.scanpy.container\n  \tpublishDir \"${params.global.outdir}/notebooks/intermediate\", mode: 'link', overwrite: true\n    label 'compute_resources__report'\n\n\tinput:\n\t\tfile ipynb\n\t\ttuple val(sampleId), path(adata)\n\t\tval(reportTitle)\n\n\toutput:\n\t\ttuple val(sampleId), path(\"${sampleId}.${reportTitle}.ipynb\")\n\n\tscript:\n\t\tdef reportParams = new Yaml().dump(annotations_to_plot: params.sc.scanpy.report.annotations_to_plot)\n\t\t\"\"\"\n\t\tpapermill ${ipynb} \\\n\t\t    --report-mode \\\n\t\t\t${sampleId}.${reportTitle}.ipynb \\\n\t\t\t-p FILE $adata \\\n\t\t\t-y \"${reportParams}\" \\\n\t\t\t-p WORKFLOW_MANIFEST '${params.misc.manifestAsJSON}' \\\n\t\t\t-p WORKFLOW_PARAMETERS '${params.misc.paramsAsJSON}'\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 26,
        "string_script": "\t\tdef reportParams = new Yaml().dump(annotations_to_plot: params.sc.scanpy.report.annotations_to_plot)\n\t\t\"\"\"\n\t\tpapermill ${ipynb} \\\n\t\t    --report-mode \\\n\t\t\t${sampleId}.${reportTitle}.ipynb \\\n\t\t\t-p FILE $adata \\\n\t\t\t-y \"${reportParams}\" \\\n\t\t\t-p WORKFLOW_MANIFEST '${params.misc.manifestAsJSON}' \\\n\t\t\t-p WORKFLOW_PARAMETERS '${params.misc.paramsAsJSON}'\n\t\t\"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ipynb",
            "sampleId",
            "adata",
            "reportTitle"
        ],
        "nb_inputs": 4,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/notebooks/intermediate\", mode: 'link', overwrite: true",
            "label 'compute_resources__report'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__PARAM_EXPLORE_CLUSTERING_GENERATE_REPORT": {
        "name_process": "SC__SCANPY__PARAM_EXPLORE_CLUSTERING_GENERATE_REPORT",
        "string_process": "\nprocess SC__SCANPY__PARAM_EXPLORE_CLUSTERING_GENERATE_REPORT {\n\n  \tcontainer params.sc.scanpy.container\n  \tpublishDir \"${params.global.outdir}/notebooks/intermediate/clustering/${isParamNull(method) ? \"default\": method.toLowerCase()}/${isParamNull(resolution) ? \"res_\": resolution}\", mode: 'symlink', overwrite: true\n    label 'compute_resources__report'\n\n\tinput:\n\t\tfile ipynb\n\t\ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(adata), \\\n\t\t\tval(method), \\\n\t\t\tval(resolution)\n\t\tval(reportTitle)\n\n\toutput:\n\t\ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(\"${sampleId}.${reportTitle}.${uuid}.ipynb\"), \\\n\t\t\tval(method), \\\n\t\t\tval(resolution)\n\n\tscript:\n\t\t                                                                                                      \n\t\t                                                       \n\t\tstashedParams = [method, resolution]\n\t\tif(!isParamNull(stashedParams))\n\t\t\tuuid = stashedParams.findAll { it != 'NULL' }.join('_')\n\t\tdef reportParams = new Yaml().dump(annotations_to_plot: params.sc.scanpy.report.annotations_to_plot)\n\t\t\"\"\"\n\t\tpapermill ${ipynb} \\\n\t\t    --report-mode \\\n\t\t\t${sampleId}.${reportTitle}.${uuid}.ipynb \\\n\t\t\t-p FILE $adata \\\n\t\t\t-y \"${reportParams}\" \\\n\t\t\t-p WORKFLOW_MANIFEST '${params.misc.manifestAsJSON}' \\\n\t\t\t-p WORKFLOW_PARAMETERS '${params.misc.paramsAsJSON}'\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 39,
        "string_script": "\t\tstashedParams = [method, resolution]\n\t\tif(!isParamNull(stashedParams))\n\t\t\tuuid = stashedParams.findAll { it != 'NULL' }.join('_')\n\t\tdef reportParams = new Yaml().dump(annotations_to_plot: params.sc.scanpy.report.annotations_to_plot)\n\t\t\"\"\"\n\t\tpapermill ${ipynb} \\\n\t\t    --report-mode \\\n\t\t\t${sampleId}.${reportTitle}.${uuid}.ipynb \\\n\t\t\t-p FILE $adata \\\n\t\t\t-y \"${reportParams}\" \\\n\t\t\t-p WORKFLOW_MANIFEST '${params.misc.manifestAsJSON}' \\\n\t\t\t-p WORKFLOW_PARAMETERS '${params.misc.paramsAsJSON}'\n\t\t\"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ipynb",
            "sampleId",
            "adata",
            "method",
            "resolution",
            "reportTitle"
        ],
        "nb_inputs": 6,
        "outputs": [
            "sampleId",
            "method",
            "resolution"
        ],
        "nb_outputs": 3,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/notebooks/intermediate/clustering/${isParamNull(method) ? \"default\": method.toLowerCase()}/${isParamNull(resolution) ? \"res_\": resolution}\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__report'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__GENERATE_DUAL_INPUT_REPORT": {
        "name_process": "SC__SCANPY__GENERATE_DUAL_INPUT_REPORT",
        "string_process": "\nprocess SC__SCANPY__GENERATE_DUAL_INPUT_REPORT {\n\n\tcontainer params.sc.scanpy.container\n\tpublishDir \"${params.global.outdir}/notebooks/intermediate\", mode: 'link', overwrite: true\n    label 'compute_resources__report'\n\n  \tinput:\n\t\tfile(ipynb)\n\t\ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tfile(data1), \\\n\t\t\tfile(data2), \\\n\t\t\tval(stashedParams)\n\t\tval(reportTitle)\n\t\tval(isParameterExplorationModeOn)\n\n  \toutput:\n    \ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tfile(\"${sampleId}.${reportTitle}.${isParameterExplorationModeOn ? uuid + \".\" : ''}ipynb\"), \\\n\t\t\tval(stashedParams)\n\n  \tscript:\n\t\tif(!isParamNull(stashedParams))\n\t\t\tuuid = stashedParams.findAll { it != 'NULL' }.join('_')\n\t\tdef reportParams = new Yaml().dump(annotations_to_plot: params.sc.scanpy.report.annotations_to_plot)\n\t\t\"\"\"\n\t\tpapermill ${ipynb} \\\n\t\t    --report-mode \\\n\t\t\t${sampleId}.${reportTitle}.${isParameterExplorationModeOn ? uuid + \".\" : ''}ipynb \\\n\t\t\t-p FILE1 $data1 -p FILE2 $data2 \\\n\t\t\t-y \"${reportParams}\" \\\n\t\t\t-p WORKFLOW_MANIFEST '${params.misc.manifestAsJSON}' \\\n\t\t\t-p WORKFLOW_PARAMETERS '${params.misc.paramsAsJSON}'\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 36,
        "string_script": "\t\tif(!isParamNull(stashedParams))\n\t\t\tuuid = stashedParams.findAll { it != 'NULL' }.join('_')\n\t\tdef reportParams = new Yaml().dump(annotations_to_plot: params.sc.scanpy.report.annotations_to_plot)\n\t\t\"\"\"\n\t\tpapermill ${ipynb} \\\n\t\t    --report-mode \\\n\t\t\t${sampleId}.${reportTitle}.${isParameterExplorationModeOn ? uuid + \".\" : ''}ipynb \\\n\t\t\t-p FILE1 $data1 -p FILE2 $data2 \\\n\t\t\t-y \"${reportParams}\" \\\n\t\t\t-p WORKFLOW_MANIFEST '${params.misc.manifestAsJSON}' \\\n\t\t\t-p WORKFLOW_PARAMETERS '${params.misc.paramsAsJSON}'\n\t\t\"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ipynb",
            "sampleId",
            "data1",
            "data2",
            "stashedParams",
            "reportTitle",
            "isParameterExplorationModeOn"
        ],
        "nb_inputs": 7,
        "outputs": [
            "sampleId",
            "stashedParams"
        ],
        "nb_outputs": 2,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/notebooks/intermediate\", mode: 'link', overwrite: true",
            "label 'compute_resources__report'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__REPORT_TO_HTML": {
        "name_process": "SC__SCANPY__REPORT_TO_HTML",
        "string_process": "\nprocess SC__SCANPY__REPORT_TO_HTML {\n\n\tcontainer params.sc.scanpy.container\n\tpublishDir \"${params.global.outdir}/notebooks/intermediate\", mode: 'link', overwrite: true\n\t                                               \n\tpublishDir \"${params.global.outdir}/notebooks\", pattern: '*merged_report*', mode: 'link', overwrite: true\n    label 'compute_resources__report'\n\n\tinput:\n\t\ttuple val(sampleId), path(ipynb)\n\n\toutput:\n\t\tfile(\"*.html\")\n\n\tscript:\n\t\t\"\"\"\n\t\tjupyter nbconvert ${ipynb} --to html\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 19,
        "string_script": "\t\t\"\"\"\n\t\tjupyter nbconvert ${ipynb} --to html\n\t\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "Jupyterhub"
        ],
        "tools_url": [
            "https://bio.tools/Jupyterhub"
        ],
        "tools_dico": [
            {
                "name": "Jupyterhub",
                "uri": "https://bio.tools/Jupyterhub",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Workflows"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3372",
                            "term": "Software engineering"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Pipelines"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3372",
                            "term": "Computer programming"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3372",
                            "term": "Software development"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Jupyter notebooks in science gateways.\n\nJupyter Notebooks empower scientists to create executable documents that include text, equations, code and figures. Notebooks are a simple way to create reproducible and shareable workflows. The Jupyter developers have also released a multi-user notebook environment: Jupyterhub. Jupyterhub provides an extensible platform for handling user authentication and spawning the Notebook application to each user. I developed a plugin for Jupyterhub to spawn notebooks on a Supercomputer and integrated the authentication with CILogon and XSEDE. Scientists can authenticate on their browser and connect to a Jupyter Notebook instance running on the computing node of a Supercomputer, in my test deployment SDSC Comet. Jupyterhub can benefit Science Gateways by providing an expressive interface to a centralized environment with many software tools pre-installed and allow scientists to access Gateway functionality via web API.\n\n||| HOMEPAGE MISSING!",
                "homepage": "https://doi.org/10.7287/PEERJ.PREPRINTS.2577V2"
            }
        ],
        "inputs": [
            "sampleId",
            "ipynb"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/notebooks/intermediate\", mode: 'link', overwrite: true",
            "publishDir \"${params.global.outdir}/notebooks\", pattern: '*merged_report*', mode: 'link', overwrite: true",
            "label 'compute_resources__report'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__MERGE_REPORTS": {
        "name_process": "SC__SCANPY__MERGE_REPORTS",
        "string_process": "\nprocess SC__SCANPY__MERGE_REPORTS {\n\n\tcontainer params.sc.scanpy.container\n\tpublishDir \"${params.global.outdir}/notebooks/intermediate\", mode: 'link', overwrite: true\n\t                                                \n\tpublishDir \"${params.global.outdir}/notebooks\", pattern: '*merged_report*', mode: 'link', overwrite: true\n    label 'compute_resources__report'\n\n\tinput:\n\t\ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(ipynbs), \\\n\t\t\tval(stashedParams)\n\t\tval(reportTitle)\n\t\tval(isParameterExplorationModeOn)\n\n\toutput:\n\t\ttuple val(sampleId), path(\"${sampleId}.${reportTitle}.${isParameterExplorationModeOn ? uuid + '.' : ''}ipynb\")\n\n\tscript:\n\t\tif(!isParamNull(stashedParams))\n\t\t\tuuid = stashedParams.findAll { it != 'NULL' }.join('_')\n\t\t\"\"\"\n\t\tnbmerge \\\n\t\t\t${ipynbs} \\\n\t\t\t-o \"${sampleId}.${reportTitle}.${isParameterExplorationModeOn ? uuid + '.' : ''}ipynb\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 28,
        "string_script": "\t\tif(!isParamNull(stashedParams))\n\t\t\tuuid = stashedParams.findAll { it != 'NULL' }.join('_')\n\t\t\"\"\"\n\t\tnbmerge \\\n\t\t\t${ipynbs} \\\n\t\t\t-o \"${sampleId}.${reportTitle}.${isParameterExplorationModeOn ? uuid + '.' : ''}ipynb\"\n\t\t\"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "ipynbs",
            "stashedParams",
            "reportTitle",
            "isParameterExplorationModeOn"
        ],
        "nb_inputs": 5,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/notebooks/intermediate\", mode: 'link', overwrite: true",
            "publishDir \"${params.global.outdir}/notebooks\", pattern: '*merged_report*', mode: 'link', overwrite: true",
            "label 'compute_resources__report'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__CELLRANGER_ATAC__COUNT": {
        "name_process": "SC__CELLRANGER_ATAC__COUNT",
        "string_process": "\nprocess SC__CELLRANGER_ATAC__COUNT {\n\n    cache 'deep'\n    container toolParams.container\n    publishDir \"${params.global.outdir}/counts\", mode: 'link', overwrite: true\n    label 'compute_resources__cellranger_count'\n\n    input:\n        file(reference)\n        tuple val(sampleId), file(fastqs)\n\n    output:\n        tuple val(sampleId), file(\"${sampleId}/outs\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, toolParams.count)\n        processParams = sampleParams.local\n        if(processParams.sample == '') {\n            throw new Exception(\"Regards params.sc.cellranger_atac.count: sample parameter cannot be empty\")\n        }\n        runCellRangerAtacCount(\n            sampleId,\n            sampleId,\n            fastqs,\n            processParams,\n            task\n        )\n\n}",
        "nb_lignes_process": 28,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, toolParams.count)\n        processParams = sampleParams.local\n        if(processParams.sample == '') {\n            throw new Exception(\"Regards params.sc.cellranger_atac.count: sample parameter cannot be empty\")\n        }\n        runCellRangerAtacCount(\n            sampleId,\n            sampleId,\n            fastqs,\n            processParams,\n            task\n        )",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "MD-TASK"
        ],
        "tools_url": [
            "https://bio.tools/md-task"
        ],
        "tools_dico": [
            {
                "name": "MD-TASK",
                "uri": "https://bio.tools/md-task",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0176",
                            "term": "Molecular dynamics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2258",
                            "term": "Cheminformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Structure analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_2258",
                            "term": "Chemoinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2258",
                            "term": "Chemical informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Structural bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0081",
                            "term": "Biomolecular structure"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2476",
                                    "term": "Molecular dynamics"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2476",
                                    "term": "Molecular dynamics simulation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Tool suite for analysing molecular dynamics trajectories using network analysis and PRS.",
                "homepage": "https://github.com/RUBi-ZA/MD-TASK"
            }
        ],
        "inputs": [
            "reference",
            "sampleId",
            "fastqs"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "container toolParams.container",
            "publishDir \"${params.global.outdir}/counts\", mode: 'link', overwrite: true",
            "label 'compute_resources__cellranger_count'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__CELLRANGER_ATAC__COUNT_WITH_METADATA": {
        "name_process": "SC__CELLRANGER_ATAC__COUNT_WITH_METADATA",
        "string_process": "\nprocess SC__CELLRANGER_ATAC__COUNT_WITH_METADATA {\n\n    cache 'deep'\n    container toolParams.container\n    publishDir \"${params.global.outdir}/counts\", mode: 'link', overwrite: true\n    label 'compute_resources__cellranger'\n\n    input:\n        tuple \\\n            val(sampleId), \\\n            val(samplePrefix), \\\n            file(fastqs), \\\n            val(expectCells)\n\n    output:\n        tuple \\\n            val(sampleId), \\\n            file(\"${sampleId}/outs\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, toolParams.count)\n        processParams = sampleParams.local\n        runCellRangerAtacCount(\n            sampleId,\n            samplePrefix,\n            fastqs,\n            processParams,\n            task,\n            expectCells\n        )\n\n}",
        "nb_lignes_process": 31,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, toolParams.count)\n        processParams = sampleParams.local\n        runCellRangerAtacCount(\n            sampleId,\n            samplePrefix,\n            fastqs,\n            processParams,\n            task,\n            expectCells\n        )",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "samplePrefix",
            "fastqs",
            "expectCells"
        ],
        "nb_inputs": 4,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "container toolParams.container",
            "publishDir \"${params.global.outdir}/counts\", mode: 'link', overwrite: true",
            "label 'compute_resources__cellranger'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__CELLRANGER__COUNT": {
        "name_process": "SC__CELLRANGER__COUNT",
        "string_process": "\nprocess SC__CELLRANGER__COUNT {\n\n\tcache 'deep'\n\tcontainer toolParams.container\n\tpublishDir \"${params.global.outdir}/counts\", saveAs: {\"${sampleId}/outs\"}, mode: 'link', overwrite: true\n    label 'compute_resources__cellranger_count'\n\n    input:\n\t\tpath(transcriptome)\n\t\ttuple \\\n\t\t\tval(sampleId), \n\t\t\tpath(fastqs, stageAs: \"fastqs_??/*\")\n\n  \toutput:\n    \ttuple val(sampleId), path(\"${sampleId}/outs\")\n\n  \tscript:\n\t  \tdef sampleParams = params.parseConfig(sampleId, params.global, toolParams.count)\n\t\tprocessParams = sampleParams.local\n\t\tif(processParams.sample == '') {\n\t\t\tthrow new Exception(\"Regards params.sc.cellranger.count: sample parameter cannot be empty\")\n\t\t}\n\t\t                                                           \n\t\tfastqs = fastqs instanceof List ? fastqs.join(',') : fastqs\n\t\trunCellRangerCount(\n\t\t\tprocessParams,\n\t\t\ttranscriptome,\n            task,\n\t\t\tsampleId,\n\t\t\tsampleId,\n\t\t\tfastqs\n\t\t)\n\n}",
        "nb_lignes_process": 33,
        "string_script": "\t  \tdef sampleParams = params.parseConfig(sampleId, params.global, toolParams.count)\n\t\tprocessParams = sampleParams.local\n\t\tif(processParams.sample == '') {\n\t\t\tthrow new Exception(\"Regards params.sc.cellranger.count: sample parameter cannot be empty\")\n\t\t}\n\t\t                                                           \n\t\tfastqs = fastqs instanceof List ? fastqs.join(',') : fastqs\n\t\trunCellRangerCount(\n\t\t\tprocessParams,\n\t\t\ttranscriptome,\n            task,\n\t\t\tsampleId,\n\t\t\tsampleId,\n\t\t\tfastqs\n\t\t)",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "transcriptome",
            "sampleId",
            "fastqs"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "container toolParams.container",
            "publishDir \"${params.global.outdir}/counts\", saveAs: {\"${sampleId}/outs\"}, mode: 'link', overwrite: true",
            "label 'compute_resources__cellranger_count'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__CELLRANGER__COUNT_WITH_LIBRARIES": {
        "name_process": "SC__CELLRANGER__COUNT_WITH_LIBRARIES",
        "string_process": "\nprocess SC__CELLRANGER__COUNT_WITH_LIBRARIES {\n\n\tcache 'deep'\n\tcontainer toolParams.container\n\tpublishDir \"${params.global.outdir}/counts\", saveAs: {\"${sampleId}/outs\"}, mode: 'link', overwrite: true\n    label 'compute_resources__cellranger'\n\n    input:\n\t\tpath(transcriptome)\n\t\tpath(featureRef)\n\t\ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(fastqs, stageAs: \"fastqs_??/*\"),\n\t\t\tval(sampleNames),\n\t\t\tval(assays)\n\n  \toutput:\n    \ttuple val(sampleId), path(\"${sampleId}/outs\")\n\n  \tscript:\n\t  \tdef sampleParams = params.parseConfig(sampleId, params.global, toolParams.count)\n\t\tprocessParams = sampleParams.local\n\n\t\tif(processParams.sample == '') {\n\t\t\tthrow new Exception(\"Regards params.sc.cellranger.count: sample parameter cannot be empty\")\n\t\t}\n\n\t\t                                                                                \n\n\t\tcsvData = \"fastqs,sample,library_type\\n\"\n\t\tfastqs.eachWithIndex { fastq, ix -> \n\t\t\tcsvData += \"\\$PWD/${fastq},${sampleNames[ix]},${assays[ix]}\\n\"\n\t\t}\n\n\t\t\"\"\"\n\t\techo \"${csvData}\" > libraries.csv\n\t\t\"\"\" + runCellRangerCountLibraries(\n\t\t\tprocessParams,\n\t\t\ttranscriptome,\n            task,\n\t\t\tsampleId,\n\t\t\tfeatureRef,\n\t\t\t\"libraries.csv\"\n\t\t)\n\n}",
        "nb_lignes_process": 45,
        "string_script": "\t  \tdef sampleParams = params.parseConfig(sampleId, params.global, toolParams.count)\n\t\tprocessParams = sampleParams.local\n\n\t\tif(processParams.sample == '') {\n\t\t\tthrow new Exception(\"Regards params.sc.cellranger.count: sample parameter cannot be empty\")\n\t\t}\n\n\t\t                                                                                \n\n\t\tcsvData = \"fastqs,sample,library_type\\n\"\n\t\tfastqs.eachWithIndex { fastq, ix -> \n\t\t\tcsvData += \"\\$PWD/${fastq},${sampleNames[ix]},${assays[ix]}\\n\"\n\t\t}\n\n\t\t\"\"\"\n\t\techo \"${csvData}\" > libraries.csv\n\t\t\"\"\" + runCellRangerCountLibraries(\n\t\t\tprocessParams,\n\t\t\ttranscriptome,\n            task,\n\t\t\tsampleId,\n\t\t\tfeatureRef,\n\t\t\t\"libraries.csv\"\n\t\t)",
        "nb_lignes_script": 23,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "transcriptome",
            "featureRef",
            "sampleId",
            "sampleNames",
            "assays",
            "fastqs"
        ],
        "nb_inputs": 6,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "container toolParams.container",
            "publishDir \"${params.global.outdir}/counts\", saveAs: {\"${sampleId}/outs\"}, mode: 'link', overwrite: true",
            "label 'compute_resources__cellranger'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__CELLRANGER__COUNT_WITH_METADATA": {
        "name_process": "SC__CELLRANGER__COUNT_WITH_METADATA",
        "string_process": "\nprocess SC__CELLRANGER__COUNT_WITH_METADATA {\n\n\tcache 'deep'\n\tcontainer toolParams.container\n\tpublishDir \"${params.global.outdir}/counts\", saveAs: {\"${sampleId}/outs\"}, mode: 'link', overwrite: true\n    label 'compute_resources__cellranger'\n\n    input:\n\t\tpath(transcriptome)\n\t\ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tval(samplePrefix), \\\n\t\t\tpath(fastqs, stageAs: \"fastqs_??/*\"), \\\n\t\t\tval(expectCells), \\\n\t\t\tval(chemistry)\n\n  \toutput:\n    \ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(\"${sampleId}/outs\")\n\n  \tscript:\n\t  \tdef sampleParams = params.parseConfig(sampleId, params.global, toolParams.count)\n\t\tprocessParams = sampleParams.local\n\t\tfastqs = fastqs instanceof List ? fastqs.join(',') : fastqs\n\n\t\trunCellRangerCount(\n\t\t\tprocessParams,\n\t\t\ttranscriptome,\n            task,\n\t\t\tsampleId,\n\t\t\tsamplePrefix,\n\t\t\tfastqs,\n\t\t\texpectCells,\n\t\t\tchemistry\n\t\t)\n\n}",
        "nb_lignes_process": 37,
        "string_script": "\t  \tdef sampleParams = params.parseConfig(sampleId, params.global, toolParams.count)\n\t\tprocessParams = sampleParams.local\n\t\tfastqs = fastqs instanceof List ? fastqs.join(',') : fastqs\n\n\t\trunCellRangerCount(\n\t\t\tprocessParams,\n\t\t\ttranscriptome,\n            task,\n\t\t\tsampleId,\n\t\t\tsamplePrefix,\n\t\t\tfastqs,\n\t\t\texpectCells,\n\t\t\tchemistry\n\t\t)",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "transcriptome",
            "sampleId",
            "samplePrefix",
            "fastqs",
            "expectCells",
            "chemistry"
        ],
        "nb_inputs": 6,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "container toolParams.container",
            "publishDir \"${params.global.outdir}/counts\", saveAs: {\"${sampleId}/outs\"}, mode: 'link', overwrite: true",
            "label 'compute_resources__cellranger'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__DROP_SEQ_TOOLS__TRIM_POLYA_UNALIGNED_TAGGED_TRIMMED_SMART": {
        "name_process": "SC__DROP_SEQ_TOOLS__TRIM_POLYA_UNALIGNED_TAGGED_TRIMMED_SMART",
        "string_process": "\nprocess SC__DROP_SEQ_TOOLS__TRIM_POLYA_UNALIGNED_TAGGED_TRIMMED_SMART {\n\n    container params.sc.dropseqtools.container\n    publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n    tuple val(sample), path(bam)\n\n    output:\n    tuple val(sample), path('*.unaligned_tagged_polyA_filtered.bam'), emit: bam\n    tuple file('*.polyA_trimming_report.txt'), emit: report\n\n    script:\n    def sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.trim_polya_unaligned_tagged_trimmed_smart)\n\t\tprocessParams = sampleParams.local\n    \"\"\"\n    PolyATrimmer \\\n        INPUT=${bam} \\\n        OUTPUT=${sample}.unaligned_tagged_polyA_filtered.bam \\\n        OUTPUT_SUMMARY=${sample}.polyA_trimming_report.txt \\\n        MISMATCHES=${processParams.mismatches} \\\n        NUM_BASES=${processParams.numBases}\n    \"\"\"\n\n}",
        "nb_lignes_process": 25,
        "string_script": "    def sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.trim_polya_unaligned_tagged_trimmed_smart)\n\t\tprocessParams = sampleParams.local\n    \"\"\"\n    PolyATrimmer \\\n        INPUT=${bam} \\\n        OUTPUT=${sample}.unaligned_tagged_polyA_filtered.bam \\\n        OUTPUT_SUMMARY=${sample}.polyA_trimming_report.txt \\\n        MISMATCHES=${processParams.mismatches} \\\n        NUM_BASES=${processParams.numBases}\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "bam"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.dropseqtools.container",
            "publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'",
            "label 'compute_resources__cpu','compute_resources__24hqueue'"
        ],
        "when": "",
        "stub": ""
    },
    "CISTARGET": {
        "name_process": "CISTARGET",
        "string_process": "\nprocess CISTARGET {\n\n    cache 'deep'\n    container toolParams.container\n    publishDir \"${toolParams.scenicoutdir}/${sampleId}/cistarget/${\"numRuns\" in toolParams && toolParams.numRuns > 1 ? \"run_\" + runId : \"\"}\", mode: 'link', overwrite: true\n    label 'compute_resources__scenic_cistarget'\n\n    input:\n        tuple \\\n            val(sampleId), \\\n            path(filteredLoom), \\\n            path(f), \\\n            val(runId)\n        file featherDB\n        file annotation\n        val type\n\n    output:\n        tuple \\\n            val(sampleId), \\\n            path(filteredLoom), \\\n            path(\"${outputFileName}\"), \\\n            val(runId)\n\n    script:\n        if(toolParams.numRuns > 2 && task.maxForks > 1 && task.executor == \"local\")\n            throw new Exception(\"Running multi-runs SCENIC is quite computationally extensive. Please submit it as a job instead.\")\n        outputFileName = \"numRuns\" in toolParams && toolParams.numRuns > 1 ? sampleId + \"__run_\" + runId +\"__reg_\" + type + \".csv\" : sampleId + \"__reg_\" + type + \".csv\"\n        \"\"\"\n        pyscenic ctx \\\n            ${f} \\\n            ${featherDB} \\\n            ${processParams.containsKey('all_modules') && processParams.all_modules ? '--all_modules': ''} \\\n            --annotations_fname ${annotation} \\\n            --expression_mtx_fname ${filteredLoom} \\\n            --cell_id_attribute ${toolParams.cell_id_attribute} \\\n            --gene_attribute ${toolParams.gene_attribute} \\\n            --mode \"dask_multiprocessing\" \\\n            --output ${outputFileName} \\\n            --num_workers ${task.cpus} \\\n        \"\"\"\n\n}",
        "nb_lignes_process": 42,
        "string_script": "        if(toolParams.numRuns > 2 && task.maxForks > 1 && task.executor == \"local\")\n            throw new Exception(\"Running multi-runs SCENIC is quite computationally extensive. Please submit it as a job instead.\")\n        outputFileName = \"numRuns\" in toolParams && toolParams.numRuns > 1 ? sampleId + \"__run_\" + runId +\"__reg_\" + type + \".csv\" : sampleId + \"__reg_\" + type + \".csv\"\n        \"\"\"\n        pyscenic ctx \\\n            ${f} \\\n            ${featherDB} \\\n            ${processParams.containsKey('all_modules') && processParams.all_modules ? '--all_modules': ''} \\\n            --annotations_fname ${annotation} \\\n            --expression_mtx_fname ${filteredLoom} \\\n            --cell_id_attribute ${toolParams.cell_id_attribute} \\\n            --gene_attribute ${toolParams.gene_attribute} \\\n            --mode \"dask_multiprocessing\" \\\n            --output ${outputFileName} \\\n            --num_workers ${task.cpus} \\\n        \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "filteredLoom",
            "f",
            "runId",
            "featherDB",
            "annotation",
            "type"
        ],
        "nb_inputs": 7,
        "outputs": [
            "sampleId",
            "filteredLoom",
            "runId"
        ],
        "nb_outputs": 3,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "container toolParams.container",
            "publishDir \"${toolParams.scenicoutdir}/${sampleId}/cistarget/${\"numRuns\" in toolParams && toolParams.numRuns > 1 ? \"run_\" + runId : \"\"}\", mode: 'link', overwrite: true",
            "label 'compute_resources__scenic_cistarget'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__MARKER_GENES": {
        "name_process": "SC__SCANPY__MARKER_GENES",
        "string_process": "\nprocess SC__SCANPY__MARKER_GENES {\n\n  \tcontainer params.sc.scanpy.container\n  \tpublishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n  \n  \tinput:\n\t\t           \n\t\t                                                                              \n    \ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(normalizedTransformedData), \\\n\t\t\tpath(clusteredData)\n  \n  \toutput:\n    \ttuple val(sampleId), path(\"${sampleId}.SC__SCANPY__MARKER_GENES.${processParams.off}\")\n  \n  \tscript:\n    \tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.marker_genes)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}/cluster/sc_marker_genes.py \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t${(processParams.containsKey('groupby')) ? '--groupby ' + processParams.groupby : ''} \\\n\t\t\t${(processParams.containsKey('ngenes')) ? '--ngenes ' + processParams.ngenes : ''} \\\n\t\t\t$normalizedTransformedData \\\n\t\t\t$clusteredData \\\n\t\t\t\"${sampleId}.SC__SCANPY__MARKER_GENES.${processParams.off}\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 30,
        "string_script": "    \tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.marker_genes)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}/cluster/sc_marker_genes.py \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t${(processParams.containsKey('groupby')) ? '--groupby ' + processParams.groupby : ''} \\\n\t\t\t${(processParams.containsKey('ngenes')) ? '--ngenes ' + processParams.ngenes : ''} \\\n\t\t\t$normalizedTransformedData \\\n\t\t\t$clusteredData \\\n\t\t\t\"${sampleId}.SC__SCANPY__MARKER_GENES.${processParams.off}\"\n\t\t\"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "normalizedTransformedData",
            "clusteredData"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__PARAM_EXPLORE_MARKER_GENES": {
        "name_process": "SC__SCANPY__PARAM_EXPLORE_MARKER_GENES",
        "string_process": "\nprocess SC__SCANPY__PARAM_EXPLORE_MARKER_GENES {\n\n  \tcontainer params.sc.scanpy.container\n  \tpublishDir \"${params.global.outdir}/data/intermediate/markers/${isParamNull(clusteringMethod) ? \"default\": clusteringMethod.toLowerCase()}/${isParamNull(clusteringResolution) ? \"res_\": clusteringResolution}\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n  \n  \tinput:\n\t\t           \n\t\t                                                                              \n    \ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(normalizedTransformedData), \\\n\t\t\tpath(clusteredData), \\\n\t\t\tval(clusteringMethod), \\\n\t\t\tval(clusteringResolution)\n  \n  \toutput:\n    \ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(\"${sampleId}.SC__SCANPY__PARAM_EXPLORE_MARKER_GENES.${uuid}.${processParams.off}\"), \\\n\t\t\tval(clusteringMethod), \\\n\t\t\tval(clusteringResolution)\n  \n  \tscript:\n    \tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.marker_genes)\n\t\tprocessParams = sampleParams.local\n\t\t                                                                                                      \n\t\t                                                       \n\t\tstashedParams = [clusteringMethod, clusteringResolution]\n\t\tif(!isParamNull(stashedParams))\n\t\t\tuuid = stashedParams.findAll { it != 'NULL' }.join('_')\n\t\t\"\"\"\n\t\t${binDir}/cluster/sc_marker_genes.py \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t${!isParamNull(clusteringMethod) ? '--groupby ' + clusteringMethod : ''} \\\n\t\t\t${(processParams.containsKey('ngenes')) ? '--ngenes ' + processParams.ngenes : ''} \\\n\t\t\t$normalizedTransformedData \\\n\t\t\t$clusteredData \\\n\t\t\t\"${sampleId}.SC__SCANPY__PARAM_EXPLORE_MARKER_GENES.${uuid}.${processParams.off}\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 41,
        "string_script": "    \tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.marker_genes)\n\t\tprocessParams = sampleParams.local\n\t\t                                                                                                      \n\t\t                                                       \n\t\tstashedParams = [clusteringMethod, clusteringResolution]\n\t\tif(!isParamNull(stashedParams))\n\t\t\tuuid = stashedParams.findAll { it != 'NULL' }.join('_')\n\t\t\"\"\"\n\t\t${binDir}/cluster/sc_marker_genes.py \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t${!isParamNull(clusteringMethod) ? '--groupby ' + clusteringMethod : ''} \\\n\t\t\t${(processParams.containsKey('ngenes')) ? '--ngenes ' + processParams.ngenes : ''} \\\n\t\t\t$normalizedTransformedData \\\n\t\t\t$clusteredData \\\n\t\t\t\"${sampleId}.SC__SCANPY__PARAM_EXPLORE_MARKER_GENES.${uuid}.${processParams.off}\"\n\t\t\"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "normalizedTransformedData",
            "clusteredData",
            "clusteringMethod",
            "clusteringResolution"
        ],
        "nb_inputs": 5,
        "outputs": [
            "sampleId",
            "clusteringMethod",
            "clusteringResolution"
        ],
        "nb_outputs": 3,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate/markers/${isParamNull(clusteringMethod) ? \"default\": clusteringMethod.toLowerCase()}/${isParamNull(clusteringResolution) ? \"res_\": clusteringResolution}\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__H5AD_MERGE": {
        "name_process": "SC__H5AD_MERGE",
        "string_process": "\nprocess SC__H5AD_MERGE {\n\n\tcontainer params.sc.scanpy.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n\tinput:\n\t\t           \n\t\t                                                                             \n\t\ttuple \\\n            val(sampleId), \\\n\t\t\tpath(data)\n\n\toutput:\n\t\ttuple \\\n            val(sampleId), \\\n\t\t    path(\"${sampleId}.SC__H5AD_MERGE.h5ad\")\n\n\tscript:\n\t\t\"\"\"\n        ${binDir}/sc_h5ad_merge.py \\\n            * \\\n            \"${sampleId}.SC__H5AD_MERGE.h5ad\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 25,
        "string_script": "\t\t\"\"\"\n        ${binDir}/sc_h5ad_merge.py \\\n            * \\\n            \"${sampleId}.SC__H5AD_MERGE.h5ad\"\n\t\t\"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "data"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__POPSCLE__DSC_PILEUP": {
        "name_process": "SC__POPSCLE__DSC_PILEUP",
        "string_process": "\nprocess SC__POPSCLE__DSC_PILEUP {\n\n    container params.sc.popscle.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink'\n    label 'compute_resources__cpu'\n\n    input:\n        tuple val(sampleId), path(f)\n        file vcf\n\n    output:\n        tuple val(sampleId), path(\"${sampleId}_dsc-pileup*.gz\")\n\n    script:\n        \"\"\"\n        popscle dsc-pileup \\\n            --sam ${f} \\\n            --vcf ${vcf} \\\n            --out ${sampleId}_dsc-pileup\n        \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "        \"\"\"\n        popscle dsc-pileup \\\n            --sam ${f} \\\n            --vcf ${vcf} \\\n            --out ${sampleId}_dsc-pileup\n        \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f",
            "vcf"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.popscle.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink'",
            "label 'compute_resources__cpu'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__POPSCLE__PREFILTER_DSC_PILEUP": {
        "name_process": "SC__POPSCLE__PREFILTER_DSC_PILEUP",
        "string_process": "\nprocess SC__POPSCLE__PREFILTER_DSC_PILEUP {\n\n    container params.sc.popscle.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink'\n    label 'compute_resources__cpu'\n\n    input:\n        tuple val(sampleId), path(f)\n        file vcf\n\n    output:\n        tuple val(sampleId), path(\"${sampleId}_filtered_possorted_genome_bam.bam\")\n\n    script:\n        \"\"\"\n        ${binDir}/filter_bam_file_for_popscle_dsc_pileup.sh \\\n            ${f}/possorted_genome_bam.bam \\\n            ${f}/filtered_*_bc_matrix/barcodes.tsv* \\\n            ${vcf} \\\n            ${sampleId}_filtered_possorted_genome_bam.bam\n        \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "        \"\"\"\n        ${binDir}/filter_bam_file_for_popscle_dsc_pileup.sh \\\n            ${f}/possorted_genome_bam.bam \\\n            ${f}/filtered_*_bc_matrix/barcodes.tsv* \\\n            ${vcf} \\\n            ${sampleId}_filtered_possorted_genome_bam.bam\n        \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f",
            "vcf"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.popscle.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink'",
            "label 'compute_resources__cpu'"
        ],
        "when": "",
        "stub": ""
    },
    "DOWNLOAD_FASTQS_FROM_SRA_ACC_ID": {
        "name_process": "DOWNLOAD_FASTQS_FROM_SRA_ACC_ID",
        "string_process": "\nprocess DOWNLOAD_FASTQS_FROM_SRA_ACC_ID {\n\n    container toolParams.container\n    publishDir \"${params.global.outdir}/data/raw/fastqs\", mode: 'symlink', overwrite: true\n    label 'compute_resources__sratoolkit'\n\n    input:\n        tuple val(sraId), val(sampleId)\n    \n    output:\n        tuple val(sraId), file(\"${sraId}*.fastq.gz\")\n    \n    script:\n        if(sampleId == null || sampleId.length() < 1) {\n            throw new Exception(\"DOWNLOAD_FASTQS_FROM_SRA_ACC_ID: Sample ID is empty.\")\n        }\n        \"\"\"\n        SRA_FILE_LOCK=~/ncbi/public/sra/${sraId}.sra.lock\n        if [[ -f \"\\${SRA_FILE_LOCK}\" ]]; then\n            echo \"SRA file lock found for ${sraId}. Removing file lock...\"\n            rm \\${SRA_FILE_LOCK}\n        fi\n        prefetch -v -p 1 ${sraId}\n        fasterq-dump -S -v -p -e ${task.cpus} -O . ${sraId}\n        pigz -p ${task.cpus} *.fastq\n        \"\"\"\n\n}",
        "nb_lignes_process": 27,
        "string_script": "        if(sampleId == null || sampleId.length() < 1) {\n            throw new Exception(\"DOWNLOAD_FASTQS_FROM_SRA_ACC_ID: Sample ID is empty.\")\n        }\n        \"\"\"\n        SRA_FILE_LOCK=~/ncbi/public/sra/${sraId}.sra.lock\n        if [[ -f \"\\${SRA_FILE_LOCK}\" ]]; then\n            echo \"SRA file lock found for ${sraId}. Removing file lock...\"\n            rm \\${SRA_FILE_LOCK}\n        fi\n        prefetch -v -p 1 ${sraId}\n        fasterq-dump -S -v -p -e ${task.cpus} -O . ${sraId}\n        pigz -p ${task.cpus} *.fastq\n        \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sraId",
            "sampleId"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sraId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container toolParams.container",
            "publishDir \"${params.global.outdir}/data/raw/fastqs\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__sratoolkit'"
        ],
        "when": "",
        "stub": ""
    },
    "PICARD__FASTQ_TO_BAM": {
        "name_process": "PICARD__FASTQ_TO_BAM",
        "string_process": "\nprocess PICARD__FASTQ_TO_BAM {\n\n    container params.picard.container\n    publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n        file(reads)\n        file(tmpDir)\n    \n    output:\n        tuple val(sample), path('*.unaligned.bam'), emit: bam\n    \n    script:\n        sample = reads[0].toString() - ~/(_R1)?(\\.clean)?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        \"\"\"\n        java -Djava.io.tmpdir=$tmpDir -jar \\\n            /picard.jar \\\n                FastqToSam \\\n                    FASTQ=${reads[0]} \\\n                    FASTQ2=${reads[1]} \\\n                    O=${sample}.unaligned.bam \\\n                    SAMPLE_NAME=${sample}\n        \"\"\"\n\n}",
        "nb_lignes_process": 25,
        "string_script": "        sample = reads[0].toString() - ~/(_R1)?(\\.clean)?(\\.fq)?(\\.fastq)?(\\.gz)?$/\n        \"\"\"\n        java -Djava.io.tmpdir=$tmpDir -jar \\\n            /picard.jar \\\n                FastqToSam \\\n                    FASTQ=${reads[0]} \\\n                    FASTQ2=${reads[1]} \\\n                    O=${sample}.unaligned.bam \\\n                    SAMPLE_NAME=${sample}\n        \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [
            "SAMPLE"
        ],
        "tools_url": [
            "https://bio.tools/sample"
        ],
        "tools_dico": [
            {
                "name": "SAMPLE",
                "uri": "https://bio.tools/sample",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3516",
                            "term": "Genotyping experiment"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Genetic mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Genetic map construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Linkage mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Functional mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Genetic cartography"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Genetic map generation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The tool is designed to identify regions that are linked to a recessive disease by analysing genotype data from the parents and unaffected sibs of affected individuals. Since this analysis does not use data from affected patients, it is suited to the identification of lethal recessive genes, when the patients may have died before DNA samples could be obtained.",
                "homepage": "http://dna.leeds.ac.uk/sample/"
            }
        ],
        "inputs": [
            "reads",
            "tmpDir"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.picard.container",
            "publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'",
            "label 'compute_resources__cpu','compute_resources__24hqueue'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__STAR__UNLOAD_GENOME": {
        "name_process": "SC__STAR__UNLOAD_GENOME",
        "string_process": "\nprocess SC__STAR__UNLOAD_GENOME {\n\n\tcontainer params.sc.star.container\n    label 'compute_resources__default'\n\n\tinput:\n\t\tfile(transcriptome)\n\t\tval allDone\n\n\tscript:\n\t\t\"\"\"\n\t\techo \"--genomeDir ${transcriptome}\"\n\t\tSTAR \\\n\t\t\t--genomeLoad Remove \\\n\t\t\t--genomeDir ${transcriptome}\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 17,
        "string_script": "\t\t\"\"\"\n\t\techo \"--genomeDir ${transcriptome}\"\n\t\tSTAR \\\n\t\t\t--genomeLoad Remove \\\n\t\t\t--genomeDir ${transcriptome}\n\t\t\"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "STAR"
        ],
        "tools_url": [
            "https://bio.tools/star"
        ],
        "tools_dico": [
            {
                "name": "STAR",
                "uri": "https://bio.tools/star",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Ultrafast universal RNA-seq aligner",
                "homepage": "http://code.google.com/p/rna-star/"
            }
        ],
        "inputs": [
            "transcriptome",
            "allDone"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.star.container",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__CELDA__DECONTX": {
        "name_process": "SC__CELDA__DECONTX",
        "string_process": "\nprocess SC__CELDA__DECONTX {\n    \n    container params.sc.celda.container\n    publishDir \"${params.global.outdir}/data/${moduleName}\", mode: 'link'\n    label 'compute_resources__default'\n\n    input:\n        tuple \\\n            val(sampleId), \\\n            path(f)\n\n    output:\n        tuple \\\n            val(sampleId), \\\n            path(\"${sampleId}.CELDA__DECONTX.Rds\"), \\\n            emit: main\n        tuple \\\n            val(sampleId), \\\n            path(\"${sampleId}.CELDA__DECONTX.Contamination_Outlier_Table.tsv\"), \\\n            emit: outlier_table\n        tuple \\\n            val(sampleId), \\\n            path(\"${sampleId}.CELDA__DECONTX.{*.pdf,*.tsv}\"), \\\n            emit: other\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.celda.decontx)\n        processParams = sampleParams.local\n        \n        def filterNumMadsThresholdsAsArguments = ''\n        def filterContaminationScoreThresholdsAsArguments = ''\n\n        if(processParams?.filters) {\n            filterNumMadsThresholdsAsArguments = processParams.filters.containsKey('numMadsThresholds') ?\n                processParams.filters.numMadsThresholds.collect({\n                    '--num-mads-threshold ' + ' ' + it \n                }).join(' ') :\n                ''\n            filterContaminationScoreThresholdsAsArguments = processParams.filters.containsKey('contaminationScoreThresholds') ?\n                processParams.filters.contaminationScoreThresholds.collect({ \n                    '--custom-threshold ' + ' ' + it\n                }).join(' ') :\n                ''\n        }\n\n        def roundToIntAsArgument = ''\n        if(processParams?.roundToInt) {\n            roundToIntAsArgument = '--round-to-int '+ processParams.roundToInt\n        }\n\n        \"\"\"\n        ${binDir}/run_decontx.R \\\n            --sample-id ${sampleId} \\\n            --seed ${params.global.seed} \\\n            ${filterNumMadsThresholdsAsArguments} \\\n            ${filterContaminationScoreThresholdsAsArguments} \\\n            ${roundToIntAsArgument} \\\n            --output-prefix \"${sampleId}.CELDA__DECONTX\" \\\n            $f\n        \"\"\"\n\n}",
        "nb_lignes_process": 61,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.celda.decontx)\n        processParams = sampleParams.local\n        \n        def filterNumMadsThresholdsAsArguments = ''\n        def filterContaminationScoreThresholdsAsArguments = ''\n\n        if(processParams?.filters) {\n            filterNumMadsThresholdsAsArguments = processParams.filters.containsKey('numMadsThresholds') ?\n                processParams.filters.numMadsThresholds.collect({\n                    '--num-mads-threshold ' + ' ' + it \n                }).join(' ') :\n                ''\n            filterContaminationScoreThresholdsAsArguments = processParams.filters.containsKey('contaminationScoreThresholds') ?\n                processParams.filters.contaminationScoreThresholds.collect({ \n                    '--custom-threshold ' + ' ' + it\n                }).join(' ') :\n                ''\n        }\n\n        def roundToIntAsArgument = ''\n        if(processParams?.roundToInt) {\n            roundToIntAsArgument = '--round-to-int '+ processParams.roundToInt\n        }\n\n        \"\"\"\n        ${binDir}/run_decontx.R \\\n            --sample-id ${sampleId} \\\n            --seed ${params.global.seed} \\\n            ${filterNumMadsThresholdsAsArguments} \\\n            ${filterContaminationScoreThresholdsAsArguments} \\\n            ${roundToIntAsArgument} \\\n            --output-prefix \"${sampleId}.CELDA__DECONTX\" \\\n            $f\n        \"\"\"",
        "nb_lignes_script": 33,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId",
            "sampleId",
            "sampleId"
        ],
        "nb_outputs": 3,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.celda.container",
            "publishDir \"${params.global.outdir}/data/${moduleName}\", mode: 'link'",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__DIM_REDUCTION": {
        "name_process": "SC__SCANPY__DIM_REDUCTION",
        "string_process": "\nprocess SC__SCANPY__DIM_REDUCTION {\n\n\tcontainer params.sc.scanpy.container\n\tpublishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__cpu'\n\n\tinput:\n\t\ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(data), \\\n\t\t\tval(stashedParams), \\\n\t\t\tval(nComps)\n\n\toutput:\n\t\ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(\"${sampleId}.SC__SCANPY__DIM_REDUCTION_${method}.${!isParamNull(stashedParams) ? uuid + '.' : ''}${processParams.off}\"), \\\n\t\t\tval(stashedParams), \\\n\t\t\tval(nComps)\n\n\tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.dim_reduction.get(params.method))\n\t\tprocessParams = sampleParams.local\n\t\t                                                                                                      \n\t\t                                                       \n\t\tif(!isParamNull(stashedParams))\n\t\t\tuuid = stashedParams.findAll { it != 'NULL' }.join('_')\n\t\tmethod = processParams.method.replaceAll('-','').toUpperCase()\n\t\t                                                                                                 \n\t\tdef _processParams = new SC__SCANPY__DIM_REDUCTION_PARAMS()\n\t\t_processParams.setEnv(this)\n\t\t_processParams.setConfigParams(processParams)\n\t\t\"\"\"\n\t\t${binDir}/dim_reduction/sc_dim_reduction.py \\\n\t\t\t--seed ${params.global.seed} \\\n\t\t\t--method ${processParams.method} \\\n\t\t\t${(processParams.containsKey('svdSolver')) ? '--svd-solver ' + processParams.svdSolver : ''} \\\n\t\t\t${(processParams.containsKey('nNeighbors')) ? '--n-neighbors ' + processParams.nNeighbors : ''} \\\n\t\t\t${_processParams.getNCompsAsArgument(nComps)} \\\n\t\t\t${(processParams.containsKey('nPcs')) ? '--n-pcs ' + processParams.nPcs : ''} \\\n            --n-jobs ${task.cpus} \\\n\t\t\t${(processParams.containsKey('useFastTsne')) ? '--use-fast-tsne ' + processParams.useFastTsne : ''} \\\n\t\t\t$data \\\n\t\t\t\"${sampleId}.SC__SCANPY__DIM_REDUCTION_${method}.${!isParamNull(stashedParams) ? uuid + '.' : ''}${processParams.off}\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 46,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.dim_reduction.get(params.method))\n\t\tprocessParams = sampleParams.local\n\t\t                                                                                                      \n\t\t                                                       \n\t\tif(!isParamNull(stashedParams))\n\t\t\tuuid = stashedParams.findAll { it != 'NULL' }.join('_')\n\t\tmethod = processParams.method.replaceAll('-','').toUpperCase()\n\t\t                                                                                                 \n\t\tdef _processParams = new SC__SCANPY__DIM_REDUCTION_PARAMS()\n\t\t_processParams.setEnv(this)\n\t\t_processParams.setConfigParams(processParams)\n\t\t\"\"\"\n\t\t${binDir}/dim_reduction/sc_dim_reduction.py \\\n\t\t\t--seed ${params.global.seed} \\\n\t\t\t--method ${processParams.method} \\\n\t\t\t${(processParams.containsKey('svdSolver')) ? '--svd-solver ' + processParams.svdSolver : ''} \\\n\t\t\t${(processParams.containsKey('nNeighbors')) ? '--n-neighbors ' + processParams.nNeighbors : ''} \\\n\t\t\t${_processParams.getNCompsAsArgument(nComps)} \\\n\t\t\t${(processParams.containsKey('nPcs')) ? '--n-pcs ' + processParams.nPcs : ''} \\\n            --n-jobs ${task.cpus} \\\n\t\t\t${(processParams.containsKey('useFastTsne')) ? '--use-fast-tsne ' + processParams.useFastTsne : ''} \\\n\t\t\t$data \\\n\t\t\t\"${sampleId}.SC__SCANPY__DIM_REDUCTION_${method}.${!isParamNull(stashedParams) ? uuid + '.' : ''}${processParams.off}\"\n\t\t\"\"\"",
        "nb_lignes_script": 23,
        "language_script": "bash",
        "tools": [
            "pcaMethods"
        ],
        "tools_url": [
            "https://bio.tools/pcamethods"
        ],
        "tools_dico": [
            {
                "name": "pcaMethods",
                "uri": "https://bio.tools/pcamethods",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Imputation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Data imputation"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2048",
                                "term": "Report"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2048",
                                "term": "Report"
                            },
                            {
                                "uri": "http://edamontology.org/data_1772",
                                "term": "Score"
                            },
                            {
                                "uri": "http://edamontology.org/data_2884",
                                "term": "Plot"
                            }
                        ]
                    }
                ],
                "description": "This tool provides BPCA, PPCA and NipalsPCA that can be used to perform PCA on incomplete data as well as for accurate missing value estimation. A set of methods for printing and plotting the results is also provided. All PCA methods make use of the same data structure to provide a common interface to the PCA results.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/pcaMethods.html"
            }
        ],
        "inputs": [
            "sampleId",
            "data",
            "stashedParams",
            "nComps"
        ],
        "nb_inputs": 4,
        "outputs": [
            "sampleId",
            "stashedParams",
            "nComps"
        ],
        "nb_outputs": 3,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__cpu'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__DROP_SEQ_TOOLS__FILTER_UNALIGNED_TAGGED_BAM": {
        "name_process": "SC__DROP_SEQ_TOOLS__FILTER_UNALIGNED_TAGGED_BAM",
        "string_process": "\nprocess SC__DROP_SEQ_TOOLS__FILTER_UNALIGNED_TAGGED_BAM {\n\n    container params.sc.dropseqtools.container\n    publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n        tuple val(sample), path(bam)\n\n    output:\n        tuple val(sample), path('*.unaligned_tagged_filtered.bam'), emit: bam\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.filter_unaligned_tagged_bam)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        FilterBAM \\\n            TAG_REJECT=${processParams.tagReject} \\\n            INPUT=${bam} \\\n            OUTPUT=${sample}.unaligned_tagged_filtered.bam\n        \"\"\"\n\n}",
        "nb_lignes_process": 22,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.filter_unaligned_tagged_bam)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        FilterBAM \\\n            TAG_REJECT=${processParams.tagReject} \\\n            INPUT=${bam} \\\n            OUTPUT=${sample}.unaligned_tagged_filtered.bam\n        \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "bam"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.dropseqtools.container",
            "publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'",
            "label 'compute_resources__cpu','compute_resources__24hqueue'"
        ],
        "when": "",
        "stub": ""
    },
    "FORMAT_GTF": {
        "name_process": "FORMAT_GTF",
        "string_process": "\nprocess FORMAT_GTF {\n\n    publishDir \"${params.global.outdir}/00.refdata\", mode: 'symlink'\n    label 'compute_resources__default'\n\n    input:\n        file(annotation)\n\n    output:\n        file \"*.formatted.gtf\"\n\n    script:\n        \"\"\"\n        sed -r 's/(.*); transcript_id (.*); (.*); gene_name (.*); \\$/\\\\1; transcript_id \\\\2; \\\\3; gene_name \\\\4; transcript_name \\\\2;/' \\\n            ${annotation} \\\n            > ${annotation.baseName}.formatted.gtf\n        \"\"\"\n\n}",
        "nb_lignes_process": 18,
        "string_script": "        \"\"\"\n        sed -r 's/(.*); transcript_id (.*); (.*); gene_name (.*); \\$/\\\\1; transcript_id \\\\2; \\\\3; gene_name \\\\4; transcript_name \\\\2;/' \\\n            ${annotation} \\\n            > ${annotation.baseName}.formatted.gtf\n        \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "annotation"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "publishDir \"${params.global.outdir}/00.refdata\", mode: 'symlink'",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "FORMAT_GTF_IGENOMES": {
        "name_process": "FORMAT_GTF_IGENOMES",
        "string_process": "\nprocess FORMAT_GTF_IGENOMES {\n\n    publishDir \"${params.global.outdir}/00.refdata\", mode: 'symlink'\n    label 'compute_resources__default'\n\n    input:\n        file(annotation)\n\n    output:\n        file \"*.formatted.gtf\"\n\n    script:\n        \"\"\"\n        sed -r 's/(.*); gene_name (.*); transcript_id (.*); (.*);\\$/\\\\1; gene_name \\\\2; transcript_id \\\\3; \\\\4; transcript_name \\\\3;/' \\\n            ${annotation} \\\n            > ${annotation.baseName}.formatted.gtf\n        \"\"\"\n\n}",
        "nb_lignes_process": 18,
        "string_script": "        \"\"\"\n        sed -r 's/(.*); gene_name (.*); transcript_id (.*); (.*);\\$/\\\\1; gene_name \\\\2; transcript_id \\\\3; \\\\4; transcript_name \\\\3;/' \\\n            ${annotation} \\\n            > ${annotation.baseName}.formatted.gtf\n        \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "annotation"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "publishDir \"${params.global.outdir}/00.refdata\", mode: 'symlink'",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__NEIGHBORHOOD_GRAPH": {
        "name_process": "SC__SCANPY__NEIGHBORHOOD_GRAPH",
        "string_process": "\nprocess SC__SCANPY__NEIGHBORHOOD_GRAPH {\n\n  \tcontainer params.sc.scanpy.container\n    label 'compute_resources__mem'\n\n  \tinput:\n        tuple \\\n            val(sampleId), \\\n            path(f), \\\n            val(stashedParams), \\\n\t\t\tval(nPcs)\n\n\toutput:\n        tuple \\\n            val(sampleId), \\\n            path(\"${sampleId}.SC__SCANPY__NEIGHBORHOOD_GRAPH.${processParams.off}\"), \\\n            val(stashedParams),\n\t\t\tval(nPcs)\n\n\tscript:\n        def sampleParams = params.parseConfig(\n\t\t\tsampleId,\n\t\t\tparams.global,\n\t\t\tparams.sc.scanpy.neighborhood_graph\n\t\t)\n\t\tprocessParams = sampleParams.local\n                                                                                                              \n\t\t                                                       \n\t\tif(!isParamNull(stashedParams))\n\t\t\tuuid = stashedParams.findAll { it != 'NULL' }.join('_')\n\t\t                                                                                               \n\t\tdef _processParams = new SC__SCANPY__NEIGHBORHOOD_GRAPH_PARAMS()\n\t\t_processParams.setEnv(this)\n\t\t_processParams.setConfigParams(processParams)\n        \"\"\"\n        ${binDir}/nn/sc_neighborhood_graph.py \\\n            $f \\\n            ${sampleId}.SC__SCANPY__NEIGHBORHOOD_GRAPH.${processParams.off} \\\n\t\t\t--seed ${params.global.seed} \\\n            ${(processParams.containsKey('nNeighbors')) ? '--n-neighbors ' + processParams.nNeighbors : ''} \\\n\t\t\t${_processParams.getNPcsAsArgument(nPcs)}\n        \"\"\"\n\n}",
        "nb_lignes_process": 43,
        "string_script": "        def sampleParams = params.parseConfig(\n\t\t\tsampleId,\n\t\t\tparams.global,\n\t\t\tparams.sc.scanpy.neighborhood_graph\n\t\t)\n\t\tprocessParams = sampleParams.local\n                                                                                                              \n\t\t                                                       \n\t\tif(!isParamNull(stashedParams))\n\t\t\tuuid = stashedParams.findAll { it != 'NULL' }.join('_')\n\t\t                                                                                               \n\t\tdef _processParams = new SC__SCANPY__NEIGHBORHOOD_GRAPH_PARAMS()\n\t\t_processParams.setEnv(this)\n\t\t_processParams.setConfigParams(processParams)\n        \"\"\"\n        ${binDir}/nn/sc_neighborhood_graph.py \\\n            $f \\\n            ${sampleId}.SC__SCANPY__NEIGHBORHOOD_GRAPH.${processParams.off} \\\n\t\t\t--seed ${params.global.seed} \\\n            ${(processParams.containsKey('nNeighbors')) ? '--n-neighbors ' + processParams.nNeighbors : ''} \\\n\t\t\t${_processParams.getNPcsAsArgument(nPcs)}\n        \"\"\"",
        "nb_lignes_script": 21,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f",
            "stashedParams",
            "nPcs"
        ],
        "nb_inputs": 4,
        "outputs": [
            "sampleId",
            "nPcs"
        ],
        "nb_outputs": 2,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "CONVERT_MULTI_RUNS_FEATURES_TO_REGULONS": {
        "name_process": "CONVERT_MULTI_RUNS_FEATURES_TO_REGULONS",
        "string_process": "\nprocess CONVERT_MULTI_RUNS_FEATURES_TO_REGULONS {\n\n    cache 'deep'\n    container toolParams.container\n    publishDir \"${toolParams.scenicoutdir}/${sampleId}/multi_runs_cistarget/\", mode: 'link', overwrite: true\n                                                                                                            \n                                                                                                 \n    label 'compute_resources__scenic_multiruns_motifs2regulons'\n\n    input:\n        tuple val(sampleId), path(multiRunsAggrMotifEnrichmentTable), path(multiRunsAggrRegulonsFolder)\n        val type\n\n    output:\n        tuple val(sampleId), path(\"multi_runs_regulons_${type}.pkl.gz\")\n\n    script:\n        \"\"\"\n        ${binDir}convert_multi_runs_features_to_regulons.py \\\n            $multiRunsAggrMotifEnrichmentTable \\\n            $multiRunsAggrRegulonsFolder \\\n            -o \"multi_runs_regulons_${type}.pkl.gz\"\n        \"\"\"\n\n}",
        "nb_lignes_process": 24,
        "string_script": "        \"\"\"\n        ${binDir}convert_multi_runs_features_to_regulons.py \\\n            $multiRunsAggrMotifEnrichmentTable \\\n            $multiRunsAggrRegulonsFolder \\\n            -o \"multi_runs_regulons_${type}.pkl.gz\"\n        \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "multiRunsAggrMotifEnrichmentTable",
            "multiRunsAggrRegulonsFolder",
            "type"
        ],
        "nb_inputs": 4,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "container toolParams.container",
            "publishDir \"${toolParams.scenicoutdir}/${sampleId}/multi_runs_cistarget/\", mode: 'link', overwrite: true",
            "label 'compute_resources__scenic_multiruns_motifs2regulons'"
        ],
        "when": "",
        "stub": ""
    },
    "UTILS__GENERATE_WORKFLOW_CONFIG_REPORT": {
        "name_process": "UTILS__GENERATE_WORKFLOW_CONFIG_REPORT",
        "string_process": "\nprocess UTILS__GENERATE_WORKFLOW_CONFIG_REPORT {\n\n  \tcontainer params.utils.container\n  \tpublishDir \"${params.global.outdir}/notebooks/intermediate\", mode: 'link', overwrite: true\n    label 'compute_resources__report'\n\n    input:\n        path(ipynb)\n\n\toutput:\n\t\tpath(\"workflow_configuration_report.ipynb\")\n\n\tscript:\n\t\t\"\"\"\n\t\tpapermill ${ipynb} \\\n\t\t\tworkflow_configuration_report.ipynb \\\n\t\t\t-p WORKFLOW_MANIFEST '${params.misc.manifestAsJSON}' \\\n\t\t\t-p WORKFLOW_PARAMETERS '${params.misc.paramsAsJSON}'\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 20,
        "string_script": "\t\t\"\"\"\n\t\tpapermill ${ipynb} \\\n\t\t\tworkflow_configuration_report.ipynb \\\n\t\t\t-p WORKFLOW_MANIFEST '${params.misc.manifestAsJSON}' \\\n\t\t\t-p WORKFLOW_PARAMETERS '${params.misc.paramsAsJSON}'\n\t\t\"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ipynb"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.utils.container",
            "publishDir \"${params.global.outdir}/notebooks/intermediate\", mode: 'link', overwrite: true",
            "label 'compute_resources__report'"
        ],
        "when": "",
        "stub": ""
    },
    "UTILS__REPORT_TO_HTML": {
        "name_process": "UTILS__REPORT_TO_HTML",
        "string_process": "\nprocess UTILS__REPORT_TO_HTML {\n\n\tcontainer params.utils.container\n\tpublishDir \"${params.global.outdir}/notebooks/intermediate\", mode: 'link', overwrite: true\n\t                                               \n\tpublishDir \"${params.global.outdir}/notebooks\", pattern: '*merged_report*', mode: 'link', overwrite: true\n\tlabel 'compute_resources__report'\n\n\tinput:\n\t\ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(ipynb)\n\n\toutput:\n\t\tfile(\"*.html\")\n\n\tscript:\n\t\t\"\"\"\n\t\tjupyter nbconvert ${ipynb} --to html\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 21,
        "string_script": "\t\t\"\"\"\n\t\tjupyter nbconvert ${ipynb} --to html\n\t\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "Jupyterhub"
        ],
        "tools_url": [
            "https://bio.tools/Jupyterhub"
        ],
        "tools_dico": [
            {
                "name": "Jupyterhub",
                "uri": "https://bio.tools/Jupyterhub",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Workflows"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3372",
                            "term": "Software engineering"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Pipelines"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3372",
                            "term": "Computer programming"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3372",
                            "term": "Software development"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Jupyter notebooks in science gateways.\n\nJupyter Notebooks empower scientists to create executable documents that include text, equations, code and figures. Notebooks are a simple way to create reproducible and shareable workflows. The Jupyter developers have also released a multi-user notebook environment: Jupyterhub. Jupyterhub provides an extensible platform for handling user authentication and spawning the Notebook application to each user. I developed a plugin for Jupyterhub to spawn notebooks on a Supercomputer and integrated the authentication with CILogon and XSEDE. Scientists can authenticate on their browser and connect to a Jupyter Notebook instance running on the computing node of a Supercomputer, in my test deployment SDSC Comet. Jupyterhub can benefit Science Gateways by providing an expressive interface to a centralized environment with many software tools pre-installed and allow scientists to access Gateway functionality via web API.\n\n||| HOMEPAGE MISSING!",
                "homepage": "https://doi.org/10.7287/PEERJ.PREPRINTS.2577V2"
            }
        ],
        "inputs": [
            "sampleId",
            "ipynb"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.utils.container",
            "publishDir \"${params.global.outdir}/notebooks/intermediate\", mode: 'link', overwrite: true",
            "publishDir \"${params.global.outdir}/notebooks\", pattern: '*merged_report*', mode: 'link', overwrite: true",
            "label 'compute_resources__report'"
        ],
        "when": "",
        "stub": ""
    },
    "ARBORETO_WITH_MULTIPROCESSING": {
        "name_process": "ARBORETO_WITH_MULTIPROCESSING",
        "string_process": "\nprocess ARBORETO_WITH_MULTIPROCESSING {\n\n    cache 'deep'\n    container toolParams.container\n    publishDir \"${toolParams.scenicoutdir}/${sampleId}/arboreto_with_multiprocessing/${\"numRuns\" in toolParams && toolParams.numRuns > 1 ? \"run_\" + runId : \"\"}\", mode: 'link', overwrite: true\n    label 'compute_resources__scenic_grn'\n\n    input:\n        tuple \\\n            val(sampleId), \\\n            path(filteredLoom), \\\n            val(runId)\n        file tfs\n\n    output:\n        tuple val(sampleId), \\\n        file(filteredLoom), \\\n        file(\"${outputFileName}\"), \\\n        val(runId)\n\n    script:\n        if(toolParams.numRuns > 2 && task.maxForks > 1 && task.executor == \"local\")\n            throw new Exception(\"Running multi-runs SCENIC is quite computationally extensive. Please submit it as a job instead.\")\n        outputFileName = \"numRuns\" in toolParams && toolParams.numRuns > 1 ? sampleId + \"__run_\" + runId +\"__adj.tsv\" : sampleId + \"__adj.tsv\"\n        seed = \"numRuns\" in toolParams && toolParams.numRuns > 1 ? (params.global.seed + runId) : params.global.seed\n        \"\"\"\n        ${binDir}arboreto_with_multiprocessing.py \\\n            $filteredLoom \\\n            $tfs \\\n            --output ${outputFileName} \\\n            --num_workers ${task.cpus} \\\n            --cell_id_attribute ${toolParams.cell_id_attribute} \\\n            --gene_attribute ${toolParams.gene_attribute} \\\n            --method ${processParams.algorithm} \\\n            --seed ${seed}\n        \"\"\"\n\n}",
        "nb_lignes_process": 37,
        "string_script": "        if(toolParams.numRuns > 2 && task.maxForks > 1 && task.executor == \"local\")\n            throw new Exception(\"Running multi-runs SCENIC is quite computationally extensive. Please submit it as a job instead.\")\n        outputFileName = \"numRuns\" in toolParams && toolParams.numRuns > 1 ? sampleId + \"__run_\" + runId +\"__adj.tsv\" : sampleId + \"__adj.tsv\"\n        seed = \"numRuns\" in toolParams && toolParams.numRuns > 1 ? (params.global.seed + runId) : params.global.seed\n        \"\"\"\n        ${binDir}arboreto_with_multiprocessing.py \\\n            $filteredLoom \\\n            $tfs \\\n            --output ${outputFileName} \\\n            --num_workers ${task.cpus} \\\n            --cell_id_attribute ${toolParams.cell_id_attribute} \\\n            --gene_attribute ${toolParams.gene_attribute} \\\n            --method ${processParams.algorithm} \\\n            --seed ${seed}\n        \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [
            "Seed"
        ],
        "tools_url": [
            "https://bio.tools/seed-eco"
        ],
        "tools_dico": [
            {
                "name": "Seed",
                "uri": "https://bio.tools/seed-eco",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0610",
                            "term": "Ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3365",
                            "term": "Data architecture, analysis and design"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An R/Shiny package for visualizing ecological data. It provides a visual interface for generating a wide variety of plots, including histograms, scatterplots, bar plots, stacked bar plots, PCoA plots, cluster dendrograms, and heatmaps.",
                "homepage": "https://github.com/danlbek/Seed"
            }
        ],
        "inputs": [
            "sampleId",
            "filteredLoom",
            "runId",
            "tfs"
        ],
        "nb_inputs": 4,
        "outputs": [
            "sampleId",
            "filteredLoom",
            "runId"
        ],
        "nb_outputs": 3,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "container toolParams.container",
            "publishDir \"${toolParams.scenicoutdir}/${sampleId}/arboreto_with_multiprocessing/${\"numRuns\" in toolParams && toolParams.numRuns > 1 ? \"run_\" + runId : \"\"}\", mode: 'link', overwrite: true",
            "label 'compute_resources__scenic_grn'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__DROPLET_UTILS__BARCODE_SELECTION": {
        "name_process": "SC__DROPLET_UTILS__BARCODE_SELECTION",
        "string_process": "\nprocess SC__DROPLET_UTILS__BARCODE_SELECTION {\n    \n    container params.sc.dropletutils.container\n    publishDir \"03.count\", mode: 'symlink'\n    label 'compute_resources__default'\n\n    input:\n        tuple val(sample), path(readCounts)\n    \n    output:\n        tuple val(sample), val(\"knee\"), path(\"*.selected_cell_barcodes_by_knee.txt\"), emit: selectedCellBarcodesByKnee\n        tuple val(sample), val(\"inflection\"), path(\"*.selected_cell_barcodes_by_inflection.txt\"), emit: selectedCellBarcodesByInflection\n        tuple file(\"*.barcode_rank_vs_total_umi_plot.png\"), emit: plot\n    \n    script:\n        \"\"\"\n        Rscript ${workflow.projectDir}/lib/modules/dropletutils/bin/barcode_selection.R \\\n            ${readCounts} \\\n            ${sample}\n        \"\"\"\n\n}",
        "nb_lignes_process": 21,
        "string_script": "        \"\"\"\n        Rscript ${workflow.projectDir}/lib/modules/dropletutils/bin/barcode_selection.R \\\n            ${readCounts} \\\n            ${sample}\n        \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "readCounts"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.dropletutils.container",
            "publishDir \"03.count\", mode: 'symlink'",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__H5AD_UPDATE_X_PCA": {
        "name_process": "SC__H5AD_UPDATE_X_PCA",
        "string_process": "\nprocess SC__H5AD_UPDATE_X_PCA {\n\n\tcontainer params.sc.scanpy.container\n    label 'compute_resources__mem'\n\n\tinput:\n\t\ttuple \\\n            val(sampleId), \\\n\t\t    path(data), \\\n            path(xPca)\n\n\toutput:\n\t\ttuple \\\n            val(sampleId), \\\n\t\t    path(\"${sampleId}.SC__H5AD_UPDATE_X_PCA.h5ad\")\n\n\tscript:\n\t\t\"\"\"\n\t\t${binDir}/sc_h5ad_update.py \\\n\t\t\t--x-pca ${xPca} \\\n\t\t\t$data \\\n\t\t\t\"${sampleId}.SC__H5AD_UPDATE_X_PCA.h5ad\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 24,
        "string_script": "\t\t\"\"\"\n\t\t${binDir}/sc_h5ad_update.py \\\n\t\t\t--x-pca ${xPca} \\\n\t\t\t$data \\\n\t\t\t\"${sampleId}.SC__H5AD_UPDATE_X_PCA.h5ad\"\n\t\t\"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "data",
            "xPca"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__H5AD_CLEAN": {
        "name_process": "SC__H5AD_CLEAN",
        "string_process": "\nprocess SC__H5AD_CLEAN {\n\n\tcontainer params.sc.scanpy.container\n    label 'compute_resources__mem'\n\n\tinput:\n\t\ttuple \\\n            val(sampleId), \\\n\t\t    path(data), \\\n\t\t\tval(stashedParams)\n\n\toutput:\n\t\ttuple \\\n            val(sampleId), \\\n\t\t    path(\"${sampleId}.SC__H5AD_CLEAN.h5ad\"), \\\n\t\t\tval(stashedParams)\n\n\tscript:\n\t\t\"\"\"\n\t\t${binDir}/sc_h5ad_update.py \\\n\t\t\t--empty-x \\\n\t\t\t$data \\\n\t\t\t\"${sampleId}.SC__H5AD_CLEAN.h5ad\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 25,
        "string_script": "\t\t\"\"\"\n\t\t${binDir}/sc_h5ad_update.py \\\n\t\t\t--empty-x \\\n\t\t\t$data \\\n\t\t\t\"${sampleId}.SC__H5AD_CLEAN.h5ad\"\n\t\t\"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "data",
            "stashedParams"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId",
            "stashedParams"
        ],
        "nb_outputs": 2,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__H5AD_BEAUTIFY": {
        "name_process": "SC__H5AD_BEAUTIFY",
        "string_process": "\nprocess SC__H5AD_BEAUTIFY {\n\n\tcontainer params.sc.scanpy.container\n\tpublishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n\tinput:\n\t\ttuple \\\n            val(sampleId), \\\n\t\t    path(data), \\\n\t\t\tval(stashedParams)\n\n\toutput:\n\t\ttuple \\\n            val(sampleId), \\\n\t\t    path(\"${sampleId}.SC__H5AD_BEAUTIFY.h5ad\"), \\\n\t\t\tval(stashedParams)\n\n\tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.file_cleaner)\n        processParams = sampleParams.local\n\n\t\tobsColumnsToRemoveAsArgument = processParams.containsKey(\"obsColumnsToRemove\") ? \n\t\t\tprocessParams.obsColumnsToRemove.collect({ '--obs-column-to-remove' + ' ' + it }).join(' ') : \n\t\t\t''\n\t\t\"\"\"\n\t\t${binDir}/sc_h5ad_update.py \\\n\t\t\t${obsColumnsToRemoveAsArgument} \\\n\t\t\t${processParams.containsKey(\"obsColumnMapper\") ? \"--obs-column-mapper '\" + toJson(processParams.obsColumnMapper) + \"'\": ''} \\\n\t\t\t${processParams.containsKey(\"obsColumnValueMapper\") ? \"--obs-column-value-mapper '\" + toJson(processParams.obsColumnValueMapper) + \"'\": ''} \\\n\t\t\t$data \\\n\t\t\t\"${sampleId}.SC__H5AD_BEAUTIFY.h5ad\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 34,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.file_cleaner)\n        processParams = sampleParams.local\n\n\t\tobsColumnsToRemoveAsArgument = processParams.containsKey(\"obsColumnsToRemove\") ? \n\t\t\tprocessParams.obsColumnsToRemove.collect({ '--obs-column-to-remove' + ' ' + it }).join(' ') : \n\t\t\t''\n\t\t\"\"\"\n\t\t${binDir}/sc_h5ad_update.py \\\n\t\t\t${obsColumnsToRemoveAsArgument} \\\n\t\t\t${processParams.containsKey(\"obsColumnMapper\") ? \"--obs-column-mapper '\" + toJson(processParams.obsColumnMapper) + \"'\": ''} \\\n\t\t\t${processParams.containsKey(\"obsColumnValueMapper\") ? \"--obs-column-value-mapper '\" + toJson(processParams.obsColumnValueMapper) + \"'\": ''} \\\n\t\t\t$data \\\n\t\t\t\"${sampleId}.SC__H5AD_BEAUTIFY.h5ad\"\n\t\t\"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "data",
            "stashedParams"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId",
            "stashedParams"
        ],
        "nb_outputs": 2,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__CELLRANGER__MKFASTQ": {
        "name_process": "SC__CELLRANGER__MKFASTQ",
        "string_process": "\nprocess SC__CELLRANGER__MKFASTQ {\n\n\tpublishDir \"${params.global.outdir}/fastqs\", saveAs: { outputF = file(it); \"${outputF.getParent().getName()}/${outputF.name}\" }, mode: 'link', overwrite: true\n  \tcontainer toolParams.container\n    label 'compute_resources__cellranger_mkfastq'\n\n  \tinput:\n\t\tfile(csv)\n    \tfile(runFolder)\n\n  \toutput:\n    \tpath \"*/outs/fastq_path/*/*/*.fastq.gz\"\n\n  \tscript:\n\t\t\"\"\"\n\t\tcellranger mkfastq \\\n\t\t\t--run=${runFolder} \\\n\t\t\t--csv=${csv} \\\n\t\t\t${(toolParams.mkfastq.containsKey('runID')) ? '--id ' + toolParams.mkfastq.runID: params.global.containsKey('project_name') ? '--id ' + params.global.project_name: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('samplesheet')) ? '--samplesheet ' + toolParams.mkfastq.samplesheet: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('ignoreDualIndex')) ? '--ignore-dual-index ' + toolParams.mkfastq.ignoreDualIndex: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('qc')) ? '--qc ' + toolParams.mkfastq.qc: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('lanes')) ? '--lanes ' + toolParams.mkfastq.lanes: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('useBasesMask')) ? '--use-bases-mask ' + toolParams.mkfastq.useBasesMask: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('deleteUndetermined')) ? '--delete-undetermined ' + toolParams.mkfastq.deleteUndetermined: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('outputDir')) ? '--output-dir ' + toolParams.mkfastq.outputDir: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('project')) ? '--project ' + toolParams.mkfastq.project: ''} \\\n            --jobmode=local \\\n            --localcores=${task.cpus} \\\n            --localmem=${task.memory.toGiga()}\n\t\t\"\"\"\n\n\n}",
        "nb_lignes_process": 33,
        "string_script": "\t\t\"\"\"\n\t\tcellranger mkfastq \\\n\t\t\t--run=${runFolder} \\\n\t\t\t--csv=${csv} \\\n\t\t\t${(toolParams.mkfastq.containsKey('runID')) ? '--id ' + toolParams.mkfastq.runID: params.global.containsKey('project_name') ? '--id ' + params.global.project_name: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('samplesheet')) ? '--samplesheet ' + toolParams.mkfastq.samplesheet: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('ignoreDualIndex')) ? '--ignore-dual-index ' + toolParams.mkfastq.ignoreDualIndex: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('qc')) ? '--qc ' + toolParams.mkfastq.qc: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('lanes')) ? '--lanes ' + toolParams.mkfastq.lanes: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('useBasesMask')) ? '--use-bases-mask ' + toolParams.mkfastq.useBasesMask: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('deleteUndetermined')) ? '--delete-undetermined ' + toolParams.mkfastq.deleteUndetermined: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('outputDir')) ? '--output-dir ' + toolParams.mkfastq.outputDir: ''} \\\n\t\t\t${(toolParams.mkfastq.containsKey('project')) ? '--project ' + toolParams.mkfastq.project: ''} \\\n            --jobmode=local \\\n            --localcores=${task.cpus} \\\n            --localmem=${task.memory.toGiga()}\n\t\t\"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "csv",
            "runFolder"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "publishDir \"${params.global.outdir}/fastqs\", saveAs: { outputF = file(it); \"${outputF.getParent().getName()}/${outputF.name}\" }, mode: 'link', overwrite: true",
            "container toolParams.container",
            "label 'compute_resources__cellranger_mkfastq'"
        ],
        "when": "",
        "stub": ""
    },
    "FASTP__CLEAN_AND_FASTQC": {
        "name_process": "FASTP__CLEAN_AND_FASTQC",
        "string_process": "\nprocess FASTP__CLEAN_AND_FASTQC {\n\n    container params.fastp.container\n    publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n        set val(sample), path(reads)\n    \n    output:\n        tuple file('*_R{1,2}.clean.fastq.gz'), emit: fastq\n        tuple file('*_fastp.{json,html}'), emit: report\n    \n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.fastp)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        fastp --thread ${processParams.thread} \\\n            -i ${reads[0]} \\\n            -I ${reads[1]} \\\n            -o ${sample}_R1.clean.fastq.gz \\\n            -O ${sample}_R2.clean.fastq.gz \\\n            --length_required ${processParams.clean_and_fastqc.length_required} \\\n            --adapter_fasta ${processParams.clean_and_fastqc.adapter_fasta} \\\n            -j ${sample}_fastp.json \\\n            -h ${sample}_fastp.html\n        \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.fastp)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        fastp --thread ${processParams.thread} \\\n            -i ${reads[0]} \\\n            -I ${reads[1]} \\\n            -o ${sample}_R1.clean.fastq.gz \\\n            -O ${sample}_R2.clean.fastq.gz \\\n            --length_required ${processParams.clean_and_fastqc.length_required} \\\n            --adapter_fasta ${processParams.clean_and_fastqc.adapter_fasta} \\\n            -j ${sample}_fastp.json \\\n            -h ${sample}_fastp.html\n        \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "fastPHASE"
        ],
        "tools_url": [
            "https://bio.tools/fastphase"
        ],
        "tools_dico": [
            {
                "name": "fastPHASE",
                "uri": "https://bio.tools/fastphase",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3056",
                            "term": "Population genetics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3454",
                                    "term": "Phasing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Imputation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Data imputation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "fastPHASE is a program to estimate missing genotypes and unobserved haplotypes. It is an implementation of the model described in Scheet & Stephens (2006). This is a cluster-based model for haplotype variation, and gains its utility from implicitly modeling the genealogy of chromosomes in a random sample from a population as a tree but summarizing all haplotype variation in the \"tips\" of the trees.",
                "homepage": "http://scheet.org/software.html"
            }
        ],
        "inputs": [
            "sample",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.fastp.container",
            "publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'",
            "label 'compute_resources__cpu','compute_resources__24hqueue'"
        ],
        "when": "",
        "stub": ""
    },
    "GENERATE_REPORT": {
        "name_process": "GENERATE_REPORT",
        "string_process": "\nprocess GENERATE_REPORT {\n\n\tcontainer toolParams.container\n\tpublishDir \"${toolParams.scenicoutdir}/${sampleId}/notebooks\", mode: 'link', overwrite: true\n    label 'compute_resources__report'\n\n\tinput:\n\t\tfile ipynb\n\t\ttuple val(sampleId), path(loom)\n\t\tval reportTitle\n\n\toutput:\n\t\ttuple val(sampleId), path(\"${reportTitle}.ipynb\")\n\n\tscript:\n\t\t\"\"\"\n\t\tpapermill ${ipynb} \\\n\t\t\t${reportTitle}.ipynb \\\n\t\t\t-p FILE $loom\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 21,
        "string_script": "\t\t\"\"\"\n\t\tpapermill ${ipynb} \\\n\t\t\t${reportTitle}.ipynb \\\n\t\t\t-p FILE $loom\n\t\t\"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ipynb",
            "sampleId",
            "loom",
            "reportTitle"
        ],
        "nb_inputs": 4,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container toolParams.container",
            "publishDir \"${toolParams.scenicoutdir}/${sampleId}/notebooks\", mode: 'link', overwrite: true",
            "label 'compute_resources__report'"
        ],
        "when": "",
        "stub": ""
    },
    "REPORT_TO_HTML": {
        "name_process": "REPORT_TO_HTML",
        "string_process": "\nprocess REPORT_TO_HTML {\n\n\tcontainer toolParams.container\n\tpublishDir \"${toolParams.scenicoutdir}/${sampleId}/notebooks\", mode: 'link', overwrite: true\n    label 'compute_resources__report'\n\n\tinput:\n\t\ttuple val(sampleId), path(ipynb)\n\n\toutput:\n\t\ttuple val(sampleId), path(\"*.html\")\n\n\tscript:\n\t\t\"\"\"\n\t\tjupyter nbconvert ${ipynb} --to html\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 17,
        "string_script": "\t\t\"\"\"\n\t\tjupyter nbconvert ${ipynb} --to html\n\t\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "Jupyterhub"
        ],
        "tools_url": [
            "https://bio.tools/Jupyterhub"
        ],
        "tools_dico": [
            {
                "name": "Jupyterhub",
                "uri": "https://bio.tools/Jupyterhub",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Workflows"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3372",
                            "term": "Software engineering"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Pipelines"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3372",
                            "term": "Computer programming"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3372",
                            "term": "Software development"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Jupyter notebooks in science gateways.\n\nJupyter Notebooks empower scientists to create executable documents that include text, equations, code and figures. Notebooks are a simple way to create reproducible and shareable workflows. The Jupyter developers have also released a multi-user notebook environment: Jupyterhub. Jupyterhub provides an extensible platform for handling user authentication and spawning the Notebook application to each user. I developed a plugin for Jupyterhub to spawn notebooks on a Supercomputer and integrated the authentication with CILogon and XSEDE. Scientists can authenticate on their browser and connect to a Jupyter Notebook instance running on the computing node of a Supercomputer, in my test deployment SDSC Comet. Jupyterhub can benefit Science Gateways by providing an expressive interface to a centralized environment with many software tools pre-installed and allow scientists to access Gateway functionality via web API.\n\n||| HOMEPAGE MISSING!",
                "homepage": "https://doi.org/10.7287/PEERJ.PREPRINTS.2577V2"
            }
        ],
        "inputs": [
            "sampleId",
            "ipynb"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container toolParams.container",
            "publishDir \"${toolParams.scenicoutdir}/${sampleId}/notebooks\", mode: 'link', overwrite: true",
            "label 'compute_resources__report'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__DROP_SEQ_TOOLS__CONVERT_TO_REFFLAT": {
        "name_process": "SC__DROP_SEQ_TOOLS__CONVERT_TO_REFFLAT",
        "string_process": "\nprocess SC__DROP_SEQ_TOOLS__CONVERT_TO_REFFLAT {\n    \n    container params.sc.dropseqtools.container\n    publishDir \"${params.global.outdir}/00.refdata\", mode: 'symlink'\n    label 'compute_resources__default'\n\n    input:\n        file(annotation)\n        file(seqdict)\n    \n    output:\n        file(\"${seqdict.baseName}.refFlat\")\n    \n    script:\n        \"\"\"\n        ConvertToRefFlat \\\n            ANNOTATIONS_FILE=${annotation} \\\n            SEQUENCE_DICTIONARY=${seqdict} \\\n            OUTPUT=${seqdict.baseName}.refFlat\n        \"\"\"\n\n}",
        "nb_lignes_process": 21,
        "string_script": "        \"\"\"\n        ConvertToRefFlat \\\n            ANNOTATIONS_FILE=${annotation} \\\n            SEQUENCE_DICTIONARY=${seqdict} \\\n            OUTPUT=${seqdict.baseName}.refFlat\n        \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "annotation",
            "seqdict"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.dropseqtools.container",
            "publishDir \"${params.global.outdir}/00.refdata\", mode: 'symlink'",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__UTILS__EXTRACT_FEATURE_METADATA": {
        "name_process": "SC__UTILS__EXTRACT_FEATURE_METADATA",
        "string_process": "\nprocess SC__UTILS__EXTRACT_FEATURE_METADATA {\n\n    container params.sc.scanpy.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'link', overwrite: true\n    label 'compute_resources__default'\n\n    input:\n        tuple val(sampleId), path(f)\n\n    output:\n        tuple val(sampleId), path(\"${sampleId}.SC__UTILS__EXTRACT_FEATURE_METADATA.tsv\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.utils.extract_feature_metadata)\n\t\tprocessParams = sampleParams.local\n        columnNamesAsArguments = processParams.columnNames.collect({ '--column-name' + ' ' + it }).join(' ')\n        \"\"\"\n        ${binDir}/sc_h5ad_extract_metadata.py \\\n            --axis feature \\\n            ${columnNamesAsArguments} \\\n            $f \\\n            \"${sampleId}.SC__UTILS__EXTRACT_FEATURE_METADATA.tsv\"\n        \"\"\"\n\n}",
        "nb_lignes_process": 24,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.utils.extract_feature_metadata)\n\t\tprocessParams = sampleParams.local\n        columnNamesAsArguments = processParams.columnNames.collect({ '--column-name' + ' ' + it }).join(' ')\n        \"\"\"\n        ${binDir}/sc_h5ad_extract_metadata.py \\\n            --axis feature \\\n            ${columnNamesAsArguments} \\\n            $f \\\n            \"${sampleId}.SC__UTILS__EXTRACT_FEATURE_METADATA.tsv\"\n        \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'link', overwrite: true",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SOUPX": {
        "name_process": "SC__SOUPX",
        "string_process": "\nprocess SC__SOUPX {\n    \n    container toolParams.container\n    publishDir \"${params.global.outdir}/data/${moduleName}\", mode: 'link'\n    label 'compute_resources__default'\n\n    input:\n        tuple \\\n            val(sampleId), \\\n            path(f)\n\n    output:\n        tuple \\\n            val(sampleId), \\\n            path(\"${sampleId}.SC__SOUPX.Rds\"), \\\n            emit: main\n        tuple \\\n            val(sampleId), \\\n            path(\"${sampleId}.SC__SOUPX.*.pdf\"), \\\n            emit: other\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, toolParams)\n        processParams = sampleParams.local\n\n        def roundToIntAsArgument = ''\n        if(processParams?.roundToInt) {\n            roundToIntAsArgument = '--round-to-int '+ processParams.roundToInt\n        }\n\n        \"\"\"\n        export NXF_BIN_DIR=$binDir\n        ${binDir}/run_soupx.R \\\n            --sample-id ${sampleId} \\\n            --seed ${params.global.seed} \\\n            ${roundToIntAsArgument} \\\n            --output-prefix \"${sampleId}.SC__SOUPX\" \\\n            $f\n        \"\"\"\n\n}",
        "nb_lignes_process": 40,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, toolParams)\n        processParams = sampleParams.local\n\n        def roundToIntAsArgument = ''\n        if(processParams?.roundToInt) {\n            roundToIntAsArgument = '--round-to-int '+ processParams.roundToInt\n        }\n\n        \"\"\"\n        export NXF_BIN_DIR=$binDir\n        ${binDir}/run_soupx.R \\\n            --sample-id ${sampleId} \\\n            --seed ${params.global.seed} \\\n            ${roundToIntAsArgument} \\\n            --output-prefix \"${sampleId}.SC__SOUPX\" \\\n            $f\n        \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId",
            "sampleId"
        ],
        "nb_outputs": 2,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container toolParams.container",
            "publishDir \"${params.global.outdir}/data/${moduleName}\", mode: 'link'",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCRUBLET__DOUBLET_DETECTION": {
        "name_process": "SC__SCRUBLET__DOUBLET_DETECTION",
        "string_process": "\nprocess SC__SCRUBLET__DOUBLET_DETECTION {\n\n\tcontainer params.sc.scrublet.container\n\tpublishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n\tinput:\n\t\ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(adataRaw), \\\n            path(adataWithHvgInfo), \\\n\t\t\tval(stashedParams), \\\n\t\t\tval(nPrinComps)\n\n\toutput:\n\t\ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(\"${sampleId}.SC__SCRUBLET__DOUBLET_DETECTION.ScrubletDoubletTable.tsv\"), \\\n\t\t\tpath(\"${sampleId}.SC__SCRUBLET__DOUBLET_DETECTION.ScrubletObject.pklz\"), \\\n\t\t\tval(stashedParams), \\\n\t\t\tval(nPrinComps)\n\n\tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scrublet.doublet_detection)\n\t\tprocessParams = sampleParams.local\n\t\tdef _processParams = new SC__SCRUBLET__DOUBLET_DETECTION_PARAMS()\n\t\t_processParams.setEnv(this)\n\t\t_processParams.setConfigParams(processParams)\n\t\t\"\"\"\n\t\t${binDir}/sc_doublet_detection.py \\\n\t\t\t${(processParams.containsKey('useVariableFeatures')) ? '--use-variable-features ' + processParams.useVariableFeatures : ''} \\\n            ${(processParams.containsKey('syntheticDoubletUmiSubsampling')) ? '--synthetic-doublet-umi-subsampling ' + processParams.syntheticDoubletUmiSubsampling : ''} \\\n            ${(processParams.containsKey('minCounts')) ? '--min-counts ' + processParams.minCounts : ''} \\\n            ${(processParams.containsKey('minCells')) ? '--min-cells ' + processParams.minCells : ''} \\\n            ${(processParams.containsKey('minGeneVariabilityPctl')) ? '--min-gene-variability-pctl ' + processParams.minGeneVariabilityPctl : ''} \\\n            ${(processParams.containsKey('logTransform')) ? '--log-transform ' + processParams.logTransform : ''} \\\n            ${(processParams.containsKey('meanCenter')) ? '--mean-center ' + processParams.meanCenter : ''} \\\n            ${(processParams.containsKey('normalizeVariance')) ? '--normalize-variance ' + processParams.normalizeVariance : ''} \\\n            ${_processParams.getNPrinCompsAsArgument(nPrinComps)} \\\n            ${(processParams.containsKey('technology')) ? '--technology ' + processParams.technology : ''} \\\n\t\t\t${(processParams.containsKey('useVariableFeatures')) ? '--h5ad-with-variable-features-info ' + adataWithHvgInfo : ''} \\\n\t\t\t--output-prefix \"${sampleId}.SC__SCRUBLET__DOUBLET_DETECTION\" \\\n\t\t\t$adataRaw\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 45,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scrublet.doublet_detection)\n\t\tprocessParams = sampleParams.local\n\t\tdef _processParams = new SC__SCRUBLET__DOUBLET_DETECTION_PARAMS()\n\t\t_processParams.setEnv(this)\n\t\t_processParams.setConfigParams(processParams)\n\t\t\"\"\"\n\t\t${binDir}/sc_doublet_detection.py \\\n\t\t\t${(processParams.containsKey('useVariableFeatures')) ? '--use-variable-features ' + processParams.useVariableFeatures : ''} \\\n            ${(processParams.containsKey('syntheticDoubletUmiSubsampling')) ? '--synthetic-doublet-umi-subsampling ' + processParams.syntheticDoubletUmiSubsampling : ''} \\\n            ${(processParams.containsKey('minCounts')) ? '--min-counts ' + processParams.minCounts : ''} \\\n            ${(processParams.containsKey('minCells')) ? '--min-cells ' + processParams.minCells : ''} \\\n            ${(processParams.containsKey('minGeneVariabilityPctl')) ? '--min-gene-variability-pctl ' + processParams.minGeneVariabilityPctl : ''} \\\n            ${(processParams.containsKey('logTransform')) ? '--log-transform ' + processParams.logTransform : ''} \\\n            ${(processParams.containsKey('meanCenter')) ? '--mean-center ' + processParams.meanCenter : ''} \\\n            ${(processParams.containsKey('normalizeVariance')) ? '--normalize-variance ' + processParams.normalizeVariance : ''} \\\n            ${_processParams.getNPrinCompsAsArgument(nPrinComps)} \\\n            ${(processParams.containsKey('technology')) ? '--technology ' + processParams.technology : ''} \\\n\t\t\t${(processParams.containsKey('useVariableFeatures')) ? '--h5ad-with-variable-features-info ' + adataWithHvgInfo : ''} \\\n\t\t\t--output-prefix \"${sampleId}.SC__SCRUBLET__DOUBLET_DETECTION\" \\\n\t\t\t$adataRaw\n\t\t\"\"\"",
        "nb_lignes_script": 20,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "adataRaw",
            "adataWithHvgInfo",
            "stashedParams",
            "nPrinComps"
        ],
        "nb_inputs": 5,
        "outputs": [
            "sampleId",
            "stashedParams",
            "nPrinComps"
        ],
        "nb_outputs": 3,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scrublet.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "PICARD__MERGE_BAM_ALIGNMENT": {
        "name_process": "PICARD__MERGE_BAM_ALIGNMENT",
        "string_process": "\nprocess PICARD__MERGE_BAM_ALIGNMENT {\n\n    container params.picard.container\n    publishDir \"${params.global.outdir}/02.map\", mode: 'symlink'\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n        tuple val(sample), path(unmappedBam)\n        tuple val(sample), path(mappedBam)\n        file(genome)\n        file(dict)\n        file(tmpDir)\n\n    output:\n        tuple val(sample), path(\"*.merged.bam\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.picard.merge_bam_alignment)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        java -Djava.io.tmpdir=$tmpDir -jar \\\n            /picard.jar \\\n                MergeBamAlignment \\\n                    REFERENCE_SEQUENCE=${genome} \\\n                    UNMAPPED_BAM=${unmappedBam} \\\n                    ALIGNED_BAM=${mappedBam} \\\n                    OUTPUT=${sample}.merged.bam \\\n                    INCLUDE_SECONDARY_ALIGNMENTS=${processParams.includeSecondaryAlignments} \\\n                    PAIRED_RUN=${processParams.pairedRun}\n        \"\"\"\n\n}",
        "nb_lignes_process": 31,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.picard.merge_bam_alignment)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        java -Djava.io.tmpdir=$tmpDir -jar \\\n            /picard.jar \\\n                MergeBamAlignment \\\n                    REFERENCE_SEQUENCE=${genome} \\\n                    UNMAPPED_BAM=${unmappedBam} \\\n                    ALIGNED_BAM=${mappedBam} \\\n                    OUTPUT=${sample}.merged.bam \\\n                    INCLUDE_SECONDARY_ALIGNMENTS=${processParams.includeSecondaryAlignments} \\\n                    PAIRED_RUN=${processParams.pairedRun}\n        \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "unmappedBam",
            "sample",
            "mappedBam",
            "genome",
            "dict",
            "tmpDir"
        ],
        "nb_inputs": 7,
        "outputs": [
            "sample"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.picard.container",
            "publishDir \"${params.global.outdir}/02.map\", mode: 'symlink'",
            "label 'compute_resources__cpu','compute_resources__24hqueue'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__STAR__LOAD_GENOME": {
        "name_process": "SC__STAR__LOAD_GENOME",
        "string_process": "\nprocess SC__STAR__LOAD_GENOME {\n\n  \tcontainer params.sc.star.container\n    label 'compute_resources__default'\n\n\tinput:\n\t\tfile(starIndex)\n\n\toutput:\n\t\tval starIndexLoaded \n\n\tscript:\n\t\tstarIndexLoaded = true\n\t\t\"\"\"\n\t\tSTAR \\\n\t\t\t--genomeLoad LoadAndExit \\\n\t\t\t--genomeDir ${starIndex}\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 19,
        "string_script": "\t\tstarIndexLoaded = true\n\t\t\"\"\"\n\t\tSTAR \\\n\t\t\t--genomeLoad LoadAndExit \\\n\t\t\t--genomeDir ${starIndex}\n\t\t\"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "STAR"
        ],
        "tools_url": [
            "https://bio.tools/star"
        ],
        "tools_dico": [
            {
                "name": "STAR",
                "uri": "https://bio.tools/star",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Ultrafast universal RNA-seq aligner",
                "homepage": "http://code.google.com/p/rna-star/"
            }
        ],
        "inputs": [
            "starIndex"
        ],
        "nb_inputs": 1,
        "outputs": [
            "starIndexLoaded"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.star.container",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SAVE_MULTI_RUNS_TO_LOOM": {
        "name_process": "SAVE_MULTI_RUNS_TO_LOOM",
        "string_process": "\nprocess SAVE_MULTI_RUNS_TO_LOOM {\n\n    cache 'deep'\n    container toolParams.container\n    publishDir \"${toolParams.scenicoutdir}/${sampleId}/multi_runs_looms/\", mode: 'link', overwrite: true\n    label 'compute_resources__scenic_multiruns'\n\n    input:\n\t\ttuple val(sampleId), path(filteredLoom), path(multiRunsAggrRegulons), path(multiRunsAggrRegulonsAUC)\n\t\tval type\n\n    output:\n    \ttuple val(sampleId), path(\"multi_runs_regulons_auc_${type}.loom\")\n\n\tscript:\n\t\t\"\"\"\n\t\t${binDir}save_multi_runs_to_loom.py \\\n\t\t\t$filteredLoom \\\n\t\t\t$multiRunsAggrRegulons \\\n\t\t\t$multiRunsAggrRegulonsAUC \\\n\t\t\t-o \"multi_runs_regulons_auc_${type}.loom\" \\\n\t\t\t--min-genes-regulon ${toolParams.aucell.min_genes_regulon} \\\n\t\t\t--min-regulon-gene-occurrence ${toolParams.aucell.min_regulon_gene_occurrence} \\\n\t\t\t--cell-id-attribute ${toolParams.cell_id_attribute} \\\n\t\t\t--gene-attribute ${toolParams.gene_attribute} \\\n\t\t\t--title \"${sampleId} - pySCENIC (${type})\" \\\n\t\t\t--nomenclature \"${params.sc.scope.genome}\" \\\n\t\t\t--scope-tree-level-1 \"${params.sc.scope.tree.level_1}\" \\\n\t\t\t--scope-tree-level-2 \"${params.sc.scope.tree.level_2}\" \\\n\t\t\t--scope-tree-level-3 \"${params.sc.scope.tree.level_3}\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 32,
        "string_script": "\t\t\"\"\"\n\t\t${binDir}save_multi_runs_to_loom.py \\\n\t\t\t$filteredLoom \\\n\t\t\t$multiRunsAggrRegulons \\\n\t\t\t$multiRunsAggrRegulonsAUC \\\n\t\t\t-o \"multi_runs_regulons_auc_${type}.loom\" \\\n\t\t\t--min-genes-regulon ${toolParams.aucell.min_genes_regulon} \\\n\t\t\t--min-regulon-gene-occurrence ${toolParams.aucell.min_regulon_gene_occurrence} \\\n\t\t\t--cell-id-attribute ${toolParams.cell_id_attribute} \\\n\t\t\t--gene-attribute ${toolParams.gene_attribute} \\\n\t\t\t--title \"${sampleId} - pySCENIC (${type})\" \\\n\t\t\t--nomenclature \"${params.sc.scope.genome}\" \\\n\t\t\t--scope-tree-level-1 \"${params.sc.scope.tree.level_1}\" \\\n\t\t\t--scope-tree-level-2 \"${params.sc.scope.tree.level_2}\" \\\n\t\t\t--scope-tree-level-3 \"${params.sc.scope.tree.level_3}\"\n\t\t\"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "filteredLoom",
            "multiRunsAggrRegulons",
            "multiRunsAggrRegulonsAUC",
            "type"
        ],
        "nb_inputs": 5,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "container toolParams.container",
            "publishDir \"${toolParams.scenicoutdir}/${sampleId}/multi_runs_looms/\", mode: 'link', overwrite: true",
            "label 'compute_resources__scenic_multiruns'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__CELDA__DECONTX_MERGE_OUTLIER_TABLES": {
        "name_process": "SC__CELDA__DECONTX_MERGE_OUTLIER_TABLES",
        "string_process": "\nprocess SC__CELDA__DECONTX_MERGE_OUTLIER_TABLES {\n\n    publishDir \"${params.global.outdir}/data/${moduleName}\", mode: 'link'\n    label 'compute_resources__default'\n\n    input:\n        file(\"*\")\n\n    output:\n        path(\"all.CELDA__DECONTX.Contamination_Outlier_Table.tsv.gz\")\n\n    script:\n        \"\"\"\n        cat * > tmp.CELDA__DECONTX.Contamination_Outlier_Table.tsv\n        cat \\\n            <(head -n1 tmp.CELDA__DECONTX.Contamination_Outlier_Table.tsv) \\\n            <(cat tmp.CELDA__DECONTX.Contamination_Outlier_Table.tsv | sed \"/^index/d\") \\\n            | gzip -c > all.CELDA__DECONTX.Contamination_Outlier_Table.tsv.gz\n        rm tmp.CELDA__DECONTX.Contamination_Outlier_Table.tsv\n        \"\"\"\n\n}",
        "nb_lignes_process": 21,
        "string_script": "        \"\"\"\n        cat * > tmp.CELDA__DECONTX.Contamination_Outlier_Table.tsv\n        cat \\\n            <(head -n1 tmp.CELDA__DECONTX.Contamination_Outlier_Table.tsv) \\\n            <(cat tmp.CELDA__DECONTX.Contamination_Outlier_Table.tsv | sed \"/^index/d\") \\\n            | gzip -c > all.CELDA__DECONTX.Contamination_Outlier_Table.tsv.gz\n        rm tmp.CELDA__DECONTX.Contamination_Outlier_Table.tsv\n        \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "publishDir \"${params.global.outdir}/data/${moduleName}\", mode: 'link'",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__FILE_CONVERTER": {
        "name_process": "SC__FILE_CONVERTER",
        "string_process": "\nprocess SC__FILE_CONVERTER {\n\n    cache 'deep'\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    container \"${getConverterContainer(params,converterToUse)}\"\n    label 'compute_resources__mem'\n\n    input:\n        tuple \\\n            val(sampleId), \\\n            path(f), \\\n            val(inputDataType), \\\n            val(outputDataType)\n\n    output:\n        tuple \\\n            val(sampleId), \\\n            path(\"${sampleId}.SC__FILE_CONVERTER.${outputExtension}\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.file_converter)\n        processParams = sampleParams.local\n\n        switch(inputDataType) {\n            case \"10x_cellranger_mex\":\n                                          \n            break;\n            case \"10x_cellranger_mex_outs\":\n                                                                                                                                                 \n                                                                         \n                cellranger_outs_v2_mex = file(\"${f.toRealPath()}/${processParams.useFilteredMatrix ? \"filtered\" : \"raw\"}_gene_bc_matrices/\")\n                cellranger_outs_v3_mex = file(\"${f.toRealPath()}/${processParams.useFilteredMatrix ? \"filtered\" : \"raw\"}_feature_bc_matrix/\")\n                cellRangerData = detectCellRangerVersionData(cellranger_outs_v2_mex, cellranger_outs_v3_mex)\n                f = cellRangerData.path\n                inputDataType = \"10x_cellranger_mex\"\n            break;\n            case \"10x_cellranger_h5\":\n                                          \n            break;\n            case \"10x_cellranger_h5_outs\":\n                                                                         \n                cellranger_outs_v2_h5 = file(\"${f.toRealPath()}/${processParams.useFilteredMatrix ? \"filtered\" : \"raw\"}_gene_bc_matrices.h5\")\n                cellranger_outs_v3_h5 = file(\"${f.toRealPath()}/${processParams.useFilteredMatrix ? \"filtered\" : \"raw\"}_feature_bc_matrix.h5\")\n                cellRangerData = detectCellRangerVersionData(cellranger_outs_v2_h5, cellranger_outs_v3_h5)\n                f = cellRangerData.path\n                inputDataType = \"10x_cellranger_h5\"\n            case \"10x_atac_cellranger_mex_outs\":\n                                          \n            break;\n            case \"csv\":\n            case \"tsv\":\n            case \"h5ad\":\n            case \"loom\":\n            case \"seurat_rds\":\n                                          \n            break;\n            \n            default:\n                throw new Exception(\"VSN ERROR: The given input format ${inputDataType} is not recognized.\")\n            break;\n        }\n\n                                                                              \n        converterToUse = getConverter(\n            inputDataType,\n            outputDataType\n        )\n        outputExtension = getOutputExtension(outputDataType)\n\n        switch(converterToUse) {\n            case \"cistopic\":\n                \"\"\"\n                ${binDir}/create_cistopic_object.R \\\n                    --tenx_path ${f} \\\n                    --sampleId ${sampleId} \\\n                    --output ${sampleId}.SC__FILE_CONVERTER.${outputExtension}\n                \"\"\"\n                break;\n            case \"r\":\n                runRConverter(\n                    \"SC__FILE_CONVERTER\",\n                    processParams,\n                    sampleId,\n                    inputDataType,\n                    outputDataType,\n                    outputExtension,\n                    f\n                )\n                break;\n            case \"python\":\n                runPythonConverter(\n                    processParams,\n                    sampleId,\n                    inputDataType,\n                    outputDataType,\n                    outputExtension,\n                    f\n                )\n                break;\n            default:\n                throw new Exception(\"VSN ERROR: Unrecognized file converter.\")\n        }\n\n}",
        "nb_lignes_process": 103,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.file_converter)\n        processParams = sampleParams.local\n\n        switch(inputDataType) {\n            case \"10x_cellranger_mex\":\n                                          \n            break;\n            case \"10x_cellranger_mex_outs\":\n                                                                                                                                                 \n                                                                         \n                cellranger_outs_v2_mex = file(\"${f.toRealPath()}/${processParams.useFilteredMatrix ? \"filtered\" : \"raw\"}_gene_bc_matrices/\")\n                cellranger_outs_v3_mex = file(\"${f.toRealPath()}/${processParams.useFilteredMatrix ? \"filtered\" : \"raw\"}_feature_bc_matrix/\")\n                cellRangerData = detectCellRangerVersionData(cellranger_outs_v2_mex, cellranger_outs_v3_mex)\n                f = cellRangerData.path\n                inputDataType = \"10x_cellranger_mex\"\n            break;\n            case \"10x_cellranger_h5\":\n                                          \n            break;\n            case \"10x_cellranger_h5_outs\":\n                                                                         \n                cellranger_outs_v2_h5 = file(\"${f.toRealPath()}/${processParams.useFilteredMatrix ? \"filtered\" : \"raw\"}_gene_bc_matrices.h5\")\n                cellranger_outs_v3_h5 = file(\"${f.toRealPath()}/${processParams.useFilteredMatrix ? \"filtered\" : \"raw\"}_feature_bc_matrix.h5\")\n                cellRangerData = detectCellRangerVersionData(cellranger_outs_v2_h5, cellranger_outs_v3_h5)\n                f = cellRangerData.path\n                inputDataType = \"10x_cellranger_h5\"\n            case \"10x_atac_cellranger_mex_outs\":\n                                          \n            break;\n            case \"csv\":\n            case \"tsv\":\n            case \"h5ad\":\n            case \"loom\":\n            case \"seurat_rds\":\n                                          \n            break;\n            \n            default:\n                throw new Exception(\"VSN ERROR: The given input format ${inputDataType} is not recognized.\")\n            break;\n        }\n\n                                                                              \n        converterToUse = getConverter(\n            inputDataType,\n            outputDataType\n        )\n        outputExtension = getOutputExtension(outputDataType)\n\n        switch(converterToUse) {\n            case \"cistopic\":\n                \"\"\"\n                ${binDir}/create_cistopic_object.R \\\n                    --tenx_path ${f} \\\n                    --sampleId ${sampleId} \\\n                    --output ${sampleId}.SC__FILE_CONVERTER.${outputExtension}\n                \"\"\"\n                break;\n            case \"r\":\n                runRConverter(\n                    \"SC__FILE_CONVERTER\",\n                    processParams,\n                    sampleId,\n                    inputDataType,\n                    outputDataType,\n                    outputExtension,\n                    f\n                )\n                break;\n            case \"python\":\n                runPythonConverter(\n                    processParams,\n                    sampleId,\n                    inputDataType,\n                    outputDataType,\n                    outputExtension,\n                    f\n                )\n                break;\n            default:\n                throw new Exception(\"VSN ERROR: Unrecognized file converter.\")\n        }",
        "nb_lignes_script": 81,
        "language_script": "bash",
        "tools": [
            "CASE",
            "BreakSeq"
        ],
        "tools_url": [
            "https://bio.tools/CASE",
            "https://bio.tools/breakseq"
        ],
        "tools_dico": [
            {
                "name": "CASE",
                "uri": "https://bio.tools/CASE",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0749",
                            "term": "Transcription factors and regulatory sites"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0089",
                            "term": "Ontology and terminology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3436",
                                    "term": "Aggregation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3435",
                                    "term": "Standardisation and normalisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3559",
                                    "term": "Ontology visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3559",
                                    "term": "Ontology browsing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Advancing Coordinated Cyber-investigations and Tool Interoperability using a Community Developed Specification Language.\n\nSource files for the CASE website.\n\nAPI used for instantiating CASE objects (includes ontological verification and type checking).\n\nCyber-investigation Analysis Standard Expression (CASE).\n\nRead the CASE Wiki tab to learn everything you need to know about the Cyber-investigation Analysis Standard Expression (CASE) ontology. For learning about the Unified Cyber Ontology, CASE's parent, see UCO.\n\n\"@vocab\": \"http://case.example.org/core#\",.\n\nDET ER DINE PENGER DET DREIER SEG OM...\n\nVi er ikke st\ufffdrst, men garanterer effektiv behandling.\n\nLast ned v\ufffdr brosjyre i PDF format.\n\n||| COMMON LINK WITH (PUB. & NAME DIFFERENT) bio.tools/pymzml (GITHUB.COM).\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'UCO', 'cyber-investigation', 'cyber-investigations', 'plaso'",
                "homepage": "http://CASE.as"
            },
            {
                "name": "BreakSeq",
                "uri": "https://bio.tools/breakseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "Structural variation"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "Genomic structural variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "DNA structural variation"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Database of known human breakpoint junctions and software to search short reads against them.",
                "homepage": "http://sv.gersteinlab.org/breakseq/"
            }
        ],
        "inputs": [
            "sampleId",
            "f",
            "inputDataType",
            "outputDataType"
        ],
        "nb_inputs": 4,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "container \"${getConverterContainer(params,converterToUse)}\"",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__FILE_CONVERTER_FROM_SCE": {
        "name_process": "SC__FILE_CONVERTER_FROM_SCE",
        "string_process": "\nprocess SC__FILE_CONVERTER_FROM_SCE {\n\n    cache 'deep'\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    container \"${getConverterContainer(params,converterToUse)}\"\n    label 'compute_resources__mem'\n\n    input:\n        tuple \\\n            val(sampleId), \\\n            path(f)\n        val(outputDataType)\n        val(mainLayer)\n\n    output:\n        tuple \\\n            val(sampleId), \\\n            path(\"${sampleId}.SC__FILE_CONVERTER_FROM_SCE.${outputDataType}\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.file_converter)\n        processParams = sampleParams.local\n        def _outputDataType = outputDataType\n        converterToUse = getConverter(\n            \"sce_rds\",\n            _outputDataType\n        )\n        def outputExtension = getOutputExtension(_outputDataType)\n\n        runRConverter(\n            \"SC__FILE_CONVERTER_FROM_SCE\",\n            processParams,\n            sampleId,\n            \"sce_rds\",\n            _outputDataType,\n            outputExtension,\n            f,\n            mainLayer\n        )\n\n}",
        "nb_lignes_process": 40,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.file_converter)\n        processParams = sampleParams.local\n        def _outputDataType = outputDataType\n        converterToUse = getConverter(\n            \"sce_rds\",\n            _outputDataType\n        )\n        def outputExtension = getOutputExtension(_outputDataType)\n\n        runRConverter(\n            \"SC__FILE_CONVERTER_FROM_SCE\",\n            processParams,\n            sampleId,\n            \"sce_rds\",\n            _outputDataType,\n            outputExtension,\n            f,\n            mainLayer\n        )",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f",
            "outputDataType",
            "mainLayer"
        ],
        "nb_inputs": 4,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "container \"${getConverterContainer(params,converterToUse)}\"",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__FILE_CONCATENATOR": {
        "name_process": "SC__FILE_CONCATENATOR",
        "string_process": "\nprocess SC__FILE_CONCATENATOR {\n\n    cache 'deep'\n    container params.sc.scanpy.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n    input:\n        file(\"*\")\n\n    output:\n        tuple val(params.global.project_name), path(\"${params.global.project_name}.SC__FILE_CONCATENATOR.${processParams.off}\")\n\n    script:\n        processParams = params.sc.file_concatenator\n        \"\"\"\n        ${binDir}/sc_file_concatenator.py \\\n            --file-format $processParams.off \\\n            ${(processParams.containsKey('join')) ? '--join ' + processParams.join : ''} \\\n            --output \"${params.global.project_name}.SC__FILE_CONCATENATOR.${processParams.off}\" *\n        \"\"\"\n\n}",
        "nb_lignes_process": 22,
        "string_script": "        processParams = params.sc.file_concatenator\n        \"\"\"\n        ${binDir}/sc_file_concatenator.py \\\n            --file-format $processParams.off \\\n            ${(processParams.containsKey('join')) ? '--join ' + processParams.join : ''} \\\n            --output \"${params.global.project_name}.SC__FILE_CONCATENATOR.${processParams.off}\" *\n        \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [
            "params"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__PUBLISH_PROXY": {
        "name_process": "SC__PUBLISH_PROXY",
        "string_process": "\nprocess SC__PUBLISH_PROXY {\n\n    publishDir \"${params.global.outdir}/data/intermediate\", \\\n        mode: 'symlink', \\\n        overwrite: true, \\\n        saveAs: {\n            filename -> \"${outputFileName}\" \n        }\n\n    label 'compute_resources__minimal'\n\n    input:\n        tuple \\\n            val(tag), \\\n            path(f), \\\n            val(stashedParams)\n        val(fileOutputSuffix)\n        val(toolName)\n        val(isParameterExplorationModeOn)\n\n    output:\n        tuple \\\n            val(tag), \\\n            path(outputFileName), \\\n            val(stashedParams)\n\n    script:\n        outputFileName = getOutputFileName(\n            params,\n            tag,\n            f,\n            fileOutputSuffix,\n            false,\n            null\n        )\n        \"\"\"\n        if [ ! -f ${outputFileName} ]; then\n            ln -s $f \"${outputFileName}\"\n        fi\n        \"\"\"\n\n}",
        "nb_lignes_process": 41,
        "string_script": "        outputFileName = getOutputFileName(\n            params,\n            tag,\n            f,\n            fileOutputSuffix,\n            false,\n            null\n        )\n        \"\"\"\n        if [ ! -f ${outputFileName} ]; then\n            ln -s $f \"${outputFileName}\"\n        fi\n        \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "NullSeq"
        ],
        "tools_url": [
            "https://bio.tools/nullseq"
        ],
        "tools_dico": [
            {
                "name": "NullSeq",
                "uri": "https://bio.tools/nullseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0364",
                                    "term": "Random sequence generation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Creates Random Coding Sequences with specified GC content and Amino Acid usage.",
                "homepage": "https://github.com/amarallab/NullSeq"
            }
        ],
        "inputs": [
            "tag",
            "f",
            "stashedParams",
            "fileOutputSuffix",
            "toolName",
            "isParameterExplorationModeOn"
        ],
        "nb_inputs": 6,
        "outputs": [
            "tag",
            "outputFileName",
            "stashedParams"
        ],
        "nb_outputs": 3,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true, saveAs: { filename -> \"${outputFileName}\" }",
            "label 'compute_resources__minimal'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__PUBLISH": {
        "name_process": "SC__PUBLISH",
        "string_process": "\nprocess SC__PUBLISH {\n\n    publishDir \\\n        \"${getPublishDir(params.global.outdir,toolName)}\", \\\n        mode: 'link', \\\n        overwrite: true, \\\n        saveAs: {\n            filename -> \"${outputFileName}\" \n        }\n\n    label 'compute_resources__minimal'\n    \n    input:\n        tuple \\\n            val(tag), \\\n            path(f), \\\n            val(stashedParams)\n        val(fileOutputSuffix)\n        val(toolName)\n        val(isParameterExplorationModeOn)\n\n    output:\n        tuple \\\n            val(tag), \\\n            path(outputFileName), \\\n            val(stashedParams)\n\n    script:\n        outputFileName = getOutputFileName(\n            params,\n            tag,\n            f,\n            fileOutputSuffix,\n            isParameterExplorationModeOn,\n            stashedParams\n        )\n        \"\"\"\n        cp -RL $f tmp\n        rm $f\n        ln tmp \"${outputFileName}\"\n        rm tmp\n        \"\"\"\n\n}",
        "nb_lignes_process": 43,
        "string_script": "        outputFileName = getOutputFileName(\n            params,\n            tag,\n            f,\n            fileOutputSuffix,\n            isParameterExplorationModeOn,\n            stashedParams\n        )\n        \"\"\"\n        cp -RL $f tmp\n        rm $f\n        ln tmp \"${outputFileName}\"\n        rm tmp\n        \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "tag",
            "f",
            "stashedParams",
            "fileOutputSuffix",
            "toolName",
            "isParameterExplorationModeOn"
        ],
        "nb_inputs": 6,
        "outputs": [
            "tag",
            "outputFileName",
            "stashedParams"
        ],
        "nb_outputs": 3,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "publishDir \"${getPublishDir(params.global.outdir,toolName)}\", mode: 'link', overwrite: true, saveAs: { filename -> \"${outputFileName}\" }",
            "label 'compute_resources__minimal'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__FIND_HIGHLY_VARIABLE_GENES": {
        "name_process": "SC__SCANPY__FIND_HIGHLY_VARIABLE_GENES",
        "string_process": "\nprocess SC__SCANPY__FIND_HIGHLY_VARIABLE_GENES {\n\n  \tcontainer params.sc.scanpy.container\n  \tpublishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n  \tinput:\n    \ttuple val(sampleId), path(f)\n\n  \toutput:\n    \ttuple val(sampleId), path(\"${sampleId}.SC__SCANPY__FIND_HIGHLY_VARIABLE_GENES.${processParams.off}\")\n\n  \tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.feature_selection)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}/feature_selection/sc_find_variable_genes.py \\\n\t\t\t--method ${processParams.method} \\\n\t\t\t${(processParams.containsKey('minMean')) ? '--min-mean ' + processParams.minMean : ''} \\\n\t\t\t${(processParams.containsKey('maxMean')) ? '--max-mean ' + processParams.maxMean : ''} \\\n\t\t\t${(processParams.containsKey('minDisp')) ? '--min-disp ' + processParams.minDisp : ''} \\\n\t\t\t${(processParams.containsKey('maxDisp')) ? '--max-disp ' + processParams.maxDisp : ''} \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__FIND_HIGHLY_VARIABLE_GENES.${processParams.off}\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 26,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.feature_selection)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}/feature_selection/sc_find_variable_genes.py \\\n\t\t\t--method ${processParams.method} \\\n\t\t\t${(processParams.containsKey('minMean')) ? '--min-mean ' + processParams.minMean : ''} \\\n\t\t\t${(processParams.containsKey('maxMean')) ? '--max-mean ' + processParams.maxMean : ''} \\\n\t\t\t${(processParams.containsKey('minDisp')) ? '--min-disp ' + processParams.minDisp : ''} \\\n\t\t\t${(processParams.containsKey('maxDisp')) ? '--max-disp ' + processParams.maxDisp : ''} \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__FIND_HIGHLY_VARIABLE_GENES.${processParams.off}\"\n\t\t\"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__SUBSET_HIGHLY_VARIABLE_GENES": {
        "name_process": "SC__SCANPY__SUBSET_HIGHLY_VARIABLE_GENES",
        "string_process": "\nprocess SC__SCANPY__SUBSET_HIGHLY_VARIABLE_GENES {\n\n  \tcontainer params.sc.scanpy.container\n  \tpublishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n  \tinput:\n    \ttuple val(sampleId), path(f)\n\n  \toutput:\n    \ttuple val(sampleId), path(\"${sampleId}.SC__SCANPY__SUBSET_HIGHLY_VARIABLE_GENES.${processParams.off}\")\n\n  \tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.feature_selection)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}/feature_selection/sc_subset_variable_genes.py \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__SUBSET_HIGHLY_VARIABLE_GENES.${processParams.off}\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 21,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.feature_selection)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}/feature_selection/sc_subset_variable_genes.py \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__SUBSET_HIGHLY_VARIABLE_GENES.${processParams.off}\"\n\t\t\"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "GZIP": {
        "name_process": "GZIP",
        "string_process": "\nprocess GZIP {\n\n    container params.sc.dropseqtools.container\n    publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n        tuple val(sample), path(f)\n\n    output:\n        tuple val(sample), path(\"*.unaligned_tagged_polyA_filtered.fastq.gz\"), emit: fastq_gz\n\n    script:\n        \"\"\"\n        gzip --force ${f}\n        \"\"\"\n\n}",
        "nb_lignes_process": 17,
        "string_script": "        \"\"\"\n        gzip --force ${f}\n        \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.dropseqtools.container",
            "publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'",
            "label 'compute_resources__cpu','compute_resources__24hqueue'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__NORMALIZATION": {
        "name_process": "SC__SCANPY__NORMALIZATION",
        "string_process": "\nprocess SC__SCANPY__NORMALIZATION {\n\n\tcontainer params.sc.scanpy.container\n\tpublishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n\tinput:\n\t\ttuple val(sampleId), path(f)\n\n\toutput:\n\t\ttuple val(sampleId), path(\"${sampleId}.SC__SCANPY__NORMALIZATION.${processParams.off}\")\n\n\tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.normalization)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}/transform/sc_normalization.py \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t${(processParams.containsKey('countsPerCellAfter')) ? '--counts-per-cell-after ' + processParams.countsPerCellAfter : ''} \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__NORMALIZATION.${processParams.off}\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 23,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.normalization)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}/transform/sc_normalization.py \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t${(processParams.containsKey('countsPerCellAfter')) ? '--counts-per-cell-after ' + processParams.countsPerCellAfter : ''} \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__NORMALIZATION.${processParams.off}\"\n\t\t\"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__DATA_TRANSFORMATION": {
        "name_process": "SC__SCANPY__DATA_TRANSFORMATION",
        "string_process": "\nprocess SC__SCANPY__DATA_TRANSFORMATION {\n\n\tcontainer params.sc.scanpy.container\n\tpublishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n\tinput:\n\t\ttuple val(sampleId), path(f)\n\t\n\toutput:\n\t\ttuple val(sampleId), path(\"${sampleId}.SC__SCANPY__DATA_TRANSFORMATION.${processParams.off}\")\n\t\n\tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.data_transformation)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}/transform/sc_data_transformation.py \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__DATA_TRANSFORMATION.${processParams.off}\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 22,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.data_transformation)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}/transform/sc_data_transformation.py \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__DATA_TRANSFORMATION.${processParams.off}\"\n\t\t\"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__FEATURE_SCALING": {
        "name_process": "SC__SCANPY__FEATURE_SCALING",
        "string_process": "\nprocess SC__SCANPY__FEATURE_SCALING {\n\n\tcontainer params.sc.scanpy.container\n\tpublishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n\tinput:\n\t\ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(f)\n\t\n\toutput:\n\t\ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(\"${sampleId}.SC__SCANPY__FEATURE_SCALING.${processParams.off}\")\n\t\n\tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.feature_scaling)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}/transform/sc_feature_scaling.py \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t${(processParams.containsKey('maxSD')) ? '--max-sd ' + processParams.maxSD : ''} \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__FEATURE_SCALING.${processParams.off}\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 27,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.feature_scaling)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}/transform/sc_feature_scaling.py \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t${(processParams.containsKey('maxSD')) ? '--max-sd ' + processParams.maxSD : ''} \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__FEATURE_SCALING.${processParams.off}\"\n\t\t\"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__DROP_SEQ_TOOLS__TRIM_SMART_UNALIGNED_TAGGED_FILTERED_BAM": {
        "name_process": "SC__DROP_SEQ_TOOLS__TRIM_SMART_UNALIGNED_TAGGED_FILTERED_BAM",
        "string_process": "\nprocess SC__DROP_SEQ_TOOLS__TRIM_SMART_UNALIGNED_TAGGED_FILTERED_BAM {\n\n    container params.sc.dropseqtools.container\n    publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n        tuple val(sample), path(bam)\n\n    output:\n        tuple val(sample), path('*.unaligned_tagged_trimmed_smart.bam'), emit: bam\n        tuple file('*.adapter_trimming_report.txt'), emit: report\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.trim_smart_unaligned_tagged_filtered_bam)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        TrimStartingSequence \\\n            INPUT=${bam} \\\n            OUTPUT=${sample}.unaligned_tagged_trimmed_smart.bam \\\n            OUTPUT_SUMMARY=${sample}.adapter_trimming_report.txt \\\n            SEQUENCE=${processParams.adapterSequence} \\\n            MISMATCHES=${processParams.mismatches} \\\n            NUM_BASES=${processParams.numBases}\n        \"\"\"\n\n}",
        "nb_lignes_process": 26,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.trim_smart_unaligned_tagged_filtered_bam)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        TrimStartingSequence \\\n            INPUT=${bam} \\\n            OUTPUT=${sample}.unaligned_tagged_trimmed_smart.bam \\\n            OUTPUT_SUMMARY=${sample}.adapter_trimming_report.txt \\\n            SEQUENCE=${processParams.adapterSequence} \\\n            MISMATCHES=${processParams.mismatches} \\\n            NUM_BASES=${processParams.numBases}\n        \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "bam"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.dropseqtools.container",
            "publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'",
            "label 'compute_resources__cpu','compute_resources__24hqueue'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__REGRESS_OUT": {
        "name_process": "SC__SCANPY__REGRESS_OUT",
        "string_process": "\nprocess SC__SCANPY__REGRESS_OUT {\n\n\tcontainer params.sc.scanpy.container\n\tpublishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n\tinput:\n\t\ttuple val(sampleId), path(f)\n\n\toutput:\n\t\ttuple val(sampleId), path(\"${sampleId}.SC__SCANPY__REGRESS_OUT.${processParams.off}\")\n\n\tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.regress_out)\n\t\tprocessParams = sampleParams.local\n\t\tvariablesToRegressOutAsArguments = processParams.variablesToRegressOut.collect({ '--variable-to-regress-out' + ' ' + it }).join(' ')\n\t\t\"\"\"\n\t\t${binDir}adjust/sc_regress_out.py \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t${(processParams.containsKey('variablesToRegressOut')) ? variablesToRegressOutAsArguments : ''} \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__REGRESS_OUT.${processParams.off}\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 24,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.regress_out)\n\t\tprocessParams = sampleParams.local\n\t\tvariablesToRegressOutAsArguments = processParams.variablesToRegressOut.collect({ '--variable-to-regress-out' + ' ' + it }).join(' ')\n\t\t\"\"\"\n\t\t${binDir}adjust/sc_regress_out.py \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t${(processParams.containsKey('variablesToRegressOut')) ? variablesToRegressOutAsArguments : ''} \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__REGRESS_OUT.${processParams.off}\"\n\t\t\"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__CLUSTERING": {
        "name_process": "SC__SCANPY__CLUSTERING",
        "string_process": "\nprocess SC__SCANPY__CLUSTERING {\n\n  \tcontainer params.sc.scanpy.container\n  \tpublishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n  \tinput:\n    \ttuple val(sampleId), path(f)\n\n  \toutput:\n    \ttuple val(sampleId), path(\"${sampleId}.SC__SCANPY__CLUSTERING.${processParams.off}\")\n\n  \tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.clustering)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}/cluster/sc_clustering.py \\\n\t\t\t--seed ${params.global.seed} \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t${(processParams.containsKey('resolution')) ? '--resolution ' + processParams.resolution : ''} \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__CLUSTERING.${processParams.off}\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 24,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.clustering)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}/cluster/sc_clustering.py \\\n\t\t\t--seed ${params.global.seed} \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t${(processParams.containsKey('resolution')) ? '--resolution ' + processParams.resolution : ''} \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__CLUSTERING.${processParams.off}\"\n\t\t\"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__PARAM_EXPLORE_CLUSTERING": {
        "name_process": "SC__SCANPY__PARAM_EXPLORE_CLUSTERING",
        "string_process": "\nprocess SC__SCANPY__PARAM_EXPLORE_CLUSTERING {\n\n  \tcontainer params.sc.scanpy.container\n  \tpublishDir \"${params.global.outdir}/data/intermediate/clustering/${isParamNull(method) ? \"default\": method.toLowerCase()}/${isParamNull(resolution) ? \"default\" : \"res_\" + resolution}\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n  \tinput:\n    \ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(f), \\\n\t\t\tval(stashedParams), \\\n\t\t\tval(method), \\\n\t\t\tval(resolution)\n\n  \toutput:\n    \ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(\"${sampleId}.SC__SCANPY__PARAM_EXPLORE_CLUSTERING.${processParams.off}\"), \\\n\t\t\tval(method), \\\n\t\t\tval(resolution)\n\n  \tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.clustering)\n\t\tprocessParams = sampleParams.local\n\t\tdef _processParams = new SC__SCANPY__CLUSTERING_PARAMS()\n\t\t_processParams.setEnv(this)\n\t\t_processParams.setConfigParams(processParams)\n\t\t\"\"\"\n\t\t${binDir}/cluster/sc_clustering.py \\\n\t\t\t--seed ${params.global.seed} \\\n\t\t\t${_processParams.getMethodAsArgument(method)} \\\n\t\t\t${_processParams.getResolutionAsArgument(resolution)} \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__PARAM_EXPLORE_CLUSTERING.${processParams.off}\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 36,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.clustering)\n\t\tprocessParams = sampleParams.local\n\t\tdef _processParams = new SC__SCANPY__CLUSTERING_PARAMS()\n\t\t_processParams.setEnv(this)\n\t\t_processParams.setConfigParams(processParams)\n\t\t\"\"\"\n\t\t${binDir}/cluster/sc_clustering.py \\\n\t\t\t--seed ${params.global.seed} \\\n\t\t\t${_processParams.getMethodAsArgument(method)} \\\n\t\t\t${_processParams.getResolutionAsArgument(resolution)} \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.SC__SCANPY__PARAM_EXPLORE_CLUSTERING.${processParams.off}\"\n\t\t\"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f",
            "stashedParams",
            "method",
            "resolution"
        ],
        "nb_inputs": 5,
        "outputs": [
            "sampleId",
            "method",
            "resolution"
        ],
        "nb_outputs": 3,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate/clustering/${isParamNull(method) ? \"default\": method.toLowerCase()}/${isParamNull(resolution) ? \"default\" : \"res_\" + resolution}\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__DROP_SEQ_TOOLS__TAG_READ_WITH_GENE_EXON": {
        "name_process": "SC__DROP_SEQ_TOOLS__TAG_READ_WITH_GENE_EXON",
        "string_process": "\nprocess SC__DROP_SEQ_TOOLS__TAG_READ_WITH_GENE_EXON {\n\n    publishDir \"${params.global.outdir}/02.map\", mode: 'symlink'\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n        tuple val(sample), path(bam)\n        file(annotation)\n\n    output:\n        tuple val(sample), path(\"*.merged_gene-exon-tagged.bam\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.tag_read_with_gene_exon)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        source $DWMAX/documents/aertslab/scripts/src_dwmax/bash-utils/utils.sh\n        software load drop-seq_tools/1.12\n        TagReadWithGeneExon \\\n            I=${bam} \\\n            O=${sample}.merged_gene-exon-tagged.bam \\\n            ANNOTATIONS_FILE=${annotation} \\\n            TAG=${processParams.tag}\n        \"\"\"\n\n}",
        "nb_lignes_process": 25,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.tag_read_with_gene_exon)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        source $DWMAX/documents/aertslab/scripts/src_dwmax/bash-utils/utils.sh\n        software load drop-seq_tools/1.12\n        TagReadWithGeneExon \\\n            I=${bam} \\\n            O=${sample}.merged_gene-exon-tagged.bam \\\n            ANNOTATIONS_FILE=${annotation} \\\n            TAG=${processParams.tag}\n        \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [
            "ABI software"
        ],
        "tools_url": [
            "https://bio.tools/ABI_software"
        ],
        "tools_dico": [
            {
                "name": "ABI software",
                "uri": "https://bio.tools/ABI_software",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3334",
                            "term": "Neurology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3334",
                            "term": "https://en.wikipedia.org/wiki/Neurology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Experimental Brain Computer Interface Software for the ModularEEG.",
                "homepage": "https://users.dcc.uchile.cl/~peortega/abi/"
            }
        ],
        "inputs": [
            "sample",
            "bam",
            "annotation"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sample"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "publishDir \"${params.global.outdir}/02.map\", mode: 'symlink'",
            "label 'compute_resources__cpu','compute_resources__24hqueue'"
        ],
        "when": "",
        "stub": ""
    },
    "AGGR_MULTI_RUNS_FEATURES": {
        "name_process": "AGGR_MULTI_RUNS_FEATURES",
        "string_process": "\nprocess AGGR_MULTI_RUNS_FEATURES {\n\n    cache 'deep'\n    container toolParams.container\n    publishDir \"${toolParams.scenicoutdir}/${sampleId}/multi_runs_cistarget/\", mode: 'link', overwrite: true\n                                                                                                                              \n                                                                                                 \n    label 'compute_resources__scenic_multiruns'\n\n    input:\n\t\ttuple val(sampleId), path(f)\n\t\tval type\n\n    output:\n    \ttuple val(sampleId), path(\"multi_runs_features_${type}.${output_format_ext}${compression_ext}\")\n\n    script:\n\t\toutput_format = processParams.output_format\n\t\toutput_format_ext = output_format\n\t\tif(output_format == 'pickle') {\n\t\toutput_format_ext = 'pkl'\n\t\t}\n\t\tcompression = processParams.compression\n\t\tcompression_ext = ''\n\t\tif(compression == 'gzip') {\n\t\tcompression_ext = '.gz'\n\t\t}\n\t\t\"\"\"\n\t\t${binDir}aggregate_multi_runs_features.py \\\n\t\t\t${f} \\\n\t\t\t--output \"multi_runs_features_${type}.${output_format_ext}${compression_ext}\" \\\n\t\t\t--output-format ${output_format} \\\n\t\t\t--use-chunking ${processParams.use_chunking}\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 35,
        "string_script": "\t\toutput_format = processParams.output_format\n\t\toutput_format_ext = output_format\n\t\tif(output_format == 'pickle') {\n\t\toutput_format_ext = 'pkl'\n\t\t}\n\t\tcompression = processParams.compression\n\t\tcompression_ext = ''\n\t\tif(compression == 'gzip') {\n\t\tcompression_ext = '.gz'\n\t\t}\n\t\t\"\"\"\n\t\t${binDir}aggregate_multi_runs_features.py \\\n\t\t\t${f} \\\n\t\t\t--output \"multi_runs_features_${type}.${output_format_ext}${compression_ext}\" \\\n\t\t\t--output-format ${output_format} \\\n\t\t\t--use-chunking ${processParams.use_chunking}\n\t\t\"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f",
            "type"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "container toolParams.container",
            "publishDir \"${toolParams.scenicoutdir}/${sampleId}/multi_runs_cistarget/\", mode: 'link', overwrite: true",
            "label 'compute_resources__scenic_multiruns'"
        ],
        "when": "",
        "stub": ""
    },
    "EDIRECT__SRAID_TO_SAMPLENAME": {
        "name_process": "EDIRECT__SRAID_TO_SAMPLENAME",
        "string_process": "\nprocess EDIRECT__SRAID_TO_SAMPLENAME {\n    \n    container params.edirect.container\n    label 'compute_resources__default'\n    maxForks 1\n\n    input:\n        val(sraId)\n    output:\n        tuple val(sraId), stdout\n    shell:\n        \"\"\"\n        esearch -db sra -query ${sraId} \\\n           | efetch --format native \\\n           | sed -r 's/(.*)<TITLE>(.*)<\\\\/TITLE>(.*)/\\\\2/' \\\n           | grep \"^[^<;]\" \\\n           | tr -d '\\\\n' \n        \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "        \"\"\"\n        esearch -db sra -query ${sraId} \\\n           | efetch --format native \\\n           | sed -r 's/(.*)<TITLE>(.*)<\\\\/TITLE>(.*)/\\\\2/' \\\n           | grep \"^[^<;]\" \\\n           | tr -d '\\\\n' \n        \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "QResearch",
            "eFetch Snp"
        ],
        "tools_url": [
            "https://bio.tools/QResearch",
            "https://bio.tools/efetch_snp"
        ],
        "tools_dico": [
            {
                "name": "QResearch",
                "uri": "https://bio.tools/QResearch",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatric medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3335",
                            "term": "Cardiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "https://en.wikipedia.org/wiki/Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3335",
                            "term": "Cardiovascular medicine"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3283",
                                    "term": "Anonymisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Imputation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3283",
                                    "term": "Data anonymisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Data imputation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "General practice database for research.",
                "homepage": "http://www.qresearch.org"
            },
            {
                "name": "eFetch Snp",
                "uri": "https://bio.tools/efetch_snp",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Biological databases"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Data management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Information systems"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Databases and information systems"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data retrieval"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data extraction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Retrieval"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Get SNPs information given SNP ID list.",
                "homepage": "http://www.ncbi.nlm.nih.gov/corehtml/query/static/efetchseq_help.html"
            }
        ],
        "inputs": [
            "sraId"
        ],
        "nb_inputs": 1,
        "outputs": [
            "sraId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.edirect.container",
            "label 'compute_resources__default'",
            "maxForks 1"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__COMPUTE_QC_STATS": {
        "name_process": "SC__SCANPY__COMPUTE_QC_STATS",
        "string_process": "\nprocess SC__SCANPY__COMPUTE_QC_STATS {\n\n  \tcontainer params.sc.scanpy.container\n    label 'compute_resources__mem'\n\n  \tinput:\n        tuple val(sampleId), path(f)\n\n\toutput:\n        tuple val(sampleId), path(\"${sampleId}.SC__SCANPY__COMPUTE_QC_STATS.${processParams.off}\")\n\n\tscript:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.filter)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        ${binDir}/filter/sc_cell_gene_filtering.py \\\n            compute \\\n            $f \\\n            ${sampleId}.SC__SCANPY__COMPUTE_QC_STATS.${processParams.off} \\\n            ${(processParams.containsKey('cellFilterMinNCounts')) ? '--min-n-counts ' + processParams.cellFilterMinNCounts : ''} \\\n            ${(processParams.containsKey('cellFilterMaxNCounts')) ? '--max-n-counts ' + processParams.cellFilterMaxNCounts : ''} \\\n            ${(processParams.containsKey('cellFilterMinNGenes')) ? '--min-n-genes ' + processParams.cellFilterMinNGenes : ''} \\\n            ${(processParams.containsKey('cellFilterMaxNGenes')) ? '--max-n-genes ' + processParams.cellFilterMaxNGenes : ''} \\\n            ${(processParams.containsKey('cellFilterMaxPercentMito')) ? '--max-percent-mito ' + processParams.cellFilterMaxPercentMito : ''} \\\n            ${(processParams.containsKey('geneFilterMinNCells')) ? '--min-number-cells ' + processParams.geneFilterMinNCells : ''}\n        \"\"\"\n\n}",
        "nb_lignes_process": 27,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.filter)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        ${binDir}/filter/sc_cell_gene_filtering.py \\\n            compute \\\n            $f \\\n            ${sampleId}.SC__SCANPY__COMPUTE_QC_STATS.${processParams.off} \\\n            ${(processParams.containsKey('cellFilterMinNCounts')) ? '--min-n-counts ' + processParams.cellFilterMinNCounts : ''} \\\n            ${(processParams.containsKey('cellFilterMaxNCounts')) ? '--max-n-counts ' + processParams.cellFilterMaxNCounts : ''} \\\n            ${(processParams.containsKey('cellFilterMinNGenes')) ? '--min-n-genes ' + processParams.cellFilterMinNGenes : ''} \\\n            ${(processParams.containsKey('cellFilterMaxNGenes')) ? '--max-n-genes ' + processParams.cellFilterMaxNGenes : ''} \\\n            ${(processParams.containsKey('cellFilterMaxPercentMito')) ? '--max-percent-mito ' + processParams.cellFilterMaxPercentMito : ''} \\\n            ${(processParams.containsKey('geneFilterMinNCells')) ? '--min-number-cells ' + processParams.geneFilterMinNCells : ''}\n        \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "Computel"
        ],
        "tools_url": [
            "https://bio.tools/computel"
        ],
        "tools_dico": [
            {
                "name": "Computel",
                "uri": "https://bio.tools/computel",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0160",
                            "term": "Sequence sites, features and motifs"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Whole genome sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Genome sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "WGS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0415",
                                    "term": "Nucleic acid feature detection"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0415",
                                    "term": "Sequence feature detection (nucleic acid)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Computation of Mean Telomere Length from Whole-Genome Next-Generation Sequencing Data.",
                "homepage": "https://github.com/lilit-nersisyan/computel"
            }
        ],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__GENE_FILTER": {
        "name_process": "SC__SCANPY__GENE_FILTER",
        "string_process": "\nprocess SC__SCANPY__GENE_FILTER {\n\n    container params.sc.scanpy.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n    input:\n        tuple val(sampleId), path(f)\n\n\toutput:\n        tuple val(sampleId), path(\"${sampleId}.SC__SCANPY__GENE_FILTER.${processParams.off}\")\n\n\tscript:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.filter)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        ${binDir}/filter/sc_cell_gene_filtering.py \\\n            genefilter \\\n            $f \\\n            ${sampleId}.SC__SCANPY__GENE_FILTER.${processParams.off} \\\n            ${(processParams.containsKey('geneFilterMinNCells')) ? '--min-number-cells ' + processParams.geneFilterMinNCells : ''}\n        \"\"\"\n\n}",
        "nb_lignes_process": 23,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.filter)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        ${binDir}/filter/sc_cell_gene_filtering.py \\\n            genefilter \\\n            $f \\\n            ${sampleId}.SC__SCANPY__GENE_FILTER.${processParams.off} \\\n            ${(processParams.containsKey('geneFilterMinNCells')) ? '--min-number-cells ' + processParams.geneFilterMinNCells : ''}\n        \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "genefilter"
        ],
        "tools_url": [
            "https://bio.tools/genefilter"
        ],
        "tools_dico": [
            {
                "name": "genefilter",
                "uri": "https://bio.tools/genefilter",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3321",
                            "term": "Molecular genetics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3695",
                                    "term": "Filtering"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Some basic functions for filtering genes.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/genefilter.html"
            }
        ],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__CELL_FILTER": {
        "name_process": "SC__SCANPY__CELL_FILTER",
        "string_process": "\nprocess SC__SCANPY__CELL_FILTER {\n\n    container params.sc.scanpy.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n    input:\n        tuple val(sampleId), path(f)\n\n\toutput:\n        tuple val(sampleId), path(\"${sampleId}.SC__SCANPY__CELL_FILTER.${processParams.off}\")\n    \n\tscript:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.filter)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        ${binDir}/filter/sc_cell_gene_filtering.py \\\n            cellfilter \\\n            $f \\\n            ${sampleId}.SC__SCANPY__CELL_FILTER.${processParams.off} \\\n            ${(processParams.containsKey('cellFilterMinNCounts')) ? '--min-n-counts ' + processParams.cellFilterMinNCounts : ''} \\\n            ${(processParams.containsKey('cellFilterMaxNCounts')) ? '--max-n-counts ' + processParams.cellFilterMaxNCounts : ''} \\\n            ${(processParams.containsKey('cellFilterMinNGenes')) ? '--min-n-genes ' + processParams.cellFilterMinNGenes : ''} \\\n            ${(processParams.containsKey('cellFilterMaxNGenes')) ? '--max-n-genes ' + processParams.cellFilterMaxNGenes : ''} \\\n            ${(processParams.containsKey('cellFilterMaxPercentMito')) ? '--max-percent-mito ' + processParams.cellFilterMaxPercentMito : ''}\n        \"\"\"\n\n}",
        "nb_lignes_process": 27,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.filter)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        ${binDir}/filter/sc_cell_gene_filtering.py \\\n            cellfilter \\\n            $f \\\n            ${sampleId}.SC__SCANPY__CELL_FILTER.${processParams.off} \\\n            ${(processParams.containsKey('cellFilterMinNCounts')) ? '--min-n-counts ' + processParams.cellFilterMinNCounts : ''} \\\n            ${(processParams.containsKey('cellFilterMaxNCounts')) ? '--max-n-counts ' + processParams.cellFilterMaxNCounts : ''} \\\n            ${(processParams.containsKey('cellFilterMinNGenes')) ? '--min-n-genes ' + processParams.cellFilterMinNGenes : ''} \\\n            ${(processParams.containsKey('cellFilterMaxNGenes')) ? '--max-n-genes ' + processParams.cellFilterMaxNGenes : ''} \\\n            ${(processParams.containsKey('cellFilterMaxPercentMito')) ? '--max-percent-mito ' + processParams.cellFilterMaxPercentMito : ''}\n        \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "PCACV__FIND_OPTIMAL_NPCS": {
        "name_process": "PCACV__FIND_OPTIMAL_NPCS",
        "string_process": "\nprocess PCACV__FIND_OPTIMAL_NPCS {\n    \n    container params.pcacv.container\n    publishDir \"${params.global.outdir}/data/pcacv\", mode: 'link'\n    label 'compute_resources__pcacv'\n\n    input:\n        tuple \\\n            val(sampleId), \\\n            path(f)\n\n    output:\n        tuple \\\n            val(sampleId), \\\n            stdout, \\\n            emit: optimalNumberPC\n        tuple \\\n            val(sampleId), \\\n            path(\"${sampleId}.PCACV__FIND_OPTIMAL_NPCS.*\"), \\\n            emit: files\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.pcacv.find_optimal_npcs)\n        processParams = sampleParams.local\n        \"\"\"\n        export OPENBLAS_NUM_THREADS=1\n        ${binDir}/run_pca_cv.R \\\n            --input-file ${f} \\\n            --seed ${params.global.seed} \\\n            ${(processParams.containsKey('accessor')) ? '--accessor \"' + processParams.accessor.replace('$','\\\\$') + '\"': ''} \\\n            ${(processParams.containsKey('useVariableFeatures')) ? '--use-variable-features ' + processParams.useVariableFeatures: ''} \\\n            ${(processParams.containsKey('kFold')) ? '--k-fold ' + processParams.libraries: ''} \\\n            ${(processParams.containsKey('fromNPC')) ? '--from-n-pc ' + processParams.fromNPC: ''} \\\n            ${(processParams.containsKey('toNPC')) ? '--to-n-pc ' + processParams.toNPC: ''} \\\n            ${(processParams.containsKey('byNPC')) ? '--by-n-pc ' + processParams.byNPC: ''} \\\n            ${(processParams.containsKey('maxIters')) ? '--max-iters ' + processParams.libraries: ''} \\\n            --n-cores ${task.cpus} \\\n            ${(processParams.containsKey('defaultSVD')) ? '--default-svd ' + processParams.defaultSVD: ''} \\\n            ${(processParams.containsKey('verbose')) ? '--verbose ' + processParams.verbose: ''} \\\n            --output-prefix \"${sampleId}.PCACV__FIND_OPTIMAL_NPCS\" \\\n            > .command.log 2>&1\n        cat \"${sampleId}.PCACV__FIND_OPTIMAL_NPCS.OPTIMAL_NPCS.txt\"\n        \"\"\"\n\n}",
        "nb_lignes_process": 44,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.pcacv.find_optimal_npcs)\n        processParams = sampleParams.local\n        \"\"\"\n        export OPENBLAS_NUM_THREADS=1\n        ${binDir}/run_pca_cv.R \\\n            --input-file ${f} \\\n            --seed ${params.global.seed} \\\n            ${(processParams.containsKey('accessor')) ? '--accessor \"' + processParams.accessor.replace('$','\\\\$') + '\"': ''} \\\n            ${(processParams.containsKey('useVariableFeatures')) ? '--use-variable-features ' + processParams.useVariableFeatures: ''} \\\n            ${(processParams.containsKey('kFold')) ? '--k-fold ' + processParams.libraries: ''} \\\n            ${(processParams.containsKey('fromNPC')) ? '--from-n-pc ' + processParams.fromNPC: ''} \\\n            ${(processParams.containsKey('toNPC')) ? '--to-n-pc ' + processParams.toNPC: ''} \\\n            ${(processParams.containsKey('byNPC')) ? '--by-n-pc ' + processParams.byNPC: ''} \\\n            ${(processParams.containsKey('maxIters')) ? '--max-iters ' + processParams.libraries: ''} \\\n            --n-cores ${task.cpus} \\\n            ${(processParams.containsKey('defaultSVD')) ? '--default-svd ' + processParams.defaultSVD: ''} \\\n            ${(processParams.containsKey('verbose')) ? '--verbose ' + processParams.verbose: ''} \\\n            --output-prefix \"${sampleId}.PCACV__FIND_OPTIMAL_NPCS\" \\\n            > .command.log 2>&1\n        cat \"${sampleId}.PCACV__FIND_OPTIMAL_NPCS.OPTIMAL_NPCS.txt\"\n        \"\"\"",
        "nb_lignes_script": 20,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId",
            "sampleId"
        ],
        "nb_outputs": 2,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.pcacv.container",
            "publishDir \"${params.global.outdir}/data/pcacv\", mode: 'link'",
            "label 'compute_resources__pcacv'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__STAR__MAP_COUNT": {
        "name_process": "SC__STAR__MAP_COUNT",
        "string_process": "\nprocess SC__STAR__MAP_COUNT {\n\n\tcontainer params.sc.star.container\n    label 'compute_resources__star_map_count'\n\n\tinput:\n\t\tfile(starIndex)\n\t\tval starIndexLoaded\n\t\ttuple val(sample), path(fastqs)\n\n\toutput:\n\t\tval success, emit: isDone\n\t\ttuple val(sample), path(\"*ReadsPerGene.out.tab\"), emit: counts optional processParams.containsKey('quantMode') && processParams.quantMode == \"GeneCounts\" ? true: false\n\t\ttuple val(sample), path(\"*.STAR_Aligned.sortedByCoord.out.bam\"), emit: bam\n\n\tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.star.map_count)\n\t\tprocessParams = sampleParams.local\n\t\tsuccess = true\n\t\t\"\"\"\n\t\tSTAR \\\n\t\t\t--genomeLoad LoadAndKeep \\\n\t\t\t--genomeDir ${starIndex} \\\n            --runThreadN ${task.cpus} \\\n\t\t\t${(processParams.containsKey('limitBAMsortRAM')) ? '--limitBAMsortRAM ' + processParams.limitBAMsortRAM: ''} \\\n\t\t\t${(processParams.containsKey('outSAMtype')) ? '--outSAMtype ' + processParams.outSAMtype: ''} \\\n\t\t\t${(processParams.containsKey('quantMode')) ? '--quantMode ' + processParams.quantMode: ''} \\\n\t\t\t${(processParams.containsKey('outReadsUnmapped')) ? '--outReadsUnmapped ' + processParams.outReadsUnmapped: ''} \\\n\t\t\t--readFilesIn ${fastqs} \\\n\t\t\t${(fastqs.name.endsWith(\".gz\")) ? '--readFilesCommand zcat' : ''} \\\n\t\t\t--outFileNamePrefix ${sample}.STAR_\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 33,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.star.map_count)\n\t\tprocessParams = sampleParams.local\n\t\tsuccess = true\n\t\t\"\"\"\n\t\tSTAR \\\n\t\t\t--genomeLoad LoadAndKeep \\\n\t\t\t--genomeDir ${starIndex} \\\n            --runThreadN ${task.cpus} \\\n\t\t\t${(processParams.containsKey('limitBAMsortRAM')) ? '--limitBAMsortRAM ' + processParams.limitBAMsortRAM: ''} \\\n\t\t\t${(processParams.containsKey('outSAMtype')) ? '--outSAMtype ' + processParams.outSAMtype: ''} \\\n\t\t\t${(processParams.containsKey('quantMode')) ? '--quantMode ' + processParams.quantMode: ''} \\\n\t\t\t${(processParams.containsKey('outReadsUnmapped')) ? '--outReadsUnmapped ' + processParams.outReadsUnmapped: ''} \\\n\t\t\t--readFilesIn ${fastqs} \\\n\t\t\t${(fastqs.name.endsWith(\".gz\")) ? '--readFilesCommand zcat' : ''} \\\n\t\t\t--outFileNamePrefix ${sample}.STAR_\n\t\t\"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "STAR"
        ],
        "tools_url": [
            "https://bio.tools/star"
        ],
        "tools_dico": [
            {
                "name": "STAR",
                "uri": "https://bio.tools/star",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Ultrafast universal RNA-seq aligner",
                "homepage": "http://code.google.com/p/rna-star/"
            }
        ],
        "inputs": [
            "starIndex",
            "starIndexLoaded",
            "sample",
            "fastqs"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.star.container",
            "label 'compute_resources__star_map_count'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__STAR__BUILD_INDEX": {
        "name_process": "SC__STAR__BUILD_INDEX",
        "string_process": "\nprocess SC__STAR__BUILD_INDEX {\n\n    container params.sc.star.container\n    label 'compute_resources__star_build_genome'\n\n    input:\n        file(annotation)\n        file(genome)\n\n    output:\n        file(\"STAR_index\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.star.build_genome)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        mkdir STAR_index\n        STAR \\\n            --runThreadN ${task.cpus} \\\n            --runMode genomeGenerate \\\n            --genomeDir STAR_index \\\n            --genomeFastaFiles ${genome} \\\n            --sjdbGTFfile ${annotation} \\\n            --sjdbOverhang ${processParams.sjdbOverhang} \\\n            --genomeSAindexNbases ${processParams.genomeSAindexNbases} # Suggested by STAR (default: 14), otherwise keeps on hanging\n        \"\"\"\n\n}",
        "nb_lignes_process": 27,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.star.build_genome)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        mkdir STAR_index\n        STAR \\\n            --runThreadN ${task.cpus} \\\n            --runMode genomeGenerate \\\n            --genomeDir STAR_index \\\n            --genomeFastaFiles ${genome} \\\n            --sjdbGTFfile ${annotation} \\\n            --sjdbOverhang ${processParams.sjdbOverhang} \\\n            --genomeSAindexNbases ${processParams.genomeSAindexNbases} # Suggested by STAR (default: 14), otherwise keeps on hanging\n        \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "STAR"
        ],
        "tools_url": [
            "https://bio.tools/star"
        ],
        "tools_dico": [
            {
                "name": "STAR",
                "uri": "https://bio.tools/star",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Ultrafast universal RNA-seq aligner",
                "homepage": "http://code.google.com/p/rna-star/"
            }
        ],
        "inputs": [
            "annotation",
            "genome"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.star.container",
            "label 'compute_resources__star_build_genome'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__POPSCLE__DEMUXLET": {
        "name_process": "SC__POPSCLE__DEMUXLET",
        "string_process": "\nprocess SC__POPSCLE__DEMUXLET {\n\n    container params.sc.popscle.container\n    publishDir \"${params.global.outdir}/data\", mode: 'symlink'\n    label 'compute_resources__cpu'\n\n    input:\n        tuple val(sampleId), path(f)\n        file vcf\n\n    output:\n        tuple val(sampleId), path(\"${sampleId}_demuxlet*\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.popscle.demuxlet)\n\t\tprocessParams = sampleParams.local\n\n        \"\"\"\n        popscle demuxlet \\\n            --vcf ${vcf} \\\n            ${(processParams.containsKey('field')) ? '--field ' + processParams.field : ''} \\\n            --plp ${sampleId}_dsc-pileup \\\n            --out ${sampleId}_demuxlet\n        \"\"\"\n}",
        "nb_lignes_process": 24,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.popscle.demuxlet)\n\t\tprocessParams = sampleParams.local\n\n        \"\"\"\n        popscle demuxlet \\\n            --vcf ${vcf} \\\n            ${(processParams.containsKey('field')) ? '--field ' + processParams.field : ''} \\\n            --plp ${sampleId}_dsc-pileup \\\n            --out ${sampleId}_demuxlet\n        \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f",
            "vcf"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.popscle.container",
            "publishDir \"${params.global.outdir}/data\", mode: 'symlink'",
            "label 'compute_resources__cpu'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__POPSCLE__FREEMUXLET": {
        "name_process": "SC__POPSCLE__FREEMUXLET",
        "string_process": "\nprocess SC__POPSCLE__FREEMUXLET {\n\n    container params.sc.popscle.container\n    publishDir \"${params.global.outdir}/data\", mode: 'symlink'\n    label 'compute_resources__cpu'\n\n    input:\n        tuple val(sampleId), path(f)\n\n    output:\n        tuple val(sampleId), path(\"${sampleId}_freemuxlet*\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.popscle.freemuxlet)\n\t\tprocessParams = sampleParams.local\n\n        \"\"\"\n        popscle freemuxlet \\\n            --nsample ${processParams.nSamples} \\\n            --plp ${sampleId}_dsc-pileup \\\n            --out ${sampleId}_freemuxlet\n        \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.popscle.freemuxlet)\n\t\tprocessParams = sampleParams.local\n\n        \"\"\"\n        popscle freemuxlet \\\n            --nsample ${processParams.nSamples} \\\n            --plp ${sampleId}_dsc-pileup \\\n            --out ${sampleId}_freemuxlet\n        \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.popscle.container",
            "publishDir \"${params.global.outdir}/data\", mode: 'symlink'",
            "label 'compute_resources__cpu'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__HARMONY__HARMONY_MATRIX": {
        "name_process": "SC__HARMONY__HARMONY_MATRIX",
        "string_process": "\nprocess SC__HARMONY__HARMONY_MATRIX {\n    \n    container params.sc.harmony.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink'\n    label 'compute_resources__default'\n\n    input:\n        tuple val(sampleId), path(f)\n\n    output:\n        tuple val(sampleId), path(\"${sampleId}.SC__HARMONY__HARMONY_MATRIX.tsv\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.harmony)\n\t\tprocessParams = sampleParams.local\n        varsUseAsArguments = processParams.varsUse.collect({ '--vars-use' + ' ' + it }).join(' ')\n        \"\"\"\n        ${binDir}run_harmony.R \\\n            --seed ${params.global.seed} \\\n            --input-file ${f} \\\n            ${varsUseAsArguments} \\\n            --output-prefix \"${sampleId}.SC__HARMONY__HARMONY_MATRIX\"\n        \"\"\"\n\n}",
        "nb_lignes_process": 24,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.harmony)\n\t\tprocessParams = sampleParams.local\n        varsUseAsArguments = processParams.varsUse.collect({ '--vars-use' + ' ' + it }).join(' ')\n        \"\"\"\n        ${binDir}run_harmony.R \\\n            --seed ${params.global.seed} \\\n            --input-file ${f} \\\n            ${varsUseAsArguments} \\\n            --output-prefix \"${sampleId}.SC__HARMONY__HARMONY_MATRIX\"\n        \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.harmony.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink'",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "PICARD__BAM_TO_FASTQ": {
        "name_process": "PICARD__BAM_TO_FASTQ",
        "string_process": "\nprocess PICARD__BAM_TO_FASTQ {\n\n    container params.picard.container\n    publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n        tuple val(sample), path(bam)\n        file(tmpDir)\n\n    output:\n        tuple val(sample), path('*.unaligned_tagged_polyA_filtered.fastq'), emit: fastq\n\n    script:\n        \"\"\"\n        java -Djava.io.tmpdir=$tmpDir -jar \\\n            /picard.jar \\\n                SamToFastq \\\n                    INPUT=${bam} \\\n                    FASTQ=${sample}.unaligned_tagged_polyA_filtered.fastq\n        \"\"\"\n\n}",
        "nb_lignes_process": 22,
        "string_script": "        \"\"\"\n        java -Djava.io.tmpdir=$tmpDir -jar \\\n            /picard.jar \\\n                SamToFastq \\\n                    INPUT=${bam} \\\n                    FASTQ=${sample}.unaligned_tagged_polyA_filtered.fastq\n        \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "bam",
            "tmpDir"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.picard.container",
            "publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'",
            "label 'compute_resources__cpu','compute_resources__24hqueue'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__DROP_SEQ_TOOLS__DETECT_REPAIR_BARCODE_SYNTHESIS_ERRORS": {
        "name_process": "SC__DROP_SEQ_TOOLS__DETECT_REPAIR_BARCODE_SYNTHESIS_ERRORS",
        "string_process": "\nprocess SC__DROP_SEQ_TOOLS__DETECT_REPAIR_BARCODE_SYNTHESIS_ERRORS {\n\n\tcontainer params.sc.dropseqtools.container\n\tpublishDir \"${params.global.outdir}/02.map\", mode: 'symlink'\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n    \ttuple val(sample), path(bam)\n\n\toutput:\n\t\ttuple val(sample), path(\"*.final_cleaned.bam\"), emit: bam\n\t\ttuple file(\"*.synthesis_stats.txt\"), emit: stats\n\t\t                                                                  \n\n\tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.detect_repair_barcode_synthesis_errors)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\tDetectBeadSynthesisErrors \\\n\t\t\tI=${bam} \\\n\t\t\tO=${sample}.final_cleaned.bam \\\n\t\t\tOUTPUT_STATS=${sample}.synthesis_stats.txt \\\n\t\t\tSUMMARY=${sample}.synthesis_stats.summary.txt \\\n\t\t\tNUM_BARCODES=${processParams.numBarcodes * 2} \\\n\t\t\tPRIMER_SEQUENCE=${processParams.primerSequence} \\\n\t\t\tTMP_DIR=$DWMAX/tmp\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 28,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.detect_repair_barcode_synthesis_errors)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\tDetectBeadSynthesisErrors \\\n\t\t\tI=${bam} \\\n\t\t\tO=${sample}.final_cleaned.bam \\\n\t\t\tOUTPUT_STATS=${sample}.synthesis_stats.txt \\\n\t\t\tSUMMARY=${sample}.synthesis_stats.summary.txt \\\n\t\t\tNUM_BARCODES=${processParams.numBarcodes * 2} \\\n\t\t\tPRIMER_SEQUENCE=${processParams.primerSequence} \\\n\t\t\tTMP_DIR=$DWMAX/tmp\n\t\t\"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "TMPD"
        ],
        "tools_url": [
            "https://bio.tools/tmpd"
        ],
        "tools_dico": [
            {
                "name": "TMPD",
                "uri": "https://bio.tools/tmpd",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plant biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0160",
                            "term": "Sequence sites, features and motifs"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plant science"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plants"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Botany"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plant"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data retrieval"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data extraction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Retrieval"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The Tobacco Markers & Primers Database.",
                "homepage": "http://biodb.sdau.edu.cn/tmpd/index.html"
            }
        ],
        "inputs": [
            "sample",
            "bam"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.dropseqtools.container",
            "publishDir \"${params.global.outdir}/02.map\", mode: 'symlink'",
            "label 'compute_resources__cpu','compute_resources__24hqueue'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__DROP_SEQ_TOOLS__DIGITAL_EXPRESSION": {
        "name_process": "SC__DROP_SEQ_TOOLS__DIGITAL_EXPRESSION",
        "string_process": "\nprocess SC__DROP_SEQ_TOOLS__DIGITAL_EXPRESSION {\n\n    container params.sc.dropseqtools.container\n    publishDir \"03.count\", mode: 'symlink'\n    label 'compute_resources__default'\n\n    input:\n        tuple val(sample), path(bam), val(tag), path(selectedBarcodes)\n\n    output:\n        tuple file(\"*.${tag}.cells_dge.txt.gz\"), emit: dgem\n\n    shell:\n        \"\"\"\n        DigitalExpression \\\n            I=${bam} \\\n            O=${sample}.${tag}.cells_dge.txt.gz \\\n            SUMMARY=${sample}.${tag}.cells_dge.summary.txt \\\n            CELL_BC_FILE=${selectedBarcodes} \\\n            TMP_DIR=$DWMAX/tmp\n        \"\"\"\n\n}",
        "nb_lignes_process": 22,
        "string_script": "        \"\"\"\n        DigitalExpression \\\n            I=${bam} \\\n            O=${sample}.${tag}.cells_dge.txt.gz \\\n            SUMMARY=${sample}.${tag}.cells_dge.summary.txt \\\n            CELL_BC_FILE=${selectedBarcodes} \\\n            TMP_DIR=$DWMAX/tmp\n        \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [
            "TMPD"
        ],
        "tools_url": [
            "https://bio.tools/tmpd"
        ],
        "tools_dico": [
            {
                "name": "TMPD",
                "uri": "https://bio.tools/tmpd",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plant biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0160",
                            "term": "Sequence sites, features and motifs"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plant science"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plants"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Botany"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0780",
                            "term": "Plant"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data retrieval"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data extraction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Retrieval"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The Tobacco Markers & Primers Database.",
                "homepage": "http://biodb.sdau.edu.cn/tmpd/index.html"
            }
        ],
        "inputs": [
            "sample",
            "tag",
            "bam",
            "selectedBarcodes"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.dropseqtools.container",
            "publishDir \"03.count\", mode: 'symlink'",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__DROP_SEQ_TOOLS__BAM_TAG_HISTOGRAM": {
        "name_process": "SC__DROP_SEQ_TOOLS__BAM_TAG_HISTOGRAM",
        "string_process": "\nprocess SC__DROP_SEQ_TOOLS__BAM_TAG_HISTOGRAM {\n    \n    container params.sc.dropseqtools.container\n    publishDir \"${params.global.outdir}/03.count\", mode: 'symlink'\n    label 'compute_resources__default'\n\n    input:\n        tuple val(sample), path(bam)\n    \n    output:\n\t    tuple val(sample), path(\"*.cell_readcounts.txt.gz\")\n    \n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.bam_tag_histogram)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        BAMTagHistogram \\\n            I=${bam} \\\n            O=${sample}.cell_readcounts.txt.gz \\\n            TAG=${processParams.tag}\n        \"\"\"\n\n}",
        "nb_lignes_process": 22,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.bam_tag_histogram)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        BAMTagHistogram \\\n            I=${bam} \\\n            O=${sample}.cell_readcounts.txt.gz \\\n            TAG=${processParams.tag}\n        \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "bam"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sample"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.dropseqtools.container",
            "publishDir \"${params.global.outdir}/03.count\", mode: 'symlink'",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "AUCELL_FROM_FOLDER": {
        "name_process": "AUCELL_FROM_FOLDER",
        "string_process": "\nprocess AUCELL_FROM_FOLDER {\n\n    cache 'deep'\n    container toolParams.container\n    publishDir \"${toolParams.scenicoutdir}/${sampleId}/multi_runs_aucell/\", mode: 'link', overwrite: true\n    label 'compute_resources__scenic_multiruns'\n\n    input:\n        tuple val(sampleId), path(filteredLoom), path(multiRunsAggrRegulonsFolder)\n        val type\n\n    output:\n        tuple val(sampleId), path(\"multi_runs_regulons_auc_${type}.tsv\")\n\n    script:\n        \"\"\"\n        ${binDir}aucell_from_folder.py \\\n            $filteredLoom \\\n            $multiRunsAggrRegulonsFolder \\\n            -o \"multi_runs_regulons_auc_${type}.tsv\" \\\n            --min-genes ${processParams.min_genes_regulon} \\\n            --auc-threshold ${processParams.auc_threshold} \\\n            ${processParams.containsKey('percentile_threshold') ? \"--percentile-threshold \" + processParams.percentile_threshold : \"\"} \\\n            --min-regulon-gene-occurrence ${processParams.min_regulon_gene_occurrence} \\\n            --num-workers ${task.cpus} \\\n            --cell-id-attribute ${toolParams.cell_id_attribute} \\\n            --gene-attribute ${toolParams.gene_attribute}\n        \"\"\"\n\n}",
        "nb_lignes_process": 29,
        "string_script": "        \"\"\"\n        ${binDir}aucell_from_folder.py \\\n            $filteredLoom \\\n            $multiRunsAggrRegulonsFolder \\\n            -o \"multi_runs_regulons_auc_${type}.tsv\" \\\n            --min-genes ${processParams.min_genes_regulon} \\\n            --auc-threshold ${processParams.auc_threshold} \\\n            ${processParams.containsKey('percentile_threshold') ? \"--percentile-threshold \" + processParams.percentile_threshold : \"\"} \\\n            --min-regulon-gene-occurrence ${processParams.min_regulon_gene_occurrence} \\\n            --num-workers ${task.cpus} \\\n            --cell-id-attribute ${toolParams.cell_id_attribute} \\\n            --gene-attribute ${toolParams.gene_attribute}\n        \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "filteredLoom",
            "multiRunsAggrRegulonsFolder",
            "type"
        ],
        "nb_inputs": 4,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "container toolParams.container",
            "publishDir \"${toolParams.scenicoutdir}/${sampleId}/multi_runs_aucell/\", mode: 'link', overwrite: true",
            "label 'compute_resources__scenic_multiruns'"
        ],
        "when": "",
        "stub": ""
    },
    "AGGR_MULTI_RUNS_REGULONS": {
        "name_process": "AGGR_MULTI_RUNS_REGULONS",
        "string_process": "\nprocess AGGR_MULTI_RUNS_REGULONS {\n\n    cache 'deep'\n    container toolParams.container\n    publishDir \"${toolParams.scenicoutdir}/${sampleId}\", mode: 'link', overwrite: true\n    label 'compute_resources__scenic_multiruns'\n\n    input:\n\t\ttuple val(sampleId), path(f)\n\t\tval type\n\n    output:\n    \ttuple val(sampleId), path(\"multi_runs_regulons_${type}\")\n\n\tscript:\n\t\t\"\"\"\n\t\t${binDir}aggregate_multi_runs_regulons.py \\\n\t\t\t${f} \\\n\t\t\t--output \"multi_runs_regulons_${type}\" \\\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 21,
        "string_script": "\t\t\"\"\"\n\t\t${binDir}aggregate_multi_runs_regulons.py \\\n\t\t\t${f} \\\n\t\t\t--output \"multi_runs_regulons_${type}\" \\\n\t\t\"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f",
            "type"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "cache 'deep'",
            "container toolParams.container",
            "publishDir \"${toolParams.scenicoutdir}/${sampleId}\", mode: 'link', overwrite: true",
            "label 'compute_resources__scenic_multiruns'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__CELLRANGER__PREPARE_FOLDER": {
        "name_process": "SC__CELLRANGER__PREPARE_FOLDER",
        "string_process": "\nprocess SC__CELLRANGER__PREPARE_FOLDER {\n\n    publishDir \"${params.global.outdir}/data/raw/cellranger_fastq_folders\", mode: 'symlink', overwrite: true\n    label 'compute_resources__minimal'\n\n    input:\n        tuple val(sampleId), val(fastqs)\n\n    output:\n        tuple val(sampleId), path(\"${sampleId}_s*\")\n\n    script:\n        def cmd = ''\n        for(int i = 0; i < fastqs.size(); i++) {\n            cmd += \"mkdir ${sampleId}_s${i+1}; \"\n            cmd += \"ln -s ${fastqs[i].join(' ')} ${sampleId}_s${i+1}; \"\n        }\n        \"\"\"\n        $cmd\n        \"\"\"\n\n}",
        "nb_lignes_process": 21,
        "string_script": "        def cmd = ''\n        for(int i = 0; i < fastqs.size(); i++) {\n            cmd += \"mkdir ${sampleId}_s${i+1}; \"\n            cmd += \"ln -s ${fastqs[i].join(' ')} ${sampleId}_s${i+1}; \"\n        }\n        \"\"\"\n        $cmd\n        \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "fastqs"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "publishDir \"${params.global.outdir}/data/raw/cellranger_fastq_folders\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__minimal'"
        ],
        "when": "",
        "stub": ""
    },
    "PICARD__SORT_SAM": {
        "name_process": "PICARD__SORT_SAM",
        "string_process": "\nprocess PICARD__SORT_SAM {\n\n    container params.picard.container\n    publishDir \"${params.global.outdir}/02.map\", mode: 'symlink'\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n        tuple val(sample), path(bam)\n        file(tmpDir)\n\n    output:\n        tuple val(sample), path(\"*.STAR_aligned_sorted.bam\")\n    \n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.picard.sort_sam)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        java -Djava.io.tmpdir=$tmpDir -jar \\\n            /picard.jar \\\n                SortSam \\\n                    I=${bam} \\\n                    O=${sample}.STAR_aligned_sorted.bam \\\n                    SO=${processParams.so}\n        \"\"\"\n\n}",
        "nb_lignes_process": 25,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.picard.sort_sam)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        java -Djava.io.tmpdir=$tmpDir -jar \\\n            /picard.jar \\\n                SortSam \\\n                    I=${bam} \\\n                    O=${sample}.STAR_aligned_sorted.bam \\\n                    SO=${processParams.so}\n        \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "bam",
            "tmpDir"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sample"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.picard.container",
            "publishDir \"${params.global.outdir}/02.map\", mode: 'symlink'",
            "label 'compute_resources__cpu','compute_resources__24hqueue'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__ANNOTATE_BY_CELL_METADATA": {
        "name_process": "SC__ANNOTATE_BY_CELL_METADATA",
        "string_process": "\nprocess SC__ANNOTATE_BY_CELL_METADATA {\n\n    container params.sc.scanpy.container\n    publishDir \"${getPublishDir(params.global.outdir,tool)}\", mode: \"${getMode(tool)}\", overwrite: true\n    label 'compute_resources__default'\n\n    input:\n        tuple \\\n            val(sampleId), \\\n            path(f), \\\n            path(metadata)\n                                             \n        val(tool)\n\n    output:\n        tuple \\\n            val(sampleId), \\\n            path(\"${sampleId}.${toolTag}SC__ANNOTATE_BY_CELL_METADATA.h5ad\")\n\n    script:\n        def sampleParams = params.parseConfig(\n            sampleId,\n            params.global,\n            isParamNull(tool) ? params.sc.cell_annotate : getToolParams(params.sc, tool)[\"cell_annotate\"]\n        )\n\t\tprocessParams = sampleParams.local\n        toolTag = isParamNull(tool) ? '' : tool.toUpperCase() + '.'\n        annotationColumnNamesAsArguments = processParams.containsKey(\"annotationColumnNames\") ?\n            processParams.annotationColumnNames.collect({ '--annotation-column-name' + ' ' + it }).join(' ')\n            : ''\n        \"\"\"\n        ${binDir}/sc_h5ad_annotate_by_cell_metadata.py \\\n            ${processParams.containsKey('method') ? '--method ' + processParams.method : ''} \\\n            --index-column-name ${processParams.indexColumnName} \\\n            --sample-id ${sampleId} \\\n            ${processParams.containsKey('sampleColumnName') ? '--sample-column-name ' + processParams.sampleColumnName : ''} \\\n            ${annotationColumnNamesAsArguments} \\\n            $f \\\n            ${metadata} \\\n            --output \"${sampleId}.${toolTag}SC__ANNOTATE_BY_CELL_METADATA.h5ad\"\n        \"\"\"\n\n}",
        "nb_lignes_process": 42,
        "string_script": "        def sampleParams = params.parseConfig(\n            sampleId,\n            params.global,\n            isParamNull(tool) ? params.sc.cell_annotate : getToolParams(params.sc, tool)[\"cell_annotate\"]\n        )\n\t\tprocessParams = sampleParams.local\n        toolTag = isParamNull(tool) ? '' : tool.toUpperCase() + '.'\n        annotationColumnNamesAsArguments = processParams.containsKey(\"annotationColumnNames\") ?\n            processParams.annotationColumnNames.collect({ '--annotation-column-name' + ' ' + it }).join(' ')\n            : ''\n        \"\"\"\n        ${binDir}/sc_h5ad_annotate_by_cell_metadata.py \\\n            ${processParams.containsKey('method') ? '--method ' + processParams.method : ''} \\\n            --index-column-name ${processParams.indexColumnName} \\\n            --sample-id ${sampleId} \\\n            ${processParams.containsKey('sampleColumnName') ? '--sample-column-name ' + processParams.sampleColumnName : ''} \\\n            ${annotationColumnNamesAsArguments} \\\n            $f \\\n            ${metadata} \\\n            --output \"${sampleId}.${toolTag}SC__ANNOTATE_BY_CELL_METADATA.h5ad\"\n        \"\"\"",
        "nb_lignes_script": 20,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f",
            "metadata",
            "tool"
        ],
        "nb_inputs": 4,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${getPublishDir(params.global.outdir,tool)}\", mode: \"${getMode(tool)}\", overwrite: true",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__ANNOTATE_BY_SAMPLE_METADATA": {
        "name_process": "SC__ANNOTATE_BY_SAMPLE_METADATA",
        "string_process": "\nprocess SC__ANNOTATE_BY_SAMPLE_METADATA {\n\n    container params.sc.scanpy.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'link', overwrite: true\n    label 'compute_resources__default'\n\n    input:\n        tuple \\\n            val(sampleId), \\\n            path(f)\n\n    output:\n        tuple \\\n            val(sampleId), \\\n            path(\"${sampleId}.SC__ANNOTATE_BY_SAMPLE_METADATA.${processParams.off}\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.sample_annotate)\n        processParams = sampleParams.local\n\n                              \n        methodAsArgument = ''\n        if(processParams.containsKey(\"by\")) {\n            methodAsArgument = processParams.by.containsKey('method') ? processParams.by.method : ''\n        } else {\n                                                                              \n            methodAsArgument = processParams.containsKey('type') ? processParams.type : methodAsArgument\n        }\n\n                                 \n        metadataFilePathAsArgument = getMetadataFilePath(processParams)\n\n        compIndexColumnNamesFromAdataAsArguments = ''\n        compIndexColumnNamesFromMetadataAsArguments = ''\n        annotationColumnNamesAsArguments = ''\n        if(processParams.containsKey(\"by\")) {\n            compIndexColumnNamesFromAdataAsArguments = processParams.by.containsKey('compIndexColumnNames') ?\n                processParams.by.compIndexColumnNames.collect { key, value -> return key }.collect({ '--adata-comp-index-column-name ' + ' ' + it }).join(' ') :\n                ''\n            compIndexColumnNamesFromMetadataAsArguments = processParams.by.containsKey('compIndexColumnNames') ?\n                processParams.by.compIndexColumnNames.collect { key, value -> return value }.collect({ '--metadata-comp-index-column-name ' + ' ' + it }).join(' ') :\n                ''\n            annotationColumnNamesAsArguments = processParams.by.containsKey('annotationColumnNames') ?\n                processParams.by.annotationColumnNames.collect({ '--annotation-column-name' + ' ' + it }).join(' ') :\n                ''\n        }\n\n                            \n        sampleColumnName = ''\n        if(processParams.containsKey(\"by\")) {\n            if(!processParams.by.containsKey(\"sampleColumnName\")) {\n                throw new Exception(\"VSN ERROR: Missing sampleColumnName param in params.sc.sample_annotate.by.\")\n            }\n            sampleColumnName = processParams.by.sampleColumnName\n        } else {\n            if(!processParams.containsKey(\"sampleColumnName\")) {\n                throw new Exception(\"VSN ERROR: Missing sampleColumnName param in params.sc.sample_annotate.\")\n            }\n                                                                              \n            sampleColumnName = processParams.sampleColumnName\n        }\n\n        \"\"\"\n        ${binDir}/sc_h5ad_annotate_by_sample_metadata.py \\\n            --sample-id ${sampleId} \\\n            ${methodAsArgument != '' ? '--method ' + methodAsArgument : '' } \\\n            ${metadataFilePathAsArgument != '' ? '--metadata-file-path ' + metadataFilePathAsArgument : '' } \\\n            ${'--sample-column-name ' + sampleColumnName} \\\n            ${compIndexColumnNamesFromAdataAsArguments} \\\n            ${compIndexColumnNamesFromMetadataAsArguments} \\\n            ${annotationColumnNamesAsArguments} \\\n            $f \\\n            \"${sampleId}.SC__ANNOTATE_BY_SAMPLE_METADATA.${processParams.off}\"\n        \"\"\"\n\n}",
        "nb_lignes_process": 75,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.sample_annotate)\n        processParams = sampleParams.local\n\n                              \n        methodAsArgument = ''\n        if(processParams.containsKey(\"by\")) {\n            methodAsArgument = processParams.by.containsKey('method') ? processParams.by.method : ''\n        } else {\n                                                                              \n            methodAsArgument = processParams.containsKey('type') ? processParams.type : methodAsArgument\n        }\n\n                                 \n        metadataFilePathAsArgument = getMetadataFilePath(processParams)\n\n        compIndexColumnNamesFromAdataAsArguments = ''\n        compIndexColumnNamesFromMetadataAsArguments = ''\n        annotationColumnNamesAsArguments = ''\n        if(processParams.containsKey(\"by\")) {\n            compIndexColumnNamesFromAdataAsArguments = processParams.by.containsKey('compIndexColumnNames') ?\n                processParams.by.compIndexColumnNames.collect { key, value -> return key }.collect({ '--adata-comp-index-column-name ' + ' ' + it }).join(' ') :\n                ''\n            compIndexColumnNamesFromMetadataAsArguments = processParams.by.containsKey('compIndexColumnNames') ?\n                processParams.by.compIndexColumnNames.collect { key, value -> return value }.collect({ '--metadata-comp-index-column-name ' + ' ' + it }).join(' ') :\n                ''\n            annotationColumnNamesAsArguments = processParams.by.containsKey('annotationColumnNames') ?\n                processParams.by.annotationColumnNames.collect({ '--annotation-column-name' + ' ' + it }).join(' ') :\n                ''\n        }\n\n                            \n        sampleColumnName = ''\n        if(processParams.containsKey(\"by\")) {\n            if(!processParams.by.containsKey(\"sampleColumnName\")) {\n                throw new Exception(\"VSN ERROR: Missing sampleColumnName param in params.sc.sample_annotate.by.\")\n            }\n            sampleColumnName = processParams.by.sampleColumnName\n        } else {\n            if(!processParams.containsKey(\"sampleColumnName\")) {\n                throw new Exception(\"VSN ERROR: Missing sampleColumnName param in params.sc.sample_annotate.\")\n            }\n                                                                              \n            sampleColumnName = processParams.sampleColumnName\n        }\n\n        \"\"\"\n        ${binDir}/sc_h5ad_annotate_by_sample_metadata.py \\\n            --sample-id ${sampleId} \\\n            ${methodAsArgument != '' ? '--method ' + methodAsArgument : '' } \\\n            ${metadataFilePathAsArgument != '' ? '--metadata-file-path ' + metadataFilePathAsArgument : '' } \\\n            ${'--sample-column-name ' + sampleColumnName} \\\n            ${compIndexColumnNamesFromAdataAsArguments} \\\n            ${compIndexColumnNamesFromMetadataAsArguments} \\\n            ${annotationColumnNamesAsArguments} \\\n            $f \\\n            \"${sampleId}.SC__ANNOTATE_BY_SAMPLE_METADATA.${processParams.off}\"\n        \"\"\"",
        "nb_lignes_script": 56,
        "language_script": "bash",
        "tools": [
            "noreturn"
        ],
        "tools_url": [
            "https://bio.tools/noreturn"
        ],
        "tools_dico": [
            {
                "name": "noreturn",
                "uri": "https://bio.tools/noreturn",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Biological databases"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Data management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Information systems"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Databases and information systems"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2048",
                                "term": "Report"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2048",
                                "term": "Report"
                            }
                        ]
                    }
                ],
                "description": "Remove carriage return from ASCII files.",
                "homepage": "http://emboss.open-bio.org/rel/rel6/apps/noreturn.html"
            }
        ],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'link', overwrite: true",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCRUBLET__DOUBLET_DETECTION_REPORT": {
        "name_process": "SC__SCRUBLET__DOUBLET_DETECTION_REPORT",
        "string_process": "\nprocess SC__SCRUBLET__DOUBLET_DETECTION_REPORT {\n\n\tcontainer params.sc.scrublet.container\n\tpublishDir \"${params.global.outdir}/notebooks/intermediate\", mode: 'link', overwrite: true\n\tlabel 'compute_resources__report'\n\n  \tinput:\n\t\tfile(ipynb)\n\t\ttuple \\\n\t\t\tval(sampleId), \\\n            file(scrubletObjectFile), \\\n\t\t\tfile(adataWithScrubletInfo), \\\n\t\t\tfile(adataWithDimRed)\n\n  \toutput:\n    \ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tfile(\"${sampleId}.SC_Scrublet_doublet_detection_report.ipynb\")\n\n  \tscript:\n\t\t\"\"\"\n\t\tpapermill ${ipynb} \\\n\t\t    --report-mode \\\n\t\t\t${sampleId}.SC_Scrublet_doublet_detection_report.ipynb \\\n\t\t\t-p SCRUBLET_OBJECT_FILE $scrubletObjectFile \\\n            -p H5AD_WITH_SCRUBLET_INFO $adataWithScrubletInfo \\\n            -p H5AD_WITH_DIM_RED $adataWithDimRed \\\n\t\t\t-p WORKFLOW_MANIFEST '${params.misc.manifestAsJSON}' \\\n\t\t\t-p WORKFLOW_PARAMETERS '${params.misc.paramsAsJSON}'\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 31,
        "string_script": "\t\t\"\"\"\n\t\tpapermill ${ipynb} \\\n\t\t    --report-mode \\\n\t\t\t${sampleId}.SC_Scrublet_doublet_detection_report.ipynb \\\n\t\t\t-p SCRUBLET_OBJECT_FILE $scrubletObjectFile \\\n            -p H5AD_WITH_SCRUBLET_INFO $adataWithScrubletInfo \\\n            -p H5AD_WITH_DIM_RED $adataWithDimRed \\\n\t\t\t-p WORKFLOW_MANIFEST '${params.misc.manifestAsJSON}' \\\n\t\t\t-p WORKFLOW_PARAMETERS '${params.misc.paramsAsJSON}'\n\t\t\"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ipynb",
            "sampleId",
            "scrubletObjectFile",
            "adataWithScrubletInfo",
            "adataWithDimRed"
        ],
        "nb_inputs": 5,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scrublet.container",
            "publishDir \"${params.global.outdir}/notebooks/intermediate\", mode: 'link', overwrite: true",
            "label 'compute_resources__report'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__CELLRANGER_ATAC__MKFASTQ": {
        "name_process": "SC__CELLRANGER_ATAC__MKFASTQ",
        "string_process": "\nprocess SC__CELLRANGER_ATAC__MKFASTQ {\n\n    publishDir \"${params.global.outdir}/fastqs\", saveAs: { filename -> dirname = filename =~ /(.*)_fastqOut/; \"${dirname[0][1]}\" }, mode: 'link', overwrite: true\n    container toolParams.container\n    label 'compute_resources__cellranger_mkfastq'\n\n    input:\n        file(csv)\n        file(runFolder)\n\n    output:\n        file \"*_fastqOut\"\n\n    script:\n        \"\"\"\n        cellranger-atac mkfastq \\\n            --run=${runFolder} \\\n            --csv=${csv} \\\n            ${(toolParams.mkfastq.containsKey('samplesheet')) ? '--samplesheet ' + toolParams.mkfastq.samplesheet: ''} \\\n            ${(toolParams.mkfastq.containsKey('qc')) ? '--qc ' + toolParams.mkfastq.qc: ''} \\\n            ${(toolParams.mkfastq.containsKey('lanes')) ? '--lanes ' + toolParams.mkfastq.lanes: ''} \\\n            ${(toolParams.mkfastq.containsKey('useBasesMask')) ? '--use-bases-mask ' + toolParams.mkfastq.useBasesMask: ''} \\\n            ${(toolParams.mkfastq.containsKey('deleteUndetermined')) ? '--delete-undetermined ' + toolParams.mkfastq.deleteUndetermined: ''} \\\n            ${(toolParams.mkfastq.containsKey('outputDir')) ? '--output-dir ' + toolParams.mkfastq.outputDir: ''} \\\n            ${(toolParams.mkfastq.containsKey('project')) ? '--project ' + toolParams.mkfastq.project: ''} \\\n            --jobmode=local \\\n            --localcores=${task.cpus} \\\n            --localmem=${task.memory.toGiga()}\n        \n        for sample in \\$(tail -n+2 ${csv} | cut -f2 -d','); do\n            ln -s ${(params.global.containsKey('outputDir')) ? params.global.outputDir + \"*/\\${sample}\" : \"*/outs/fastq_path/*/\\${sample}\"} \\${sample}_fastqOut\n        done\n        \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "        \"\"\"\n        cellranger-atac mkfastq \\\n            --run=${runFolder} \\\n            --csv=${csv} \\\n            ${(toolParams.mkfastq.containsKey('samplesheet')) ? '--samplesheet ' + toolParams.mkfastq.samplesheet: ''} \\\n            ${(toolParams.mkfastq.containsKey('qc')) ? '--qc ' + toolParams.mkfastq.qc: ''} \\\n            ${(toolParams.mkfastq.containsKey('lanes')) ? '--lanes ' + toolParams.mkfastq.lanes: ''} \\\n            ${(toolParams.mkfastq.containsKey('useBasesMask')) ? '--use-bases-mask ' + toolParams.mkfastq.useBasesMask: ''} \\\n            ${(toolParams.mkfastq.containsKey('deleteUndetermined')) ? '--delete-undetermined ' + toolParams.mkfastq.deleteUndetermined: ''} \\\n            ${(toolParams.mkfastq.containsKey('outputDir')) ? '--output-dir ' + toolParams.mkfastq.outputDir: ''} \\\n            ${(toolParams.mkfastq.containsKey('project')) ? '--project ' + toolParams.mkfastq.project: ''} \\\n            --jobmode=local \\\n            --localcores=${task.cpus} \\\n            --localmem=${task.memory.toGiga()}\n        \n        for sample in \\$(tail -n+2 ${csv} | cut -f2 -d','); do\n            ln -s ${(params.global.containsKey('outputDir')) ? params.global.outputDir + \"*/\\${sample}\" : \"*/outs/fastq_path/*/\\${sample}\"} \\${sample}_fastqOut\n        done\n        \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "csv",
            "runFolder"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "publishDir \"${params.global.outdir}/fastqs\", saveAs: { filename -> dirname = filename =~ /(.*)_fastqOut/; \"${dirname[0][1]}\" }, mode: 'link', overwrite: true",
            "container toolParams.container",
            "label 'compute_resources__cellranger_mkfastq'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__UTILS__UPDATE_FEATURE_METADATA_INDEX": {
        "name_process": "SC__UTILS__UPDATE_FEATURE_METADATA_INDEX",
        "string_process": "\nprocess SC__UTILS__UPDATE_FEATURE_METADATA_INDEX {\n\n    container params.sc.scanpy.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'link', overwrite: true\n    label 'compute_resources__default'\n\n    input:\n        tuple val(sampleId), path(f), path(additionalMetadata)\n\n    output:\n        tuple val(sampleId), path(\"${sampleId}.SC__UTILS__UPDATE_FEATURE_METADATA_INDEX.h5ad\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.utils.update_feature_metadata_index)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        ${binDir}/sc_h5ad_update_metadata.py \\\n            --additional-metadata ${additionalMetadata} \\\n            --axis feature \\\n            --index-column-name ${processParams.indexColumnName} \\\n            --join-key ${processParams.joinKey} \\\n            $f \\\n            \"${sampleId}.SC__UTILS__UPDATE_FEATURE_METADATA_INDEX.h5ad\"\n        \"\"\"\n\n}",
        "nb_lignes_process": 25,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.utils.update_feature_metadata_index)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        ${binDir}/sc_h5ad_update_metadata.py \\\n            --additional-metadata ${additionalMetadata} \\\n            --axis feature \\\n            --index-column-name ${processParams.indexColumnName} \\\n            --join-key ${processParams.joinKey} \\\n            $f \\\n            \"${sampleId}.SC__UTILS__UPDATE_FEATURE_METADATA_INDEX.h5ad\"\n        \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f",
            "additionalMetadata"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'link', overwrite: true",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__STAR__SOLO_MAP_COUNT": {
        "name_process": "SC__STAR__SOLO_MAP_COUNT",
        "string_process": "\nprocess SC__STAR__SOLO_MAP_COUNT {\n\n  container params.sc.star.container\n  label 'compute_resources__star_map_count'\n\n  input:\n    file(transcriptome)\n    val genome_loaded\n    file(fastqs)\n\n  output:\n    val success\n                                   \n\n  script:\n    sample = fastqs.getName()\n    _sampleName = sample\n    success = true\n\n    \"\"\"\n    STAR \\\n      --genomeLoad LoadAndKeep \\\n      --soloType Droplet \\\n      --genomeDir ${transcriptome} \\\n      --runThreadN ${task.cpus} \\\n      ${(params.sc.star.map_count.containsKey('limitBAMsortRAM')) ? '--limitBAMsortRAM ' + params.sc.star.map_count.limitBAMsortRAM: ''} \\\n      ${(params.sc.star.map_count.containsKey('outSAMtype')) ? '--outSAMtype ' + params.sc.star.map_count.outSAMtype: ''} \\\n      ${(params.sc.star.map_count.containsKey('quantMode')) ? '--quantMode ' + params.sc.star.map_count.quantMode: ''} \\\n      ${(params.sc.star.map_count.containsKey('outReadsUnmapped')) ? '--outReadsUnmapped ' + params.sc.star.map_count.outReadsUnmapped: ''} \\\n      --readFilesIn ${fastqs} \\\n      ${(fastqs.name.endsWith(\".gz\")) ? '--readFilesCommand zcat' : ''} \\\n      --outFileNamePrefix ${_sampleName}\n    \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "    sample = fastqs.getName()\n    _sampleName = sample\n    success = true\n\n    \"\"\"\n    STAR \\\n      --genomeLoad LoadAndKeep \\\n      --soloType Droplet \\\n      --genomeDir ${transcriptome} \\\n      --runThreadN ${task.cpus} \\\n      ${(params.sc.star.map_count.containsKey('limitBAMsortRAM')) ? '--limitBAMsortRAM ' + params.sc.star.map_count.limitBAMsortRAM: ''} \\\n      ${(params.sc.star.map_count.containsKey('outSAMtype')) ? '--outSAMtype ' + params.sc.star.map_count.outSAMtype: ''} \\\n      ${(params.sc.star.map_count.containsKey('quantMode')) ? '--quantMode ' + params.sc.star.map_count.quantMode: ''} \\\n      ${(params.sc.star.map_count.containsKey('outReadsUnmapped')) ? '--outReadsUnmapped ' + params.sc.star.map_count.outReadsUnmapped: ''} \\\n      --readFilesIn ${fastqs} \\\n      ${(fastqs.name.endsWith(\".gz\")) ? '--readFilesCommand zcat' : ''} \\\n      --outFileNamePrefix ${_sampleName}\n    \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [
            "SAMPLE",
            "STAR"
        ],
        "tools_url": [
            "https://bio.tools/sample",
            "https://bio.tools/star"
        ],
        "tools_dico": [
            {
                "name": "SAMPLE",
                "uri": "https://bio.tools/sample",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3516",
                            "term": "Genotyping experiment"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Genetic mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Genetic map construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Linkage mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Functional mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Genetic cartography"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0282",
                                    "term": "Genetic map generation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The tool is designed to identify regions that are linked to a recessive disease by analysing genotype data from the parents and unaffected sibs of affected individuals. Since this analysis does not use data from affected patients, it is suited to the identification of lethal recessive genes, when the patients may have died before DNA samples could be obtained.",
                "homepage": "http://dna.leeds.ac.uk/sample/"
            },
            {
                "name": "STAR",
                "uri": "https://bio.tools/star",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Ultrafast universal RNA-seq aligner",
                "homepage": "http://code.google.com/p/rna-star/"
            }
        ],
        "inputs": [
            "transcriptome",
            "genome_loaded",
            "fastqs"
        ],
        "nb_inputs": 3,
        "outputs": [
            "success"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.star.container",
            "label 'compute_resources__star_map_count'"
        ],
        "when": "",
        "stub": ""
    },
    "FLYBASER__CONVERT_FBGN_TO_GENE_SYMBOL": {
        "name_process": "FLYBASER__CONVERT_FBGN_TO_GENE_SYMBOL",
        "string_process": "\nprocess FLYBASER__CONVERT_FBGN_TO_GENE_SYMBOL {\n    \n    container params.flybaser.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink'\n    label 'compute_resources__default'\n\n    input:\n        tuple val(sampleId), path(f)\n    \n    output:\n        tuple val(sampleId), path(\"${sampleId}.FLYBASER__CONVERT_FBGN_TO_GENE_SYMBOL.tsv\")\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.flybaser.convert_fbgn_to_gene_symbol)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        ${binDir}convertFBgnToGeneSymbol.R \\\n            ${f} \\\n            ${processParams.columnName} \\\n            \"${sampleId}.FLYBASER__CONVERT_FBGN_TO_GENE_SYMBOL.tsv\"\n        \"\"\"\n\n}",
        "nb_lignes_process": 22,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.flybaser.convert_fbgn_to_gene_symbol)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        ${binDir}convertFBgnToGeneSymbol.R \\\n            ${f} \\\n            ${processParams.columnName} \\\n            \"${sampleId}.FLYBASER__CONVERT_FBGN_TO_GENE_SYMBOL.tsv\"\n        \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.flybaser.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink'",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__H5AD_TO_LOOM": {
        "name_process": "SC__H5AD_TO_LOOM",
        "string_process": "\nprocess SC__H5AD_TO_LOOM {\n\n\tcontainer params.sc.scanpy.container\n    publishDir \"${params.global.outdir}/loom\", mode: 'link', overwrite: true, saveAs: { filename -> \"${sampleId}.SCope_output.loom\" }\n    label 'compute_resources__mem'\n\n\tinput:\n\t\t           \n\t\t                                                                                            \n\t\t                                                                                            \n\t\ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(rawFilteredData), \\\n\t\t\tpath(data)\n\n\toutput:\n\t\ttuple val(sampleId), \\\n\t\tpath(\"${sampleId}.SC__H5AD_TO_LOOM.loom\")\n\n\tscript:\n\t\t\"\"\"\n\t\t${binDir}/h5ad_to_loom.py \\\n\t\t\t${(params.sc.containsKey('scope') && params.sc.scope.genome.length() > 0) ? '--nomenclature \"' + params.sc.scope.genome + '\"' : ''} \\\n\t\t\t${(params.sc.containsKey('scope') && params.sc.scope.tree.level_1.length() > 0 ) ? '--scope-tree-level-1 \"' + params.sc.scope.tree.level_1 + '\"'  : ''} \\\n\t\t\t${(params.sc.containsKey('scope') && params.sc.scope.tree.level_2.length() > 0 ) ? '--scope-tree-level-2 \"' + params.sc.scope.tree.level_2 + '\"'  : ''} \\\n\t\t\t${(params.sc.containsKey('scope') && params.sc.scope.tree.level_3.length() > 0 ) ? '--scope-tree-level-3 \"' + params.sc.scope.tree.level_3 + '\"'  : ''} \\\n\t\t\t${(params.sc.containsKey('scope') && params.sc.scope.containsKey('markers') && params.sc.scope.markers.log_fc_threshold.length() > 0 ) ? '--markers-log-fc-threshold ' + params.sc.scope.markers.log_fc_threshold : ''} \\\n\t\t\t${(params.sc.containsKey('scope') && params.sc.scope.containsKey('markers') && params.sc.scope.markers.fdr_threshold.length() > 0 ) ? '--markers-fdr-threshold ' + params.sc.scope.markers.fdr_threshold : ''} \\\n\t\t\t$data \\\n\t\t\t$rawFilteredData \\\n\t\t\t\"${sampleId}.SC__H5AD_TO_LOOM.loom\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 33,
        "string_script": "\t\t\"\"\"\n\t\t${binDir}/h5ad_to_loom.py \\\n\t\t\t${(params.sc.containsKey('scope') && params.sc.scope.genome.length() > 0) ? '--nomenclature \"' + params.sc.scope.genome + '\"' : ''} \\\n\t\t\t${(params.sc.containsKey('scope') && params.sc.scope.tree.level_1.length() > 0 ) ? '--scope-tree-level-1 \"' + params.sc.scope.tree.level_1 + '\"'  : ''} \\\n\t\t\t${(params.sc.containsKey('scope') && params.sc.scope.tree.level_2.length() > 0 ) ? '--scope-tree-level-2 \"' + params.sc.scope.tree.level_2 + '\"'  : ''} \\\n\t\t\t${(params.sc.containsKey('scope') && params.sc.scope.tree.level_3.length() > 0 ) ? '--scope-tree-level-3 \"' + params.sc.scope.tree.level_3 + '\"'  : ''} \\\n\t\t\t${(params.sc.containsKey('scope') && params.sc.scope.containsKey('markers') && params.sc.scope.markers.log_fc_threshold.length() > 0 ) ? '--markers-log-fc-threshold ' + params.sc.scope.markers.log_fc_threshold : ''} \\\n\t\t\t${(params.sc.containsKey('scope') && params.sc.scope.containsKey('markers') && params.sc.scope.markers.fdr_threshold.length() > 0 ) ? '--markers-fdr-threshold ' + params.sc.scope.markers.fdr_threshold : ''} \\\n\t\t\t$data \\\n\t\t\t$rawFilteredData \\\n\t\t\t\"${sampleId}.SC__H5AD_TO_LOOM.loom\"\n\t\t\"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "rawFilteredData",
            "data"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/loom\", mode: 'link', overwrite: true, saveAs: { filename -> \"${sampleId}.SCope_output.loom\" }",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__H5AD_TO_FILTERED_LOOM": {
        "name_process": "SC__H5AD_TO_FILTERED_LOOM",
        "string_process": "\nprocess SC__H5AD_TO_FILTERED_LOOM {\n\n\tcontainer params.sc.scanpy.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n\tinput:\n\t\ttuple val(sampleId), path(f)\n\n\toutput:\n\t\ttuple val(sampleId), path(\"${sampleId}.filtered.loom\")\n\n\tscript:\n\t\t\"\"\"\n\t\t${binDir}/h5ad_to_filtered_loom.py \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.filtered.loom\"\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 19,
        "string_script": "\t\t\"\"\"\n\t\t${binDir}/h5ad_to_filtered_loom.py \\\n\t\t\t$f \\\n\t\t\t\"${sampleId}.filtered.loom\"\n\t\t\"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "PICARD__CREATE_SEQUENCE_DICTIONARY": {
        "name_process": "PICARD__CREATE_SEQUENCE_DICTIONARY",
        "string_process": "\nprocess PICARD__CREATE_SEQUENCE_DICTIONARY {\n\n    container params.picard.container\n    publishDir \"${params.global.outdir}/00.refdata\", mode: 'symlink'\n    label 'compute_resources__default'\n\n    input:\n        file(genome)\n        file(tmpDir)\n    \n    output:\n        file \"${genome.baseName}.dict\"\n\n    script:\n        \"\"\"\n        java -Djava.io.tmpdir=$tmpDir -jar \\\n            /picard.jar \\\n                CreateSequenceDictionary \\\n                    R=${genome} \\\n                    O=${genome.baseName}.dict\n        \"\"\"\n\n}",
        "nb_lignes_process": 22,
        "string_script": "        \"\"\"\n        java -Djava.io.tmpdir=$tmpDir -jar \\\n            /picard.jar \\\n                CreateSequenceDictionary \\\n                    R=${genome} \\\n                    O=${genome.baseName}.dict\n        \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "genome",
            "tmpDir"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.picard.container",
            "publishDir \"${params.global.outdir}/00.refdata\", mode: 'symlink'",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__DROP_SEQ_TOOLS__TAG_UNALIGNED_BAM_WITH_CELLBARCODE": {
        "name_process": "SC__DROP_SEQ_TOOLS__TAG_UNALIGNED_BAM_WITH_CELLBARCODE",
        "string_process": "\nprocess SC__DROP_SEQ_TOOLS__TAG_UNALIGNED_BAM_WITH_CELLBARCODE {\n\n\tcontainer params.sc.dropseqtools.container\n    publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n    \ttuple val(sample), path(bam)\n\n\toutput:\n\t\ttuple val(sample), path('*.unaligned_tagged_Cell.bam'), emit: bam\n\t\ttuple file('*.unaligned_tagged_Cellular.bam_summary.txt'), emit: report\n\n\tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.tag_unaligned_bam_with_cellbarcode)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\tTagBamWithReadSequenceExtended \\\n\t\t\tINPUT=${bam} \\\n\t\t\tOUTPUT=${sample}.unaligned_tagged_Cell.bam \\\n\t\t\tSUMMARY=${sample}.unaligned_tagged_Cellular.bam_summary.txt \\\n\t\t\tBASE_RANGE=${processParams.baseRange} \\\n\t\t\tBASE_QUALITY=${processParams.baseQuality} \\\n\t\t\tBARCODED_READ=${processParams.barcodedRead} \\\n\t\t\tDISCARD_READ=${processParams.discardRead} \\\n\t\t\tTAG_NAME=${processParams.barcodeTagName} \\\n\t\t\tNUM_BASES_BELOW_QUALITY=${processParams.numBasesBelowQuality}        \n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 29,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.tag_unaligned_bam_with_cellbarcode)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\tTagBamWithReadSequenceExtended \\\n\t\t\tINPUT=${bam} \\\n\t\t\tOUTPUT=${sample}.unaligned_tagged_Cell.bam \\\n\t\t\tSUMMARY=${sample}.unaligned_tagged_Cellular.bam_summary.txt \\\n\t\t\tBASE_RANGE=${processParams.baseRange} \\\n\t\t\tBASE_QUALITY=${processParams.baseQuality} \\\n\t\t\tBARCODED_READ=${processParams.barcodedRead} \\\n\t\t\tDISCARD_READ=${processParams.discardRead} \\\n\t\t\tTAG_NAME=${processParams.barcodeTagName} \\\n\t\t\tNUM_BASES_BELOW_QUALITY=${processParams.numBasesBelowQuality}        \n\t\t\"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "bam"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.dropseqtools.container",
            "publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'",
            "label 'compute_resources__cpu','compute_resources__24hqueue'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__DROP_SEQ_TOOLS__TAG_UNALIGNED_BAM_WITH_CELLMOLECULAR": {
        "name_process": "SC__DROP_SEQ_TOOLS__TAG_UNALIGNED_BAM_WITH_CELLMOLECULAR",
        "string_process": "\nprocess SC__DROP_SEQ_TOOLS__TAG_UNALIGNED_BAM_WITH_CELLMOLECULAR {\n\n    publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'\n    label 'compute_resources__cpu','compute_resources__24hqueue'\n\n    input:\n    \ttuple val(sample), path(bam)\n\n\toutput:\n\t\ttuple val(sample), path('*.unaligned_tagged_CellMolecular.bam'), emit: bam\n\t\ttuple file('*.unaligned_tagged_Molecular.bam_summary.txt'), emit: report\n\n\tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.tag_unaligned_bam_with_cellmolecular)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\tsource $DWMAX/documents/aertslab/scripts/src_dwmax/bash-utils/utils.sh\n\t\tsoftware load drop-seq_tools/1.12\n\t\tTagBamWithReadSequenceExtended \\\n\t\t\tINPUT=${bam} \\\n\t\t\tOUTPUT=${sample}.unaligned_tagged_CellMolecular.bam \\\n\t\t\tSUMMARY=${sample}.unaligned_tagged_Molecular.bam_summary.txt \\\n\t\t\tBASE_RANGE=${processParams.baseRange} \\\n\t\t\tBASE_QUALITY=${processParams.baseQuality} \\\n\t\t\tBARCODED_READ=${processParams.barcodedRead} \\\n\t\t\tDISCARD_READ=${processParams.discardRead} \\\n\t\t\tTAG_NAME=${processParams.barcodeTagName} \\\n\t\t\tNUM_BASES_BELOW_QUALITY=${processParams.numBasesBelowQuality}        \n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 30,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.dropseqtools.tag_unaligned_bam_with_cellmolecular)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\tsource $DWMAX/documents/aertslab/scripts/src_dwmax/bash-utils/utils.sh\n\t\tsoftware load drop-seq_tools/1.12\n\t\tTagBamWithReadSequenceExtended \\\n\t\t\tINPUT=${bam} \\\n\t\t\tOUTPUT=${sample}.unaligned_tagged_CellMolecular.bam \\\n\t\t\tSUMMARY=${sample}.unaligned_tagged_Molecular.bam_summary.txt \\\n\t\t\tBASE_RANGE=${processParams.baseRange} \\\n\t\t\tBASE_QUALITY=${processParams.baseQuality} \\\n\t\t\tBARCODED_READ=${processParams.barcodedRead} \\\n\t\t\tDISCARD_READ=${processParams.discardRead} \\\n\t\t\tTAG_NAME=${processParams.barcodeTagName} \\\n\t\t\tNUM_BASES_BELOW_QUALITY=${processParams.numBasesBelowQuality}        \n\t\t\"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "ABI software"
        ],
        "tools_url": [
            "https://bio.tools/ABI_software"
        ],
        "tools_dico": [
            {
                "name": "ABI software",
                "uri": "https://bio.tools/ABI_software",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3334",
                            "term": "Neurology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3334",
                            "term": "https://en.wikipedia.org/wiki/Neurology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Experimental Brain Computer Interface Software for the ModularEEG.",
                "homepage": "https://users.dcc.uchile.cl/~peortega/abi/"
            }
        ],
        "inputs": [
            "sample",
            "bam"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "publishDir \"${params.global.outdir}/01.clean\", mode: 'symlink'",
            "label 'compute_resources__cpu','compute_resources__24hqueue'"
        ],
        "when": "",
        "stub": ""
    },
    "GET_SRA_DB": {
        "name_process": "GET_SRA_DB",
        "string_process": "\nprocess GET_SRA_DB {\n\n    container params.utils.container\n    publishDir \"${processParams.sraDbOutDir}\", mode: 'link', overwrite: true\n    label 'compute_resources__default'\n\n    output:\n        file(\"SRAmetadb.sqlite\")\n\n    script:\n        \"\"\"\n        pysradb metadb \\\n            --out-dir \".\"\n        \"\"\"\n\n}",
        "nb_lignes_process": 15,
        "string_script": "        \"\"\"\n        pysradb metadb \\\n            --out-dir \".\"\n        \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "pysradb"
        ],
        "tools_url": [
            "https://bio.tools/pysradb"
        ],
        "tools_dico": [
            {
                "name": "pysradb",
                "uri": "https://bio.tools/pysradb",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0091",
                            "term": "Bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "Gene transcripts"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "mRNA features"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Deposition"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data extraction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data deposition"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Python package to query next-generation sequencing metadata and data from NCBI Sequence Read Archive.",
                "homepage": "https://github.com/saketkc/pysradb"
            }
        ],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.utils.container",
            "publishDir \"${processParams.sraDbOutDir}\", mode: 'link', overwrite: true",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SRA_TO_METADATA": {
        "name_process": "SRA_TO_METADATA",
        "string_process": "\nprocess SRA_TO_METADATA {\n\n    container params.utils.container\n    publishDir \"${params.global.outdir}/metadata\", mode: 'link', overwrite: true\n    errorStrategy 'retry'\n    maxRetries 5\n    label 'compute_resources__default'\n\n    input:\n        tuple val(sraId), val(sampleFilters)\n        file(sraDb)\n\n    output:\n        file \"${sraId}_metadata.tsv\"\n\n    script:\n        if(processParams.mode == 'db') {\n            if(sraDb.name != 'NO_FILE') {\n                sraDbAsArgument = \"--sra-db ${sraDb}\"\n            } else {\n                if(!processParams.containsKey('sraDb') || processParams.sraDb == '')\n                    throw new Exception(\"The db modue requires sraDb to be specified\")\n                sraDbAsArgument = '--sra-db ' + processParams.sraDb\n            }\n        } else if(processParams.mode == 'web') {\n            sraDbAsArgument = ''\n        } else {\n            throw new Exception(\"The \"+ processParams.mode +\" mode does not exist. Choose one of: web, db.\")\n        }\n        def sampleFiltersAsArguments = sampleFilters.collect({ '--sample-filter' + ' \"' + it + '\"'}).join(' ')\n        \"\"\"\n        ${binDir}/sra_to_metadata.py \\\n            ${sraId} \\\n            ${sraDbAsArgument} \\\n            ${sampleFiltersAsArguments} \\\n            --output \"${sraId}_metadata.tsv\"\n        \"\"\"\n\n}",
        "nb_lignes_process": 38,
        "string_script": "        if(processParams.mode == 'db') {\n            if(sraDb.name != 'NO_FILE') {\n                sraDbAsArgument = \"--sra-db ${sraDb}\"\n            } else {\n                if(!processParams.containsKey('sraDb') || processParams.sraDb == '')\n                    throw new Exception(\"The db modue requires sraDb to be specified\")\n                sraDbAsArgument = '--sra-db ' + processParams.sraDb\n            }\n        } else if(processParams.mode == 'web') {\n            sraDbAsArgument = ''\n        } else {\n            throw new Exception(\"The \"+ processParams.mode +\" mode does not exist. Choose one of: web, db.\")\n        }\n        def sampleFiltersAsArguments = sampleFilters.collect({ '--sample-filter' + ' \"' + it + '\"'}).join(' ')\n        \"\"\"\n        ${binDir}/sra_to_metadata.py \\\n            ${sraId} \\\n            ${sraDbAsArgument} \\\n            ${sampleFiltersAsArguments} \\\n            --output \"${sraId}_metadata.tsv\"\n        \"\"\"",
        "nb_lignes_script": 20,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sraId",
            "sampleFilters",
            "sraDb"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.utils.container",
            "publishDir \"${params.global.outdir}/metadata\", mode: 'link', overwrite: true",
            "errorStrategy 'retry'",
            "maxRetries 5",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "NORMALIZE_SRA_FASTQS": {
        "name_process": "NORMALIZE_SRA_FASTQS",
        "string_process": "\nprocess NORMALIZE_SRA_FASTQS {\n\n                                                                 \n    label 'compute_resources__default'\n\n    input:\n        tuple val(sampleId), file(fastqs)\n\n    output:\n        tuple val(sampleId), path(\"*.fastq.gz\")\n\n    script:\n        def normalizedFastqs = fastqs\n            .collect {\n                fastq -> normalizeSRAFastQ(fastq, sampleId)\n            }\n        def cmd = ''\n        for(int i = 0; i < normalizedFastqs.size(); i++)\n            cmd += \"ln -s ${normalizedFastqs[i][0]} ${normalizedFastqs[i][1]}; \" \n        \"\"\"\n        $cmd\n        \"\"\"\n\n}",
        "nb_lignes_process": 23,
        "string_script": "        def normalizedFastqs = fastqs\n            .collect {\n                fastq -> normalizeSRAFastQ(fastq, sampleId)\n            }\n        def cmd = ''\n        for(int i = 0; i < normalizedFastqs.size(); i++)\n            cmd += \"ln -s ${normalizedFastqs[i][0]} ${normalizedFastqs[i][1]}; \" \n        \"\"\"\n        $cmd\n        \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [
            "FastQC"
        ],
        "tools_url": [
            "https://bio.tools/fastqc"
        ],
        "tools_dico": [
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            }
        ],
        "inputs": [
            "sampleId",
            "fastqs"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__DIRECTS__SELECT_DEFAULT_CLUSTERING": {
        "name_process": "SC__DIRECTS__SELECT_DEFAULT_CLUSTERING",
        "string_process": "\nprocess SC__DIRECTS__SELECT_DEFAULT_CLUSTERING {\n\n    container params.sc.directs.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink'\n    label 'compute_resources__default'\n\n    input:\n        tuple \\\n            val(sampleId), \\\n            path(f), \\\n            val(stashedParams)\n\n    output:\n        tuple \\\n            val(sampleId), \\\n            path(\"${sampleId}.SC__DIRECTS__SELECT_DEFAULT_CLUSTERING.loom\"), \\\n            val(stashedParams)\n\n    script:\n        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.directs.select_default_clustering)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        ${binDir}select_default_clustering.py \\\n            ${f} \\\n            ${sampleId}.SC__DIRECTS__SELECT_DEFAULT_CLUSTERING.loom \\\n            ${(processParams.containsKey('cellEmbeddingsIndex')) ? '--cell-embeddings-index ' + processParams.cellEmbeddingsIndex : ''} \\\n            ${(processParams.containsKey('fromMinClusterSize')) ? '--from-min-cluster-size ' + processParams.fromMinClusterSize : ''} \\\n            ${(processParams.containsKey('toMinClusterSize')) ? '--to-min-cluster-size ' + processParams.toMinClusterSize : ''} \\\n            ${(processParams.containsKey('byMinClusterSize')) ? '--by-min-cluster-size ' + processParams.byMinClusterSize : ''} \\\n            ${(processParams.containsKey('fromMinSamples')) ? '--from-min-samples ' + processParams.fromMinSamples : ''} \\\n            ${(processParams.containsKey('toMinSamples')) ? '--to-min-samples ' + processParams.toMinSamples : ''} \\\n            ${(processParams.containsKey('byMinSamples')) ? '--by-min-samples ' + processParams.byMinSamples : ''}\n        \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "        def sampleParams = params.parseConfig(sampleId, params.global, params.sc.directs.select_default_clustering)\n\t\tprocessParams = sampleParams.local\n        \"\"\"\n        ${binDir}select_default_clustering.py \\\n            ${f} \\\n            ${sampleId}.SC__DIRECTS__SELECT_DEFAULT_CLUSTERING.loom \\\n            ${(processParams.containsKey('cellEmbeddingsIndex')) ? '--cell-embeddings-index ' + processParams.cellEmbeddingsIndex : ''} \\\n            ${(processParams.containsKey('fromMinClusterSize')) ? '--from-min-cluster-size ' + processParams.fromMinClusterSize : ''} \\\n            ${(processParams.containsKey('toMinClusterSize')) ? '--to-min-cluster-size ' + processParams.toMinClusterSize : ''} \\\n            ${(processParams.containsKey('byMinClusterSize')) ? '--by-min-cluster-size ' + processParams.byMinClusterSize : ''} \\\n            ${(processParams.containsKey('fromMinSamples')) ? '--from-min-samples ' + processParams.fromMinSamples : ''} \\\n            ${(processParams.containsKey('toMinSamples')) ? '--to-min-samples ' + processParams.toMinSamples : ''} \\\n            ${(processParams.containsKey('byMinSamples')) ? '--by-min-samples ' + processParams.byMinSamples : ''}\n        \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f",
            "stashedParams"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId",
            "stashedParams"
        ],
        "nb_outputs": 2,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.directs.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink'",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__PREPARE_OBS_FILTER": {
        "name_process": "SC__PREPARE_OBS_FILTER",
        "string_process": "\nprocess SC__PREPARE_OBS_FILTER {\n\n    container params.sc.scanpy.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'link', overwrite: true\n    label 'compute_resources__default'\n\n    input:\n        tuple \\\n            val(sampleId), \\\n            path(f), \\\n            val(filterConfig)\n                                             \n        val(tool)\n\n    output:\n        tuple \\\n            val(sampleId), \\\n            path(f), \\\n            path(\"${sampleId}.${toolTag}SC__PREPARE_OBS_FILTER.${filterConfig.id}.txt\")\n\n    script:\n        def sampleParams = params.parseConfig(\n            sampleId,\n            params.global,\n            isParamNull(tool) ? params.sc.cell_filter : getToolParams(params.sc, tool)[\"cell_filter\"]\n        )\n\t\tprocessParams = sampleParams.local\n        toolTag = isParamNull(tool) ? '' : tool.toUpperCase() + '.'\n\n        input = null\n        if(processParams.method == 'internal') {\n            input = f\n        } else if (processParams.method == 'external') {\n            input = filterConfig.cellMetaDataFilePath\n        } else {\n            throw new Exception(\"The given method\" + args.method + \" is not implemented. Choose either: internal or external.\")\n        }\n        valuesToKeepFromFilterColumnAsArguments = filterConfig.valuesToKeepFromFilterColumn.collect({ '--value-to-keep-from-filter-column' + ' ' + it }).join(' ')\n        \"\"\"\n        ${binDir}/sc_h5ad_prepare_obs_filter.py \\\n            ${processParams.containsKey('method') ? '--method ' + processParams.method : ''} \\\n            --sample-id ${sampleId} \\\n            --filter-column-name ${filterConfig.filterColumnName} \\\n            ${valuesToKeepFromFilterColumnAsArguments} \\\n            ${filterConfig.containsKey('indexColumnName') ? '--index-column-name ' + filterConfig.indexColumnName : ''} \\\n            ${filterConfig.containsKey('sampleColumnName') ? '--sample-column-name ' + filterConfig.sampleColumnName : ''} \\\n            $input \\\n            \"${sampleId}.${toolTag}SC__PREPARE_OBS_FILTER.${filterConfig.id}.txt\"\n        \"\"\"\n\n}",
        "nb_lignes_process": 50,
        "string_script": "        def sampleParams = params.parseConfig(\n            sampleId,\n            params.global,\n            isParamNull(tool) ? params.sc.cell_filter : getToolParams(params.sc, tool)[\"cell_filter\"]\n        )\n\t\tprocessParams = sampleParams.local\n        toolTag = isParamNull(tool) ? '' : tool.toUpperCase() + '.'\n\n        input = null\n        if(processParams.method == 'internal') {\n            input = f\n        } else if (processParams.method == 'external') {\n            input = filterConfig.cellMetaDataFilePath\n        } else {\n            throw new Exception(\"The given method\" + args.method + \" is not implemented. Choose either: internal or external.\")\n        }\n        valuesToKeepFromFilterColumnAsArguments = filterConfig.valuesToKeepFromFilterColumn.collect({ '--value-to-keep-from-filter-column' + ' ' + it }).join(' ')\n        \"\"\"\n        ${binDir}/sc_h5ad_prepare_obs_filter.py \\\n            ${processParams.containsKey('method') ? '--method ' + processParams.method : ''} \\\n            --sample-id ${sampleId} \\\n            --filter-column-name ${filterConfig.filterColumnName} \\\n            ${valuesToKeepFromFilterColumnAsArguments} \\\n            ${filterConfig.containsKey('indexColumnName') ? '--index-column-name ' + filterConfig.indexColumnName : ''} \\\n            ${filterConfig.containsKey('sampleColumnName') ? '--sample-column-name ' + filterConfig.sampleColumnName : ''} \\\n            $input \\\n            \"${sampleId}.${toolTag}SC__PREPARE_OBS_FILTER.${filterConfig.id}.txt\"\n        \"\"\"",
        "nb_lignes_script": 27,
        "language_script": "bash",
        "tools": [
            "wossinput"
        ],
        "tools_url": [
            "https://bio.tools/wossinput"
        ],
        "tools_dico": [
            {
                "name": "wossinput",
                "uri": "https://bio.tools/wossinput",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0219",
                            "term": "Data submission, annotation and curation"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data retrieval"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data extraction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Retrieval"
                                }
                            ]
                        ],
                        "input": [],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0958",
                                "term": "Tool metadata"
                            }
                        ]
                    }
                ],
                "description": "Find programs by EDAM input data.",
                "homepage": "http://emboss.open-bio.org/rel/rel6/apps/wossinput.html"
            }
        ],
        "inputs": [
            "sampleId",
            "f",
            "filterConfig",
            "tool"
        ],
        "nb_inputs": 4,
        "outputs": [
            "sampleId",
            "f"
        ],
        "nb_outputs": 2,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'link', overwrite: true",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__APPLY_OBS_FILTER": {
        "name_process": "SC__APPLY_OBS_FILTER",
        "string_process": "\nprocess SC__APPLY_OBS_FILTER {\n\n    container params.sc.scanpy.container\n    publishDir \"${params.global.outdir}/data/intermediate\", mode: 'link', overwrite: true\n    label 'compute_resources__default'\n\n    input:\n        tuple \\\n            val(sampleId), \\\n            path(f), \\\n            path(filters)\n                                             \n        val(tool)\n\n    output:\n        tuple \\\n            val(sampleId), \\\n            path(\"${sampleId}.${toolTag}SC__APPLY_OBS_FILTER.${processParams.off}\")\n\n    script:\n        def sampleParams = params.parseConfig(\n            sampleId,\n            params.global,\n            isParamNull(tool) ? params.sc.cell_filter : getToolParams(params.sc, tool)[\"cell_filter\"]\n        )\n\t\tprocessParams = sampleParams.local\n        toolTag = isParamNull(tool) ? '' : tool.toUpperCase() + '.'\n\n        filtersAsArguments = filters.collect({ '--filter-file-path' + ' ' + it }).join(' ')\n        \"\"\"\n        ${binDir}/sc_h5ad_apply_obs_filter.py \\\n            $f \\\n            --output \"${sampleId}.${toolTag}SC__APPLY_OBS_FILTER.${processParams.off}\" \\\n            $filtersAsArguments\n        \"\"\"\n\n}",
        "nb_lignes_process": 36,
        "string_script": "        def sampleParams = params.parseConfig(\n            sampleId,\n            params.global,\n            isParamNull(tool) ? params.sc.cell_filter : getToolParams(params.sc, tool)[\"cell_filter\"]\n        )\n\t\tprocessParams = sampleParams.local\n        toolTag = isParamNull(tool) ? '' : tool.toUpperCase() + '.'\n\n        filtersAsArguments = filters.collect({ '--filter-file-path' + ' ' + it }).join(' ')\n        \"\"\"\n        ${binDir}/sc_h5ad_apply_obs_filter.py \\\n            $f \\\n            --output \"${sampleId}.${toolTag}SC__APPLY_OBS_FILTER.${processParams.off}\" \\\n            $filtersAsArguments\n        \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f",
            "filters",
            "tool"
        ],
        "nb_inputs": 4,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'link', overwrite: true",
            "label 'compute_resources__default'"
        ],
        "when": "",
        "stub": ""
    },
    "SC__SCANPY__BATCH_EFFECT_CORRECTION": {
        "name_process": "SC__SCANPY__BATCH_EFFECT_CORRECTION",
        "string_process": "\nprocess SC__SCANPY__BATCH_EFFECT_CORRECTION {\n\n  \tcontainer params.sc.scanpy.container\n  \tpublishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true\n    label 'compute_resources__mem'\n\n  \tinput:\n    \ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(f), \\\n\t\t\tval(stashedParams)\n\n  \toutput:\n    \ttuple \\\n\t\t\tval(sampleId), \\\n\t\t\tpath(\"${sampleId}.SC__SCANPY__BATCH_EFFECT_CORRECTION.${processParams.off}\"), \\\n\t\t\tval(stashedParams)\n\n\tscript:\n\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.batch_effect_correct)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}aggregate/sc_batch_effect_correction.py \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t--output-file \"${sampleId}.SC__SCANPY__BATCH_EFFECT_CORRECTION.${processParams.off}\" \\\n\t\t\t${(processParams.containsKey('key')) ? '--key ' + processParams.key : ''} \\\n\t\t\t${(processParams.containsKey('batchKey')) ? '--batch-key ' + processParams.batchKey : ''} \\\n\t\t\t${(processParams.containsKey('nPcs')) ? '--n-pcs ' + processParams.nPcs : ''} \\\n\t\t\t${(processParams.containsKey('k')) ? '--k ' + processParams.k : ''} \\\n\t\t\t${(processParams.containsKey('varIndex')) ? '--var-index ' + processParams.varIndex : ''} \\\n\t\t\t${(processParams.containsKey('varSubset')) ? '--var-subset ' + processParams.varSubset : ''} \\\n            --n-jobs ${task.cpus} \\\n\t\t\t${(processParams.containsKey('neighborsWithinBatch')) ? '--neighbors-within-batch ' + processParams.neighborsWithinBatch : ''} \\\n\t\t\t${(processParams.containsKey('trim')) ? '--trim ' + processParams.trim : ''} \\\n\t\t\t$f\n\t\t\"\"\"\n\n}",
        "nb_lignes_process": 37,
        "string_script": "\t\tdef sampleParams = params.parseConfig(sampleId, params.global, params.sc.scanpy.batch_effect_correct)\n\t\tprocessParams = sampleParams.local\n\t\t\"\"\"\n\t\t${binDir}aggregate/sc_batch_effect_correction.py \\\n\t\t\t${(processParams.containsKey('method')) ? '--method ' + processParams.method : ''} \\\n\t\t\t--output-file \"${sampleId}.SC__SCANPY__BATCH_EFFECT_CORRECTION.${processParams.off}\" \\\n\t\t\t${(processParams.containsKey('key')) ? '--key ' + processParams.key : ''} \\\n\t\t\t${(processParams.containsKey('batchKey')) ? '--batch-key ' + processParams.batchKey : ''} \\\n\t\t\t${(processParams.containsKey('nPcs')) ? '--n-pcs ' + processParams.nPcs : ''} \\\n\t\t\t${(processParams.containsKey('k')) ? '--k ' + processParams.k : ''} \\\n\t\t\t${(processParams.containsKey('varIndex')) ? '--var-index ' + processParams.varIndex : ''} \\\n\t\t\t${(processParams.containsKey('varSubset')) ? '--var-subset ' + processParams.varSubset : ''} \\\n            --n-jobs ${task.cpus} \\\n\t\t\t${(processParams.containsKey('neighborsWithinBatch')) ? '--neighbors-within-batch ' + processParams.neighborsWithinBatch : ''} \\\n\t\t\t${(processParams.containsKey('trim')) ? '--trim ' + processParams.trim : ''} \\\n\t\t\t$f\n\t\t\"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f",
            "stashedParams"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId",
            "stashedParams"
        ],
        "nb_outputs": 2,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container params.sc.scanpy.container",
            "publishDir \"${params.global.outdir}/data/intermediate\", mode: 'symlink', overwrite: true",
            "label 'compute_resources__mem'"
        ],
        "when": "",
        "stub": ""
    },
    "PUBLISH_LOOM": {
        "name_process": "PUBLISH_LOOM",
        "string_process": "\nprocess PUBLISH_LOOM {\n\n    publishDir \"${toolParams.scenicoutdir}/${sampleId}\", mode: 'link', overwrite: true, saveAs: { filename -> toolParams.scenicScopeOutputLoom }\n    label 'compute_resources__minimal'\n\n    input:\n        tuple val(sampleId), path(f)\n\n    output:\n        tuple val(sampleId), path(f)\n\n    script:\n        \"\"\"\n        \"\"\"\n\n}",
        "nb_lignes_process": 15,
        "string_script": "        \"\"\"\n        \"\"\"",
        "nb_lignes_script": 1,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "f"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "publishDir \"${toolParams.scenicoutdir}/${sampleId}\", mode: 'link', overwrite: true, saveAs: { filename -> toolParams.scenicScopeOutputLoom }",
            "label 'compute_resources__minimal'"
        ],
        "when": "",
        "stub": ""
    },
    "VISUALIZE": {
        "name_process": "VISUALIZE",
        "string_process": "\nprocess VISUALIZE {\n\n    container toolParams.container\n    label 'compute_resources__scenic'\n\n    input:\n        tuple val(sampleId), path(inputLoom)\n\n    output:\n        tuple val(sampleId), path(\"scenic_visualize.loom\")\n\n    script:\n        \"\"\"\n        ${binDir}add_visualization.py \\\n            --loom_input ${inputLoom} \\\n            --loom_output scenic_visualize.loom \\\n            --num_workers ${task.cpus}\n        \"\"\"\n\n}",
        "nb_lignes_process": 19,
        "string_script": "        \"\"\"\n        ${binDir}add_visualization.py \\\n            --loom_input ${inputLoom} \\\n            --loom_output scenic_visualize.loom \\\n            --num_workers ${task.cpus}\n        \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "inputLoom"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container toolParams.container",
            "label 'compute_resources__scenic'"
        ],
        "when": "",
        "stub": ""
    },
    "MERGE_MOTIF_TRACK_LOOMS": {
        "name_process": "MERGE_MOTIF_TRACK_LOOMS",
        "string_process": "\nprocess MERGE_MOTIF_TRACK_LOOMS {\n\n    container toolParams.container\n    publishDir \"${toolParams.scenicoutdir}/${sampleId}\", mode: 'link', overwrite: true\n    label 'compute_resources__scenic'\n\n    input:\n        tuple val(sampleId), path(motifLoom), path(trackLoom)\n\n    output:\n        tuple val(sampleId), path(toolParams.scenicOutputLoom)\n\n    script:\n        toolParams = params.sc.scenic\n        \"\"\"\n        ${binDir}merge_motif_track_loom.py \\\n            --loom_motif ${motifLoom} \\\n            --loom_track ${trackLoom} \\\n            --loom_output ${toolParams.scenicOutputLoom}\n        \"\"\"\n\n}",
        "nb_lignes_process": 21,
        "string_script": "        toolParams = params.sc.scenic\n        \"\"\"\n        ${binDir}merge_motif_track_loom.py \\\n            --loom_motif ${motifLoom} \\\n            --loom_track ${trackLoom} \\\n            --loom_output ${toolParams.scenicOutputLoom}\n        \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "motifLoom",
            "trackLoom"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container toolParams.container",
            "publishDir \"${toolParams.scenicoutdir}/${sampleId}\", mode: 'link', overwrite: true",
            "label 'compute_resources__scenic'"
        ],
        "when": "",
        "stub": ""
    },
    "APPEND_SCENIC_LOOM": {
        "name_process": "APPEND_SCENIC_LOOM",
        "string_process": "\nprocess APPEND_SCENIC_LOOM {\n\n    container toolParams.container\n    publishDir \"${params.global.outdir}/loom\", mode: 'link', overwrite: true\n    label 'compute_resources__scenic'\n\n    input:\n        tuple val(sampleId), path(scopeLoom), path(scenicLoom)\n\n    output:\n        tuple val(sampleId), path(\"${sampleId}.${toolParams.scenicScopeOutputLoom}\")\n\n    script:\n        toolParams = params.sc.scenic\n        \"\"\"\n        ${binDir}append_results_to_existing_loom.py \\\n            --loom_scope ${scopeLoom} \\\n            --loom_scenic ${scenicLoom} \\\n            --loom_output ${sampleId}.${toolParams.scenicScopeOutputLoom}\n        \"\"\"\n\n}",
        "nb_lignes_process": 21,
        "string_script": "        toolParams = params.sc.scenic\n        \"\"\"\n        ${binDir}append_results_to_existing_loom.py \\\n            --loom_scope ${scopeLoom} \\\n            --loom_scenic ${scenicLoom} \\\n            --loom_output ${sampleId}.${toolParams.scenicScopeOutputLoom}\n        \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sampleId",
            "scopeLoom",
            "scenicLoom"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sampleId"
        ],
        "nb_outputs": 1,
        "name_workflow": "FredHutch__dvc-pipelines",
        "directive": [
            "container toolParams.container",
            "publishDir \"${params.global.outdir}/loom\", mode: 'link', overwrite: true",
            "label 'compute_resources__scenic'"
        ],
        "when": "",
        "stub": ""
    }
}