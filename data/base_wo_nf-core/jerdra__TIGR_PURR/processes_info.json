{
    "apply_n4": {
        "name_process": "apply_n4",
        "string_process": "\nprocess apply_n4{\n\n    label 'niworkflows'\n    publishDir \"$params.out/$params.application/$sub\", \\\n                mode: 'copy', \\\n                pattern: \"${sub}*_corrected.nii.gz\", \\\n                saveAs: { it.replace(\"_corrected.nii.gz\", \"_n4.nii.gz\") }\n\n\n    input:\n    tuple val(sub), path(t1)\n\n    output:\n    tuple val(sub), path(\"${sub}*_corrected.nii.gz\"), emit: n4\n\n    shell:\n    '''\n    t1=$(basename !{t1})\n    sub_w_desc=${t1%.nii.gz}\n    python /scripts/process_file.py !{t1} !{params.bspline} !{params.niter}\n    mv n4_wf/corrected_img/${sub_w_desc}_corrected.nii.gz .\n    '''\n\n}",
        "nb_lignes_process": 23,
        "string_script": "    '''\n    t1=$(basename !{t1})\n    sub_w_desc=${t1%.nii.gz}\n    python /scripts/process_file.py !{t1} !{params.bspline} !{params.niter}\n    mv n4_wf/corrected_img/${sub_w_desc}_corrected.nii.gz .\n    '''",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sub",
            "t1"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "label 'niworkflows'",
            "publishDir \"$params.out/$params.application/$sub\", mode: 'copy', pattern: \"${sub}*_corrected.nii.gz\", saveAs: { it.replace(\"_corrected.nii.gz\", \"_n4.nii.gz\") }"
        ],
        "when": "",
        "stub": ""
    },
    "publish_pepolar": {
        "name_process": "publish_pepolar",
        "string_process": "\nprocess publish_pepolar{\n\n      \n                                                            \n      \n\n    publishDir path: \"${params.out}/${sub}\",\\\n               mode: 'copy',\\\n               saveAs: { \"${name}.nii.gz\" },\n               pattern: \"*.nii.gz\",\n               overwrite: params.rewrite\n\n    publishDir path: \"${params.out}/${sub}\",\\\n               mode: 'copy',\\\n               saveAs: { \"${name}.json\" },\n               pattern: \"*.json\",\n               overwrite: params.rewrite\n\n    input:\n    tuple val(sub), path(img), path(json), val(name)\n\n    output:\n    tuple path(img), path(json)\n\n    shell:\n    '''\n    echo \"Transferring !{img} to !{name}\"\n    '''\n}",
        "nb_lignes_process": 28,
        "string_script": "    '''\n    echo \"Transferring !{img} to !{name}\"\n    '''",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sub",
            "name",
            "img",
            "json"
        ],
        "nb_inputs": 4,
        "outputs": [
            "json"
        ],
        "nb_outputs": 1,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "publishDir path: \"${params.out}/${sub}\", mode: 'copy', saveAs: { \"${name}.nii.gz\" } , pattern: \"*.nii.gz\" , overwrite: params.rewrite",
            "publishDir path: \"${params.out}/${sub}\", mode: 'copy', saveAs: { \"${name}.json\" } , pattern: \"*.json\" , overwrite: params.rewrite"
        ],
        "when": "",
        "stub": ""
    },
    "gen_pepolar": {
        "name_process": "gen_pepolar",
        "string_process": "\nprocess gen_pepolar{\n\n      \n                                                              \n      \n\n    label 'FSL'\n\n    input:\n    tuple val(sub), val(ser), path(bold)\n\n    output:\n    tuple val(sub), val(ser), path(\"${sub}_${ser}_boldref.nii.gz\"),\\\n    emit: boldref\n\n    shell:\n    '''\n    #!/bin/bash\n    fslroi !{bold} !{sub}_!{ser}_boldref.nii.gz 0 1\n    '''\n}",
        "nb_lignes_process": 20,
        "string_script": "    '''\n    #!/bin/bash\n    fslroi !{bold} !{sub}_!{ser}_boldref.nii.gz 0 1\n    '''",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sub",
            "ser",
            "bold"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "label 'FSL'"
        ],
        "when": "",
        "stub": ""
    },
    "denoise_image": {
        "name_process": "denoise_image",
        "string_process": "\nprocess denoise_image{\n\n    label 'fmriprep'\n\n    input:\n    tuple val(sub), path(t1w), path(mask)\n\n    output:\n    tuple val(sub), path(\"${sub}_desc-denoised.nii.gz\"), emit: denoised\n\n    shell:\n    '''\n    DenoiseImage -d 3 -i !{t1w} -x !{mask} -o !{sub}_desc-denoised.nii.gz\n    '''\n\n}",
        "nb_lignes_process": 15,
        "string_script": "    '''\n    DenoiseImage -d 3 -i !{t1w} -x !{mask} -o !{sub}_desc-denoised.nii.gz\n    '''",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sub",
            "t1w",
            "mask"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "label 'fmriprep'"
        ],
        "when": "",
        "stub": ""
    },
    "apply_mask": {
        "name_process": "apply_mask",
        "string_process": "\nprocess apply_mask{\n\n    label 'fmriprep'\n\n    input:\n    tuple val(sub), path(t1w), path(mask)\n\n    output:\n    tuple val(sub), path(\"${sub}_desc-masked.nii.gz\"), emit: masked\n\n    shell:\n    '''\n    fslmaths !{t1w} -mas !{mask} !{sub}_desc-masked.nii.gz\n    '''\n}",
        "nb_lignes_process": 14,
        "string_script": "    '''\n    fslmaths !{t1w} -mas !{mask} !{sub}_desc-masked.nii.gz\n    '''",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sub",
            "t1w",
            "mask"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "label 'fmriprep'"
        ],
        "when": "",
        "stub": ""
    },
    "fast": {
        "name_process": "fast",
        "string_process": "\nprocess fast{\n\n    label 'fmriprep'\n\n    input:\n    tuple val(sub), path(t1w)\n\n    output:\n    tuple val(sub),\\\n    path(\"${sub}_desc-prob_2.nii.gz\"), path(\"${sub}_desc-prob_0.nii.gz\"), emit: tpm\n\n    shell:\n    '''\n    fast -p -g --nobias -o t1 !{t1w}\n    rename \"s/t1_/!{sub}_desc-/g\" t1*\n    '''\n\n}",
        "nb_lignes_process": 17,
        "string_script": "    '''\n    fast -p -g --nobias -o t1 !{t1w}\n    rename \"s/t1_/!{sub}_desc-/g\" t1*\n    '''",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "DFAST"
        ],
        "tools_url": [
            "https://bio.tools/dfast"
        ],
        "tools_dico": [
            {
                "name": "DFAST",
                "uri": "https://bio.tools/dfast",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0097",
                            "term": "Nucleic acid structure analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0097",
                            "term": "Nucleic acid structure"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0362",
                                    "term": "Genome annotation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Flexible prokaryotic genome annotation pipeline for faster genome publication.",
                "homepage": "https://dfast.nig.ac.jp/"
            }
        ],
        "inputs": [
            "sub",
            "t1w"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sub"
        ],
        "nb_outputs": 1,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "label 'fmriprep'"
        ],
        "when": "",
        "stub": ""
    },
    "gen_confounds": {
        "name_process": "gen_confounds",
        "string_process": "\nprocess gen_confounds{\n\n    label 'fmriprep'\n\n    input:\n    tuple val(sub), val(ses),\\\n    path(t1), path(t1_bm),\\\n    path(wm), path(csf),\\\n    path(func), path(func_bm), path(func_json),\\\n    val(base)\n\n    output:\n    tuple val(sub), val(base),  path(\"${base}_new_confounds.tsv\"), emit: confounds\n    tuple val(sub), val(base),  path(\"${base}_new_confounds.json\"), emit: confounds_metadata\n    tuple val(sub), val(base),  path(\"${base}_wm_roi.nii.gz\"), emit: wm\n    tuple val(sub), val(base),  path(\"${base}_csf_roi.nii.gz\"), emit: csf\n    tuple val(sub), val(base),  path(\"${base}_acc_roi.nii.gz\"), emit: acc\n\n    shell:\n    '''\n    PYTHONPATH=/scripts\n    /scripts/confounds.py $(pwd)/!{t1} $(pwd)/!{t1_bm} $(pwd)/!{wm} $(pwd)/!{csf} \\\n                         $(pwd)/!{func} $(pwd)/!{func_bm} $(pwd)/!{func_json} \\\n                         --workdir $(pwd) $(pwd)/!{base}\n    rename 's/_confounds/_new_confounds/g' *confounds*\n    '''\n\n}",
        "nb_lignes_process": 27,
        "string_script": "    '''\n    PYTHONPATH=/scripts\n    /scripts/confounds.py $(pwd)/!{t1} $(pwd)/!{t1_bm} $(pwd)/!{wm} $(pwd)/!{csf} \\\n                         $(pwd)/!{func} $(pwd)/!{func_bm} $(pwd)/!{func_json} \\\n                         --workdir $(pwd) $(pwd)/!{base}\n    rename 's/_confounds/_new_confounds/g' *confounds*\n    '''",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sub",
            "ses",
            "t1",
            "t1_bm",
            "wm",
            "csf",
            "func",
            "func_bm",
            "func_json",
            "base"
        ],
        "nb_inputs": 10,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "label 'fmriprep'"
        ],
        "when": "",
        "stub": ""
    },
    "update_confounds": {
        "name_process": "update_confounds",
        "string_process": "\nprocess update_confounds{\n\n    label 'fmriprep'\n\n    input:\n    tuple val(sub), val(base), val(ses),\\\n    path(new_confounds), path(confounds)\n\n    output:\n    tuple val(sub), val(base), val(ses),\\\n    path(\"${base}_merged_confounds.tsv\"), emit: confounds\n\n    shell:\n    '''\n    #!/usr/bin/env python\n    import numpy as np\n    import pandas as pd\n\n    # Load in TSV files\n    old_tsv = pd.read_csv(\"!{confounds}\", delimiter=\"\\t\")\n    new_tsv = pd.read_csv(\"!{new_confounds}\", delimiter=\"\\t\")\n\n    # Calculate derivates and powers\n    deriv_cols = [\"white_matter\", \"csf\"]\n    for d in deriv_cols:\n        deriv_name = d + \"_derivative1\"\n        sq_name = d + \"_power2\"\n        deriv_sq_name = d + \"_derivative1_power2\"\n        new_tsv[deriv_name] = new_tsv[d].diff()\n        new_tsv[sq_name] = new_tsv[d] ** 2\n        new_tsv[deriv_sq_name] = new_tsv[deriv_name]**2\n\n    # Rename heads in\n    cols = new_tsv.columns\n    new_tsv.columns = [\"{}_fixed\".format(c) for c in cols]\n\n    # Drop component based columns in old CSV\n    drop_cols = [c for c in old_tsv.columns if \"a_comp_cor\" in c.lower()]\n    old_tsv.drop(columns = drop_cols, inplace=True)\n\n    # Drop columns in old tsv\n    old_tsv.drop(columns = cols, inplace=True, errors=\"ignore\")\n    out = old_tsv.merge(new_tsv, left_index=True, right_index=True)\n    out.to_csv(\"!{base}_merged_confounds.tsv\", sep=\"\\\\t\", index=False)\n\n    '''\n}",
        "nb_lignes_process": 46,
        "string_script": "    '''\n    #!/usr/bin/env python\n    import numpy as np\n    import pandas as pd\n\n    # Load in TSV files\n    old_tsv = pd.read_csv(\"!{confounds}\", delimiter=\"\\t\")\n    new_tsv = pd.read_csv(\"!{new_confounds}\", delimiter=\"\\t\")\n\n    # Calculate derivates and powers\n    deriv_cols = [\"white_matter\", \"csf\"]\n    for d in deriv_cols:\n        deriv_name = d + \"_derivative1\"\n        sq_name = d + \"_power2\"\n        deriv_sq_name = d + \"_derivative1_power2\"\n        new_tsv[deriv_name] = new_tsv[d].diff()\n        new_tsv[sq_name] = new_tsv[d] ** 2\n        new_tsv[deriv_sq_name] = new_tsv[deriv_name]**2\n\n    # Rename heads in\n    cols = new_tsv.columns\n    new_tsv.columns = [\"{}_fixed\".format(c) for c in cols]\n\n    # Drop component based columns in old CSV\n    drop_cols = [c for c in old_tsv.columns if \"a_comp_cor\" in c.lower()]\n    old_tsv.drop(columns = drop_cols, inplace=True)\n\n    # Drop columns in old tsv\n    old_tsv.drop(columns = cols, inplace=True, errors=\"ignore\")\n    out = old_tsv.merge(new_tsv, left_index=True, right_index=True)\n    out.to_csv(\"!{base}_merged_confounds.tsv\", sep=\"\\\\t\", index=False)\n\n    '''",
        "nb_lignes_script": 32,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sub",
            "base",
            "ses",
            "new_confounds",
            "confounds"
        ],
        "nb_inputs": 5,
        "outputs": [
            "ses"
        ],
        "nb_outputs": 1,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "label 'fmriprep'"
        ],
        "when": "",
        "stub": ""
    },
    "dump_masks": {
        "name_process": "dump_masks",
        "string_process": "\nprocess dump_masks{\n\n    publishDir path: \"$params.dump_masks\",\\\n               pattern: \"*.nii.gz\",\\\n               mode: 'copy'\n\n\n    input:\n    tuple val(sub), val(base),\\\n    path(csf), path(wm), path(acc)\n\n    output:\n    tuple path(csf), path(wm), path(acc)\n\n    shell:\n    '''\n    echo \"Dumping !{base} ROIs into !{params.dump_masks}\"\n    '''\n}",
        "nb_lignes_process": 18,
        "string_script": "    '''\n    echo \"Dumping !{base} ROIs into !{params.dump_masks}\"\n    '''",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sub",
            "base",
            "csf",
            "wm",
            "acc"
        ],
        "nb_inputs": 5,
        "outputs": [
            "acc"
        ],
        "nb_outputs": 1,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "publishDir path: \"$params.dump_masks\", pattern: \"*.nii.gz\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "update_metadata": {
        "name_process": "update_metadata",
        "string_process": "\nprocess update_metadata{\n\n    label 'fmriprep'\n\n    input:\n    tuple val(sub), val(base), val(ses),\\\n    path(new_meta), path(meta)\n\n    output:\n    tuple val(sub), val(base), val(ses),\\\n    path(\"${base}_merged_confounds.json\"), emit: metadata\n\n    shell:\n    '''\n    #!/usr/bin/env python\n\n    import json\n\n    with open(\"!{meta}\", \"r\") as f:\n        meta = json.load(f)\n\n    with open(\"!{new_meta}\", \"r\") as f:\n        new_meta = json.load(f)\n\n    # Remove columns being replaced\n    cleaned_meta = {k: v for k,v in meta.items() if not ((\"a_comp\" in k) or (\"dropped\" in k))}\n    out_meta = {**cleaned_meta, **new_meta}\n\n    with open(\"!{base}_merged_confounds.json\", \"w\") as f:\n        json.dump(out_meta, f, indent=2)\n    '''\n}",
        "nb_lignes_process": 31,
        "string_script": "    '''\n    #!/usr/bin/env python\n\n    import json\n\n    with open(\"!{meta}\", \"r\") as f:\n        meta = json.load(f)\n\n    with open(\"!{new_meta}\", \"r\") as f:\n        new_meta = json.load(f)\n\n    # Remove columns being replaced\n    cleaned_meta = {k: v for k,v in meta.items() if not ((\"a_comp\" in k) or (\"dropped\" in k))}\n    out_meta = {**cleaned_meta, **new_meta}\n\n    with open(\"!{base}_merged_confounds.json\", \"w\") as f:\n        json.dump(out_meta, f, indent=2)\n    '''",
        "nb_lignes_script": 17,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sub",
            "base",
            "ses",
            "new_meta",
            "meta"
        ],
        "nb_inputs": 5,
        "outputs": [
            "ses"
        ],
        "nb_outputs": 1,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "label 'fmriprep'"
        ],
        "when": "",
        "stub": ""
    },
    "write_to_fmriprep": {
        "name_process": "write_to_fmriprep",
        "string_process": "\nprocess write_to_fmriprep{\n\n    label 'fmriprep'\n    stageInMode 'copy'\n\n    publishDir path: \"$params.fmriprep/$sub/$ses/func\",\\\n               pattern: \"*tsv\",\\\n               saveAs: { f -> \"${base}_desc-confounds_fixedregressors.tsv\" },\\\n               mode: 'copy'\n\n    publishDir path: \"$params.fmriprep/$sub/$ses/func\",\\\n               pattern: \"*json\",\\\n               saveAs: { f -> \"${base}_desc-confounds_fixedregressors.json\" },\\\n               mode: 'copy'\n\n    input:\n    tuple val(sub), val(base), val(ses),\\\n    path(confounds), path(metadata)\n\n    output:\n    tuple path(confounds), path(metadata)\n\n    shell:\n    '''\n    echo \"Writing confounds to !{params.fmriprep}/!{sub}/!{ses}/func/!{base}_desc-confound_fixedregressors.tsv\"\n    echo \"Writing confounds to !{params.fmriprep}/!{sub}/!{ses}/func/!{base}_desc-confound_fixedregressors.json\"\n    '''\n}",
        "nb_lignes_process": 27,
        "string_script": "    '''\n    echo \"Writing confounds to !{params.fmriprep}/!{sub}/!{ses}/func/!{base}_desc-confound_fixedregressors.tsv\"\n    echo \"Writing confounds to !{params.fmriprep}/!{sub}/!{ses}/func/!{base}_desc-confound_fixedregressors.json\"\n    '''",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sub",
            "base",
            "ses",
            "confounds",
            "metadata"
        ],
        "nb_inputs": 5,
        "outputs": [
            "metadata"
        ],
        "nb_outputs": 1,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "label 'fmriprep'",
            "stageInMode 'copy'",
            "publishDir path: \"$params.fmriprep/$sub/$ses/func\", pattern: \"*tsv\", saveAs: { f -> \"${base}_desc-confounds_fixedregressors.tsv\" }, mode: 'copy'",
            "publishDir path: \"$params.fmriprep/$sub/$ses/func\", pattern: \"*json\", saveAs: { f -> \"${base}_desc-confounds_fixedregressors.json\" }, mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "save_invocation": {
        "name_process": "save_invocation",
        "string_process": "\nprocess save_invocation{\n\n    input:\n    path invocation\n\n    shell:\n    '''\n\n    invoke_name=$(basename !{params.invocation})\n    invoke_name=${invoke_name%.json}\n    datestr=$(date +\"%d-%m-%Y\")\n\n    # If file with same date is available, check if they are the same\n    if [ -f !{params.out}/${invoke_name}_${datestr}.json ]; then\n\n        DIFF=$(diff !{params.invocation} !{params.out}/${invoke_name}_${datestr}.json)\n\n        if [ \"$DIFF\" != \"\" ]; then\n            >&2 echo \"Error invocations have identical names but are not identical!\"\n            exit 1\n        fi\n\n    else\n        cp -n !{params.invocation} !{params.out}/${invoke_name}_${datestr}.json\n    fi\n    '''\n}",
        "nb_lignes_process": 26,
        "string_script": "    '''\n\n    invoke_name=$(basename !{params.invocation})\n    invoke_name=${invoke_name%.json}\n    datestr=$(date +\"%d-%m-%Y\")\n\n    # If file with same date is available, check if they are the same\n    if [ -f !{params.out}/${invoke_name}_${datestr}.json ]; then\n\n        DIFF=$(diff !{params.invocation} !{params.out}/${invoke_name}_${datestr}.json)\n\n        if [ \"$DIFF\" != \"\" ]; then\n            >&2 echo \"Error invocations have identical names but are not identical!\"\n            exit 1\n        fi\n\n    else\n        cp -n !{params.invocation} !{params.out}/${invoke_name}_${datestr}.json\n    fi\n    '''",
        "nb_lignes_script": 19,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "invocation"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "modify_invocation": {
        "name_process": "modify_invocation",
        "string_process": "\nprocess modify_invocation{\n\n    input:\n\n    val sub\n    output:\n    tuple val(sub), path(\"${sub}.json\"), emit: json\n\n    \"\"\"\n\n    #!/usr/bin/env python\n\n    import json\n    import sys\n\n    out_file = '${sub}.json'\n    invoke_file = '${params.invocation}'\n\n    x = '${sub}'.replace('sub-','')\n\n    with open(invoke_file,'r') as f:\n        j_dict = json.load(f)\n\n    j_dict.update({'participant_label' : [x]})\n\n    with open(out_file,'w') as f:\n        json.dump(j_dict,f,indent=4)\n\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "\"\"\"\n\n    #!/usr/bin/env python\n\n    import json\n    import sys\n\n    out_file = '${sub}.json'\n    invoke_file = '${params.invocation}'\n\n    x = '${sub}'.replace('sub-','')\n\n    with open(invoke_file,'r') as f:\n        j_dict = json.load(f)\n\n    j_dict.update({'participant_label' : [x]})\n\n    with open(out_file,'w') as f:\n        json.dump(j_dict,f,indent=4)\n\n    \"\"\"",
        "nb_lignes_script": 20,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sub"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "run_bids": {
        "name_process": "run_bids",
        "string_process": "\nprocess run_bids{\n\n    time { params.cluster_time(s) }\n    queue { params.cluster_queue(s) }\n\n    input:\n    tuple path(sub_input), val(s)\n\n    scratch params.scratchDir\n    stageInMode 'copy'\n\n    shell:\n    '''\n\n    #Stop error rejection\n    set +e\n\n    #Make logging folder\n    logging_dir=!{params.out}/pipeline_logs/!{params.application}\n    mkdir -p ${logging_dir}\n\n    #Set up logging output\n    sub_json=!{sub_input}\n    sub=${sub_json%.json}\n    log_out=${logging_dir}/${sub}.out\n    log_err=${logging_dir}/${sub}.err\n\n\n    echo \"TASK ATTEMPT !{task.attempt}\" >> ${log_out}\n    echo \"============================\" >> ${log_out}\n    echo \"TASK ATTEMPT !{task.attempt}\" >> ${log_err}\n    echo \"============================\" >> ${log_err}\n\n    mkdir work\n    bosh exec launch \\\n    -v !{params.bids}:/bids \\\n    -v !{params.out}:/output \\\n    -v !{params.license}:/license \\\n    !{ (params.resources) ? \"-v $params.resources:/resources\" : \"\"} \\\n    -v $(pwd)/work:/work \\\n    !{params.descriptor} $(pwd)/!{sub_input} \\\n    --imagepath !{params.simg} -x --stream 2>> ${log_out} \\\n                                           1>> ${log_err}\n\n    '''\n}",
        "nb_lignes_process": 45,
        "string_script": "    '''\n\n    #Stop error rejection\n    set +e\n\n    #Make logging folder\n    logging_dir=!{params.out}/pipeline_logs/!{params.application}\n    mkdir -p ${logging_dir}\n\n    #Set up logging output\n    sub_json=!{sub_input}\n    sub=${sub_json%.json}\n    log_out=${logging_dir}/${sub}.out\n    log_err=${logging_dir}/${sub}.err\n\n\n    echo \"TASK ATTEMPT !{task.attempt}\" >> ${log_out}\n    echo \"============================\" >> ${log_out}\n    echo \"TASK ATTEMPT !{task.attempt}\" >> ${log_err}\n    echo \"============================\" >> ${log_err}\n\n    mkdir work\n    bosh exec launch \\\n    -v !{params.bids}:/bids \\\n    -v !{params.out}:/output \\\n    -v !{params.license}:/license \\\n    !{ (params.resources) ? \"-v $params.resources:/resources\" : \"\"} \\\n    -v $(pwd)/work:/work \\\n    !{params.descriptor} $(pwd)/!{sub_input} \\\n    --imagepath !{params.simg} -x --stream 2>> ${log_out} \\\n                                           1>> ${log_err}\n\n    '''",
        "nb_lignes_script": 32,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "s",
            "sub_input"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "time { params.cluster_time(s) }",
            "queue { params.cluster_queue(s) }"
        ],
        "when": "",
        "stub": ""
    },
    "split_invalid": {
        "name_process": "split_invalid",
        "string_process": " process split_invalid{\n\n            publishDir \"$params.out/pipeline_logs/$params.application/\", \\\n                     mode: 'copy', \\\n                     saveAs: { 'invalid_subjects.log' }, \\\n                     pattern: 'invalid'\n\n            input:\n            val subs from input_sub_channel.collect()\n            val available_subs from Channel.from(input_sessions).collect()\n\n            output:\n            file 'valid' into valid_subs\n            file 'invalid' optional true into invalid_subs\n\n\n            \"\"\"\n            #!/usr/bin/env python\n\n            import os\n            print(os.getcwd())\n\n            def nflist_2_pylist(x):\n                x = x.strip('[').strip(']')\n                x = [x.strip(' ').strip(\"\\\\n\") for x in x.split(',')]\n                return x\n\n            #Process full BIDS subjects\n            bids_subs = nflist_2_pylist(\"$available_subs\")\n            input_subs = nflist_2_pylist(\"$subs\")\n\n            print(input_subs)\n            valid_subs = [x for x in input_subs if x in bids_subs]\n            invalid_subs = [x for x in input_subs if x not in valid_subs]\n\n            with open('valid','w') as f:\n                f.writelines(\"\\\\n\".join(valid_subs))\n\n            if invalid_subs:\n\n                with open('invalid','w') as f:\n                    f.writelines(\"\\\\n\".join(invalid_subs))\n                    f.write(\"\\\\n\")\n\n            \"\"\"\n\n    }",
        "nb_lignes_process": 45,
        "string_script": "\"\"\"\n            #!/usr/bin/env python\n\n            import os\n            print(os.getcwd())\n\n            def nflist_2_pylist(x):\n                x = x.strip('[').strip(']')\n                x = [x.strip(' ').strip(\"\\\\n\") for x in x.split(',')]\n                return x\n\n            #Process full BIDS subjects\n            bids_subs = nflist_2_pylist(\"$available_subs\")\n            input_subs = nflist_2_pylist(\"$subs\")\n\n            print(input_subs)\n            valid_subs = [x for x in input_subs if x in bids_subs]\n            invalid_subs = [x for x in input_subs if x not in valid_subs]\n\n            with open('valid','w') as f:\n                f.writelines(\"\\\\n\".join(valid_subs))\n\n            if invalid_subs:\n\n                with open('invalid','w') as f:\n                    f.writelines(\"\\\\n\".join(invalid_subs))\n                    f.write(\"\\\\n\")\n\n            \"\"\"",
        "nb_lignes_script": 28,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "input_sub_channel"
        ],
        "nb_inputs": 1,
        "outputs": [
            "valid_subs",
            "invalid_subs"
        ],
        "nb_outputs": 2,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "publishDir \"$params.out/pipeline_logs/$params.application/\", mode: 'copy', saveAs: { 'invalid_subjects.log' }, pattern: 'invalid'"
        ],
        "when": "",
        "stub": ""
    },
    "resample": {
        "name_process": "resample",
        "string_process": "\nprocess resample {\n\n    stageInMode 'copy'\n    module \"FSL/5.0.11\"\n\n    input:\n    set val(sub), file(echo1), file(echo2) from fieldmap_input\n\n    output:\n    set val(sub), file(echo1), file(echo2) into resampled_fieldmaps\n\n    shell:\n    '''\n    #!/bin/bash\n\n    THRES=0.0001\n\n    # Grab info from image 1\n    img1_x=$(fslinfo !{echo1} | grep pixdim1 | awk '{print $2}')\n    img1_y=$(fslinfo !{echo1} | grep pixdim2 | awk '{print $2}')\n    img1_z=$(fslinfo !{echo1} | grep pixdim3 | awk '{print $2}')\n    img1_voxarea=$(echo \"$img1_x*$img1_y*$img1_z\" | bc -l)\n\n    # Grab info from image 1\n    img2_x=$(fslinfo !{echo2} | grep pixdim1 | awk '{print $2}')\n    img2_y=$(fslinfo !{echo2} | grep pixdim2 | awk '{print $2}')\n    img2_z=$(fslinfo !{echo2} | grep pixdim3 | awk '{print $2}')\n    img2_voxarea=$(echo \"$img2_x*$img2_y*$img2_z\" | bc -l)\n\n    #Calculate difference\n    diff=$(echo \"$img1_voxarea - $img2_voxarea\" | bc -l)\n\n    #Check image areas then downsample if needed\n    if (( $(echo \"$diff > $THRES\" | bc -l) )); then\n        in=!{echo2}\n        ref=!{echo1}\n    elif (( $(echo \"$diff < -1*$THRES\" | bc -l) )); then\n        in=!{echo1}\n        ref=!{echo2}\n    else\n        exit 0\n    fi\n\n    #Split images then generate transforms\n    fslsplit $in image1_ -t\n    fslsplit $ref image2_ -t\n\n    #Resample image\n    flirt -in image1_0000.nii.gz -ref image2_0000.nii.gz -omat resamp_mat\n\n    #Apply transformation\n    flirt -in $in -ref image2_0000.nii.gz -applyxfm -init resamp_mat -out transformed.nii.gz\n\n    #Replace\n    mv transformed.nii.gz $in\n    '''\n\n}",
        "nb_lignes_process": 57,
        "string_script": "    '''\n    #!/bin/bash\n\n    THRES=0.0001\n\n    # Grab info from image 1\n    img1_x=$(fslinfo !{echo1} | grep pixdim1 | awk '{print $2}')\n    img1_y=$(fslinfo !{echo1} | grep pixdim2 | awk '{print $2}')\n    img1_z=$(fslinfo !{echo1} | grep pixdim3 | awk '{print $2}')\n    img1_voxarea=$(echo \"$img1_x*$img1_y*$img1_z\" | bc -l)\n\n    # Grab info from image 1\n    img2_x=$(fslinfo !{echo2} | grep pixdim1 | awk '{print $2}')\n    img2_y=$(fslinfo !{echo2} | grep pixdim2 | awk '{print $2}')\n    img2_z=$(fslinfo !{echo2} | grep pixdim3 | awk '{print $2}')\n    img2_voxarea=$(echo \"$img2_x*$img2_y*$img2_z\" | bc -l)\n\n    #Calculate difference\n    diff=$(echo \"$img1_voxarea - $img2_voxarea\" | bc -l)\n\n    #Check image areas then downsample if needed\n    if (( $(echo \"$diff > $THRES\" | bc -l) )); then\n        in=!{echo2}\n        ref=!{echo1}\n    elif (( $(echo \"$diff < -1*$THRES\" | bc -l) )); then\n        in=!{echo1}\n        ref=!{echo2}\n    else\n        exit 0\n    fi\n\n    #Split images then generate transforms\n    fslsplit $in image1_ -t\n    fslsplit $ref image2_ -t\n\n    #Resample image\n    flirt -in image1_0000.nii.gz -ref image2_0000.nii.gz -omat resamp_mat\n\n    #Apply transformation\n    flirt -in $in -ref image2_0000.nii.gz -applyxfm -init resamp_mat -out transformed.nii.gz\n\n    #Replace\n    mv transformed.nii.gz $in\n    '''",
        "nb_lignes_script": 43,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fieldmap_input"
        ],
        "nb_inputs": 1,
        "outputs": [
            "resampled_fieldmaps"
        ],
        "nb_outputs": 1,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "stageInMode 'copy'",
            "module \"FSL/5.0.11\""
        ],
        "when": "",
        "stub": ""
    },
    "fieldmaps": {
        "name_process": "fieldmaps",
        "string_process": "\nprocess fieldmaps {\n\n    module \"FSL/5.0.11\"\n\n    publishDir \"$params.out/${params.application}/$sub\", \\\n                mode: 'copy', \\\n                pattern:  \"magnitude.nii.gz\" , \\\n                saveAs: { echo1.getName().replace(\"$params.echo1\",\"MAG\") }\n\n    publishDir \"$params.out/${params.application}/$sub\", \\\n                mode: 'copy', \\\n                pattern:  \"fieldmap.nii.gz\" , \\\n                saveAs: { echo1.getName().replace(\"$params.echo1\",\"FIELDMAP\") }\n\n    publishDir \"$params.out/${params.application}/$sub\", \\\n                mode: 'copy', \\\n                pattern: \"json\", \\\n                saveAs: { echo1.getName().replace(\"$params.echo1\",\"FIELDMAP\").replace('.nii.gz','.json') }\n\n    input:\n    set val(sub), file(echo1), file(echo2) from resampled_fieldmaps\n\n    output:\n    set val(sub), file(\"fieldmap.nii.gz\"), file(\"magnitude.nii.gz\"), file(\"json\") into fieldmap_output\n\n\n    shell:\n    '''\n    #!/bin/bash\n\n\n    #Set up logging\n    logging_dir=!{params.out}/pipeline_logs/!{params.application}\n    mkdir -p ${logging_dir}\n\n    #Get processID\n    pid=$$\n    log_out=${logging_dir}/!{sub}_${pid}.out\n    log_err=${logging_dir}/!{sub}_${pid}.err\n\n    echo \"TASK ATTEMPT !{task.attempt}\" >> ${log_out}\n    echo \"============================\" >> ${log_out}\n    echo \"TASK ATTEMPT !{task.attempt}\" >> ${log_err}\n    echo \"============================\" >> ${log_err}\n\n    FM65=!{echo1}\n    FM85=!{echo2}\n\n    echo \"Using ECHO1 $FM65\" >> ${log_out}\n    echo \"Using ECHO2 $FM85\" >> ${log_out}\n\n    ####split (pre) fieldmap files and log\n    (\n    fslsplit ${FM65} split65 -t\n    bet split650000 65mag -R -f 0.5 -m\n    fslmaths split650002 -mas 65mag_mask 65realm\n    fslmaths split650003 -mas 65mag_mask 65imagm\n\n    fslsplit ${FM85} split85 -t\n    bet split850000 85mag -R -f 0.5 -m\n    fslmaths split850002 -mas 85mag_mask 85realm\n    fslmaths split850003 -mas 85mag_mask 85imagm\n\n    ####calc phase difference\n    fslmaths 65realm -mul 85realm realeq1\n    fslmaths 65imagm -mul 85imagm realeq2\n    fslmaths 65realm -mul 85imagm imageq1\n    fslmaths 85realm -mul 65imagm imageq2\n    fslmaths realeq1 -add realeq2 realvol\n    fslmaths imageq1 -sub imageq2 imagvol\n\n    ####create complex image and extract phase and magnitude\n    fslcomplex -complex realvol imagvol calcomplex\n    fslcomplex -realphase calcomplex phasevolume 0 1\n    fslcomplex -realabs calcomplex magnitude 0 1\n\n    ####unwrap phase\n    prelude -a 65mag -p phasevolume -m 65mag_mask -o phasevolume_maskUW\n\n    ####divide by TE diff in seconds -> radians/sec\n    fslmaths phasevolume_maskUW -div 0.002 fieldmap\n\n    ####copy in geometry information\n    fslcpgeom ${FM65} fieldmap.nii.gz -d\n    fslcpgeom ${FM65} magnitude.nii.gz -d\n    ) 2>> ${log_err} 1>> ${log_out}\n\n    ####make a JSON file containing the units\n    echo ' { \"Units\": \"rad/s\" } ' > json\n\n    '''\n}",
        "nb_lignes_process": 91,
        "string_script": "    '''\n    #!/bin/bash\n\n\n    #Set up logging\n    logging_dir=!{params.out}/pipeline_logs/!{params.application}\n    mkdir -p ${logging_dir}\n\n    #Get processID\n    pid=$$\n    log_out=${logging_dir}/!{sub}_${pid}.out\n    log_err=${logging_dir}/!{sub}_${pid}.err\n\n    echo \"TASK ATTEMPT !{task.attempt}\" >> ${log_out}\n    echo \"============================\" >> ${log_out}\n    echo \"TASK ATTEMPT !{task.attempt}\" >> ${log_err}\n    echo \"============================\" >> ${log_err}\n\n    FM65=!{echo1}\n    FM85=!{echo2}\n\n    echo \"Using ECHO1 $FM65\" >> ${log_out}\n    echo \"Using ECHO2 $FM85\" >> ${log_out}\n\n    ####split (pre) fieldmap files and log\n    (\n    fslsplit ${FM65} split65 -t\n    bet split650000 65mag -R -f 0.5 -m\n    fslmaths split650002 -mas 65mag_mask 65realm\n    fslmaths split650003 -mas 65mag_mask 65imagm\n\n    fslsplit ${FM85} split85 -t\n    bet split850000 85mag -R -f 0.5 -m\n    fslmaths split850002 -mas 85mag_mask 85realm\n    fslmaths split850003 -mas 85mag_mask 85imagm\n\n    ####calc phase difference\n    fslmaths 65realm -mul 85realm realeq1\n    fslmaths 65imagm -mul 85imagm realeq2\n    fslmaths 65realm -mul 85imagm imageq1\n    fslmaths 85realm -mul 65imagm imageq2\n    fslmaths realeq1 -add realeq2 realvol\n    fslmaths imageq1 -sub imageq2 imagvol\n\n    ####create complex image and extract phase and magnitude\n    fslcomplex -complex realvol imagvol calcomplex\n    fslcomplex -realphase calcomplex phasevolume 0 1\n    fslcomplex -realabs calcomplex magnitude 0 1\n\n    ####unwrap phase\n    prelude -a 65mag -p phasevolume -m 65mag_mask -o phasevolume_maskUW\n\n    ####divide by TE diff in seconds -> radians/sec\n    fslmaths phasevolume_maskUW -div 0.002 fieldmap\n\n    ####copy in geometry information\n    fslcpgeom ${FM65} fieldmap.nii.gz -d\n    fslcpgeom ${FM65} magnitude.nii.gz -d\n    ) 2>> ${log_err} 1>> ${log_out}\n\n    ####make a JSON file containing the units\n    echo ' { \"Units\": \"rad/s\" } ' > json\n\n    '''",
        "nb_lignes_script": 63,
        "language_script": "bash",
        "tools": [
            "DBETH",
            "SECA"
        ],
        "tools_url": [
            "https://bio.tools/dbeth",
            "https://bio.tools/seca"
        ],
        "tools_dico": [
            {
                "name": "DBETH",
                "uri": "https://bio.tools/dbeth",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2840",
                            "term": "Toxicology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2815",
                            "term": "Human biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0602",
                            "term": "Molecular interactions, pathways and networks"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_2815",
                            "term": "Humans"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0303",
                                    "term": "Fold recognition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3461",
                                    "term": "Virulence prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0477",
                                    "term": "Protein modelling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3439",
                                    "term": "Pathway or network prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3092",
                                    "term": "Protein feature detection"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0303",
                                    "term": "Protein domain prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0303",
                                    "term": "Fold prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0303",
                                    "term": "Protein fold recognition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0303",
                                    "term": "Domain prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0303",
                                    "term": "Protein fold prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3461",
                                    "term": "Pathogenicity prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0477",
                                    "term": "Homology modelling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0477",
                                    "term": "Comparative modelling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0477",
                                    "term": "Protein structure comparative modelling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0477",
                                    "term": "Homology structure modelling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3092",
                                    "term": "Protein feature prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3092",
                                    "term": "Protein feature recognition"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Database of sequences, structures, interaction networks and analytical results for 229 exotoxins, from 26 different human pathogenic bacterial genus. All toxins are classified into 24 different Toxin classes. The aim is to provide a comprehensive database for human pathogenic bacterial exotoxins.",
                "homepage": "http://www.hpppi.iicb.res.in/btox/"
            },
            {
                "name": "SECA",
                "uri": "https://bio.tools/seca",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2533",
                            "term": "DNA mutation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2885",
                            "term": "DNA polymorphism"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3661",
                                    "term": "SNP annotation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A user-friendly web-based application to perform SNP effect concordance analysis using GWA summary results.",
                "homepage": "http://neurogenetics.qimrberghofer.edu.au/SECA/"
            }
        ],
        "inputs": [
            "resampled_fieldmaps"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fieldmap_output"
        ],
        "nb_outputs": 1,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "module \"FSL/5.0.11\"",
            "publishDir \"$params.out/${params.application}/$sub\", mode: 'copy', pattern: \"magnitude.nii.gz\" , saveAs: { echo1.getName().replace(\"$params.echo1\",\"MAG\") }",
            "publishDir \"$params.out/${params.application}/$sub\", mode: 'copy', pattern: \"fieldmap.nii.gz\" , saveAs: { echo1.getName().replace(\"$params.echo1\",\"FIELDMAP\") }",
            "publishDir \"$params.out/${params.application}/$sub\", mode: 'copy', pattern: \"json\", saveAs: { echo1.getName().replace(\"$params.echo1\",\"FIELDMAP\").replace('.nii.gz','.json') }"
        ],
        "when": "",
        "stub": ""
    },
    "transfer_preartifact": {
        "name_process": "transfer_preartifact",
        "string_process": " process transfer_preartifact {\n\n        stageInMode \"copy\"\n        module 'freesurfer'\n        publishDir \"$params.out/${params.application}\", \\\n                    mode: 'move',\n                    saveAs: { \"$sub\" }\n\n        input:\n        set val(sub), file(\"sprl.nii\") from preartifact_sprls\n\n        output:\n        set val(sub), file(\"$sub\") into preartifact_pseudo_out\n\n        shell:\n        '''\n        #!/bin/bash\n\n        #GZIP the nii file\n        gzip sprl.nii\n\n        #Fix orientation issue if exists\n        orientation=$(mri_info --orientation sprl.nii.gz)\n\n        if [ \"$orientation\" = \"PRS\" ]; then\n\n            fslorient -deleteorient sprl.nii.gz\n\n            fslswapdim sprl.nii.gz -x -y z  sprl.nii.gz\n\n            fslorient -setqformcode 1 sprl.nii.gz\n\n            #Save list of reoriented scans\n            mkdir -p !{params.out}/feenics/\n            reorient_list=!{params.out}/feenics/reoriented.log\n            echo !{sub} >> $reorient_list\n\n        fi\n\n        #Set up output directory\n        mkdir !{sub}\n        mv sprl.nii.gz !{sub}/!{sub}.sprlCOMBINED.denoised.nii.gz\n\n        '''\n    }",
        "nb_lignes_process": 43,
        "string_script": "        '''\n        #!/bin/bash\n\n        #GZIP the nii file\n        gzip sprl.nii\n\n        #Fix orientation issue if exists\n        orientation=$(mri_info --orientation sprl.nii.gz)\n\n        if [ \"$orientation\" = \"PRS\" ]; then\n\n            fslorient -deleteorient sprl.nii.gz\n\n            fslswapdim sprl.nii.gz -x -y z  sprl.nii.gz\n\n            fslorient -setqformcode 1 sprl.nii.gz\n\n            #Save list of reoriented scans\n            mkdir -p !{params.out}/feenics/\n            reorient_list=!{params.out}/feenics/reoriented.log\n            echo !{sub} >> $reorient_list\n\n        fi\n\n        #Set up output directory\n        mkdir !{sub}\n        mv sprl.nii.gz !{sub}/!{sub}.sprlCOMBINED.denoised.nii.gz\n\n        '''",
        "nb_lignes_script": 28,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "preartifact_sprls"
        ],
        "nb_inputs": 1,
        "outputs": [
            "preartifact_pseudo_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "stageInMode \"copy\"",
            "module 'freesurfer'",
            "publishDir \"$params.out/${params.application}\", mode: 'move' , saveAs: { \"$sub\" }"
        ],
        "when": "",
        "stub": ""
    },
    "gzip_nii": {
        "name_process": "gzip_nii",
        "string_process": "\nprocess gzip_nii {\n\n    stageInMode 'copy'\n\n    input:\n    set val(sub), file(sprlIN), file(sprlOUT) from sub_channel\n\n    output:\n    set val(sub), file(\"${sprlIN}.gz\"), file(\"${sprlOUT}.gz\") into gzipped_channel\n\n    shell:\n    '''\n    gzip !{sprlIN}\n    gzip !{sprlOUT}\n    '''\n\n}",
        "nb_lignes_process": 16,
        "string_script": "    '''\n    gzip !{sprlIN}\n    gzip !{sprlOUT}\n    '''",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sub_channel"
        ],
        "nb_inputs": 1,
        "outputs": [
            "gzipped_channel"
        ],
        "nb_outputs": 1,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "stageInMode 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "reorient_bad": {
        "name_process": "reorient_bad",
        "string_process": "\nprocess reorient_bad {\n\n    module 'freesurfer'\n    module 'FSL/5.0.11'\n    stageInMode 'copy'\n\n    input:\n    set val(sub), file(sprlIN), file(sprlOUT) from gzipped_channel\n\n    output:\n    set val(sub), file(sprlIN), file(sprlOUT) into oriented_subs\n\n    shell:\n    '''\n    #!/bin/bash\n\n    orientation=$(mri_info --orientation !{sprlIN})\n\n    if [ \"$orientation\" = \"PRS\" ]; then\n\n        fslorient -deleteorient !{sprlIN}\n        fslorient -deleteorient !{sprlOUT}\n\n        fslswapdim !{sprlIN} -x -y z  !{sprlIN}\n        fslswapdim !{sprlOUT} -x -y z !{sprlOUT}\n\n        fslorient -setqformcode 1 !{sprlIN}\n        fslorient -setqformcode 1 !{sprlOUT}\n\n        #Save list of reoriented scans\n        mkdir -p !{params.out}/feenics/\n        reorient_list=!{params.out}/feenics/reoriented.log\n        echo !{sub} >> $reorient_list\n\n    fi\n    '''\n\n\n}",
        "nb_lignes_process": 38,
        "string_script": "    '''\n    #!/bin/bash\n\n    orientation=$(mri_info --orientation !{sprlIN})\n\n    if [ \"$orientation\" = \"PRS\" ]; then\n\n        fslorient -deleteorient !{sprlIN}\n        fslorient -deleteorient !{sprlOUT}\n\n        fslswapdim !{sprlIN} -x -y z  !{sprlIN}\n        fslswapdim !{sprlOUT} -x -y z !{sprlOUT}\n\n        fslorient -setqformcode 1 !{sprlIN}\n        fslorient -setqformcode 1 !{sprlOUT}\n\n        #Save list of reoriented scans\n        mkdir -p !{params.out}/feenics/\n        reorient_list=!{params.out}/feenics/reoriented.log\n        echo !{sub} >> $reorient_list\n\n    fi\n    '''",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "gzipped_channel"
        ],
        "nb_inputs": 1,
        "outputs": [
            "oriented_subs"
        ],
        "nb_outputs": 1,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "module 'freesurfer'",
            "module 'FSL/5.0.11'",
            "stageInMode 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "run_feenics": {
        "name_process": "run_feenics",
        "string_process": "\nprocess run_feenics{\n\n    container \"$params.simg\"\n    stageInMode 'copy'\n    scratch \"/tmp/\"\n    container params.simg\n    containerOptions \"-B ${params.out}:${params.out}\"\n\n                                     \n    publishDir \"$params.out/${params.application}\", \\\n                mode: 'move',\n                saveAs: { \"$sub\" }\n\n    input:\n    set val(sub), file(\"sprlIN.nii.gz\"), file(\"sprlOUT.nii.gz\") from oriented_subs\n\n    output:\n    file(\"exp/$sub\") into melodic_out\n    val \"$sub\" into pseudo_out\n\n    shell:\n    '''\n\n    #Set up logging\n    logging_dir=!{params.out}/pipeline_logs/!{params.application}\n    mkdir -p ${logging_dir}\n    log_out=${logging_dir}/!{sub}.out\n    log_err=${logging_dir}/!{sub}.err\n\n    #Record task attempt\n    echo \"TASK ATTEMPT !{task.attempt}\" >> ${log_out}\n    echo \"WORKDIR: $(pwd)\"              >> ${log_out}\n    echo \"============================\" >> ${log_out}\n    echo \"TASK ATTEMPT !{task.attempt}\" >> ${log_err}\n    echo \"WORKDIR: $(pwd)\"              >> ${log_err}\n    echo \"============================\" >> ${log_err}\n\n    #Set up folder structure\n    mkdir -p ./exp/!{sub}/{sprlIN,sprlOUT}\n    mv \"sprlIN.nii.gz\" ./exp/!{sub}/sprlIN/\n    mv \"sprlOUT.nii.gz\" ./exp/!{sub}/sprlOUT/\n\n    #Run FeenICS pipeline\n    (\n    s1_folder_setup.py $(pwd)/exp\n    s2_identify_components.py $(pwd)/exp\n    s3_remove_flagged_components.py $(pwd)/exp\n    ) 2>> ${log_err} 1>> ${log_out}\n\n    #combine spiral files\n    combinesprl exp/\n\n    #Move relevant files over\n    mv exp/*sprlIN*nii.gz exp/!{sub}/\n    mv exp/*sprlOUT*nii.gz exp/!{sub}/\n    mv exp/*sprlCOMBINED*nii.gz exp/!{sub}/\n    '''\n\n\n}",
        "nb_lignes_process": 59,
        "string_script": "    '''\n\n    #Set up logging\n    logging_dir=!{params.out}/pipeline_logs/!{params.application}\n    mkdir -p ${logging_dir}\n    log_out=${logging_dir}/!{sub}.out\n    log_err=${logging_dir}/!{sub}.err\n\n    #Record task attempt\n    echo \"TASK ATTEMPT !{task.attempt}\" >> ${log_out}\n    echo \"WORKDIR: $(pwd)\"              >> ${log_out}\n    echo \"============================\" >> ${log_out}\n    echo \"TASK ATTEMPT !{task.attempt}\" >> ${log_err}\n    echo \"WORKDIR: $(pwd)\"              >> ${log_err}\n    echo \"============================\" >> ${log_err}\n\n    #Set up folder structure\n    mkdir -p ./exp/!{sub}/{sprlIN,sprlOUT}\n    mv \"sprlIN.nii.gz\" ./exp/!{sub}/sprlIN/\n    mv \"sprlOUT.nii.gz\" ./exp/!{sub}/sprlOUT/\n\n    #Run FeenICS pipeline\n    (\n    s1_folder_setup.py $(pwd)/exp\n    s2_identify_components.py $(pwd)/exp\n    s3_remove_flagged_components.py $(pwd)/exp\n    ) 2>> ${log_err} 1>> ${log_out}\n\n    #combine spiral files\n    combinesprl exp/\n\n    #Move relevant files over\n    mv exp/*sprlIN*nii.gz exp/!{sub}/\n    mv exp/*sprlOUT*nii.gz exp/!{sub}/\n    mv exp/*sprlCOMBINED*nii.gz exp/!{sub}/\n    '''",
        "nb_lignes_script": 35,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "oriented_subs"
        ],
        "nb_inputs": 1,
        "outputs": [
            "melodic_out",
            "pseudo_out"
        ],
        "nb_outputs": 2,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "container \"$params.simg\"",
            "stageInMode 'copy'",
            "scratch \"/tmp/\"",
            "container params.simg",
            "containerOptions \"-B ${params.out}:${params.out}\"",
            "publishDir \"$params.out/${params.application}\", mode: 'move' , saveAs: { \"$sub\" }"
        ],
        "when": "",
        "stub": ""
    },
    "run_icarus": {
        "name_process": "run_icarus",
        "string_process": "\nprocess run_icarus{\n\n    stageInMode 'copy'\n    container params.simg\n\n    container \"$params.simg\"\n    containerOptions \"-H $params.out:$params.out\"\n\n    publishDir \"$params.out/${params.application}\", \\\n                mode: 'copy'\n\n    input:\n    val \"*\" from pseudo_out.collect()\n\n    output:\n    val 'pseudo' into pseudo_out2\n\n    echo true\n\n    shell:\n    '''\n    #!/bin/bash\n\n    cd !{params.out}/!{params.application}\n    sprls=$(ls -1d */sprl*/)\n    icarus-report ${sprls}\n\n    '''\n\n}",
        "nb_lignes_process": 29,
        "string_script": "    '''\n    #!/bin/bash\n\n    cd !{params.out}/!{params.application}\n    sprls=$(ls -1d */sprl*/)\n    icarus-report ${sprls}\n\n    '''",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "pseudo_out"
        ],
        "nb_inputs": 1,
        "outputs": [
            "pseudo_out2"
        ],
        "nb_outputs": 1,
        "name_workflow": "jerdra__TIGR_PURR",
        "directive": [
            "stageInMode 'copy'",
            "container params.simg",
            "container \"$params.simg\"",
            "containerOptions \"-H $params.out:$params.out\"",
            "publishDir \"$params.out/${params.application}\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    }
}