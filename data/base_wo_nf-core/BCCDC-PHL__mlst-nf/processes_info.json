{
    "quast": {
        "name_process": "quast",
        "string_process": "process quast {\n\n    tag { sample_id }\n\n    input:\n      tuple val(sample_id), path(assembly)\n\n    output:\n      tuple val(sample_id), path(\"${sample_id}_quast.tsv\"), emit: tsv\n      tuple val(sample_id), path(\"${sample_id}_quast_provenance.yml\"), emit: provenance\n\n    script:\n      \"\"\"\n      printf -- \"- process_name: quast\\\\n\" > ${sample_id}_quast_provenance.yml\n      printf -- \"  tool_name: quast\\\\n  tool_version: \\$(quast --version | cut -d ' ' -f 2 | tr -d 'v')\\\\n\" >> ${sample_id}_quast_provenance.yml\n      quast --threads ${task.cpus} ${assembly} --space-efficient --fast --output-dir ${sample_id}\n      mv ${sample_id}/transposed_report.tsv ${sample_id}_quast.tsv\n      \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "      \"\"\"\n      printf -- \"- process_name: quast\\\\n\" > ${sample_id}_quast_provenance.yml\n      printf -- \"  tool_name: quast\\\\n  tool_version: \\$(quast --version | cut -d ' ' -f 2 | tr -d 'v')\\\\n\" >> ${sample_id}_quast_provenance.yml\n      quast --threads ${task.cpus} ${assembly} --space-efficient --fast --output-dir ${sample_id}\n      mv ${sample_id}/transposed_report.tsv ${sample_id}_quast.tsv\n      \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "QUAST"
        ],
        "tools_url": [
            "https://bio.tools/quast"
        ],
        "tools_dico": [
            {
                "name": "QUAST",
                "uri": "https://bio.tools/quast",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly validation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly quality evaluation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Assembly QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Assembly quality evaluation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "QUAST stands for QUality ASsessment Tool.  \nIt evaluates a quality of genome assemblies by computing various metrics and providing nice reports.",
                "homepage": "http://quast.sourceforge.net/quast"
            }
        ],
        "inputs": [
            "sample_id",
            "assembly"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "BCCDC-PHL__mlst-nf",
        "directive": [
            "tag { sample_id }"
        ],
        "when": "",
        "stub": ""
    },
    "parse_quast_report": {
        "name_process": "parse_quast_report",
        "string_process": "\nprocess parse_quast_report {\n\n    tag { sample_id }\n\n    executor 'local'\n\n    publishDir params.versioned_outdir ? \"${params.outdir}/${sample_id}/${params.pipeline_short_name}-v${params.pipeline_minor_version}-output\" : \"${params.outdir}/${sample_id}\", pattern: \"${sample_id}_quast.csv\", mode: 'copy'\n\n    input:\n      tuple val(sample_id), path(quast_report)\n\n    output:\n      tuple val(sample_id), path(\"${sample_id}_quast.csv\")\n\n    script:\n      \"\"\"\n      parse_quast_report.py ${quast_report} > ${sample_id}_quast.csv\n      \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "      \"\"\"\n      parse_quast_report.py ${quast_report} > ${sample_id}_quast.csv\n      \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_id",
            "quast_report"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sample_id"
        ],
        "nb_outputs": 1,
        "name_workflow": "BCCDC-PHL__mlst-nf",
        "directive": [
            "tag { sample_id }",
            "executor 'local'",
            "publishDir params.versioned_outdir ? \"${params.outdir}/${sample_id}/${params.pipeline_short_name}-v${params.pipeline_minor_version}-output\" : \"${params.outdir}/${sample_id}\", pattern: \"${sample_id}_quast.csv\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "collect_provenance": {
        "name_process": "collect_provenance",
        "string_process": "process collect_provenance {\n\n  tag { sample_id }\n\n  executor 'local'\n\n  publishDir params.versioned_outdir ? \"${params.outdir}/${sample_id}/${params.pipeline_short_name}-v${params.pipeline_minor_version}-output\" : \"${params.outdir}/${sample_id}\", pattern: \"${sample_id}_*_provenance.yml\", mode: 'copy'\n\n  input:\n  tuple val(sample_id), path(provenance_files)\n\n  output:\n  tuple val(sample_id), file(\"${sample_id}_*_provenance.yml\")\n\n  script:\n  \"\"\"\n  cat ${provenance_files} > ${sample_id}_\\$(date +%Y%m%d%H%M%S)_provenance.yml\n  \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "  \"\"\"\n  cat ${provenance_files} > ${sample_id}_\\$(date +%Y%m%d%H%M%S)_provenance.yml\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_id",
            "provenance_files"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sample_id"
        ],
        "nb_outputs": 1,
        "name_workflow": "BCCDC-PHL__mlst-nf",
        "directive": [
            "tag { sample_id }",
            "executor 'local'",
            "publishDir params.versioned_outdir ? \"${params.outdir}/${sample_id}/${params.pipeline_short_name}-v${params.pipeline_minor_version}-output\" : \"${params.outdir}/${sample_id}\", pattern: \"${sample_id}_*_provenance.yml\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "pipeline_provenance": {
        "name_process": "pipeline_provenance",
        "string_process": "\nprocess pipeline_provenance {\n\n  tag { pipeline_name + \" / \" + pipeline_version }\n\n  executor 'local'\n\n  input:\n  tuple val(pipeline_name), val(pipeline_version), val(analysis_start)\n\n  output:\n  file(\"pipeline_provenance.yml\")\n\n  script:\n  \"\"\"\n  printf -- \"- pipeline_name: ${pipeline_name}\\\\n  pipeline_version: ${pipeline_version}\\\\n- timestamp_analysis_start: ${analysis_start}\\\\n\" > pipeline_provenance.yml\n  \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "  \"\"\"\n  printf -- \"- pipeline_name: ${pipeline_name}\\\\n  pipeline_version: ${pipeline_version}\\\\n- timestamp_analysis_start: ${analysis_start}\\\\n\" > pipeline_provenance.yml\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "pipeline_name",
            "pipeline_version",
            "analysis_start"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "BCCDC-PHL__mlst-nf",
        "directive": [
            "tag { pipeline_name + \" / \" + pipeline_version }",
            "executor 'local'"
        ],
        "when": "",
        "stub": ""
    },
    "mlst": {
        "name_process": "mlst",
        "string_process": "process mlst {\n\n    tag { sample_id }\n\n    publishDir params.versioned_outdir ? \"${params.outdir}/${sample_id}/${params.pipeline_short_name}-v${params.pipeline_minor_version}-output\" : \"${params.outdir}/${sample_id}\", mode: 'copy', pattern: \"${sample_id}_mlst.json\"\n\n    input:\n    tuple val(sample_id), path(assembly)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_mlst.csv\"), path(\"${sample_id}_mlst.json\"), emit: mlst\n    tuple val(sample_id), path(\"${sample_id}_mlst_provenance.yml\"), emit: provenance\n    \n    script:\n    \"\"\"\n    printf -- \"- process_name: mlst\\\\n\" > ${sample_id}_mlst_provenance.yml\n    printf -- \"  tool_name: mlst\\\\n  tool_version: \\$(mlst --version | cut -d ' ' -f 2)\\\\n  parameters:\\\\n\" >> ${sample_id}_mlst_provenance.yml\n    printf -- \"  - parameter: minid\\\\n    value: ${params.minid}\\\\n\" >> ${sample_id}_mlst_provenance.yml\n    printf -- \"  - parameter: mincov\\\\n    value: ${params.mincov}\\\\n\" >> ${sample_id}_mlst_provenance.yml\n    printf -- \"  - parameter: minscore\\\\n    value: ${params.minscore}\\\\n\" >> ${sample_id}_mlst_provenance.yml\n    mlst \\\n      --minid ${params.minid} \\\n      --mincov ${params.mincov} \\\n      --minscore ${params.minscore} \\\n      --csv \\\n      --json ${sample_id}_mlst.json \\\n      ${assembly} > ${sample_id}_mlst.csv\n    \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "    \"\"\"\n    printf -- \"- process_name: mlst\\\\n\" > ${sample_id}_mlst_provenance.yml\n    printf -- \"  tool_name: mlst\\\\n  tool_version: \\$(mlst --version | cut -d ' ' -f 2)\\\\n  parameters:\\\\n\" >> ${sample_id}_mlst_provenance.yml\n    printf -- \"  - parameter: minid\\\\n    value: ${params.minid}\\\\n\" >> ${sample_id}_mlst_provenance.yml\n    printf -- \"  - parameter: mincov\\\\n    value: ${params.mincov}\\\\n\" >> ${sample_id}_mlst_provenance.yml\n    printf -- \"  - parameter: minscore\\\\n    value: ${params.minscore}\\\\n\" >> ${sample_id}_mlst_provenance.yml\n    mlst \\\n      --minid ${params.minid} \\\n      --mincov ${params.mincov} \\\n      --minscore ${params.minscore} \\\n      --csv \\\n      --json ${sample_id}_mlst.json \\\n      ${assembly} > ${sample_id}_mlst.csv\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "MLST"
        ],
        "tools_url": [
            "https://bio.tools/mlst"
        ],
        "tools_dico": [
            {
                "name": "MLST",
                "uri": "https://bio.tools/mlst",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2830",
                            "term": "Immunoproteins and antigens"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "Multi Locus Sequence Typing from an assembled genome or from a set of reads.",
                "homepage": "http://cge.cbs.dtu.dk/services/MLST/"
            }
        ],
        "inputs": [
            "sample_id",
            "assembly"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "BCCDC-PHL__mlst-nf",
        "directive": [
            "tag { sample_id }",
            "publishDir params.versioned_outdir ? \"${params.outdir}/${sample_id}/${params.pipeline_short_name}-v${params.pipeline_minor_version}-output\" : \"${params.outdir}/${sample_id}\", mode: 'copy', pattern: \"${sample_id}_mlst.json\""
        ],
        "when": "",
        "stub": ""
    },
    "parse_alleles": {
        "name_process": "parse_alleles",
        "string_process": "\nprocess parse_alleles {\n    tag { sample_id }\n    \n    executor 'local'\n\n    publishDir params.versioned_outdir ? \"${params.outdir}/${sample_id}/${params.pipeline_short_name}-v${params.pipeline_minor_version}-output\" : \"${params.outdir}/${sample_id}\", mode: 'copy', pattern: \"${sample_id}_alleles.csv\"\n    publishDir params.versioned_outdir ? \"${params.outdir}/${sample_id}/${params.pipeline_short_name}-v${params.pipeline_minor_version}-output\" : \"${params.outdir}/${sample_id}\", mode: 'copy', pattern: \"${sample_id}_sequence_type.csv\"\n\n    input:\n    tuple val(sample_id), path(mlst_csv), path(mlst_json)\n\n    output:\n    tuple val(sample_id), path(\"${sample_id}_alleles.csv\"), path(\"${sample_id}_sequence_type.csv\")\n\n    script:\n    \"\"\"\n    parse_alleles.py -s ${sample_id} ${mlst_json} > ${sample_id}_alleles.csv\n    echo 'sample_id' > sample_id.csv\n    echo ${sample_id} >> sample_id.csv\n    echo 'scheme,sequence_type' > sequence_type.csv\n    cut -d ',' -f 2,3 ${mlst_csv} >> sequence_type.csv\n    echo 'score' > score.csv\n    awk -F ',' '{sum+=\\$7}; END{print sum}' ${sample_id}_alleles.csv >> score.csv\n    paste -d ',' sample_id.csv sequence_type.csv score.csv > ${sample_id}_sequence_type.csv\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "    \"\"\"\n    parse_alleles.py -s ${sample_id} ${mlst_json} > ${sample_id}_alleles.csv\n    echo 'sample_id' > sample_id.csv\n    echo ${sample_id} >> sample_id.csv\n    echo 'scheme,sequence_type' > sequence_type.csv\n    cut -d ',' -f 2,3 ${mlst_csv} >> sequence_type.csv\n    echo 'score' > score.csv\n    awk -F ',' '{sum+=\\$7}; END{print sum}' ${sample_id}_alleles.csv >> score.csv\n    paste -d ',' sample_id.csv sequence_type.csv score.csv > ${sample_id}_sequence_type.csv\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_id",
            "mlst_csv",
            "mlst_json"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sample_id"
        ],
        "nb_outputs": 1,
        "name_workflow": "BCCDC-PHL__mlst-nf",
        "directive": [
            "tag { sample_id }",
            "executor 'local'",
            "publishDir params.versioned_outdir ? \"${params.outdir}/${sample_id}/${params.pipeline_short_name}-v${params.pipeline_minor_version}-output\" : \"${params.outdir}/${sample_id}\", mode: 'copy', pattern: \"${sample_id}_alleles.csv\"",
            "publishDir params.versioned_outdir ? \"${params.outdir}/${sample_id}/${params.pipeline_short_name}-v${params.pipeline_minor_version}-output\" : \"${params.outdir}/${sample_id}\", mode: 'copy', pattern: \"${sample_id}_sequence_type.csv\""
        ],
        "when": "",
        "stub": ""
    },
    "hash_files": {
        "name_process": "hash_files",
        "string_process": "process hash_files {\n\n    tag { sample_id + \" / \" + file_type }\n\n    input:\n    tuple  val(sample_id), path(files_to_hash), val(file_type)\n\n    output:\n    tuple  val(sample_id), path(\"${sample_id}_${file_type}.sha256.csv\"), emit: csv\n    tuple  val(sample_id), path(\"${sample_id}_${file_type}_provenance.yml\"), emit: provenance\n\n    script:\n    \"\"\"\n    shasum -a 256 ${files_to_hash} | tr -s ' ' ',' > ${sample_id}_${file_type}.sha256.csv\n    while IFS=',' read -r hash filename; do\n      printf -- \"- input_filename: \\$filename\\\\n  input_path: \\$(realpath \\$filename)\\\\n  sha256: \\$hash\\\\n\" >> ${sample_id}_${file_type}_provenance.yml\n    done < ${sample_id}_${file_type}.sha256.csv\n    \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "    \"\"\"\n    shasum -a 256 ${files_to_hash} | tr -s ' ' ',' > ${sample_id}_${file_type}.sha256.csv\n    while IFS=',' read -r hash filename; do\n      printf -- \"- input_filename: \\$filename\\\\n  input_path: \\$(realpath \\$filename)\\\\n  sha256: \\$hash\\\\n\" >> ${sample_id}_${file_type}_provenance.yml\n    done < ${sample_id}_${file_type}.sha256.csv\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_id",
            "file_type",
            "files_to_hash"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "BCCDC-PHL__mlst-nf",
        "directive": [
            "tag { sample_id + \" / \" + file_type }"
        ],
        "when": "",
        "stub": ""
    }
}