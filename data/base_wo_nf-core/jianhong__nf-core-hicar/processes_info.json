{
    "UCSC_WIGTOBIGWIG": {
        "name_process": "UCSC_WIGTOBIGWIG",
        "string_process": "\nprocess UCSC_WIGTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ucsc-wigtobigwig=377\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ucsc-wigtobigwig:377--h0b8a92a_2' :\n        'quay.io/biocontainers/ucsc-wigtobigwig:377--h0b8a92a_2' }\"\n\n    input:\n    tuple val(meta), path(wig)\n    path sizes\n\n    output:\n    tuple val(meta), path(\"*.bw\"), emit: bw\n    path \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    wigToBigWig \\\\\n        $args \\\\\n        $wig \\\\\n        $sizes \\\\\n        ${prefix}.bw\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 35,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    wigToBigWig \\\\\n        $args \\\\\n        $wig \\\\\n        $sizes \\\\\n        ${prefix}.bw\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "wig",
            "sizes"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::ucsc-wigtobigwig=377\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/ucsc-wigtobigwig:377--h0b8a92a_2' : 'quay.io/biocontainers/ucsc-wigtobigwig:377--h0b8a92a_2' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "SAMTOOLS_INDEX": {
        "name_process": "SAMTOOLS_INDEX",
        "string_process": "process SAMTOOLS_INDEX {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.bai\") , optional:true, emit: bai\n    tuple val(meta), path(\"*.csi\") , optional:true, emit: csi\n    tuple val(meta), path(\"*.crai\"), optional:true, emit: crai\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        index \\\\\n        -@ ${task.cpus-1} \\\\\n        $args \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "SAMtools",
            "CINdex"
        ],
        "tools_url": [
            "https://bio.tools/samtools",
            "https://bio.tools/cindex"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            },
            {
                "name": "CINdex",
                "uri": "https://bio.tools/cindex",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3233",
                                    "term": "Copy number estimation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3196",
                                    "term": "Genotyping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3233",
                                    "term": "Transcript copy number estimation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The package addresses important area of high-throughput genomic analysis. It allows the automated processing and analysis of the experimental DNA copy number data generated by Affymetrix SNP 6.0 arrays or similar. It calculates the chromosome instability index to quantitatively characterize genome-wide DNA copy number alterations. This package calculates not only overall genomic instability, but also instability in terms of copy number gains and losses at the chromosome and cytoband level.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/CINdex.html"
            }
        ],
        "inputs": [
            "meta",
            "input"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' : 'quay.io/biocontainers/samtools:1.15--h1170115_1' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "BWA_MEM": {
        "name_process": "BWA_MEM",
        "string_process": "process BWA_MEM {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:c56a3aabc8d64e52d5b9da1e8ecec2031668596d-0' :\n        'quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:c56a3aabc8d64e52d5b9da1e8ecec2031668596d-0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n    val   sort_bam\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa mem \\\\\n        $args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 --threads $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 43,
        "string_script": "    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def read_group = meta.read_group ? \"-R ${meta.read_group}\" : \"\"\n    def samtools_command = sort_bam ? 'sort' : 'view'\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.amb\" | sed 's/.amb//'`\n\n    bwa mem \\\\\n        $args \\\\\n        $read_group \\\\\n        -t $task.cpus \\\\\n        \\$INDEX \\\\\n        $reads \\\\\n        | samtools $samtools_command $args2 --threads $task.cpus -o ${prefix}.bam -\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 21,
        "language_script": "bash",
        "tools": [
            "BWA",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/bwa",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "meta",
            "reads",
            "index",
            "sort_bam"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::bwa=0.7.17 bioconda::samtools=1.15\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:c56a3aabc8d64e52d5b9da1e8ecec2031668596d-0' : 'quay.io/biocontainers/mulled-v2-fe8faa35dbf6dc65a0f7f5d4ea12e31a79f73e40:c56a3aabc8d64e52d5b9da1e8ecec2031668596d-0' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "BIOC_CHIPPEAKANNO": {
        "name_process": "BIOC_CHIPPEAKANNO",
        "string_process": "process BIOC_CHIPPEAKANNO {\n    tag \"$bin_size\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-chippeakanno=3.26.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-chippeakanno:3.26.0--r41hdfd78af_0' :\n        'quay.io/biocontainers/bioconductor-chippeakanno:3.26.0--r41hdfd78af_0' }\"\n\n    input:\n    tuple val(bin_size), path(diff)\n    path gtf\n\n    output:\n    tuple val(bin_size), path(\"${prefix}/anno/*\"), emit: anno\n    tuple val(bin_size), path(\"${prefix}/anno/**.anno.csv\"), emit: csv\n    path \"${prefix}/anno/*.png\", optional:true, emit: png\n    path \"versions.yml\"                       , emit: versions\n\n    script:\n    prefix   = task.ext.prefix ?: \"diffhic_bin${bin_size}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on April. 29, 2021 call ChIPpeakAnno\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"ChIPpeakAnno\", \"rtracklayer\", \"GenomicFeatures\", \"ggplot2\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    gtf <- \"${gtf}\"\n    pf <- file.path(\"${prefix}\", \"anno\")\n    bin_size <- \"${prefix}\"\n\n    detbl <- dir(\".\", \"DEtable.*.csv|sig3Dinteractions.bedpe|peaks\",\n                recursive = TRUE, full.names = TRUE)\n    detbl <- detbl[!grepl(\"anno.csv\", detbl)] ## in case of re-run\n\n    txdb <- makeTxDbFromGFF(gtf) ## create annotation data from gtf file\n    gtf <- import(gtf)\n    id2symbol <- function(gtf){ ## convert entriz id to gene symbol\n        if(is.null(gtf\\$gene_name)) return(NULL)\n        x <- data.frame(id=gtf\\$gene_id, symbol=gtf\\$gene_name)\n        x <- unique(x)\n        x <- x[!duplicated(x\\$id), ]\n        x <- x[!is.na(x\\$id), , drop=FALSE]\n        if(nrow(x)==0) return(NULL)\n        y <- x\\$symbol\n        names(y) <- x\\$id\n        y\n    }\n    id2symbol <- id2symbol(gtf)\n    anno <- toGRanges(txdb)\n    promoters <- promoters(anno, upstream=2000, downstream=500)\n    resList <- list() # save annotation results to a list\n    peaks <- list()\n    promoterList <- list() # list to save the distal sites of promoters interactions\n\n    dir.create(pf, showWarnings = FALSE, recursive = TRUE)\n    for(det in detbl){\n        if(grepl(\"csv\\$\", det)) {\n            DB <- read.csv(det)\n        }else{\n            if(grepl(\"peaks\\$\", det)){\n                DB <- read.table(det, header=TRUE)\n            }else{\n                DB <- read.delim(det)\n            }\n        }\n        if(nrow(DB)<1) next\n        rownames(DB) <- paste0(\"p\", seq.int(nrow(DB)))\n        DB.gr1 <- with(DB, GRanges(chr1, IRanges(start1, end1, name=rownames(DB))))\n        DB.gr2 <- with(DB, GRanges(chr2, IRanges(start2, end2, name=rownames(DB))))\n        # Annotation\n        DB.anno1 <- annotatePeakInBatch(DB.gr1, AnnotationData = anno,\n                                        output = \"both\",\n                                        PeakLocForDistance = \"middle\",\n                                        FeatureLocForDistance = \"TSS\",\n                                        ignore.strand = TRUE)\n        if(length(id2symbol)>0) DB.anno1\\$symbol[!is.na(DB.anno1\\$feature)] <- id2symbol[DB.anno1\\$feature[!is.na(DB.anno1\\$feature)]]\n        DB.anno2 <- annotatePeakInBatch(DB.gr2, AnnotationData = anno,\n                                        output = \"both\",\n                                        PeakLocForDistance = \"middle\",\n                                        FeatureLocForDistance = \"TSS\",\n                                        ignore.strand = TRUE)\n        if(length(id2symbol)>0) DB.anno2\\$symbol[!is.na(DB.anno2\\$feature)] <- id2symbol[DB.anno2\\$feature[!is.na(DB.anno2\\$feature)]]\n        groupName <- sub(\".sig3Dinteractions.bedpe|csv\", \"\", basename(det))\n        if(grepl(\"padj\", det)){\n            resList[[groupName]] <- c(DB.anno1, DB.anno2)\n        }else{\n            peaks[[groupName]] <- unique(c(DB.gr1, DB.gr2))\n            ol1 <- findOverlaps(DB.gr1, promoters)\n            ol2 <- findOverlaps(DB.gr2, promoters)\n            promoterList[[groupName]] <- unique(c(DB.gr2[unique(queryHits(ol1))], DB.gr1[unique(queryHits(ol2))]))\n        }\n        # Summary the annotations\n        DB.anno1 <- mcols(DB.anno1)\n        DB.anno2 <- mcols(DB.anno2)\n        DB.anno <- merge(DB.anno1, DB.anno2, by=\"peak\",\n                        suffixes = c(\".anchor1\",\".anchor2\"))\n        DB <- cbind(DB[DB.anno\\$peak, ], DB.anno)\n        pff <- file.path(pf, sub(\".(csv|bedpe|peaks)\", \".anno.csv\", det))\n        dir.create(dirname(pff), recursive = TRUE, showWarnings = FALSE)\n        write.csv(DB, pff, row.names = FALSE)\n    }\n\n\n    if(packageVersion(\"ChIPpeakAnno\")>=\"3.23.12\"){\n        if(length(resList)>0){\n            if(is.list(resList)){\n                resList <- GRangesList(resList[lengths(resList)>0])\n            }\n            out <- genomicElementDistribution(resList,\n                                            TxDb = txdb,\n                                            promoterRegion=c(upstream=2000, downstream=500),\n                                            geneDownstream=c(upstream=0, downstream=2000),\n                                            promoterLevel=list(\n                                            # from 5' -> 3', fixed precedence 3' -> 5'\n                                                breaks = c(-2000, -1000, -500, 0, 500),\n                                                labels = c(\"upstream 1-2Kb\", \"upstream 0.5-1Kb\",\n                                                        \"upstream <500b\", \"TSS - 500b\"),\n                                                colors = c(\"#FFE5CC\", \"#FFCA99\",\n                                                        \"#FFAD65\", \"#FF8E32\")),\n                                            plot = FALSE)\n\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuiton.\", bin_size, \".pdf\")), plot=out\\$plot, width=9, height=9)\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuiton.\", bin_size, \".png\")), plot=out\\$plot)\n            out <- metagenePlot(resList, txdb)\n            ggsave(file.path(pf, paste0(\"metagenePlotToTSS.\", bin_size, \".pdf\")), plot=out, width=9, height=9)\n            ggsave(file.path(pf, paste0(\"metagenePlotToTSS.\", bin_size, \".png\")), plot=out)\n        }\n        if(length(peaks)>0){\n            peaks <- GRangesList(peaks[lengths(peaks)>0])\n            out <- genomicElementDistribution(peaks,\n                                            TxDb = txdb,\n                                            promoterRegion=c(upstream=2000, downstream=500),\n                                            geneDownstream=c(upstream=0, downstream=2000),\n                                            promoterLevel=list(\n                                                # from 5' -> 3', fixed precedence 3' -> 5'\n                                                breaks = c(-2000, -1000, -500, 0, 500),\n                                                labels = c(\"upstream 1-2Kb\", \"upstream 0.5-1Kb\",\n                                                        \"upstream <500b\", \"TSS - 500b\"),\n                                                colors = c(\"#FFE5CC\", \"#FFCA99\",\n                                                        \"#FFAD65\", \"#FF8E32\")),\n                                            plot = FALSE)\n\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuitonOfEachPeakList.\", bin_size, \".pdf\")), plot=out\\$plot, width=9, height=9)\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuitonOfEachPeakList.\", bin_size, \".png\")), plot=out\\$plot)\n\n            out <- metagenePlot(peaks, txdb)\n            ggsave(file.path(pf, paste0(\"metagenePlotToTSSOfEachPeakList.\", bin_size, \".pdf\")), plot=out, width=9, height=9)\n            ggsave(file.path(pf, paste0(\"metagenePlotToTSSOfEachPeakList.\", bin_size, \".png\")), plot=out)\n\n            if(length(peaks)<=5 && length(peaks)>1){\n                ol <- findOverlapsOfPeaks(peaks)\n                png(file.path(pf, paste0(\"vennDiagram.all.\", bin_size, \".png\")))\n                vd <- makeVennDiagram(ol, connectedPeaks=\"keepAll\")\n                dev.off()\n                write.csv(vd\\$vennCounts, file.path(pf, paste0(\"vennDiagram.all.\", bin_size, \".csv\")), row.names=FALSE)\n            }\n        }\n        if(length(promoterList)>0){\n            promoterList <- GRangesList(promoterList[lengths(promoterList)>0])\n            out <- genomicElementDistribution(promoterList,\n                                            TxDb = txdb,\n                                            promoterRegion=c(upstream=2000, downstream=500),\n                                            geneDownstream=c(upstream=0, downstream=2000),\n                                            promoterLevel=list(\n                                                # from 5' -> 3', fixed precedence 3' -> 5'\n                                                breaks = c(-2000, -1000, -500, 0, 500),\n                                                labels = c(\"upstream 1-2Kb\", \"upstream 0.5-1Kb\",\n                                                        \"upstream <500b\", \"TSS - 500b\"),\n                                                colors = c(\"#FFE5CC\", \"#FFCA99\",\n                                                        \"#FFAD65\", \"#FF8E32\")),\n                                            plot = FALSE)\n\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuitonOfremoteInteractionPeaks.\", bin_size, \".pdf\")), plot=out\\$plot, width=9, height=9)\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuitonOfremoteInteractionPeaks.\", bin_size, \".png\")), plot=out\\$plot)\n\n            if(length(promoterList)<=5 && length(promoterList)>1){\n                ol <- findOverlapsOfPeaks(promoterList)\n                png(file.path(pf, paste0(\"vennDiagram.remote.interaction.peak.with.promoters.all.\", bin_size, \".png\")))\n                vd <- makeVennDiagram(ol, connectedPeaks=\"keepAll\")\n                dev.off()\n                write.csv(vd\\$vennCounts, file.path(pf, paste0(\"vennDiagram.remote.interaction.peak.with.promoters.all.\", bin_size, \".csv\")), row.names=FALSE)\n            }\n        }\n    }\n    \"\"\"\n}",
        "nb_lignes_process": 201,
        "string_script": "    prefix   = task.ext.prefix ?: \"diffhic_bin${bin_size}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on April. 29, 2021 call ChIPpeakAnno\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"ChIPpeakAnno\", \"rtracklayer\", \"GenomicFeatures\", \"ggplot2\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    gtf <- \"${gtf}\"\n    pf <- file.path(\"${prefix}\", \"anno\")\n    bin_size <- \"${prefix}\"\n\n    detbl <- dir(\".\", \"DEtable.*.csv|sig3Dinteractions.bedpe|peaks\",\n                recursive = TRUE, full.names = TRUE)\n    detbl <- detbl[!grepl(\"anno.csv\", detbl)] ## in case of re-run\n\n    txdb <- makeTxDbFromGFF(gtf) ## create annotation data from gtf file\n    gtf <- import(gtf)\n    id2symbol <- function(gtf){ ## convert entriz id to gene symbol\n        if(is.null(gtf\\$gene_name)) return(NULL)\n        x <- data.frame(id=gtf\\$gene_id, symbol=gtf\\$gene_name)\n        x <- unique(x)\n        x <- x[!duplicated(x\\$id), ]\n        x <- x[!is.na(x\\$id), , drop=FALSE]\n        if(nrow(x)==0) return(NULL)\n        y <- x\\$symbol\n        names(y) <- x\\$id\n        y\n    }\n    id2symbol <- id2symbol(gtf)\n    anno <- toGRanges(txdb)\n    promoters <- promoters(anno, upstream=2000, downstream=500)\n    resList <- list() # save annotation results to a list\n    peaks <- list()\n    promoterList <- list() # list to save the distal sites of promoters interactions\n\n    dir.create(pf, showWarnings = FALSE, recursive = TRUE)\n    for(det in detbl){\n        if(grepl(\"csv\\$\", det)) {\n            DB <- read.csv(det)\n        }else{\n            if(grepl(\"peaks\\$\", det)){\n                DB <- read.table(det, header=TRUE)\n            }else{\n                DB <- read.delim(det)\n            }\n        }\n        if(nrow(DB)<1) next\n        rownames(DB) <- paste0(\"p\", seq.int(nrow(DB)))\n        DB.gr1 <- with(DB, GRanges(chr1, IRanges(start1, end1, name=rownames(DB))))\n        DB.gr2 <- with(DB, GRanges(chr2, IRanges(start2, end2, name=rownames(DB))))\n        # Annotation\n        DB.anno1 <- annotatePeakInBatch(DB.gr1, AnnotationData = anno,\n                                        output = \"both\",\n                                        PeakLocForDistance = \"middle\",\n                                        FeatureLocForDistance = \"TSS\",\n                                        ignore.strand = TRUE)\n        if(length(id2symbol)>0) DB.anno1\\$symbol[!is.na(DB.anno1\\$feature)] <- id2symbol[DB.anno1\\$feature[!is.na(DB.anno1\\$feature)]]\n        DB.anno2 <- annotatePeakInBatch(DB.gr2, AnnotationData = anno,\n                                        output = \"both\",\n                                        PeakLocForDistance = \"middle\",\n                                        FeatureLocForDistance = \"TSS\",\n                                        ignore.strand = TRUE)\n        if(length(id2symbol)>0) DB.anno2\\$symbol[!is.na(DB.anno2\\$feature)] <- id2symbol[DB.anno2\\$feature[!is.na(DB.anno2\\$feature)]]\n        groupName <- sub(\".sig3Dinteractions.bedpe|csv\", \"\", basename(det))\n        if(grepl(\"padj\", det)){\n            resList[[groupName]] <- c(DB.anno1, DB.anno2)\n        }else{\n            peaks[[groupName]] <- unique(c(DB.gr1, DB.gr2))\n            ol1 <- findOverlaps(DB.gr1, promoters)\n            ol2 <- findOverlaps(DB.gr2, promoters)\n            promoterList[[groupName]] <- unique(c(DB.gr2[unique(queryHits(ol1))], DB.gr1[unique(queryHits(ol2))]))\n        }\n        # Summary the annotations\n        DB.anno1 <- mcols(DB.anno1)\n        DB.anno2 <- mcols(DB.anno2)\n        DB.anno <- merge(DB.anno1, DB.anno2, by=\"peak\",\n                        suffixes = c(\".anchor1\",\".anchor2\"))\n        DB <- cbind(DB[DB.anno\\$peak, ], DB.anno)\n        pff <- file.path(pf, sub(\".(csv|bedpe|peaks)\", \".anno.csv\", det))\n        dir.create(dirname(pff), recursive = TRUE, showWarnings = FALSE)\n        write.csv(DB, pff, row.names = FALSE)\n    }\n\n\n    if(packageVersion(\"ChIPpeakAnno\")>=\"3.23.12\"){\n        if(length(resList)>0){\n            if(is.list(resList)){\n                resList <- GRangesList(resList[lengths(resList)>0])\n            }\n            out <- genomicElementDistribution(resList,\n                                            TxDb = txdb,\n                                            promoterRegion=c(upstream=2000, downstream=500),\n                                            geneDownstream=c(upstream=0, downstream=2000),\n                                            promoterLevel=list(\n                                            # from 5' -> 3', fixed precedence 3' -> 5'\n                                                breaks = c(-2000, -1000, -500, 0, 500),\n                                                labels = c(\"upstream 1-2Kb\", \"upstream 0.5-1Kb\",\n                                                        \"upstream <500b\", \"TSS - 500b\"),\n                                                colors = c(\"#FFE5CC\", \"#FFCA99\",\n                                                        \"#FFAD65\", \"#FF8E32\")),\n                                            plot = FALSE)\n\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuiton.\", bin_size, \".pdf\")), plot=out\\$plot, width=9, height=9)\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuiton.\", bin_size, \".png\")), plot=out\\$plot)\n            out <- metagenePlot(resList, txdb)\n            ggsave(file.path(pf, paste0(\"metagenePlotToTSS.\", bin_size, \".pdf\")), plot=out, width=9, height=9)\n            ggsave(file.path(pf, paste0(\"metagenePlotToTSS.\", bin_size, \".png\")), plot=out)\n        }\n        if(length(peaks)>0){\n            peaks <- GRangesList(peaks[lengths(peaks)>0])\n            out <- genomicElementDistribution(peaks,\n                                            TxDb = txdb,\n                                            promoterRegion=c(upstream=2000, downstream=500),\n                                            geneDownstream=c(upstream=0, downstream=2000),\n                                            promoterLevel=list(\n                                                # from 5' -> 3', fixed precedence 3' -> 5'\n                                                breaks = c(-2000, -1000, -500, 0, 500),\n                                                labels = c(\"upstream 1-2Kb\", \"upstream 0.5-1Kb\",\n                                                        \"upstream <500b\", \"TSS - 500b\"),\n                                                colors = c(\"#FFE5CC\", \"#FFCA99\",\n                                                        \"#FFAD65\", \"#FF8E32\")),\n                                            plot = FALSE)\n\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuitonOfEachPeakList.\", bin_size, \".pdf\")), plot=out\\$plot, width=9, height=9)\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuitonOfEachPeakList.\", bin_size, \".png\")), plot=out\\$plot)\n\n            out <- metagenePlot(peaks, txdb)\n            ggsave(file.path(pf, paste0(\"metagenePlotToTSSOfEachPeakList.\", bin_size, \".pdf\")), plot=out, width=9, height=9)\n            ggsave(file.path(pf, paste0(\"metagenePlotToTSSOfEachPeakList.\", bin_size, \".png\")), plot=out)\n\n            if(length(peaks)<=5 && length(peaks)>1){\n                ol <- findOverlapsOfPeaks(peaks)\n                png(file.path(pf, paste0(\"vennDiagram.all.\", bin_size, \".png\")))\n                vd <- makeVennDiagram(ol, connectedPeaks=\"keepAll\")\n                dev.off()\n                write.csv(vd\\$vennCounts, file.path(pf, paste0(\"vennDiagram.all.\", bin_size, \".csv\")), row.names=FALSE)\n            }\n        }\n        if(length(promoterList)>0){\n            promoterList <- GRangesList(promoterList[lengths(promoterList)>0])\n            out <- genomicElementDistribution(promoterList,\n                                            TxDb = txdb,\n                                            promoterRegion=c(upstream=2000, downstream=500),\n                                            geneDownstream=c(upstream=0, downstream=2000),\n                                            promoterLevel=list(\n                                                # from 5' -> 3', fixed precedence 3' -> 5'\n                                                breaks = c(-2000, -1000, -500, 0, 500),\n                                                labels = c(\"upstream 1-2Kb\", \"upstream 0.5-1Kb\",\n                                                        \"upstream <500b\", \"TSS - 500b\"),\n                                                colors = c(\"#FFE5CC\", \"#FFCA99\",\n                                                        \"#FFAD65\", \"#FF8E32\")),\n                                            plot = FALSE)\n\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuitonOfremoteInteractionPeaks.\", bin_size, \".pdf\")), plot=out\\$plot, width=9, height=9)\n            ggsave(file.path(pf, paste0(\"genomicElementDistribuitonOfremoteInteractionPeaks.\", bin_size, \".png\")), plot=out\\$plot)\n\n            if(length(promoterList)<=5 && length(promoterList)>1){\n                ol <- findOverlapsOfPeaks(promoterList)\n                png(file.path(pf, paste0(\"vennDiagram.remote.interaction.peak.with.promoters.all.\", bin_size, \".png\")))\n                vd <- makeVennDiagram(ol, connectedPeaks=\"keepAll\")\n                dev.off()\n                write.csv(vd\\$vennCounts, file.path(pf, paste0(\"vennDiagram.remote.interaction.peak.with.promoters.all.\", bin_size, \".csv\")), row.names=FALSE)\n            }\n        }\n    }\n    \"\"\"",
        "nb_lignes_script": 179,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "bin_size",
            "diff",
            "gtf"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$bin_size\"",
            "label 'process_medium'",
            "label 'error_ignore'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-chippeakanno=3.26.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-chippeakanno:3.26.0--r41hdfd78af_0' : 'quay.io/biocontainers/bioconductor-chippeakanno:3.26.0--r41hdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "ENSEMBL_UCSC_CONVERT": {
        "name_process": "ENSEMBL_UCSC_CONVERT",
        "string_process": "process ENSEMBL_UCSC_CONVERT {\n    tag \"$fname\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-rtracklayer=1.50.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-rtracklayer:1.50.0--r40h7f5ccec_2' :\n        'quay.io/biocontainers/bioconductor-rtracklayer:1.50.0--r40h7f5ccec_2' }\"\n\n    input:\n    tuple val(bin_size), path(fname)\n\n    output:\n    tuple val(bin_size), path(\"{UCSC,ensembl}.${fname}\"), emit: tab\n    path \"versions.yml\"                                 , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    pkgs <- c(\"GenomeInfoDb\", \"rtracklayer\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    toUCSC = \"$args\"==\"toUCSC\"\n    inf = \"$fname\"\n    ## check file format\n    ## if it is bigwig file\n    isBWF <- grepl(\"\\\\\\\\.(bw|bigwig)\", inf, ignore.case=TRUE)\n    if(isBWF){## decrease the memory cost\n        bwfile <- BigWigFile(inf)\n        seqinfo <- seqinfo(bwfile)\n        seqstyle <- seqlevelsStyle(seqinfo)\n    }else{\n        data <- import(inf)\n        seqstyle <- seqlevelsStyle(data)\n    }\n    readBWFile <- function(f, seqinfo){\n        gr <- as(seqinfo, \"GRanges\")\n        data <- GRanges()\n        for(s in seq_along(gr)){\n            dat <- import.bw(f, which = gr[s])\n            dat <- coverage(dat, weight = dat\\$score)\n            dat <- as(dat, \"GRanges\")\n            dat <- dat[dat\\$score > 0] ## negative scores are not allowed\n            data <- c(data, dat)\n        }\n        data <- coverage(data, weight = data\\$score)\n        data <- as(data, \"GRanges\")\n        data <- data[data\\$score > 0]\n        return(data)\n    }\n    if(toUCSC){\n        if(!\"UCSC\" %in% seqstyle){ ## convert to UCSC style\n            if(isBWF){\n                data <- readBWFile(inf, seqinfo)\n            }\n            seqlevelsStyle(data) <- \"UCSC\"\n            ## double check\n            if(sum(grepl(\"^chr\", seqlevels(data)))==0){\n                ids <- grepl(\"^((\\\\\\\\d{1,2})|(IX|IV|V?I{0,3})|([XYMT]{1,2}))\\$\", seqlevels(data))\n                seqlevels(data)[ids] <- paste0(\"chr\", seqlevels(data)[ids])\n            }\n            export(data, file.path(dirname(inf), paste0(\"UCSC.\", basename(inf))))\n        }else{\n            file.copy(inf, file.path(dirname(inf), paste0(\"UCSC.\", basename(inf))))\n        }\n    }else{\n        if(!\"Ensembl\" %in% seqstyle){## convert to Ensembl style\n            if(isBWF){\n                data <- readBWFile(inf, seqinfo)\n            }\n            seqlevelsStyle(data) <- \"Ensembl\"\n            ## double check\n            if(sum(grepl(\"^chr\", seqlevels(data)))>0){\n                ids <- grepl(\"^(chr)((\\\\\\\\d{1,2})|(IX|IV|V?I{0,3})|([XYMT]{1,2}))\\$\", seqlevels(data))\n                seqlevels(data)[ids] <- sub(\"chr\", \"\", seqlevels(data)[ids])\n            }\n            export(data, file.path(dirname(inf), paste0(\"ENSEMBL.\", basename(inf))))\n        }else{\n            file.copy(inf, file.path(dirname(inf), paste0(\"ENSEMBL.\", basename(inf))))\n        }\n    }\n    \"\"\"\n}",
        "nb_lignes_process": 91,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    pkgs <- c(\"GenomeInfoDb\", \"rtracklayer\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    toUCSC = \"$args\"==\"toUCSC\"\n    inf = \"$fname\"\n    ## check file format\n    ## if it is bigwig file\n    isBWF <- grepl(\"\\\\\\\\.(bw|bigwig)\", inf, ignore.case=TRUE)\n    if(isBWF){## decrease the memory cost\n        bwfile <- BigWigFile(inf)\n        seqinfo <- seqinfo(bwfile)\n        seqstyle <- seqlevelsStyle(seqinfo)\n    }else{\n        data <- import(inf)\n        seqstyle <- seqlevelsStyle(data)\n    }\n    readBWFile <- function(f, seqinfo){\n        gr <- as(seqinfo, \"GRanges\")\n        data <- GRanges()\n        for(s in seq_along(gr)){\n            dat <- import.bw(f, which = gr[s])\n            dat <- coverage(dat, weight = dat\\$score)\n            dat <- as(dat, \"GRanges\")\n            dat <- dat[dat\\$score > 0] ## negative scores are not allowed\n            data <- c(data, dat)\n        }\n        data <- coverage(data, weight = data\\$score)\n        data <- as(data, \"GRanges\")\n        data <- data[data\\$score > 0]\n        return(data)\n    }\n    if(toUCSC){\n        if(!\"UCSC\" %in% seqstyle){ ## convert to UCSC style\n            if(isBWF){\n                data <- readBWFile(inf, seqinfo)\n            }\n            seqlevelsStyle(data) <- \"UCSC\"\n            ## double check\n            if(sum(grepl(\"^chr\", seqlevels(data)))==0){\n                ids <- grepl(\"^((\\\\\\\\d{1,2})|(IX|IV|V?I{0,3})|([XYMT]{1,2}))\\$\", seqlevels(data))\n                seqlevels(data)[ids] <- paste0(\"chr\", seqlevels(data)[ids])\n            }\n            export(data, file.path(dirname(inf), paste0(\"UCSC.\", basename(inf))))\n        }else{\n            file.copy(inf, file.path(dirname(inf), paste0(\"UCSC.\", basename(inf))))\n        }\n    }else{\n        if(!\"Ensembl\" %in% seqstyle){## convert to Ensembl style\n            if(isBWF){\n                data <- readBWFile(inf, seqinfo)\n            }\n            seqlevelsStyle(data) <- \"Ensembl\"\n            ## double check\n            if(sum(grepl(\"^chr\", seqlevels(data)))>0){\n                ids <- grepl(\"^(chr)((\\\\\\\\d{1,2})|(IX|IV|V?I{0,3})|([XYMT]{1,2}))\\$\", seqlevels(data))\n                seqlevels(data)[ids] <- sub(\"chr\", \"\", seqlevels(data)[ids])\n            }\n            export(data, file.path(dirname(inf), paste0(\"ENSEMBL.\", basename(inf))))\n        }else{\n            file.copy(inf, file.path(dirname(inf), paste0(\"ENSEMBL.\", basename(inf))))\n        }\n    }\n    \"\"\"",
        "nb_lignes_script": 73,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "bin_size",
            "fname"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$fname\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-rtracklayer=1.50.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-rtracklayer:1.50.0--r40h7f5ccec_2' : 'quay.io/biocontainers/bioconductor-rtracklayer:1.50.0--r40h7f5ccec_2' }\""
        ],
        "when": "",
        "stub": ""
    },
    "DIFF_HIPEAK": {
        "name_process": "DIFF_HIPEAK",
        "string_process": "process DIFF_HIPEAK {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-diffhic=1.24.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bioconductor-diffhic:1.24.0--r41h399db7b_0 \"\n    } else {\n        container \"quay.io/biocontainers/bioconductor-diffhic:1.24.0--r41h399db7b_0\"\n    }\n\n    input:\n    path peaks, stageAs: \"peaks/*\"\n    path distalpair, stageAs: \"pairs/*\"\n\n    output:\n    path \"${prefix}/*\"                        , emit: diff\n    path \"${prefix}/*.qc.json\", optional: true, emit: stats\n    path \"versions.yml\"                       , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix   = task.ext.prefix ? \"${task.ext.prefix}\" : \"diffhicar\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on April. 29, 2021 call edgeR\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"edgeR\", \"InteractionSet\", \"rhdf5\", \"BiocParallel\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"snow_type\"=c(\"--snow_type\", \"-t\", \"character\"))\n    opt <- parse_args(option_list, strsplit(\"$args\", \"\\\\\\\\s+\")[[1]])\n    PREFIX <- \"$prefix\"\n    NCORE <- as.numeric(\"$task.cpus\")\n    SNOW_TYPE <- \"SOCK\"\n    if(!is.null(opt\\$snow_type)){\n        SNOW_TYPE <- opt\\$snow_type\n    }\n\n    ## get peaks\n    pf <- dir(\"peaks\", \"peaks\", full.names = TRUE)\n    peaks <- lapply(pf, read.table, header=TRUE)\n    ### reduce the peaks\n    peaks <- unique(do.call(rbind, peaks)[, c(\"chr1\", \"start1\", \"end1\",\n                                            \"chr2\", \"start2\", \"end2\")])\n    peaks <- with(peaks, GInteractions(GRanges(chr1, IRanges(start1, end1)),\n                                        GRanges(chr2, IRanges(start2, end2))))\n    reducePeaks <- function(x){\n        y <- reduce(x)\n        ol <- findOverlaps(x, y)\n        stopifnot(all(seq_along(x) %in% queryHits(ol)))\n        ol <- as.data.frame(ol)\n        y[ol[match(seq_along(x), ol\\$queryHits), \"subjectHits\"]]\n    }\n    first <- reducePeaks(first(peaks))\n    second <- reducePeaks(second(peaks))\n    peaks <- unique(GInteractions(first, second))\n\n    ## get counts\n    if(SNOW_TYPE==\"FORK\"){\n        param <- MulticoreParam(workers = NCORE, progressbar = TRUE)\n    }else{\n        param <- SnowParam(workers = NCORE, progressbar = TRUE, type = SNOW_TYPE)\n    }\n    pc <- dir(\"pairs\", \"h5\\$\", full.names = FALSE)\n    countByOverlaps <- function(pairs, peaks, sep=\"___\"){\n        getPath <- function(root, ...){\n            paste(root, ..., sep=\"/\")\n        }\n        readPairs <- function(pair, chrom1, chrom2){\n            h5content <- rhdf5::h5ls(pair)\n            h5content <- h5content[, \"group\"]\n            h5content <- h5content[grepl(\"data.*\\\\\\\\d+_\\\\\\\\d+\", h5content)]\n            h5content <- unique(h5content)\n            n <- h5content[grepl(paste0(\"data.\", chrom1, \".\", chrom2), h5content)]\n            n <- getPath(n, \"position\")\n            inf <- rhdf5::H5Fopen(pair, flags=\"H5F_ACC_RDONLY\")\n            on.exit({rhdf5::H5Fclose(inf)})\n            pc <- lapply(n, function(.ele){\n                if(rhdf5::H5Lexists(inf, .ele)){\n                    rhdf5::h5read(inf, .ele)\n                }\n            })\n            rhdf5::H5Fclose(inf)\n            rhdf5::h5closeAll()\n            on.exit()\n            pc <- do.call(rbind, pc)\n        }\n        cnt <- lapply(names(peaks), function(chr){\n            .peak <- peaks[[chr]]\n            chr_ <- strsplit(chr, sep)[[1]]\n            chrom1 <- chr_[1]\n            chrom2 <- chr_[2]\n            ps <- readPairs(pairs, chrom1, chrom2)\n            counts_total <- rhdf5::h5read(pairs, \"header/total\")\n            if(length(ps)<1){\n                return(NULL)\n            }\n            ps <- InteractionSet::GInteractions(\n                    GenomicRanges::GRanges(chrom1, IRanges::IRanges(ps[, 1], width=150)),\n                    GenomicRanges::GRanges(chrom2, IRanges::IRanges(ps[, 2], width=150)))\n            counts_tab <- IRanges::countOverlaps(.peak, ps, use.region=\"both\")\n            counts_tab <- cbind(ID=.peak\\$ID, counts_tab)\n            list(count=counts_tab, total=counts_total)\n        })\n        cnt <- cnt[lengths(cnt)>0]\n        counts_total <- vapply(cnt, FUN=function(.ele) .ele\\$total,\n                            FUN.VALUE = numeric(1))\n        counts_total <- sum(counts_total)\n        counts_tab <- do.call(rbind, lapply(cnt, function(.ele) .ele\\$count))\n        list(count=counts_tab, total=counts_total)\n    }\n\n    peaks\\$ID <- seq_along(peaks)\n    peaks.s <- split(peaks, paste(seqnames(first(peaks)), seqnames(second(peaks)), sep=\"___\"))\n    try_res <- try({cnts <- bplapply(file.path(\"pairs\", pc), countByOverlaps, peaks=peaks.s, sep=\"___\", BPPARAM = param)})\n    sizeFactor <- vapply(cnts, FUN=function(.ele) .ele\\$total,\n                        FUN.VALUE = numeric(1))\n    if(inherits(try_res, \"try-error\") || all(sizeFactor==0)){ # check sizeFactor to make sure bplapply work\n        cnts <- lapply(file.path(\"pairs\", pc), countByOverlaps, peaks=peaks.s, sep=\"___\")\n    }\n    h5closeAll()\n    rm(peaks.s)\n    samples <- sub(\"(_REP\\\\\\\\d+)\\\\\\\\.(.*?)h5\\$\", \"\\\\\\\\1\", pc)\n    sizeFactor <- vapply(cnts, FUN=function(.ele) .ele\\$total,\n                        FUN.VALUE = numeric(1))\n    names(sizeFactor) <- samples\n    cnts <- lapply(cnts, function(.ele) .ele\\$count)\n    cnts <- mapply(cnts, samples, FUN=function(.d, .n){\n        colnames(.d)[colnames(.d)!=\"ID\"] <- .n\n        .d\n    }, SIMPLIFY=FALSE)\n    cnts <- Reduce(function(x, y) merge(x, y, by=\"ID\"), cnts)\n    cnts <- cnts[match(peaks\\$ID, cnts[, \"ID\"]), , drop=FALSE]\n    cnts <- cnts[, colnames(cnts)!=\"ID\", drop=FALSE]\n    colnames(cnts) <- samples\n    rownames(cnts) <- seq_along(peaks)\n    mcols(peaks) <- cnts\n\n    pf <- as.character(PREFIX)\n    dir.create(pf)\n\n    fname <- function(subf, ext, ...){\n        pff <- ifelse(is.na(subf), pf, file.path(pf, subf))\n        dir.create(pff, showWarnings = FALSE, recursive = TRUE)\n        file.path(pff, paste(..., ext, sep=\".\"))\n    }\n\n    ## write counts\n    write.csv(peaks, fname(NA, \"csv\", \"raw.counts\"), row.names = FALSE)\n    ## write sizeFactors\n    write.csv(sizeFactor, fname(NA, \"csv\", \"library.size\"), row.names = TRUE)\n\n    ## coldata\n    sampleNames <- colnames(cnts)\n    condition <- make.names(sub(\"_REP.*\\$\", \"\", sampleNames), allow_=TRUE)\n    coldata <- data.frame(condition=factor(condition),\n                        row.names = sampleNames)\n    ## write designtable\n    write.csv(coldata, fname(NA, \"csv\", \"designTab\"), row.names = TRUE)\n\n    contrasts.lev <- levels(coldata\\$condition)\n\n    if(length(unique(contrasts.lev))>1 && any(table(condition)>1)){\n        contrasts <- combn(contrasts.lev, 2, simplify = FALSE)\n        ## create DGEList\n        group <- coldata\\$condition\n        y <- DGEList(counts = cnts,\n                    lib.size = sizeFactor,\n                    group = group)\n\n        ## do differential analysis\n        names(contrasts) <- vapply(contrasts,\n                                    FUN=paste,\n                                    FUN.VALUE = character(1),\n                                    collapse = \"-\")\n        y <- calcNormFactors(y)\n        design <- model.matrix(~0+group)\n        colnames(design) <- levels(y\\$samples\\$group)\n        y <- estimateDisp(y,design)\n        fit <- glmQLFit(y, design)\n\n        ## PCA\n        pdf(fname(NA, \"pdf\", \"Multidimensional.scaling.plot-plot\"))\n        mds <- plotMDS(y)\n        dev.off()\n        ## PCA for multiQC\n        try_res <- try({\n        json <- data.frame(x=mds\\$x, y=mds\\$y)\n        rownames(json) <- rownames(mds\\$distance.matrix.squared)\n        json <- split(json, coldata[rownames(json), \"condition\"])\n        json <- mapply(json, rainbow(n=length(json)), FUN=function(.ele, .color){\n            .ele <- cbind(.ele, \"name\"=rownames(.ele))\n            .ele <- apply(.ele, 1, function(.e){\n                x <- names(.e)\n                y <- .e\n                .e <- paste0('{\"x\":', .e[1],\n                            ', \"y\":', .e[2],\n                            ', \"color\":\"', .color,\n                            '\", \"name\":\"', .e[3],\n                            '\"}')\n            })\n            .ele <- paste(.ele, collapse=\", \")\n            .ele <- paste(\"[\", .ele, \"]\")\n        })\n        json <- paste0('\"', names(json), '\" :', json)\n        json <- c(\n                \"{\",\n                '\"id\":\"sample_pca\",',\n                '\"data\":{',\n                paste(unlist(json), collapse=\", \"),\n                \"}\",\n                \"}\")\n        writeLines(json, fname(NA, \"json\", \"HiPeak.Multidimensional.scaling.qc\"))\n        })\n        if(inherits(try_res, \"try-error\")){\n            message(try_res)\n        }\n\n        ## plot dispersion\n        pdf(fname(NA, \"pdf\", \"DispersionEstimate-plot\"))\n        plotBCV(y)\n        dev.off()\n        ## plot QL dispersions\n        pdf(fname(NA, \"pdf\", \"Quasi-Likelihood-DispersionEstimate-plot\"))\n        plotQLDisp(fit)\n        dev.off()\n\n        res <- mapply(contrasts, names(contrasts), FUN = function(cont, name){\n            BvsA <- makeContrasts(contrasts = name, levels = design)\n            qlf <- glmQLFTest(fit, contrast = BvsA)\n            rs <- topTags(qlf, n = nrow(qlf), sort.by = \"none\")\n            ## MD-plot\n            pdf(fname(name, \"pdf\", \"Mean-Difference-plot\", name))\n            plotMD(qlf)\n            abline(h=0, col=\"red\", lty=2, lwd=2)\n            dev.off()\n            ## PValue distribution\n            pdf(fname(name, \"pdf\", \"PValue-distribution-plot\", name))\n            hist(rs\\$table\\$PValue, breaks = 20)\n            dev.off()\n            ## save res\n            res <- as.data.frame(rs)\n            res <- cbind(peaks[as.numeric(rownames(res))], res)\n            colnames(res) <- sub(\"seqnames\", \"chr\", colnames(res))\n            write.csv(res, fname(name, \"csv\", \"edgeR.DEtable\", name), row.names = FALSE)\n            ## save metadata\n            elementMetadata <- do.call(rbind, lapply(c(\"adjust.method\",\"comparison\",\"test\"), function(.ele) rs[[.ele]]))\n            rownames(elementMetadata) <- c(\"adjust.method\",\"comparison\",\"test\")\n            colnames(elementMetadata)[1] <- \"value\"\n            write.csv(elementMetadata, fname(name, \"csv\", \"edgeR.metadata\", name), row.names = TRUE)\n            ## save subset results\n            res.s <- res[res\\$FDR<0.05 & abs(res\\$logFC)>1, ]\n            write.csv(res.s, fname(name, \"csv\", \"edgeR.DEtable\", name, \"padj0.05.lfc1\"), row.names = FALSE)\n            ## Volcano plot\n            res\\$qvalue <- -10*log10(res\\$PValue)\n            pdf(fname(name, \"pdf\", \"Volcano-plot\", name))\n            plot(x=res\\$logFC, y=res\\$qvalue,\n                main = paste(\"Volcano plot for\", name),\n                xlab = \"log2 Fold Change\", ylab = \"-10*log10(P-value)\",\n                type = \"p\", col=NA)\n            res.1 <- res\n            if(nrow(res.1)>0) points(x=res.1\\$logFC, y=res.1\\$qvalue, pch = 20, cex=.5, col=\"gray80\")\n            if(nrow(res.s)>0) points(x=res.s\\$logFC, y=res.s\\$qvalue, pch = 19, cex=.5, col=ifelse(res.s\\$logFC>0, \"brown\", \"darkblue\"))\n            dev.off()\n            res\\$qvalue <- -10*log10(res\\$PValue)\n            png(fname(name, \"png\", \"Volcano-plot\", name))\n            plot(x=res\\$logFC, y=res\\$qvalue,\n                main = paste(\"Volcano plot for\", name),\n                xlab = \"log2 Fold Change\", ylab = \"-10*log10(P-value)\",\n                type = \"p\", col=NA)\n            res.1 <- res\n            if(nrow(res.1)>0) points(x=res.1\\$logFC, y=res.1\\$qvalue, pch = 20, cex=.5, col=\"gray80\")\n            if(nrow(res.s)>0) points(x=res.s\\$logFC, y=res.s\\$qvalue, pch = 19, cex=.5, col=ifelse(res.s\\$logFC>0, \"brown\", \"darkblue\"))\n            dev.off()\n        })\n    }\n    \"\"\"\n}",
        "nb_lignes_process": 303,
        "string_script": "    def args   = task.ext.args ?: ''\n    prefix   = task.ext.prefix ? \"${task.ext.prefix}\" : \"diffhicar\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on April. 29, 2021 call edgeR\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"edgeR\", \"InteractionSet\", \"rhdf5\", \"BiocParallel\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"snow_type\"=c(\"--snow_type\", \"-t\", \"character\"))\n    opt <- parse_args(option_list, strsplit(\"$args\", \"\\\\\\\\s+\")[[1]])\n    PREFIX <- \"$prefix\"\n    NCORE <- as.numeric(\"$task.cpus\")\n    SNOW_TYPE <- \"SOCK\"\n    if(!is.null(opt\\$snow_type)){\n        SNOW_TYPE <- opt\\$snow_type\n    }\n\n    ## get peaks\n    pf <- dir(\"peaks\", \"peaks\", full.names = TRUE)\n    peaks <- lapply(pf, read.table, header=TRUE)\n    ### reduce the peaks\n    peaks <- unique(do.call(rbind, peaks)[, c(\"chr1\", \"start1\", \"end1\",\n                                            \"chr2\", \"start2\", \"end2\")])\n    peaks <- with(peaks, GInteractions(GRanges(chr1, IRanges(start1, end1)),\n                                        GRanges(chr2, IRanges(start2, end2))))\n    reducePeaks <- function(x){\n        y <- reduce(x)\n        ol <- findOverlaps(x, y)\n        stopifnot(all(seq_along(x) %in% queryHits(ol)))\n        ol <- as.data.frame(ol)\n        y[ol[match(seq_along(x), ol\\$queryHits), \"subjectHits\"]]\n    }\n    first <- reducePeaks(first(peaks))\n    second <- reducePeaks(second(peaks))\n    peaks <- unique(GInteractions(first, second))\n\n    ## get counts\n    if(SNOW_TYPE==\"FORK\"){\n        param <- MulticoreParam(workers = NCORE, progressbar = TRUE)\n    }else{\n        param <- SnowParam(workers = NCORE, progressbar = TRUE, type = SNOW_TYPE)\n    }\n    pc <- dir(\"pairs\", \"h5\\$\", full.names = FALSE)\n    countByOverlaps <- function(pairs, peaks, sep=\"___\"){\n        getPath <- function(root, ...){\n            paste(root, ..., sep=\"/\")\n        }\n        readPairs <- function(pair, chrom1, chrom2){\n            h5content <- rhdf5::h5ls(pair)\n            h5content <- h5content[, \"group\"]\n            h5content <- h5content[grepl(\"data.*\\\\\\\\d+_\\\\\\\\d+\", h5content)]\n            h5content <- unique(h5content)\n            n <- h5content[grepl(paste0(\"data.\", chrom1, \".\", chrom2), h5content)]\n            n <- getPath(n, \"position\")\n            inf <- rhdf5::H5Fopen(pair, flags=\"H5F_ACC_RDONLY\")\n            on.exit({rhdf5::H5Fclose(inf)})\n            pc <- lapply(n, function(.ele){\n                if(rhdf5::H5Lexists(inf, .ele)){\n                    rhdf5::h5read(inf, .ele)\n                }\n            })\n            rhdf5::H5Fclose(inf)\n            rhdf5::h5closeAll()\n            on.exit()\n            pc <- do.call(rbind, pc)\n        }\n        cnt <- lapply(names(peaks), function(chr){\n            .peak <- peaks[[chr]]\n            chr_ <- strsplit(chr, sep)[[1]]\n            chrom1 <- chr_[1]\n            chrom2 <- chr_[2]\n            ps <- readPairs(pairs, chrom1, chrom2)\n            counts_total <- rhdf5::h5read(pairs, \"header/total\")\n            if(length(ps)<1){\n                return(NULL)\n            }\n            ps <- InteractionSet::GInteractions(\n                    GenomicRanges::GRanges(chrom1, IRanges::IRanges(ps[, 1], width=150)),\n                    GenomicRanges::GRanges(chrom2, IRanges::IRanges(ps[, 2], width=150)))\n            counts_tab <- IRanges::countOverlaps(.peak, ps, use.region=\"both\")\n            counts_tab <- cbind(ID=.peak\\$ID, counts_tab)\n            list(count=counts_tab, total=counts_total)\n        })\n        cnt <- cnt[lengths(cnt)>0]\n        counts_total <- vapply(cnt, FUN=function(.ele) .ele\\$total,\n                            FUN.VALUE = numeric(1))\n        counts_total <- sum(counts_total)\n        counts_tab <- do.call(rbind, lapply(cnt, function(.ele) .ele\\$count))\n        list(count=counts_tab, total=counts_total)\n    }\n\n    peaks\\$ID <- seq_along(peaks)\n    peaks.s <- split(peaks, paste(seqnames(first(peaks)), seqnames(second(peaks)), sep=\"___\"))\n    try_res <- try({cnts <- bplapply(file.path(\"pairs\", pc), countByOverlaps, peaks=peaks.s, sep=\"___\", BPPARAM = param)})\n    sizeFactor <- vapply(cnts, FUN=function(.ele) .ele\\$total,\n                        FUN.VALUE = numeric(1))\n    if(inherits(try_res, \"try-error\") || all(sizeFactor==0)){ # check sizeFactor to make sure bplapply work\n        cnts <- lapply(file.path(\"pairs\", pc), countByOverlaps, peaks=peaks.s, sep=\"___\")\n    }\n    h5closeAll()\n    rm(peaks.s)\n    samples <- sub(\"(_REP\\\\\\\\d+)\\\\\\\\.(.*?)h5\\$\", \"\\\\\\\\1\", pc)\n    sizeFactor <- vapply(cnts, FUN=function(.ele) .ele\\$total,\n                        FUN.VALUE = numeric(1))\n    names(sizeFactor) <- samples\n    cnts <- lapply(cnts, function(.ele) .ele\\$count)\n    cnts <- mapply(cnts, samples, FUN=function(.d, .n){\n        colnames(.d)[colnames(.d)!=\"ID\"] <- .n\n        .d\n    }, SIMPLIFY=FALSE)\n    cnts <- Reduce(function(x, y) merge(x, y, by=\"ID\"), cnts)\n    cnts <- cnts[match(peaks\\$ID, cnts[, \"ID\"]), , drop=FALSE]\n    cnts <- cnts[, colnames(cnts)!=\"ID\", drop=FALSE]\n    colnames(cnts) <- samples\n    rownames(cnts) <- seq_along(peaks)\n    mcols(peaks) <- cnts\n\n    pf <- as.character(PREFIX)\n    dir.create(pf)\n\n    fname <- function(subf, ext, ...){\n        pff <- ifelse(is.na(subf), pf, file.path(pf, subf))\n        dir.create(pff, showWarnings = FALSE, recursive = TRUE)\n        file.path(pff, paste(..., ext, sep=\".\"))\n    }\n\n    ## write counts\n    write.csv(peaks, fname(NA, \"csv\", \"raw.counts\"), row.names = FALSE)\n    ## write sizeFactors\n    write.csv(sizeFactor, fname(NA, \"csv\", \"library.size\"), row.names = TRUE)\n\n    ## coldata\n    sampleNames <- colnames(cnts)\n    condition <- make.names(sub(\"_REP.*\\$\", \"\", sampleNames), allow_=TRUE)\n    coldata <- data.frame(condition=factor(condition),\n                        row.names = sampleNames)\n    ## write designtable\n    write.csv(coldata, fname(NA, \"csv\", \"designTab\"), row.names = TRUE)\n\n    contrasts.lev <- levels(coldata\\$condition)\n\n    if(length(unique(contrasts.lev))>1 && any(table(condition)>1)){\n        contrasts <- combn(contrasts.lev, 2, simplify = FALSE)\n        ## create DGEList\n        group <- coldata\\$condition\n        y <- DGEList(counts = cnts,\n                    lib.size = sizeFactor,\n                    group = group)\n\n        ## do differential analysis\n        names(contrasts) <- vapply(contrasts,\n                                    FUN=paste,\n                                    FUN.VALUE = character(1),\n                                    collapse = \"-\")\n        y <- calcNormFactors(y)\n        design <- model.matrix(~0+group)\n        colnames(design) <- levels(y\\$samples\\$group)\n        y <- estimateDisp(y,design)\n        fit <- glmQLFit(y, design)\n\n        ## PCA\n        pdf(fname(NA, \"pdf\", \"Multidimensional.scaling.plot-plot\"))\n        mds <- plotMDS(y)\n        dev.off()\n        ## PCA for multiQC\n        try_res <- try({\n        json <- data.frame(x=mds\\$x, y=mds\\$y)\n        rownames(json) <- rownames(mds\\$distance.matrix.squared)\n        json <- split(json, coldata[rownames(json), \"condition\"])\n        json <- mapply(json, rainbow(n=length(json)), FUN=function(.ele, .color){\n            .ele <- cbind(.ele, \"name\"=rownames(.ele))\n            .ele <- apply(.ele, 1, function(.e){\n                x <- names(.e)\n                y <- .e\n                .e <- paste0('{\"x\":', .e[1],\n                            ', \"y\":', .e[2],\n                            ', \"color\":\"', .color,\n                            '\", \"name\":\"', .e[3],\n                            '\"}')\n            })\n            .ele <- paste(.ele, collapse=\", \")\n            .ele <- paste(\"[\", .ele, \"]\")\n        })\n        json <- paste0('\"', names(json), '\" :', json)\n        json <- c(\n                \"{\",\n                '\"id\":\"sample_pca\",',\n                '\"data\":{',\n                paste(unlist(json), collapse=\", \"),\n                \"}\",\n                \"}\")\n        writeLines(json, fname(NA, \"json\", \"HiPeak.Multidimensional.scaling.qc\"))\n        })\n        if(inherits(try_res, \"try-error\")){\n            message(try_res)\n        }\n\n        ## plot dispersion\n        pdf(fname(NA, \"pdf\", \"DispersionEstimate-plot\"))\n        plotBCV(y)\n        dev.off()\n        ## plot QL dispersions\n        pdf(fname(NA, \"pdf\", \"Quasi-Likelihood-DispersionEstimate-plot\"))\n        plotQLDisp(fit)\n        dev.off()\n\n        res <- mapply(contrasts, names(contrasts), FUN = function(cont, name){\n            BvsA <- makeContrasts(contrasts = name, levels = design)\n            qlf <- glmQLFTest(fit, contrast = BvsA)\n            rs <- topTags(qlf, n = nrow(qlf), sort.by = \"none\")\n            ## MD-plot\n            pdf(fname(name, \"pdf\", \"Mean-Difference-plot\", name))\n            plotMD(qlf)\n            abline(h=0, col=\"red\", lty=2, lwd=2)\n            dev.off()\n            ## PValue distribution\n            pdf(fname(name, \"pdf\", \"PValue-distribution-plot\", name))\n            hist(rs\\$table\\$PValue, breaks = 20)\n            dev.off()\n            ## save res\n            res <- as.data.frame(rs)\n            res <- cbind(peaks[as.numeric(rownames(res))], res)\n            colnames(res) <- sub(\"seqnames\", \"chr\", colnames(res))\n            write.csv(res, fname(name, \"csv\", \"edgeR.DEtable\", name), row.names = FALSE)\n            ## save metadata\n            elementMetadata <- do.call(rbind, lapply(c(\"adjust.method\",\"comparison\",\"test\"), function(.ele) rs[[.ele]]))\n            rownames(elementMetadata) <- c(\"adjust.method\",\"comparison\",\"test\")\n            colnames(elementMetadata)[1] <- \"value\"\n            write.csv(elementMetadata, fname(name, \"csv\", \"edgeR.metadata\", name), row.names = TRUE)\n            ## save subset results\n            res.s <- res[res\\$FDR<0.05 & abs(res\\$logFC)>1, ]\n            write.csv(res.s, fname(name, \"csv\", \"edgeR.DEtable\", name, \"padj0.05.lfc1\"), row.names = FALSE)\n            ## Volcano plot\n            res\\$qvalue <- -10*log10(res\\$PValue)\n            pdf(fname(name, \"pdf\", \"Volcano-plot\", name))\n            plot(x=res\\$logFC, y=res\\$qvalue,\n                main = paste(\"Volcano plot for\", name),\n                xlab = \"log2 Fold Change\", ylab = \"-10*log10(P-value)\",\n                type = \"p\", col=NA)\n            res.1 <- res\n            if(nrow(res.1)>0) points(x=res.1\\$logFC, y=res.1\\$qvalue, pch = 20, cex=.5, col=\"gray80\")\n            if(nrow(res.s)>0) points(x=res.s\\$logFC, y=res.s\\$qvalue, pch = 19, cex=.5, col=ifelse(res.s\\$logFC>0, \"brown\", \"darkblue\"))\n            dev.off()\n            res\\$qvalue <- -10*log10(res\\$PValue)\n            png(fname(name, \"png\", \"Volcano-plot\", name))\n            plot(x=res\\$logFC, y=res\\$qvalue,\n                main = paste(\"Volcano plot for\", name),\n                xlab = \"log2 Fold Change\", ylab = \"-10*log10(P-value)\",\n                type = \"p\", col=NA)\n            res.1 <- res\n            if(nrow(res.1)>0) points(x=res.1\\$logFC, y=res.1\\$qvalue, pch = 20, cex=.5, col=\"gray80\")\n            if(nrow(res.s)>0) points(x=res.s\\$logFC, y=res.s\\$qvalue, pch = 19, cex=.5, col=ifelse(res.s\\$logFC>0, \"brown\", \"darkblue\"))\n            dev.off()\n        })\n    }\n    \"\"\"",
        "nb_lignes_script": 283,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "peaks",
            "distalpair"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-diffhic=1.24.0\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/bioconductor-diffhic:1.24.0--r41h399db7b_0 \" } else { container \"quay.io/biocontainers/bioconductor-diffhic:1.24.0--r41h399db7b_0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "SAMTOOLS_MERGE": {
        "name_process": "SAMTOOLS_MERGE",
        "string_process": "process SAMTOOLS_MERGE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(input_files)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"${prefix}.bam\") , optional:true, emit: bam\n    tuple val(meta), path(\"${prefix}.cram\"), optional:true, emit: cram\n    path  \"versions.yml\"                                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def file_type = input_files[0].getExtension()\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        merge \\\\\n        --threads ${task.cpus-1} \\\\\n        $args \\\\\n        ${reference} \\\\\n        ${prefix}.${file_type} \\\\\n        $input_files\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 39,
        "string_script": "    def args = task.ext.args   ?: ''\n    prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def file_type = input_files[0].getExtension()\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        merge \\\\\n        --threads ${task.cpus-1} \\\\\n        $args \\\\\n        ${reference} \\\\\n        ${prefix}.${file_type} \\\\\n        $input_files\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [
            "SAMtools",
            "merger"
        ],
        "tools_url": [
            "https://bio.tools/samtools",
            "https://bio.tools/merger"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            },
            {
                "name": "merger",
                "uri": "https://bio.tools/merger",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0097",
                            "term": "Nucleic acid structure analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0097",
                            "term": "Nucleic acid structure"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0491",
                                    "term": "Pairwise sequence alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence merging"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0491",
                                    "term": "Pairwise alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence splicing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0874",
                                "term": "Comparison matrix"
                            },
                            {
                                "uri": "http://edamontology.org/data_0849",
                                "term": "Sequence record"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0849",
                                "term": "Sequence record"
                            },
                            {
                                "uri": "http://edamontology.org/data_1381",
                                "term": "Pair sequence alignment"
                            }
                        ]
                    }
                ],
                "description": "Merge two overlapping sequences.",
                "homepage": "http://emboss.open-bio.org/rel/rel6/apps/merger.html"
            }
        ],
        "inputs": [
            "meta",
            "input_files",
            "fasta"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' : 'quay.io/biocontainers/samtools:1.15--h1170115_1' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "BEDTOOLS_GENOMECOV": {
        "name_process": "BEDTOOLS_GENOMECOV",
        "string_process": "process BEDTOOLS_GENOMECOV {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(intervals), val(scale)\n    path  sizes\n    val   extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: genomecov\n    path  \"versions.yml\"                   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args_list = args.tokenize()\n    args += (scale > 0 && scale != 1) ? \" -scale $scale\" : \"\"\n    if (!args_list.contains('-bg') && (scale > 0 && scale != 1)) {\n        args += \" -bg\"\n    }\n\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (intervals.name =~ /\\.bam/) {\n        \"\"\"\n        bedtools \\\\\n            genomecov \\\\\n            -ibam $intervals \\\\\n            $args \\\\\n            > ${prefix}.${extension}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        bedtools \\\\\n            genomecov \\\\\n            -i $intervals \\\\\n            -g $sizes \\\\\n            $args \\\\\n            > ${prefix}.${extension}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n        END_VERSIONS\n        \"\"\"\n    }\n}",
        "nb_lignes_process": 57,
        "string_script": "    def args = task.ext.args ?: ''\n    def args_list = args.tokenize()\n    args += (scale > 0 && scale != 1) ? \" -scale $scale\" : \"\"\n    if (!args_list.contains('-bg') && (scale > 0 && scale != 1)) {\n        args += \" -bg\"\n    }\n\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (intervals.name =~ /\\.bam/) {\n        \"\"\"\n        bedtools \\\\\n            genomecov \\\\\n            -ibam $intervals \\\\\n            $args \\\\\n            > ${prefix}.${extension}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        bedtools \\\\\n            genomecov \\\\\n            -i $intervals \\\\\n            -g $sizes \\\\\n            $args \\\\\n            > ${prefix}.${extension}\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n        END_VERSIONS\n        \"\"\"\n    }",
        "nb_lignes_script": 35,
        "language_script": "bash",
        "tools": [
            "PopTargs",
            "BEDTools"
        ],
        "tools_url": [
            "https://bio.tools/PopTargs",
            "https://bio.tools/bedtools"
        ],
        "tools_dico": [
            {
                "name": "PopTargs",
                "uri": "https://bio.tools/PopTargs",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2885",
                            "term": "DNA polymorphism"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0659",
                            "term": "Functional, regulatory and non-coding RNA"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0160",
                            "term": "Sequence sites, features and motifs"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3056",
                            "term": "Population genetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0463",
                                    "term": "miRNA target prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2421",
                                    "term": "Database search"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0463",
                                    "term": "microRNA target detection"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0463",
                                    "term": "miRNA prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0463",
                                    "term": "microRNA detection"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2421",
                                    "term": "Search"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "PopTargs is a database for studying population evolutionary genetics of human microRNA target sites.\n\nThese are the scripts used to create the MySQL database that is used by PopTargs.essex.ac.uk. The pipeline can be altered to create similar databases with different species, it may need to be adjusted to fit your file names.",
                "homepage": "https://poptargs.essex.ac.uk/"
            },
            {
                "name": "BEDTools",
                "uri": "https://bio.tools/bedtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Cartography"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "BEDTools is an extensive suite of utilities for comparing genomic features in BED format.",
                "homepage": "https://github.com/arq5x/bedtools2"
            }
        ],
        "inputs": [
            "meta",
            "scale",
            "intervals",
            "sizes",
            "extension"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' : 'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "GTF2BED": {
        "name_process": "GTF2BED",
        "string_process": "process GTF2BED {\n    tag '$gtf'\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::perl-getopt-long=2.50\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/perl-getopt-long:2.50--pl526_1' :\n        'quay.io/biocontainers/perl-getopt-long:2.50--pl526_1' }\"\n\n    input:\n    path gtf\n\n    output:\n    path \"*.bed\"                  , emit: bed\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    \"\"\"\n    gtf2bed $gtf > ${gtf.baseName}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        perl: \\$(perl -e 'print \\$];')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "    \"\"\"\n    gtf2bed $gtf > ${gtf.baseName}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        perl: \\$(perl -e 'print \\$];')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "gtf"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag '$gtf'",
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::perl-getopt-long=2.50\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/perl-getopt-long:2.50--pl526_1' : 'quay.io/biocontainers/perl-getopt-long:2.50--pl526_1' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MACS2_CALLPEAK": {
        "name_process": "MACS2_CALLPEAK",
        "string_process": "process MACS2_CALLPEAK {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::macs2=2.2.7.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/macs2:2.2.7.1--py38h4a8c8d9_3' :\n        'quay.io/biocontainers/macs2:2.2.7.1--py38h4a8c8d9_3' }\"\n\n    input:\n    tuple val(meta), path(ipbam), path(controlbam)\n    val   macs2_gsize\n\n    output:\n    tuple val(meta), path(\"*.{narrowPeak,broadPeak}\"), emit: peak\n    tuple val(meta), path(\"*.xls\")                   , emit: xls\n    path  \"versions.yml\"                             , emit: versions\n\n    tuple val(meta), path(\"*.gappedPeak\"), optional:true, emit: gapped\n    tuple val(meta), path(\"*.bed\")       , optional:true, emit: bed\n    tuple val(meta), path(\"*.bdg\")       , optional:true, emit: bdg\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def args_list = args.tokenize()\n    def format    = meta.single_end ? 'BAM' : 'BAMPE'\n    def control   = controlbam ? \"--control $controlbam\" : ''\n    if(args_list.contains('--format')){\n        def id = args_list.findIndexOf{it=='--format'}\n        format = args_list[id+1]\n        args_list.remove(id+1)\n        args_list.remove(id)\n    }\n    \"\"\"\n    macs2 \\\\\n        callpeak \\\\\n        ${args_list.join(' ')} \\\\\n        --gsize $macs2_gsize \\\\\n        --format $format \\\\\n        --name $prefix \\\\\n        --treatment $ipbam \\\\\n        $control\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        macs2: \\$(macs2 --version | sed -e \"s/macs2 //g\")\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 51,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def args_list = args.tokenize()\n    def format    = meta.single_end ? 'BAM' : 'BAMPE'\n    def control   = controlbam ? \"--control $controlbam\" : ''\n    if(args_list.contains('--format')){\n        def id = args_list.findIndexOf{it=='--format'}\n        format = args_list[id+1]\n        args_list.remove(id+1)\n        args_list.remove(id)\n    }\n    \"\"\"\n    macs2 \\\\\n        callpeak \\\\\n        ${args_list.join(' ')} \\\\\n        --gsize $macs2_gsize \\\\\n        --format $format \\\\\n        --name $prefix \\\\\n        --treatment $ipbam \\\\\n        $control\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        macs2: \\$(macs2 --version | sed -e \"s/macs2 //g\")\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 25,
        "language_script": "bash",
        "tools": [
            "DEFormats"
        ],
        "tools_url": [
            "https://bio.tools/deformats"
        ],
        "tools_dico": [
            {
                "name": "DEFormats",
                "uri": "https://bio.tools/deformats",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Covert between different data formats used by differential gene expression analysis tools.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/DEFormats.html"
            }
        ],
        "inputs": [
            "meta",
            "ipbam",
            "controlbam",
            "macs2_gsize"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::macs2=2.2.7.1\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/macs2:2.2.7.1--py38h4a8c8d9_3' : 'quay.io/biocontainers/macs2:2.2.7.1--py38h4a8c8d9_3' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "COOLER_MERGE": {
        "name_process": "COOLER_MERGE",
        "string_process": "process COOLER_MERGE {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::cooler=0.8.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cooler:0.8.11--pyh3252c3a_0' :\n        'quay.io/biocontainers/cooler:0.8.11--pyh3252c3a_0' }\"\n\n    input:\n    tuple val(meta), path(cool)\n\n    output:\n    tuple val(meta), path(\"*.cool\"), emit: cool\n    path \"versions.yml\"            , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ?: \"${meta.id}${meta.bin}\"\n    def args = task.ext.args ?: ''\n    \"\"\"\n    cooler merge \\\\\n        $args \\\\\n        ${prefix}.cool \\\\\n        ${cool}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cooler: \\$(echo \\$(cooler --version 2>&1) | sed 's/cooler, version //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "    def prefix   = task.ext.prefix ?: \"${meta.id}${meta.bin}\"\n    def args = task.ext.args ?: ''\n    \"\"\"\n    cooler merge \\\\\n        $args \\\\\n        ${prefix}.cool \\\\\n        ${cool}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cooler: \\$(echo \\$(cooler --version 2>&1) | sed 's/cooler, version //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "cool"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::cooler=0.8.11\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/cooler:0.8.11--pyh3252c3a_0' : 'quay.io/biocontainers/cooler:0.8.11--pyh3252c3a_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "GENMAP_MAPPABILITY": {
        "name_process": "GENMAP_MAPPABILITY",
        "string_process": "process GENMAP_MAPPABILITY {\n    tag '$fasta'\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::genmap=1.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/genmap:1.3.0--h1b792b2_1' :\n        'quay.io/biocontainers/genmap:1.3.0--h1b792b2_1' }\"\n\n    input:\n    path index\n\n    output:\n    path \"*.wig\"        , optional:true, emit: wig\n    path \"*.bedgraph\"   , optional:true, emit: bedgraph\n    path \"*.txt\"        , optional:true, emit: txt\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    genmap \\\\\n        map \\\\\n        $args \\\\\n        -I $index \\\\\n        -O mappability\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        genmap: \\$(genmap --version 2>&1 | sed 's/GenMap version: //; s/SeqAn.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    genmap \\\\\n        map \\\\\n        $args \\\\\n        -I $index \\\\\n        -O mappability\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        genmap: \\$(genmap --version 2>&1 | sed 's/GenMap version: //; s/SeqAn.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "GenMAPP",
            "MAP"
        ],
        "tools_url": [
            "https://bio.tools/genmapp",
            "https://bio.tools/MAP"
        ],
        "tools_dico": [
            {
                "name": "GenMAPP",
                "uri": "https://bio.tools/genmapp",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarray experiment"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0602",
                            "term": "Molecular interactions, pathways and networks"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarrays"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0314",
                                    "term": "Gene expression profiling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0313",
                                    "term": "Expression profile clustering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0315",
                                    "term": "Expression profile comparison"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0314",
                                    "term": "Functional profiling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0314",
                                    "term": "Gene expression profile construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0314",
                                    "term": "Feature expression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0314",
                                    "term": "Gene transcription profiling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0314",
                                    "term": "Gene expression quantification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0314",
                                    "term": "Gene expression profile generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression data analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Microarray expression data visualization tool, allowing data to be viewed on maps representing gene groupings and biological pathways.",
                "homepage": "http://www.genmapp.org/"
            },
            {
                "name": "MAP",
                "uri": "https://bio.tools/MAP",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3520",
                            "term": "Proteomics experiment"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0108",
                            "term": "Protein expression"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3638",
                                    "term": "SILAC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3659",
                                    "term": "Regression analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression data analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Cartography"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Model-based analysis of proteomic data to detect proteins with significant abundance changes.\n\nMAP (Model-based Analysis of Proteomic data), is designed to statistically compare the proteomic profiles generated from different biological samples using the isotope labeling based mass spectrometry (MS) technique and directly identify proteins with significant abundance changes. Unlike many existing tools for this purpose, it does not require parallel/additional technical replicates to fathom technical variations; instead, MAP uses a novel step-by-step regression analysis to directly model technical variations from the profiles under comparison. Therefore, experimental designs and their expenses can be simplified and reduced for more practices",
                "homepage": "http://bioinfo.sibs.ac.cn/shaolab/MAP"
            }
        ],
        "inputs": [
            "index"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag '$fasta'",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::genmap=1.3.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/genmap:1.3.0--h1b792b2_1' : 'quay.io/biocontainers/genmap:1.3.0--h1b792b2_1' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "MAPS_RAW2BG2": {
        "name_process": "MAPS_RAW2BG2",
        "string_process": "process MAPS_RAW2BG2 {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"conda-forge::r-data.table=1.12.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/r-data.table:1.12.2' :\n        'quay.io/biocontainers/r-data.table:1.12.2' }\"\n\n    input:\n    tuple val(meta), val(bin_size), path(peak)\n\n    output:\n    tuple val(meta), val(bin_size), path(\"${prefix}.bg2\")          , emit: bg2\n    tuple val(meta), val(bin_size), path(\"${prefix}.ginteractions\"), emit: gi\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    prefix   = task.ext.prefix ? \"${task.ext.prefix}\" : \"${meta.id}_${bin_size}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #########################################\n    # Author: jianhong ou\n    # create single files from reg_raw to bedgraph for cool load and juicerbox\n    #########################################\n    versions <- c(\"${task.process}:\", \"    MAPS: 1.1.0\")\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    options(\"scipen\"=999)\n    library(data.table)\n    RESOLUTION = as.numeric($bin_size)\n    BG2_OUT = \"${prefix}.bg2\"\n    GI_OUT  = \"${prefix}.ginteractions\"\n    infs = strsplit(\"${peak}\", \"\\\\\\\\s+\")[[1]]\n\n    peaks_final_out <- lapply(infs, function(inf){\n        peaks = as.data.table(read.table(inf, header=TRUE, stringsAsFactors=FALSE))\n        if(nrow(peaks)==0){\n            peaks\\$bin1_end <- peaks\\$bin2_end <- numeric(0)\n        }else{\n            peaks\\$bin1_end = peaks\\$bin1_mid + RESOLUTION\n            peaks\\$bin2_end = peaks\\$bin2_mid + RESOLUTION\n        }\n        peaks_final = subset(peaks, select = c( \"chr\", \"bin1_mid\", \"bin1_end\",\n                                                \"chr\", \"bin2_mid\", \"bin2_end\",\n                                                \"ratio2\"))\n        colnames(peaks_final) = c('chrom1', 'start1', 'end1', 'chrom2', 'start2', 'end2', 'count')\n        peaks_final\n    })\n    peaks_final_out <- do.call(rbind, peaks_final_out)\n    peaks_final_out <- peaks_final_out[peaks_final_out\\$count>0, , drop=FALSE]\n    peaks_final_out <- peaks_final_out[order(peaks_final_out\\$chrom1,\n                                            peaks_final_out\\$start1,\n                                            peaks_final_out\\$chrom2,\n                                            peaks_final_out\\$start2), , drop=FALSE]\n    write.table(peaks_final_out, BG2_OUT, row.names = FALSE, col.names = FALSE, quote=FALSE, sep='\\t')\n    #print <strand1> <chr1> <pos1> <frag1> <strand2> <chr2> <pos2> <frag2> <score>\n    peaks_final_out = cbind(0, peaks_final_out[, c('chrom1', 'start1')], 0,\n                            0, peaks_final_out[, c('chrom2', 'start2')], 1,\n                            peaks_final_out[, 'count'])\n    write.table(peaks_final_out, GI_OUT, row.names = FALSE, col.names = FALSE, quote=FALSE, sep='\\t')\n    \"\"\"\n}",
        "nb_lignes_process": 64,
        "string_script": "    def args   = task.ext.args ?: ''\n    prefix   = task.ext.prefix ? \"${task.ext.prefix}\" : \"${meta.id}_${bin_size}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #########################################\n    # Author: jianhong ou\n    # create single files from reg_raw to bedgraph for cool load and juicerbox\n    #########################################\n    versions <- c(\"${task.process}:\", \"    MAPS: 1.1.0\")\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    options(\"scipen\"=999)\n    library(data.table)\n    RESOLUTION = as.numeric($bin_size)\n    BG2_OUT = \"${prefix}.bg2\"\n    GI_OUT  = \"${prefix}.ginteractions\"\n    infs = strsplit(\"${peak}\", \"\\\\\\\\s+\")[[1]]\n\n    peaks_final_out <- lapply(infs, function(inf){\n        peaks = as.data.table(read.table(inf, header=TRUE, stringsAsFactors=FALSE))\n        if(nrow(peaks)==0){\n            peaks\\$bin1_end <- peaks\\$bin2_end <- numeric(0)\n        }else{\n            peaks\\$bin1_end = peaks\\$bin1_mid + RESOLUTION\n            peaks\\$bin2_end = peaks\\$bin2_mid + RESOLUTION\n        }\n        peaks_final = subset(peaks, select = c( \"chr\", \"bin1_mid\", \"bin1_end\",\n                                                \"chr\", \"bin2_mid\", \"bin2_end\",\n                                                \"ratio2\"))\n        colnames(peaks_final) = c('chrom1', 'start1', 'end1', 'chrom2', 'start2', 'end2', 'count')\n        peaks_final\n    })\n    peaks_final_out <- do.call(rbind, peaks_final_out)\n    peaks_final_out <- peaks_final_out[peaks_final_out\\$count>0, , drop=FALSE]\n    peaks_final_out <- peaks_final_out[order(peaks_final_out\\$chrom1,\n                                            peaks_final_out\\$start1,\n                                            peaks_final_out\\$chrom2,\n                                            peaks_final_out\\$start2), , drop=FALSE]\n    write.table(peaks_final_out, BG2_OUT, row.names = FALSE, col.names = FALSE, quote=FALSE, sep='\\t')\n    #print <strand1> <chr1> <pos1> <frag1> <strand2> <chr2> <pos2> <frag2> <score>\n    peaks_final_out = cbind(0, peaks_final_out[, c('chrom1', 'start1')], 0,\n                            0, peaks_final_out[, c('chrom2', 'start2')], 1,\n                            peaks_final_out[, 'count'])\n    write.table(peaks_final_out, GI_OUT, row.names = FALSE, col.names = FALSE, quote=FALSE, sep='\\t')\n    \"\"\"",
        "nb_lignes_script": 45,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "bin_size",
            "peak"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"conda-forge::r-data.table=1.12.2\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/r-data.table:1.12.2' : 'quay.io/biocontainers/r-data.table:1.12.2' }\""
        ],
        "when": "",
        "stub": ""
    },
    "CREATE_PAIRS": {
        "name_process": "CREATE_PAIRS",
        "string_process": "\nprocess CREATE_PAIRS {\n    publishDir \"${params.outdir}/pairs\",\n        mode: \"copy\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0\"\n    } else {\n        container \"quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0\"\n    }\n\n    input:\n    path chromsizes\n    path gtf\n    path digest_genome_bed\n    val reads\n    val seed\n\n    output:\n    path \"$peak1\"       , emit: peak1\n    path \"$peak2\"       , emit: peak2\n    path \"$pairs\"       , emit: pairs\n    path \"$distalpairs\" , emit: distalpairs\n    path \"$interactions\", emit: interactions\n\n    script:\n    peak1=\"peak1.bed\"\n    peak2=\"peak2.bed\"\n    pairs=\"test.unselected.pairs.gz\"\n    distalpairs=\"distal.pairs.gz\"\n    cut=\"CviQI.bed\"\n    interactions=\"interaction.bedpe\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(GenomicRanges)\n    library(rtracklayer)\n    library(GenomicFeatures)\n    library(InteractionSet)\n    options(scipen=10)\n    set.seed($seed)\n    peak_num_1 <- 10000      # fake R1 events\n    peak_num_2 <- 1000       # fake R2 events\n    real_r1 <- 1000          # real R1 events\n    real_r2 <- 100           # real R2 events\n    real_conn_num  <- 1e4    # real connection events\n    peak_reads_num <- $reads # total signal reads\n    background_lambda <- .5  # background lambda\n    real_connection <- expand.grid(sample.int(peak_num_1, real_r1), sample.int(peak_num_2, real_r1))\n    real_connection <- real_connection[sample.int(nrow(real_connection), real_conn_num), ]\n    rtnorm <- function(n, mean = 0, sd = 1, min = 0, max = 1) { ## help function\n        bounds <- pnorm(c(min, max), mean, sd)\n        u <- runif(n, bounds[1], bounds[2])\n        round(qnorm(u, mean, sd))\n    }\n    ## R1 peak candidates\n    r1s <- import(\"$digest_genome_bed\")\n    r1s <- r1s[-1]\n    width(r1s) <- 1\n    export(r1s, \"$cut\")\n    chromsize <- read.delim(\"$chromsizes\", header=FALSE)\n    chromosomes <- chromsize[, 1]\n    seqlengths <- chromsize[, 2]\n    names(seqlengths) <- chromosomes\n    tileGenome <- tileGenome(seqlengths, tilewidth=500)\n    tileGenome <- unlist(tileGenome)\n    tileGenome\\$count <- countOverlaps(tileGenome, r1s)\n    tileGenome <- tileGenome[tileGenome\\$count>0]\n    stopifnot(length(tileGenome)>peak_num_1*2)\n    peaks1 <- sample(seq_along(tileGenome), peak_num_1*2, replace=FALSE)\n    peaks1 <- tileGenome[peaks1]\n    peaks1 <- reduce(peaks1)\n    peaks1 <- peaks1[sample.int(length(peaks1), peak_num_1)]\n\n    ## R2 peak candidates from promoter\n    txdb <- makeTxDbFromGFF(\"$gtf\")\n    gene <- genes(txdb)\n    pro <- promoters(gene, upstream=4000, downstream=1000)\n    pro <- reduce(pro)\n    stopifnot(length(pro)>peak_num_2)\n    peaks2 <- sample.int(length(pro), peak_num_2, replace=FALSE)\n    peaks2 <- pro[peaks2]\n    peaks2 <- shift(peaks2, shift=2000)\n    w <- rtnorm(peak_num_2, mean=800, sd=300, min=350, max=5000)\n    width(peaks2) <- w\n\n    ## real connections\n    real_gi <-GInteractions(anchor1=peaks1[real_connection[, 1]],\n                            anchor2=peaks2[real_connection[, 2]])\n    ## generate R1 background reads\n    ol <- findOverlaps(peaks1, r1s)\n    q1 <- unique(subjectHits(ol))\n    bg_ids <- rpois(length(q1), lambda=background_lambda) #background for r1\n    bg_ids <- rep(q1, bg_ids)\n    r1_background <- r1s[bg_ids]\n    ## generate R2 background reads\n    q2 <- tile(peaks2, width=1)\n    q2_bkg <- unlist(q2)\n    q2_bkg <- q2_bkg[sample.int(length(q2_bkg), length(r1s))]\n    r2_background <- q2_bkg[bg_ids]\n    ## generate ATAC reads, r1/r2 distance < 1e4\n    ol <- findOverlaps(peaks2, r1s, maxgap=1e3)\n    q1 <- split(subjectHits(ol), queryHits(ol))\n    keep <- unique(queryHits(ol))\n    mn <- peak_reads_num*.6/length(peaks2)\n    atac_ids <- rtnorm(length(keep), mean = mn, sd = mn/10, min=mn/2, max=mn*10) # 60%\n    atac_r2_ids <- mapply(lengths(q2[keep]), atac_ids, FUN=sample.int, replace=TRUE, SIMPLIFY=FALSE)\n    atac_r2_ids <- c(atac_r2_ids[[1]], mapply(cumsum(lengths(q2[keep][-length(q2[keep])])), atac_r2_ids[-1], FUN=`+`, SIMPLIFY=FALSE))\n    r2_atac <- unlist(q2[keep])[unlist(atac_r2_ids)]\n    atac_r1_ids <- mapply(lengths(q1), atac_ids, FUN=sample.int, replace=TRUE, SIMPLIFY=FALSE)\n    atac_r1_ids <- c(atac_r1_ids[[1]], mapply(cumsum(lengths(q1[-length(q1)])), atac_r1_ids[-1], FUN=`+`, SIMPLIFY=FALSE))\n    r1_atac <- r1s[unlist(q1)[unlist(atac_r1_ids)]]\n\n    ## generate real reads\n    mn <- peak_reads_num*.4/real_conn_num\n    mcols(real_gi)[, \"counts\"] <- rtnorm(real_conn_num, mean=mn, sd=mn/2, min=mn/10, max=mn*10)\n    ol <- findOverlaps(first(real_gi), r1s)\n    ols <- split(subjectHits(ol), queryHits(ol))\n    r1_ids <- mapply(lengths(ols), mcols(real_gi)[, \"counts\"], FUN=sample, replace=TRUE, SIMPLIFY=FALSE)\n    r1_ids <- mapply(ols, r1_ids, FUN=function(a, i) a[i], SIMPLIFY=FALSE)\n    r1_ids <- unlist(r1_ids)\n    r1_signal <- r1s[r1_ids]\n    q2 <- tile(second(real_gi), width=50)\n    r2_ids <- mapply(lengths(q2), mcols(real_gi)[, \"counts\"], FUN=sample, replace=TRUE, SIMPLIFY=FALSE)\n    r2_ids <- c(r2_ids[[1]], mapply(cumsum(lengths(q2[-length(q2)])), r2_ids[-1], FUN=`+`, SIMPLIFY=FALSE))\n    r2_signal <- unlist(q2)[unlist(r2_ids)]\n    r2_signal <- shift(r2_signal, shift=sample.int(50, length(r2_signal), replace=TRUE))\n    width(r2_signal) <- 1\n    reads1 <- c(r1_background, r1_atac, r1_signal)\n    reads2 <- c(r2_background, r2_atac, r2_signal)\n    ## sort the reads by r1\n    ord <- order(reads1, reads2)\n    reads1 <- reads1[ord]\n    reads2 <- reads2[ord]\n\n    N <- length(reads1)\n    readID <- paste0(\"r\", formatC(seq.int(N), width=nchar(as.character(N)), flag=\"0\"))\n    strand1 <- sample(c(\"+\", \"-\"), N, replace=TRUE)\n    strand2 <- ifelse(strand1==\"-\", \"+\", \"-\")\n\n    # write peaks1.bed\n    export(peaks1, \"$peak1\")\n    # write peaks2.bed\n    export(peaks2, \"$peak2\")\n    # write pairs\n    header <- c(\"## pairs format v1.0.0\",\n                \"#shape: whole matrix\",\n                \"#genome_assembly: unknown\",\n                paste(\"#chromosomes:\", paste(chromsize[, 1], collapse=\" \")),\n                paste(\"#chromsize:\", chromsize[, 1], chromsize[, 2]),\n                paste0(\"#samheader: @SQ\tSN:\", chromsize[, 1],\"\tLN:\", chromsize[, 2]),\n                \"#columns: readID chrom1 pos1 chrom2 pos2 strand1 strand2 pair_type\")\n    content <- paste(readID,\n                    as.character(seqnames(reads1)),\n                    start(reads1),\n                    as.character(seqnames(reads2)),\n                    start(reads2),\n                    strand1, strand2, \"UU\",\n                    sep=\"\\\\t\")\n    gz <- gzfile(\"$pairs\", \"w\")\n    writeLines(c(header, content), gz)\n    close(gz)\n    # write distalpair\n    dist <- distance(reads1, reads2)\n    content_dist <- content[dist>2000]\n    gz2 <- gzfile(\"$distalpairs\", \"w\")\n    writeLines(c(header, content_dist), gz2)\n    close(gz2)\n    # write real connections\n    export(real_gi, \"$interactions\")\n    \"\"\"\n}",
        "nb_lignes_process": 169,
        "string_script": "    peak1=\"peak1.bed\"\n    peak2=\"peak2.bed\"\n    pairs=\"test.unselected.pairs.gz\"\n    distalpairs=\"distal.pairs.gz\"\n    cut=\"CviQI.bed\"\n    interactions=\"interaction.bedpe\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(GenomicRanges)\n    library(rtracklayer)\n    library(GenomicFeatures)\n    library(InteractionSet)\n    options(scipen=10)\n    set.seed($seed)\n    peak_num_1 <- 10000      # fake R1 events\n    peak_num_2 <- 1000       # fake R2 events\n    real_r1 <- 1000          # real R1 events\n    real_r2 <- 100           # real R2 events\n    real_conn_num  <- 1e4    # real connection events\n    peak_reads_num <- $reads # total signal reads\n    background_lambda <- .5  # background lambda\n    real_connection <- expand.grid(sample.int(peak_num_1, real_r1), sample.int(peak_num_2, real_r1))\n    real_connection <- real_connection[sample.int(nrow(real_connection), real_conn_num), ]\n    rtnorm <- function(n, mean = 0, sd = 1, min = 0, max = 1) { ## help function\n        bounds <- pnorm(c(min, max), mean, sd)\n        u <- runif(n, bounds[1], bounds[2])\n        round(qnorm(u, mean, sd))\n    }\n    ## R1 peak candidates\n    r1s <- import(\"$digest_genome_bed\")\n    r1s <- r1s[-1]\n    width(r1s) <- 1\n    export(r1s, \"$cut\")\n    chromsize <- read.delim(\"$chromsizes\", header=FALSE)\n    chromosomes <- chromsize[, 1]\n    seqlengths <- chromsize[, 2]\n    names(seqlengths) <- chromosomes\n    tileGenome <- tileGenome(seqlengths, tilewidth=500)\n    tileGenome <- unlist(tileGenome)\n    tileGenome\\$count <- countOverlaps(tileGenome, r1s)\n    tileGenome <- tileGenome[tileGenome\\$count>0]\n    stopifnot(length(tileGenome)>peak_num_1*2)\n    peaks1 <- sample(seq_along(tileGenome), peak_num_1*2, replace=FALSE)\n    peaks1 <- tileGenome[peaks1]\n    peaks1 <- reduce(peaks1)\n    peaks1 <- peaks1[sample.int(length(peaks1), peak_num_1)]\n\n    ## R2 peak candidates from promoter\n    txdb <- makeTxDbFromGFF(\"$gtf\")\n    gene <- genes(txdb)\n    pro <- promoters(gene, upstream=4000, downstream=1000)\n    pro <- reduce(pro)\n    stopifnot(length(pro)>peak_num_2)\n    peaks2 <- sample.int(length(pro), peak_num_2, replace=FALSE)\n    peaks2 <- pro[peaks2]\n    peaks2 <- shift(peaks2, shift=2000)\n    w <- rtnorm(peak_num_2, mean=800, sd=300, min=350, max=5000)\n    width(peaks2) <- w\n\n    ## real connections\n    real_gi <-GInteractions(anchor1=peaks1[real_connection[, 1]],\n                            anchor2=peaks2[real_connection[, 2]])\n    ## generate R1 background reads\n    ol <- findOverlaps(peaks1, r1s)\n    q1 <- unique(subjectHits(ol))\n    bg_ids <- rpois(length(q1), lambda=background_lambda) #background for r1\n    bg_ids <- rep(q1, bg_ids)\n    r1_background <- r1s[bg_ids]\n    ## generate R2 background reads\n    q2 <- tile(peaks2, width=1)\n    q2_bkg <- unlist(q2)\n    q2_bkg <- q2_bkg[sample.int(length(q2_bkg), length(r1s))]\n    r2_background <- q2_bkg[bg_ids]\n    ## generate ATAC reads, r1/r2 distance < 1e4\n    ol <- findOverlaps(peaks2, r1s, maxgap=1e3)\n    q1 <- split(subjectHits(ol), queryHits(ol))\n    keep <- unique(queryHits(ol))\n    mn <- peak_reads_num*.6/length(peaks2)\n    atac_ids <- rtnorm(length(keep), mean = mn, sd = mn/10, min=mn/2, max=mn*10) # 60%\n    atac_r2_ids <- mapply(lengths(q2[keep]), atac_ids, FUN=sample.int, replace=TRUE, SIMPLIFY=FALSE)\n    atac_r2_ids <- c(atac_r2_ids[[1]], mapply(cumsum(lengths(q2[keep][-length(q2[keep])])), atac_r2_ids[-1], FUN=`+`, SIMPLIFY=FALSE))\n    r2_atac <- unlist(q2[keep])[unlist(atac_r2_ids)]\n    atac_r1_ids <- mapply(lengths(q1), atac_ids, FUN=sample.int, replace=TRUE, SIMPLIFY=FALSE)\n    atac_r1_ids <- c(atac_r1_ids[[1]], mapply(cumsum(lengths(q1[-length(q1)])), atac_r1_ids[-1], FUN=`+`, SIMPLIFY=FALSE))\n    r1_atac <- r1s[unlist(q1)[unlist(atac_r1_ids)]]\n\n    ## generate real reads\n    mn <- peak_reads_num*.4/real_conn_num\n    mcols(real_gi)[, \"counts\"] <- rtnorm(real_conn_num, mean=mn, sd=mn/2, min=mn/10, max=mn*10)\n    ol <- findOverlaps(first(real_gi), r1s)\n    ols <- split(subjectHits(ol), queryHits(ol))\n    r1_ids <- mapply(lengths(ols), mcols(real_gi)[, \"counts\"], FUN=sample, replace=TRUE, SIMPLIFY=FALSE)\n    r1_ids <- mapply(ols, r1_ids, FUN=function(a, i) a[i], SIMPLIFY=FALSE)\n    r1_ids <- unlist(r1_ids)\n    r1_signal <- r1s[r1_ids]\n    q2 <- tile(second(real_gi), width=50)\n    r2_ids <- mapply(lengths(q2), mcols(real_gi)[, \"counts\"], FUN=sample, replace=TRUE, SIMPLIFY=FALSE)\n    r2_ids <- c(r2_ids[[1]], mapply(cumsum(lengths(q2[-length(q2)])), r2_ids[-1], FUN=`+`, SIMPLIFY=FALSE))\n    r2_signal <- unlist(q2)[unlist(r2_ids)]\n    r2_signal <- shift(r2_signal, shift=sample.int(50, length(r2_signal), replace=TRUE))\n    width(r2_signal) <- 1\n    reads1 <- c(r1_background, r1_atac, r1_signal)\n    reads2 <- c(r2_background, r2_atac, r2_signal)\n    ## sort the reads by r1\n    ord <- order(reads1, reads2)\n    reads1 <- reads1[ord]\n    reads2 <- reads2[ord]\n\n    N <- length(reads1)\n    readID <- paste0(\"r\", formatC(seq.int(N), width=nchar(as.character(N)), flag=\"0\"))\n    strand1 <- sample(c(\"+\", \"-\"), N, replace=TRUE)\n    strand2 <- ifelse(strand1==\"-\", \"+\", \"-\")\n\n    # write peaks1.bed\n    export(peaks1, \"$peak1\")\n    # write peaks2.bed\n    export(peaks2, \"$peak2\")\n    # write pairs\n    header <- c(\"## pairs format v1.0.0\",\n                \"#shape: whole matrix\",\n                \"#genome_assembly: unknown\",\n                paste(\"#chromosomes:\", paste(chromsize[, 1], collapse=\" \")),\n                paste(\"#chromsize:\", chromsize[, 1], chromsize[, 2]),\n                paste0(\"#samheader: @SQ\tSN:\", chromsize[, 1],\"\tLN:\", chromsize[, 2]),\n                \"#columns: readID chrom1 pos1 chrom2 pos2 strand1 strand2 pair_type\")\n    content <- paste(readID,\n                    as.character(seqnames(reads1)),\n                    start(reads1),\n                    as.character(seqnames(reads2)),\n                    start(reads2),\n                    strand1, strand2, \"UU\",\n                    sep=\"\\\\t\")\n    gz <- gzfile(\"$pairs\", \"w\")\n    writeLines(c(header, content), gz)\n    close(gz)\n    # write distalpair\n    dist <- distance(reads1, reads2)\n    content_dist <- content[dist>2000]\n    gz2 <- gzfile(\"$distalpairs\", \"w\")\n    writeLines(c(header, content_dist), gz2)\n    close(gz2)\n    # write real connections\n    export(real_gi, \"$interactions\")\n    \"\"\"",
        "nb_lignes_script": 143,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "chromsizes",
            "gtf",
            "digest_genome_bed",
            "reads",
            "seed"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "publishDir \"${params.outdir}/pairs\" , mode: \"copy\"",
            "conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0\" } else { container \"quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "CHECK_PEAKS": {
        "name_process": "CHECK_PEAKS",
        "string_process": "\nprocess CHECK_PEAKS {\n    publishDir \"${params.outdir}\",\n        mode: \"copy\"\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0\"\n    } else {\n        container \"quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0\"\n    }\n    input:\n    tuple val(meta), path(peak)\n    path peak2\n    path peak1\n    path pairs\n    path interactions\n    path hdf5\n\n    output:\n    path \"results.txt\", emit: qc\n    path \"results.csv\", emit: res\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(rtracklayer)\n    library(InteractionSet)\n    library(rhdf5)\n    hdf5 <- \"$hdf5\"\n    peaks1 <- import(\"$peak1\")\n    peaks2 <- import(\"$peak2\")\n    reads <- read.table(\"$pairs\")\n    h5content <- h5ls(hdf5)\n    h5content <- h5content[, \"group\"]\n    h5content <- h5content[grepl(\"data.*\\\\\\\\d+_\\\\\\\\d+\", h5content)]\n    h5content <- unique(h5content)\n    h5reads_pos <- lapply(paste0(h5content, \"/position\"), h5read, file=hdf5)\n    h5reads_pos <- do.call(rbind, h5reads_pos)\n    stopifnot(\"There are bugs in creating pairs2hdf5.nf\"=identical(nrow(h5reads_pos), nrow(reads)))\n    peaks <- read.table(\"$peak\", header=TRUE)\n    gi <- with(reads, GInteractions(GRanges(V2, IRanges(V3, width=150)), GRanges(V4, IRanges(V5, width=150))))\n    peaks_gi <- with(peaks, GInteractions(GRanges(chr1, IRanges(start1, end1)), GRanges(chr2, IRanges(start2, end2))))\n    peaks1\\$count <- countOverlaps(peaks1, gi)\n    peaks2\\$count <- countOverlaps(peaks2, gi)\n    peaks_gi\\$count <- countOverlaps(peaks_gi, gi, use.region=\"both\")\n    pct <- sum(peaks_gi\\$count==peaks\\$count)/length(peaks_gi)\n    stopifnot(\"There are bugs in counting\"=pct>.99)\n    real_gi <- import(\"$interactions\", format=\"bedpe\")\n    detected_r <- subsetByOverlaps(real_gi, peaks_gi)\n    detected_p <- subsetByOverlaps(peaks_gi, real_gi)\n    P <- length(real_gi)                             ## condition positive\n    N <- length(peaks1)*length(peaks2) - P           ## condition negative\n    TP <- length(detected_r)                         ## True positive\n    FP <- length(peaks_gi) - length(detected_p)      ## False positive\n    sensitivity <- TP/P                              ## sensitivity\n    FDR <- FP/length(peaks_gi)                       ## false discovery rate\n    write.csv(c(\"Total reads\"=nrow(reads),\n                \"Total true connections\"=P,\n                \"True positive\"=TP,\n                \"False positive\"=FP,\n                \"sensitivity\"=sensitivity),\n                \"results.csv\")\n    writeLines(ifelse(FDR<0.1, \"YES\", \"NO\"), \"results.txt\")\n    \"\"\"\n}",
        "nb_lignes_process": 63,
        "string_script": "    \"\"\"\n    #!/usr/bin/env Rscript\n    library(rtracklayer)\n    library(InteractionSet)\n    library(rhdf5)\n    hdf5 <- \"$hdf5\"\n    peaks1 <- import(\"$peak1\")\n    peaks2 <- import(\"$peak2\")\n    reads <- read.table(\"$pairs\")\n    h5content <- h5ls(hdf5)\n    h5content <- h5content[, \"group\"]\n    h5content <- h5content[grepl(\"data.*\\\\\\\\d+_\\\\\\\\d+\", h5content)]\n    h5content <- unique(h5content)\n    h5reads_pos <- lapply(paste0(h5content, \"/position\"), h5read, file=hdf5)\n    h5reads_pos <- do.call(rbind, h5reads_pos)\n    stopifnot(\"There are bugs in creating pairs2hdf5.nf\"=identical(nrow(h5reads_pos), nrow(reads)))\n    peaks <- read.table(\"$peak\", header=TRUE)\n    gi <- with(reads, GInteractions(GRanges(V2, IRanges(V3, width=150)), GRanges(V4, IRanges(V5, width=150))))\n    peaks_gi <- with(peaks, GInteractions(GRanges(chr1, IRanges(start1, end1)), GRanges(chr2, IRanges(start2, end2))))\n    peaks1\\$count <- countOverlaps(peaks1, gi)\n    peaks2\\$count <- countOverlaps(peaks2, gi)\n    peaks_gi\\$count <- countOverlaps(peaks_gi, gi, use.region=\"both\")\n    pct <- sum(peaks_gi\\$count==peaks\\$count)/length(peaks_gi)\n    stopifnot(\"There are bugs in counting\"=pct>.99)\n    real_gi <- import(\"$interactions\", format=\"bedpe\")\n    detected_r <- subsetByOverlaps(real_gi, peaks_gi)\n    detected_p <- subsetByOverlaps(peaks_gi, real_gi)\n    P <- length(real_gi)                             ## condition positive\n    N <- length(peaks1)*length(peaks2) - P           ## condition negative\n    TP <- length(detected_r)                         ## True positive\n    FP <- length(peaks_gi) - length(detected_p)      ## False positive\n    sensitivity <- TP/P                              ## sensitivity\n    FDR <- FP/length(peaks_gi)                       ## false discovery rate\n    write.csv(c(\"Total reads\"=nrow(reads),\n                \"Total true connections\"=P,\n                \"True positive\"=TP,\n                \"False positive\"=FP,\n                \"sensitivity\"=sensitivity),\n                \"results.csv\")\n    writeLines(ifelse(FDR<0.1, \"YES\", \"NO\"), \"results.txt\")\n    \"\"\"",
        "nb_lignes_script": 40,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "peak",
            "peak2",
            "peak1",
            "pairs",
            "interactions",
            "hdf5"
        ],
        "nb_inputs": 7,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "publishDir \"${params.outdir}\" , mode: \"copy\"",
            "conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0\" } else { container \"quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "PAIRTOOLS_RESTRICT": {
        "name_process": "PAIRTOOLS_RESTRICT",
        "string_process": "process PAIRTOOLS_RESTRICT {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' :\n        'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\"\n\n    input:\n    tuple val(meta), path(pairs)\n    path frag\n\n    output:\n    tuple val(meta), path(\"*.pairs.gz\"), emit: restrict\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pairtools \\\\\n        restrict \\\\\n        -f $frag \\\\\n        $args \\\\\n        -o ${prefix}.pairs.gz \\\\\n        $pairs\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 35,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pairtools \\\\\n        restrict \\\\\n        -f $frag \\\\\n        $args \\\\\n        -o ${prefix}.pairs.gz \\\\\n        $pairs\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [
            "restrict"
        ],
        "tools_url": [
            "https://bio.tools/restrict"
        ],
        "tools_dico": [
            {
                "name": "restrict",
                "uri": "https://bio.tools/restrict",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3511",
                            "term": "Nucleic acid sites, features and motifs"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0431",
                                    "term": "Restriction site recognition"
                                }
                            ],
                            []
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2887",
                                "term": "Nucleic acid sequence record"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_1276",
                                "term": "Nucleic acid features"
                            }
                        ]
                    }
                ],
                "description": "Report restriction enzyme cleavage sites in a nucleotide sequence.",
                "homepage": "http://emboss.open-bio.org/rel/rel6/apps/restrict.html"
            }
        ],
        "inputs": [
            "meta",
            "pairs",
            "frag"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' : 'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "SAMPLESHEET_CHECK": {
        "name_process": "SAMPLESHEET_CHECK",
        "string_process": "process SAMPLESHEET_CHECK {\n    tag \"$samplesheet\"\n\n    conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/python:3.8.3' :\n        'quay.io/biocontainers/python:3.8.3' }\"\n\n    input:\n    path samplesheet\n\n    output:\n    path '*.csv'       , emit: csv\n    path \"versions.yml\", emit: versions\n\n    script:                                                                   \n    \"\"\"\n    check_samplesheet.py \\\\\n        $samplesheet \\\\\n        samplesheet.valid.csv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        python: \\$(python --version | sed 's/Python //g')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "    \"\"\"\n    check_samplesheet.py \\\\\n        $samplesheet \\\\\n        samplesheet.valid.csv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        python: \\$(python --version | sed 's/Python //g')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "samplesheet"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$samplesheet\"",
            "conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/python:3.8.3' : 'quay.io/biocontainers/python:3.8.3' }\""
        ],
        "when": "",
        "stub": ""
    },
    "CHECKSUMS": {
        "name_process": "CHECKSUMS",
        "string_process": "process CHECKSUMS {\n    tag \"${meta.id}\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"conda-forge::coreutils=8.31\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/coreutils:8.31--h14c3975_0' :\n        'quay.io/biocontainers/coreutils:8.31--h14c3975_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    path \"*.md5\"                  , emit: md5\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    \"\"\"\n    [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n    [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n    gunzip -c ${prefix}_1.fastq.gz > ${prefix}_1.fastq\n    gunzip -c ${prefix}_2.fastq.gz > ${prefix}_2.fastq\n    md5sum ${prefix}_1.fastq ${prefix}_2.fastq > ${prefix}.md5\n    rm ${prefix}_1.fastq\n    rm ${prefix}_2.fastq\n    if [ ! -z \"${meta.md5_1 ?: ''}\" ] && [ ! -z \"${meta.md5_2 ?: ''}\" ]; then\n        cat <<-END_CHECKSUM | md5sum -c\n    ${meta.md5_1}  ${prefix}_1.fastq.gz\n    ${meta.md5_2}  ${prefix}_2.fastq.gz\n    END_CHECKSUM\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        python: \\$(echo \\$(python --version) | sed 's/Python //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    \"\"\"\n    [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n    [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n    gunzip -c ${prefix}_1.fastq.gz > ${prefix}_1.fastq\n    gunzip -c ${prefix}_2.fastq.gz > ${prefix}_2.fastq\n    md5sum ${prefix}_1.fastq ${prefix}_2.fastq > ${prefix}.md5\n    rm ${prefix}_1.fastq\n    rm ${prefix}_2.fastq\n    if [ ! -z \"${meta.md5_1 ?: ''}\" ] && [ ! -z \"${meta.md5_2 ?: ''}\" ]; then\n        cat <<-END_CHECKSUM | md5sum -c\n    ${meta.md5_1}  ${prefix}_1.fastq.gz\n    ${meta.md5_2}  ${prefix}_2.fastq.gz\n    END_CHECKSUM\n    fi\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        python: \\$(echo \\$(python --version) | sed 's/Python //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 20,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"${meta.id}\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"conda-forge::coreutils=8.31\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/coreutils:8.31--h14c3975_0' : 'quay.io/biocontainers/coreutils:8.31--h14c3975_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MERGE_PEAK": {
        "name_process": "MERGE_PEAK",
        "string_process": "process MERGE_PEAK {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    path peak\n\n    output:\n    path \"merged_peak.bed\"    , emit: peak\n    path \"versions.yml\"       , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    cat $peak | \\\\\n        cut -f1-3 | \\\\\n        sort -k1,1 -k2,2n | \\\\\n        bedtools merge $args \\\\\n            -i stdin > merged_peak.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(echo \\$(bedtools --version) | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    cat $peak | \\\\\n        cut -f1-3 | \\\\\n        sort -k1,1 -k2,2n | \\\\\n        bedtools merge $args \\\\\n            -i stdin > merged_peak.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(echo \\$(bedtools --version) | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "BEDTools"
        ],
        "tools_url": [
            "https://bio.tools/bedtools"
        ],
        "tools_dico": [
            {
                "name": "BEDTools",
                "uri": "https://bio.tools/bedtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Cartography"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "BEDTools is an extensive suite of utilities for comparing genomic features in BED format.",
                "homepage": "https://github.com/arq5x/bedtools2"
            }
        ],
        "inputs": [
            "peak"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' : 'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "IGV": {
        "name_process": "IGV",
        "string_process": "process IGV {\n    label 'process_low'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"python=3.8\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/python:3.8' :\n        'quay.io/biocontainers/python:3.8' }\"\n\n    input:\n    path track\n    val species\n    val outdir\n\n    output:\n    path \"{index.html,readme.txt}\"   , emit: igv\n    path \"versions.yml\"              , emit: versions\n\n    script:\n    \"\"\"\n    create_igv.py $track $species \"${outdir.replaceAll('/$', '')}\"\n    echo \"collect in ${track} and copy all the files into relative folder in a web-server.\" > readme.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        python: \\$(echo \\$(python --version) | sed 's/Python //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "    \"\"\"\n    create_igv.py $track $species \"${outdir.replaceAll('/$', '')}\"\n    echo \"collect in ${track} and copy all the files into relative folder in a web-server.\" > readme.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        python: \\$(echo \\$(python --version) | sed 's/Python //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "track",
            "species",
            "outdir"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "label 'process_low'",
            "label 'error_ignore'",
            "conda (params.enable_conda ? \"python=3.8\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/python:3.8' : 'quay.io/biocontainers/python:3.8' }\""
        ],
        "when": "",
        "stub": ""
    },
    "GUNZIP": {
        "name_process": "GUNZIP",
        "string_process": "process GUNZIP {\n    tag \"$archive\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img' :\n        'biocontainers/biocontainers:v1.2.0_cv1' }\"\n\n    input:\n    tuple val(meta), path(archive)\n\n    output:\n    tuple val(meta), path(\"$gunzip\"), emit: gunzip\n    path \"versions.yml\"             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    gunzip = archive.toString() - '.gz'\n    \"\"\"\n    gunzip \\\\\n        -f \\\\\n        $args \\\\\n        $archive\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gunzip: \\$(echo \\$(gunzip --version 2>&1) | sed 's/^.*(gzip) //; s/ Copyright.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "    def args = task.ext.args ?: ''\n    gunzip = archive.toString() - '.gz'\n    \"\"\"\n    gunzip \\\\\n        -f \\\\\n        $args \\\\\n        $archive\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gunzip: \\$(echo \\$(gunzip --version 2>&1) | sed 's/^.*(gzip) //; s/ Copyright.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "archive"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$archive\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img' : 'biocontainers/biocontainers:v1.2.0_cv1' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "ASSIGN_TYPE": {
        "name_process": "ASSIGN_TYPE",
        "string_process": "process ASSIGN_TYPE {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-chippeakanno=3.26.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-chippeakanno:3.26.0--r41hdfd78af_0' :\n        'quay.io/biocontainers/bioconductor-chippeakanno:3.26.0--r41hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(counts)\n\n    output:\n    tuple val(meta), path(\"summary.*.txt\"), optional: true, emit: summary\n    tuple val(meta), path(\"*.peaks\")      , optional: true, emit: peak\n    tuple val(meta), path(\"*.bedpe\")      , optional: true, emit: bedpe\n    path \"versions.yml\"                                   , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on Aug. 24, 2021 assign interacion type for the peaks\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"graph\", \"RBGL\", \"InteractionSet\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    ## Options\n    ## make_option(c(\"-c\", \"--count_cutoff\"), type=\"integer\", default=12, help=\"count cutoff, default 12\", metavar=\"integer\")\n    ## make_option(c(\"-r\", \"--ratio_cutoff\"), type=\"numeric\", default=2.0, help=\"ratio cutoff, default 2.0\", metavar=\"float\")\n    ## make_option(c(\"-f\", \"--fdr\"), type=\"integer\", default=2, help=\"-log10(fdr) cutoff, default 2\", metavar=\"integer\")\n    ## make_option(c(\"-i\", \"--interactions\"), type=\"character\", default=NULL, help=\"interactions output by call hipeak\", metavar=\"string\")\n    ## make_option(c(\"-o\", \"--output\"), type=\"character\", default=\"peaks\", help=\"sample name of the output prefix\", metavar=\"string\")\n\n    OUTPUT = \".\"\n    GROUP_ID = \"$meta.id\"\n    COUNT_CUTOFF = 12\n    RATIO_CUTOFF = 2.0\n    FDR = 2\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"count_cutoff\"=c(\"--count_cutoff\", \"-c\", \"integer\"),\n                        \"ratio_cutoff\"=c(\"--ratio_cutoff\", \"-r\", \"numeric\"),\n                        \"fdr\"=c(\"--fdr\", \"-f\", \"integer\"))\n    opt <- parse_args(option_list, strsplit(\"$args\", \"\\\\\\\\s+\")[[1]])\n    if(!is.null(opt\\$count_cutoff)){\n        COUNT_CUTOFF <- opt\\$count_cutoff\n    }\n    if(!is.null(opt\\$ratio_cutoff)){\n        RATIO_CUTOFF <- opt\\$ratio_cutoff\n    }\n    if(!is.null(opt\\$fdr)){\n        FDR <- opt\\$fdr\n    }\n\n    peaks <- read.csv(\"$counts\")\n\n    if(!all(c(\"chr1\", \"start1\", \"end1\", \"width1\",\n            \"chr2\", \"start2\", \"end2\", 'width2',\n            \"count\", \"logl\", \"logn\", \"loggc\", \"logm\", \"logdist\", 'logShortCount',\n            \"ratio2\", 'fdr') %in% colnames(peaks))){\n        stop(\"count table is not in correct format.\")\n    }\n\n    classify_peaks <- function(final) {\n        # group the interactions\n        gi <- with(final, GInteractions(GRanges(chr1, IRanges(start1, end1)), GRanges(chr2, IRanges(start2, end2))))\n        ol1 <- findOverlaps(first(gi), drop.self = TRUE, drop.redundant = TRUE)\n        ol2 <- findOverlaps(second(gi), drop.self = TRUE, drop.redundant = TRUE)\n        ol <- unique(c(queryHits(ol1), subjectHits(ol1), queryHits(ol2), subjectHits(ol2)))\n        ol_ <- seq_along(gi)[-ol]\n\n        group <- unique(rbind(as.data.frame(ol1), as.data.frame(ol2)))\n        colnames(group) <- c(\"from\", \"to\")\n        group\\$weight <- rep(1L, nrow(group))\n        group <- split(group, seqnames(first(gi)[group\\$from]))\n        group <- mapply(group, names(group), FUN=function(.ele, .name){\n            if(length(unique(c(.ele[, 1], .ele[, 2])))>sqrt(.Machine\\$integer.max)){\n                .nodes <- unique(c(.ele[, 1], .ele[, 2]))\n                .max_nodes <- floor(sqrt(.Machine\\$integer.max)/2)\n                .nodes_group <- rep(seq.int(ceiling(length(.nodes)/.max_nodes)),\n                                    each=.max_nodes)[seq_along(.nodes)]\n                names(.nodes_group) <- .nodes\n                .ele <- split(.ele, paste(.nodes_group[as.character(.ele[, 1])],\n                                        .nodes_group[as.character(.ele[, 2])],\n                                        sep=\"_\"))\n                .ele <- lapply(.ele, function(.e){\n                    .e <- graphBAM(.e)\n                    .e <- connectedComp(ugraph(.e))\n                    .e <- lapply(.e, as.numeric)\n                })\n                .g <- mapply(.ele, seq_along(.ele), FUN=function(.e, .id){\n                    data.frame( nodes=unlist(.e),\n                                group=rep(paste(.id, names(.e), sep=\"_\"), lengths(.e)))\n                }, SIMPLIFY=FALSE)\n                .g <- do.call(rbind, .g)\n                .gs <- split(.g[, 2], .g[, 1])\n                .gs <- lapply(.gs, unique)\n                .gs <- .gs[!duplicated(.gs)]\n                while(any(lengths(.gs)>1)){\n                    ## merge parents\n                    .gsn <- vapply(.gs, FUN=function(.e) sort(.e)[1], FUN.VALUE=character(1))\n                    .gsn <- rep(.gsn, lengths(.gs))\n                    names(.gsn) <- unlist(.gs)\n                    .gsn <- .gsn[names(.gsn)!=.gsn]\n                    .k <- .g[, \"group\"] %in% names(.gsn)\n                    .g[.k, \"group\"] <- .gsn[.g[.k, \"group\"]]\n                    .gs <- split(.g[, 2], .g[, 1])\n                    .gs <- lapply(.gs, unique)\n                    .gs <- .gs[!duplicated(.gs)]\n                }\n                .g <- unique(.g)\n                .ele <- split(.g[, \"nodes\"], .g[, \"group\"])\n                names(.ele) <- seq_along(.ele)\n                rm(.g, .gs, .gsn, .nodes_group, .nodes)\n            }else{\n                .ele <- graphBAM(.ele)\n                .ele <- connectedComp(ugraph(.ele))\n                .ele <- lapply(.ele, as.numeric)\n            }\n            data.frame( id=unlist(.ele),\n                        g=rep(paste(.name, seq_along(.ele), sep=\"_\"), lengths(.ele)))\n        }, SIMPLIFY=FALSE)\n        group <- do.call(rbind, group)\n\n        final\\$Cluster <- NA\n        final\\$Cluster[group\\$id] <- group\\$g\n        final\\$ClusterSize <- 0\n        final\\$ClusterSize[group\\$id] <- table(group\\$g)[group\\$g]\n        if(any(is.na(final\\$Cluster))) final\\$Cluster[is.na(final\\$Cluster)] <- paste0(\"Singleton_\", seq.int(sum(is.na(final\\$Cluster))))\n        final\\$NegLog10P <- -log10( final\\$p_val_reg2 )\n        final\\$NegLog10P[is.na(final\\$NegLog10P)] <- 0\n        final\\$NegLog10P[is.infinite(final\\$NegLog10P)] <- max(final\\$NegLog10P[!is.infinite(final\\$NegLog10P)]+1)\n        NegLog10P <- rowsum(final\\$NegLog10P, final\\$Cluster)\n        final\\$NegLog10P <- NegLog10P[final\\$Cluster, 1]\n\n        x <- unique( final[ final\\$ClusterSize != 0, c('chr1', 'Cluster', 'NegLog10P', 'ClusterSize')] )\n        if(nrow(x)==0){\n            final\\$ClusterType <- 'Singleton'\n            return(final)\n        }\n\n        # sort rows by cumulative -log10 P-value\n        x <- x[ order(x\\$NegLog10P) ,]\n        y<-sort(x\\$NegLog10P)\n        z<-cbind( seq(1,length(y),1), y )\n\n        # keep a record of z before normalization\n        z0 <- z\n\n        z[,1]<-z[,1]/max(z[,1], na.rm=TRUE)\n        z[,2]<-z[,2]/max(z[,2], na.rm=TRUE)\n\n        u<-z\n        u[,1] <-  1/sqrt(2)*z[,1] + 1/sqrt(2)*z[,2]\n        u[,2] <- -1/sqrt(2)*z[,1] + 1/sqrt(2)*z[,2]\n\n        v<-cbind(u, seq(1,nrow(u),1) )\n        RefPoint <- v[ v[,2]==min(v[,2], na.rm=TRUE) , 3]\n        RefValue <- z0[RefPoint,2]\n\n        # define peak cluster type\n        final\\$ClusterType <- rep(NA, nrow(final))\n        if(length(ol_)) final\\$ClusterType[ ol_ ] <- 'Singleton'\n        if(length(ol)){\n            final\\$ClusterType[ seq_along(gi) %in% ol & final\\$NegLog10P < RefValue ] <-  'SharpPeak'\n            final\\$ClusterType[ seq_along(gi) %in% ol & final\\$NegLog10P >= RefValue ] <- 'BroadPeak'\n        }\n        return(final)\n    }\n\n    peaks <- if(nrow(peaks)>0) subset(peaks, count >= COUNT_CUTOFF & ratio2 >= RATIO_CUTOFF & -log10(fdr) > FDR) else data.frame()\n    if (dim(peaks)[1] == 0) {\n        print(paste('ERROR caller_hipeak.r: 0 bin pairs with count >= ',COUNT_CUTOFF,' observed/expected ratio >= ',RATIO_CUTOFF,' and -log10(fdr) > ',FDR,sep=''))\n        quit()\n    }\n\n    peaks = classify_peaks(peaks)\n\n    outf_name = paste(GROUP_ID, '.',FDR,'.peaks',sep='')\n    dir.create(OUTPUT, recursive=TRUE)\n    peaks <- unique(peaks)\n    write.table(peaks, file.path(OUTPUT, outf_name),\n                row.names = FALSE, col.names = TRUE, quote=FALSE)\n    peaks1 <- cbind(peaks[, c(\"chr1\", \"start1\", \"end1\", \"chr2\", \"start2\", \"end2\")], \"*\", peaks[, \"NegLog10P\", drop=FALSE])\n    peaks1 <- unique(peaks1)\n    write.table(peaks1,\n                file.path(OUTPUT, paste0(GROUP_ID, '.', FDR, '.bedpe')),\n                row.names = FALSE, col.names = FALSE, quote=FALSE, sep=\"\\t\")\n\n    summary_all_runs <- split(peaks, peaks\\$ClusterType)\n    summary_all_runs <- lapply(summary_all_runs, function(.ele){\n        c(count = nrow(.ele),\n        minWidth1 = min(.ele\\$width1),\n        medianWidth1 = median(.ele\\$width1),\n        maxWidth1 = max(.ele\\$width1),\n        minWidth2 = min(.ele\\$width2),\n        medianWidth2 = median(.ele\\$width2),\n        maxWidth2 = max(.ele\\$width2),\n        minFoldChange = min(.ele\\$ratio2),\n        medianFoldChange = median(.ele\\$ratio2),\n        maxFoldChange = max(.ele\\$ratio2))\n    })\n    summary_all_runs <- do.call(rbind, summary_all_runs)\n    summary_outf_name = paste('summary.',GROUP_ID,'.txt',sep='')\n    write.table(summary_all_runs, file.path(OUTPUT, summary_outf_name), row.names = TRUE, col.names = TRUE, quote=FALSE)\n    \"\"\"\n}",
        "nb_lignes_process": 231,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on Aug. 24, 2021 assign interacion type for the peaks\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"graph\", \"RBGL\", \"InteractionSet\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    ## Options\n    ## make_option(c(\"-c\", \"--count_cutoff\"), type=\"integer\", default=12, help=\"count cutoff, default 12\", metavar=\"integer\")\n    ## make_option(c(\"-r\", \"--ratio_cutoff\"), type=\"numeric\", default=2.0, help=\"ratio cutoff, default 2.0\", metavar=\"float\")\n    ## make_option(c(\"-f\", \"--fdr\"), type=\"integer\", default=2, help=\"-log10(fdr) cutoff, default 2\", metavar=\"integer\")\n    ## make_option(c(\"-i\", \"--interactions\"), type=\"character\", default=NULL, help=\"interactions output by call hipeak\", metavar=\"string\")\n    ## make_option(c(\"-o\", \"--output\"), type=\"character\", default=\"peaks\", help=\"sample name of the output prefix\", metavar=\"string\")\n\n    OUTPUT = \".\"\n    GROUP_ID = \"$meta.id\"\n    COUNT_CUTOFF = 12\n    RATIO_CUTOFF = 2.0\n    FDR = 2\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"count_cutoff\"=c(\"--count_cutoff\", \"-c\", \"integer\"),\n                        \"ratio_cutoff\"=c(\"--ratio_cutoff\", \"-r\", \"numeric\"),\n                        \"fdr\"=c(\"--fdr\", \"-f\", \"integer\"))\n    opt <- parse_args(option_list, strsplit(\"$args\", \"\\\\\\\\s+\")[[1]])\n    if(!is.null(opt\\$count_cutoff)){\n        COUNT_CUTOFF <- opt\\$count_cutoff\n    }\n    if(!is.null(opt\\$ratio_cutoff)){\n        RATIO_CUTOFF <- opt\\$ratio_cutoff\n    }\n    if(!is.null(opt\\$fdr)){\n        FDR <- opt\\$fdr\n    }\n\n    peaks <- read.csv(\"$counts\")\n\n    if(!all(c(\"chr1\", \"start1\", \"end1\", \"width1\",\n            \"chr2\", \"start2\", \"end2\", 'width2',\n            \"count\", \"logl\", \"logn\", \"loggc\", \"logm\", \"logdist\", 'logShortCount',\n            \"ratio2\", 'fdr') %in% colnames(peaks))){\n        stop(\"count table is not in correct format.\")\n    }\n\n    classify_peaks <- function(final) {\n        # group the interactions\n        gi <- with(final, GInteractions(GRanges(chr1, IRanges(start1, end1)), GRanges(chr2, IRanges(start2, end2))))\n        ol1 <- findOverlaps(first(gi), drop.self = TRUE, drop.redundant = TRUE)\n        ol2 <- findOverlaps(second(gi), drop.self = TRUE, drop.redundant = TRUE)\n        ol <- unique(c(queryHits(ol1), subjectHits(ol1), queryHits(ol2), subjectHits(ol2)))\n        ol_ <- seq_along(gi)[-ol]\n\n        group <- unique(rbind(as.data.frame(ol1), as.data.frame(ol2)))\n        colnames(group) <- c(\"from\", \"to\")\n        group\\$weight <- rep(1L, nrow(group))\n        group <- split(group, seqnames(first(gi)[group\\$from]))\n        group <- mapply(group, names(group), FUN=function(.ele, .name){\n            if(length(unique(c(.ele[, 1], .ele[, 2])))>sqrt(.Machine\\$integer.max)){\n                .nodes <- unique(c(.ele[, 1], .ele[, 2]))\n                .max_nodes <- floor(sqrt(.Machine\\$integer.max)/2)\n                .nodes_group <- rep(seq.int(ceiling(length(.nodes)/.max_nodes)),\n                                    each=.max_nodes)[seq_along(.nodes)]\n                names(.nodes_group) <- .nodes\n                .ele <- split(.ele, paste(.nodes_group[as.character(.ele[, 1])],\n                                        .nodes_group[as.character(.ele[, 2])],\n                                        sep=\"_\"))\n                .ele <- lapply(.ele, function(.e){\n                    .e <- graphBAM(.e)\n                    .e <- connectedComp(ugraph(.e))\n                    .e <- lapply(.e, as.numeric)\n                })\n                .g <- mapply(.ele, seq_along(.ele), FUN=function(.e, .id){\n                    data.frame( nodes=unlist(.e),\n                                group=rep(paste(.id, names(.e), sep=\"_\"), lengths(.e)))\n                }, SIMPLIFY=FALSE)\n                .g <- do.call(rbind, .g)\n                .gs <- split(.g[, 2], .g[, 1])\n                .gs <- lapply(.gs, unique)\n                .gs <- .gs[!duplicated(.gs)]\n                while(any(lengths(.gs)>1)){\n                    ## merge parents\n                    .gsn <- vapply(.gs, FUN=function(.e) sort(.e)[1], FUN.VALUE=character(1))\n                    .gsn <- rep(.gsn, lengths(.gs))\n                    names(.gsn) <- unlist(.gs)\n                    .gsn <- .gsn[names(.gsn)!=.gsn]\n                    .k <- .g[, \"group\"] %in% names(.gsn)\n                    .g[.k, \"group\"] <- .gsn[.g[.k, \"group\"]]\n                    .gs <- split(.g[, 2], .g[, 1])\n                    .gs <- lapply(.gs, unique)\n                    .gs <- .gs[!duplicated(.gs)]\n                }\n                .g <- unique(.g)\n                .ele <- split(.g[, \"nodes\"], .g[, \"group\"])\n                names(.ele) <- seq_along(.ele)\n                rm(.g, .gs, .gsn, .nodes_group, .nodes)\n            }else{\n                .ele <- graphBAM(.ele)\n                .ele <- connectedComp(ugraph(.ele))\n                .ele <- lapply(.ele, as.numeric)\n            }\n            data.frame( id=unlist(.ele),\n                        g=rep(paste(.name, seq_along(.ele), sep=\"_\"), lengths(.ele)))\n        }, SIMPLIFY=FALSE)\n        group <- do.call(rbind, group)\n\n        final\\$Cluster <- NA\n        final\\$Cluster[group\\$id] <- group\\$g\n        final\\$ClusterSize <- 0\n        final\\$ClusterSize[group\\$id] <- table(group\\$g)[group\\$g]\n        if(any(is.na(final\\$Cluster))) final\\$Cluster[is.na(final\\$Cluster)] <- paste0(\"Singleton_\", seq.int(sum(is.na(final\\$Cluster))))\n        final\\$NegLog10P <- -log10( final\\$p_val_reg2 )\n        final\\$NegLog10P[is.na(final\\$NegLog10P)] <- 0\n        final\\$NegLog10P[is.infinite(final\\$NegLog10P)] <- max(final\\$NegLog10P[!is.infinite(final\\$NegLog10P)]+1)\n        NegLog10P <- rowsum(final\\$NegLog10P, final\\$Cluster)\n        final\\$NegLog10P <- NegLog10P[final\\$Cluster, 1]\n\n        x <- unique( final[ final\\$ClusterSize != 0, c('chr1', 'Cluster', 'NegLog10P', 'ClusterSize')] )\n        if(nrow(x)==0){\n            final\\$ClusterType <- 'Singleton'\n            return(final)\n        }\n\n        # sort rows by cumulative -log10 P-value\n        x <- x[ order(x\\$NegLog10P) ,]\n        y<-sort(x\\$NegLog10P)\n        z<-cbind( seq(1,length(y),1), y )\n\n        # keep a record of z before normalization\n        z0 <- z\n\n        z[,1]<-z[,1]/max(z[,1], na.rm=TRUE)\n        z[,2]<-z[,2]/max(z[,2], na.rm=TRUE)\n\n        u<-z\n        u[,1] <-  1/sqrt(2)*z[,1] + 1/sqrt(2)*z[,2]\n        u[,2] <- -1/sqrt(2)*z[,1] + 1/sqrt(2)*z[,2]\n\n        v<-cbind(u, seq(1,nrow(u),1) )\n        RefPoint <- v[ v[,2]==min(v[,2], na.rm=TRUE) , 3]\n        RefValue <- z0[RefPoint,2]\n\n        # define peak cluster type\n        final\\$ClusterType <- rep(NA, nrow(final))\n        if(length(ol_)) final\\$ClusterType[ ol_ ] <- 'Singleton'\n        if(length(ol)){\n            final\\$ClusterType[ seq_along(gi) %in% ol & final\\$NegLog10P < RefValue ] <-  'SharpPeak'\n            final\\$ClusterType[ seq_along(gi) %in% ol & final\\$NegLog10P >= RefValue ] <- 'BroadPeak'\n        }\n        return(final)\n    }\n\n    peaks <- if(nrow(peaks)>0) subset(peaks, count >= COUNT_CUTOFF & ratio2 >= RATIO_CUTOFF & -log10(fdr) > FDR) else data.frame()\n    if (dim(peaks)[1] == 0) {\n        print(paste('ERROR caller_hipeak.r: 0 bin pairs with count >= ',COUNT_CUTOFF,' observed/expected ratio >= ',RATIO_CUTOFF,' and -log10(fdr) > ',FDR,sep=''))\n        quit()\n    }\n\n    peaks = classify_peaks(peaks)\n\n    outf_name = paste(GROUP_ID, '.',FDR,'.peaks',sep='')\n    dir.create(OUTPUT, recursive=TRUE)\n    peaks <- unique(peaks)\n    write.table(peaks, file.path(OUTPUT, outf_name),\n                row.names = FALSE, col.names = TRUE, quote=FALSE)\n    peaks1 <- cbind(peaks[, c(\"chr1\", \"start1\", \"end1\", \"chr2\", \"start2\", \"end2\")], \"*\", peaks[, \"NegLog10P\", drop=FALSE])\n    peaks1 <- unique(peaks1)\n    write.table(peaks1,\n                file.path(OUTPUT, paste0(GROUP_ID, '.', FDR, '.bedpe')),\n                row.names = FALSE, col.names = FALSE, quote=FALSE, sep=\"\\t\")\n\n    summary_all_runs <- split(peaks, peaks\\$ClusterType)\n    summary_all_runs <- lapply(summary_all_runs, function(.ele){\n        c(count = nrow(.ele),\n        minWidth1 = min(.ele\\$width1),\n        medianWidth1 = median(.ele\\$width1),\n        maxWidth1 = max(.ele\\$width1),\n        minWidth2 = min(.ele\\$width2),\n        medianWidth2 = median(.ele\\$width2),\n        maxWidth2 = max(.ele\\$width2),\n        minFoldChange = min(.ele\\$ratio2),\n        medianFoldChange = median(.ele\\$ratio2),\n        maxFoldChange = max(.ele\\$ratio2))\n    })\n    summary_all_runs <- do.call(rbind, summary_all_runs)\n    summary_outf_name = paste('summary.',GROUP_ID,'.txt',sep='')\n    write.table(summary_all_runs, file.path(OUTPUT, summary_outf_name), row.names = TRUE, col.names = TRUE, quote=FALSE)\n    \"\"\"",
        "nb_lignes_script": 211,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "counts"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-chippeakanno=3.26.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-chippeakanno:3.26.0--r41hdfd78af_0' : 'quay.io/biocontainers/bioconductor-chippeakanno:3.26.0--r41hdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "BEDTOOLS_SORT": {
        "name_process": "BEDTOOLS_SORT",
        "string_process": "process BEDTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(meta), path(intervals)\n    val   extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: sorted\n    path  \"versions.yml\"                   , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        sort \\\\\n        -i $intervals \\\\\n        $args \\\\\n        > ${prefix}.${extension}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedtools \\\\\n        sort \\\\\n        -i $intervals \\\\\n        $args \\\\\n        > ${prefix}.${extension}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(bedtools --version | sed -e \"s/bedtools v//g\")\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "BEDTools"
        ],
        "tools_url": [
            "https://bio.tools/bedtools"
        ],
        "tools_dico": [
            {
                "name": "BEDTools",
                "uri": "https://bio.tools/bedtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Cartography"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "BEDTools is an extensive suite of utilities for comparing genomic features in BED format.",
                "homepage": "https://github.com/arq5x/bedtools2"
            }
        ],
        "inputs": [
            "meta",
            "intervals",
            "extension"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' : 'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "SAMTOOLS_IDXSTATS": {
        "name_process": "SAMTOOLS_IDXSTATS",
        "string_process": "process SAMTOOLS_IDXSTATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.idxstats\"), emit: idxstats\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        idxstats \\\\\n        $bam \\\\\n        > ${bam}.idxstats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        idxstats \\\\\n        $bam \\\\\n        > ${bam}.idxstats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "meta",
            "bam",
            "bai"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' : 'quay.io/biocontainers/samtools:1.15--h1170115_1' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "COOLER_ZOOMIFY": {
        "name_process": "COOLER_ZOOMIFY",
        "string_process": "process COOLER_ZOOMIFY {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::cooler=0.8.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cooler:0.8.11--pyh3252c3a_0' :\n        'quay.io/biocontainers/cooler:0.8.11--pyh3252c3a_0' }\"\n\n    input:\n    tuple val(meta), path(cool)\n\n    output:\n    tuple val(meta), path(\"*.mcool\"), emit: mcool\n    path \"versions.yml\"             , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ?: \"${meta.id}${meta.bin}\"\n    def args = task.ext.args ?: ''\n    \"\"\"\n    cooler zoomify \\\\\n        $args \\\\\n        -r \"${meta.bin}N\" \\\\\n        -n $task.cpus \\\\\n        -o ${prefix}.mcool \\\\\n        $cool\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cooler: \\$(echo \\$(cooler --version 2>&1) | sed 's/cooler, version //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "    def prefix   = task.ext.prefix ?: \"${meta.id}${meta.bin}\"\n    def args = task.ext.args ?: ''\n    \"\"\"\n    cooler zoomify \\\\\n        $args \\\\\n        -r \"${meta.bin}N\" \\\\\n        -n $task.cpus \\\\\n        -o ${prefix}.mcool \\\\\n        $cool\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cooler: \\$(echo \\$(cooler --version 2>&1) | sed 's/cooler, version //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "cool"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::cooler=0.8.11\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/cooler:0.8.11--pyh3252c3a_0' : 'quay.io/biocontainers/cooler:0.8.11--pyh3252c3a_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MAPS_FEATURE": {
        "name_process": "MAPS_FEATURE",
        "string_process": "process MAPS_FEATURE {\n    tag \"$bin_size\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"pandas=1.1.5\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pandas:1.1.5' :\n        'quay.io/biocontainers/pandas:1.1.5' }\"\n\n    input:\n    tuple val(bin_size), path(map)\n    path chrom_sizes\n    path feature_frag2bin_source\n\n    output:\n    tuple val(bin_size), path(\"*_el.txt\")  , emit: bin_feature\n    path \"versions.yml\"                    , emit: versions\n\n    script:\n    \"\"\"\n    python ${feature_frag2bin_source} \\\\\n        -i $map \\\\\n        -o F_GC_M_${map.getSimpleName()}_${bin_size}_el.txt \\\\\n        -b $bin_size \\\\\n        -g $chrom_sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        MAPS: 1.1.0\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "    \"\"\"\n    python ${feature_frag2bin_source} \\\\\n        -i $map \\\\\n        -o F_GC_M_${map.getSimpleName()}_${bin_size}_el.txt \\\\\n        -b $bin_size \\\\\n        -g $chrom_sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        MAPS: 1.1.0\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "bin_size",
            "map",
            "chrom_sizes",
            "feature_frag2bin_source"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$bin_size\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"pandas=1.1.5\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/pandas:1.1.5' : 'quay.io/biocontainers/pandas:1.1.5' }\""
        ],
        "when": "",
        "stub": ""
    },
    "JUICER": {
        "name_process": "JUICER",
        "string_process": "process JUICER {\n    tag \"${meta.id}\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::java-jdk=8.0.112\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/java-jdk:8.0.112--1' :\n        'quay.io/biocontainers/java-jdk:8.0.112--1' }\"\n\n    input:\n    tuple val(meta), path(gi)\n    path juicer_tools_jar\n    path chromsize\n    val juicer_jvm_params\n\n    output:\n    tuple val(meta), path(\"*.hic\")               , emit: hic\n    path \"versions.yml\"                          , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def args = task.ext.args ?: ''\n    \"\"\"\n    ## thanks https://www.biostars.org/p/360254/\n    resolutions=(500 1000 2000 5000 10000 20000 50000 100000 250000 500000 1000000 2000000 5000000)\n    ## clean the chromosomes, otherwise, the patch chromosomes size may be very small\n    ## skip this line if it does not fit your model system\n    grep -v [_M] $chromsize > ${chromsize}tmp\n    grep -v EBV ${chromsize}tmp > ${chromsize}_fil.txt\n    rm ${chromsize}tmp\n    nl=\\$(wc -l < ${chromsize}_fil.txt)\n    if [ \\$nl -eq 0 ]; then\n        cp $chromsize ${chromsize}_fil.txt\n    fi\n    available_size=(\\$(awk '{print \\$NF}' ${chromsize}_fil.txt))\n    max_res=5000000\n    for i in \\${available_size[@]}; do\n        if [ \\$i -lt \\${max_res} ]; then\n            max_res=\\$i\n        fi\n    done\n    res=()\n    function join_by { local IFS=\"\\$1\"; shift; echo \"\\$*\"; }\n    for i in \\${resolutions[@]}\n    do\n        if [ \\$i -ge $meta.bin ] && [ \\$i -lt \\${max_res} ]; then\n            res+=(\\$i)\n        fi\n    done\n\n    res=\\$(join_by , \\${res[@]})\n\n    ## sort and filter the input pairs\n    seqlev=(\\$(awk '{print \\$1}' ${chromsize}_fil.txt))\n    seqlev=\\$(join_by '|' \\${seqlev[@]})\n    tail -n +2 $gi | \\\\\n        awk -v seqlev=\\$seqlev 'match(\\$2, seqlev) && match(\\$6, seqlev) {print}' | \\\\\n        sort -k2,2d -k6,6d > ${gi}.sorted\n    # count available chromsomes in the file\n    skip_do_norm=\\$(awk '{ a[\\$2]++; a[\\$6]++ } END { if(length(a)==1) print(\"-n\") }' ${gi}.sorted)\n\n    java ${juicer_jvm_params} -jar ${juicer_tools_jar} pre \\\\\n        -r \\$res \\\\\n        \\${skip_do_norm} \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        ${gi}.sorted ${prefix}.${meta.bin}.hic ${chromsize}_fil.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        java: \\$(echo \\$(java -jar ${juicer_tools_jar} --version 2>&1) | sed 's/^.*Version //; s/Usage.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 74,
        "string_script": "    def prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def args = task.ext.args ?: ''\n    \"\"\"\n    ## thanks https://www.biostars.org/p/360254/\n    resolutions=(500 1000 2000 5000 10000 20000 50000 100000 250000 500000 1000000 2000000 5000000)\n    ## clean the chromosomes, otherwise, the patch chromosomes size may be very small\n    ## skip this line if it does not fit your model system\n    grep -v [_M] $chromsize > ${chromsize}tmp\n    grep -v EBV ${chromsize}tmp > ${chromsize}_fil.txt\n    rm ${chromsize}tmp\n    nl=\\$(wc -l < ${chromsize}_fil.txt)\n    if [ \\$nl -eq 0 ]; then\n        cp $chromsize ${chromsize}_fil.txt\n    fi\n    available_size=(\\$(awk '{print \\$NF}' ${chromsize}_fil.txt))\n    max_res=5000000\n    for i in \\${available_size[@]}; do\n        if [ \\$i -lt \\${max_res} ]; then\n            max_res=\\$i\n        fi\n    done\n    res=()\n    function join_by { local IFS=\"\\$1\"; shift; echo \"\\$*\"; }\n    for i in \\${resolutions[@]}\n    do\n        if [ \\$i -ge $meta.bin ] && [ \\$i -lt \\${max_res} ]; then\n            res+=(\\$i)\n        fi\n    done\n\n    res=\\$(join_by , \\${res[@]})\n\n    ## sort and filter the input pairs\n    seqlev=(\\$(awk '{print \\$1}' ${chromsize}_fil.txt))\n    seqlev=\\$(join_by '|' \\${seqlev[@]})\n    tail -n +2 $gi | \\\\\n        awk -v seqlev=\\$seqlev 'match(\\$2, seqlev) && match(\\$6, seqlev) {print}' | \\\\\n        sort -k2,2d -k6,6d > ${gi}.sorted\n    # count available chromsomes in the file\n    skip_do_norm=\\$(awk '{ a[\\$2]++; a[\\$6]++ } END { if(length(a)==1) print(\"-n\") }' ${gi}.sorted)\n\n    java ${juicer_jvm_params} -jar ${juicer_tools_jar} pre \\\\\n        -r \\$res \\\\\n        \\${skip_do_norm} \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        ${gi}.sorted ${prefix}.${meta.bin}.hic ${chromsize}_fil.txt\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        java: \\$(echo \\$(java -jar ${juicer_tools_jar} --version 2>&1) | sed 's/^.*Version //; s/Usage.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 52,
        "language_script": "bash",
        "tools": [
            "GOFunction",
            "DSHIFT"
        ],
        "tools_url": [
            "https://bio.tools/gofunction",
            "https://bio.tools/dshift"
        ],
        "tools_dico": [
            {
                "name": "GOFunction",
                "uri": "https://bio.tools/gofunction",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3658",
                                    "term": "Statistical inference"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The GO-function package provides a tool to address the redundancy that result from the GO structure or multiple annotation genes and derive biologically relevant functions from the statistically significant functions based on some intuitive assumption and statistical testing.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/GOFunction.html"
            },
            {
                "name": "DSHIFT",
                "uri": "https://bio.tools/dshift",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0593",
                            "term": "NMR"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0082",
                            "term": "Structure prediction"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3444",
                            "term": "MRI"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0593",
                            "term": "Nuclear magnetic resonance spectroscopy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0593",
                            "term": "NMR spectroscopy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3444",
                            "term": "Nuclear magnetic resonance imaging"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3444",
                            "term": "Magnetic resonance imaging"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3444",
                            "term": "MRT"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3444",
                            "term": "Magnetic resonance tomography"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3444",
                            "term": "NMRI"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0364",
                                    "term": "Random sequence generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0470",
                                    "term": "Protein secondary structure prediction (coils)"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "DSHIFT is a web server for predicting chemical shifts of DNA sequences in random coil form or double helical B-form.",
                "homepage": "http://www.chem.cuhk.edu.hk/DSHIFT/"
            }
        ],
        "inputs": [
            "meta",
            "gi",
            "juicer_tools_jar",
            "chromsize",
            "juicer_jvm_params"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"${meta.id}\"",
            "label 'process_medium'",
            "label 'error_ignore'",
            "conda (params.enable_conda ? \"bioconda::java-jdk=8.0.112\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/java-jdk:8.0.112--1' : 'quay.io/biocontainers/java-jdk:8.0.112--1' }\""
        ],
        "when": "",
        "stub": ""
    },
    "BIOC_TRACKVIEWER": {
        "name_process": "BIOC_TRACKVIEWER",
        "string_process": "process BIOC_TRACKVIEWER {\n    tag \"$bin_size\"\n    label 'process_high'\n    label 'process_long'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(bin_size), path(events), path(mcools)\n    path raw_pairs                                   \n    path gtf\n    path chrom_sizes\n    path restrict\n\n    output:\n    path \"${prefix}/*\"            , emit: v4c\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    prefix   = task.ext.prefix ?: \"diffhic_bin${bin_size}\"\n    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on Aug. 24, 2021 for trackViewer parser\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n\n    pkgs <- c(\"trackViewer\", \"GenomicFeatures\", \"InteractionSet\", \"rtracklayer\", \"rhdf5\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    # Options\n    ## make_option(c(\"-g\", \"--gtf\"), type=\"character\", default=NULL, help=\"filename of gtf\", metavar=\"string\")\n    ## make_option(c(\"-s\", \"--chromsize\"), type=\"character\", default=NULL, help=\"filename of chrome size\", metavar=\"string\")\n    ## make_option(c(\"-x\", \"--restrict\"), type=\"character\", default=NULL, help=\"filename of restrict cut\", metavar=\"string\")\n    ## make_option(c(\"-r\", \"--resolution\"), type=\"integer\", default=NULL, help=\"resolution\", metavar=\"integer\")\n    ## make_option(c(\"-d\", \"--gap\"), type=\"integer\", default=NULL, help=\"gap, default 2*resolution\", metavar=\"integer\")\n    ## make_option(c(\"-l\", \"--readlength\"), type=\"integer\", default=NULL, help=\"reads length used to strengthen the signal, default resolution/10\", metavar=\"integer\")\n    ## make_option(c(\"-m\", \"--maxevents\"), type=\"integer\", default=25, help=\"max events to plot\", metavar=\"integer\")\n    ## make_option(c(\"-o\", \"--output\"), type=\"character\", default=\".\", help=\"output folder\", metavar=\"string\")\n    ## make_option(c(\"-c\", \"--cores\"), type=\"integer\", default=1, help=\"Number of cores\", metavar=\"integer\")\n    ## make_option(c(\"-e\", \"--events\"), type=\"character\", default=NULL, help=\"given events csv file, must be ginteractions file\", metavar=\"string\")\n    maxRegionWidth <- 1e6\n    maxEvent <- 25\n    args <- strsplit(\"${args}\", \"\\\\\\\\s+\")[[1]]\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"gap\"=c(\"--gap\", \"-d\", \"integer\"),\n                        \"readlength\"=c(\"--readlength\", \"-l\", \"integer\"),\n                        \"maxevents\"=c(\"--maxevents\", \"-m\", \"integer\"))\n    opt <- parse_args(option_list, args)\n\n    gtf <- \"${gtf}\"\n    resolution <- ${bin_size}\n    gap <- 2 * resolution\n    readwidth <- ceiling(resolution/10)\n    output <- \"${prefix}\"\n    chrom_size <- \"${chrom_sizes}\"\n    restrict_cut <- \"${restrict}\"\n    if(!is.null(opt\\$gap)){\n        gap <- as.numeric(opt\\$gap)\n    }\n    if(!is.null(opt\\$readlength)){\n        readwidth <- as.numeric(opt\\$readlength)\n    }\n    if(!is.null(opt\\$maxevents)){\n        maxEvent <- as.numeric(opt\\$maxevents)\n    }\n\n    # read the signals\n    cools <- dir(\".\", \"mcool\\$\", full.names = TRUE, recursive = TRUE)\n    pairfiles <- dir(\".\", \".h5\\$\", full.names = TRUE, recursive = TRUE)\n    if(!is.null(opt\\$events)){\n        evts <- read.csv(opt\\$events)\n        if(!\"fdr\" %in% colnames(evts)){\n            evts\\$fdr <- rep(1, nrow(evts))\n        }\n    }else{\n        evts <- dir(\".\", \".anno.csv\\$\", full.names = TRUE, recursive = TRUE)\n        if(length(evts)<1) stop(\"No events file.\")\n        evts <- lapply(evts, read.csv)\n        evts <- do.call(rbind, evts)\n    }\n    stopifnot(nrow(evts)>0)\n    colnames(evts) <- tolower(colnames(evts))\n    stopifnot(all(c(\"chr1\", \"chr2\", \"start1\", \"start2\", \"end1\", \"end2\", \"fdr\") %in%\n        colnames(evts)))\n    evts <- evts[order(evts\\$fdr), , drop=FALSE]\n    if(nrow(evts)<1) stop(\"No events in files.\")\n    dir.create(output, recursive = TRUE, showWarnings = FALSE)\n\n    ## get events ranges\n    restrict_pos <- import(restrict_cut, format = \"BED\")\n    seqlen <- read.delim(chrom_size, header=FALSE, row.names=1)\n    seql <- as.numeric(seqlen[, 1])\n    names(seql) <- rownames(seqlen)\n    txdb <- makeTxDbFromGFF(gtf)\n    gtf <- import(gtf, format = \"GTF\")\n    map <- gtf\\$gene_name\n    names(map) <- gtf\\$gene_id\n    map <- map[!duplicated(names(map))]\n    grs <- with(evts, GInteractions(anchor1 = GRanges(chr1, IRanges(start1, end1)),\n                                    anchor2 = GRanges(chr2, IRanges(start2, end2)),\n                                    mode = \"strict\",\n                                    score = fdr))\n    grs\\$score <- 1-grs\\$score\n    grs <- unique(grs)\n    grs_narrow <- grs\n    regions(grs_narrow) <-\n        resize(regions(grs), width = ceiling(width(regions(grs))/2), fix = \"center\")\n    link <- gi2track(grs_narrow)\n    setTrackStyleParam(link, \"tracktype\", \"link\")\n    setTrackStyleParam(link, \"color\", c(\"gray80\", \"yellow\", \"brown\"))\n    reg0 <- GRanges(seqnames(first(grs)),\n                            IRanges(start = start(first(grs)),\n                                    end = end(second(grs))))\n    reg0 <- reg0[width(reg0)<maxRegionWidth]\n    reg <- reduce(reg0, min.gapwidth = gap)\n    gr1 <- unique(subsetByOverlaps(reg, grs))\n    gr1 <- gr1[seq.int(min(maxEvent, length(gr1)))]\n    prettyMax <- function(x){\n        if(x<1) return(round(x, digits = 2))\n        if(x<=5) return(ceiling(x))\n        if(x<10) return(2*ceiling(x/2))\n        if(x<100) return(10*ceiling(x/10))\n        if(x<1000) return(50*ceiling(x/50))\n        n <- 10^ceiling(log10(x))\n        return(n*ceiling(x/n))\n    }\n\n    ## read file and summary counts\n    getIndex <- function(pos, tileWidth, ext=150){\n        A <- ceiling((start(pos)-ext)/rep(tileWidth, length(pos)))\n        B <- ceiling((end(pos)+ext)/rep(tileWidth, length(pos)))\n        out <- mapply(A, B, FUN=seq, SIMPLIFY=FALSE)\n        sort(unique(unlist(out)))\n    }\n    getPath <- function(root, ...){\n        paste(root, ..., sep=\"/\")\n    }\n    readPairs <- function(pair, chrom, range){\n        tileWidth <- h5read(pair, \"header/tile_width\")\n        idx <- getIndex(range, tileWidth)\n        idx <- expand.grid(idx, idx)\n        idx <- paste(idx[, 1], idx[, 2], sep=\"_\")\n        inf <- H5Fopen(pair, flags=\"H5F_ACC_RDONLY\")\n        on.exit(H5Fclose(inf))\n        pc <- lapply(idx, function(.ele){\n            n <- getPath(\"data\", chrom, chrom, .ele, \"position\")\n            if(H5Lexists(inf, n)){\n                h5read(inf, n)\n            }\n        })\n        pc <- do.call(rbind, pc)\n        strand <- lapply(idx, function(.ele){\n            n <- getPath(\"data\", chrom, chrom, .ele, \"strand\")\n            if(H5Lexists(inf, n)){\n                h5read(inf, n)\n            }\n        })\n        strand <- do.call(rbind, strand)\n        H5Fclose(inf)\n        h5closeAll()\n        on.exit()\n        GInteractions(anchor1 = GRanges(chrom, IRanges(pc[, 1], width=readwidth), strand = strand[, 1]),\n                    anchor2 = GRanges(chrom, IRanges(pc[, 2], width=readwidth), strand = strand[, 2]))\n    }\n    loadPairFile <- function(filenames, ranges, resolution){\n        stopifnot(is.character(filenames))\n        stopifnot(all(file.exists(filenames)))\n        start(ranges) <- start(ranges) - 2*resolution\n        end(ranges) <- end(ranges) + 2*resolution\n        names(filenames) <- sub(\".h5\\$\", \"\", basename(filenames))\n        total <- lapply(filenames, h5read, name=\"header/total\")\n        chrom <- as.character(seqnames(ranges)[1])\n        ranges <- ranges[seqnames(ranges)==chrom]\n        gi <- lapply(filenames, readPairs, chrom=chrom, range=ranges)\n        list(gi=gi, total=total)\n    }\n\n    readPairFile <- function(filenames, ranges, resolution){\n        stopifnot(is(ranges, \"GRanges\"))\n        chunks <- loadPairFile(filenames, ranges, resolution)\n        out <- list()\n        total <- list()\n        for(fn in names(chunks[[\"gi\"]])){\n            chunk <- chunks[[\"gi\"]][[fn]]\n            if(length(chunk)){\n                total[[fn]] <- chunks[[\"total\"]][[fn]]\n                out[[fn]] <-\n                    subsetByOverlaps(chunk, ranges = ranges, use.region=\"both\")\n            }\n            rm(chunk)\n        }\n        ## split by group\n        f <- sub(\"_REP\\\\\\\\d+\", \"\", names(out))\n        out <- split(out, f)\n        giRbind <- function(a, b){\n            GInteractions(anchor1 = c(first(a), first(b)),\n                        anchor2 = c(second(a), second(b)),\n                        regions=sort(unique(c(first(a), first(b), second(a), second(b)))))\n        }\n        out <- lapply(out, function(.ele){\n            Reduce(giRbind, .ele)\n        })\n        total <- split(unlist(total), f)\n        total <- sapply(total, sum)\n        total <- median(total)/total\n        return(list(gi=out, norm_factor=total))\n    }\n\n    for(i in seq_along(gr1)){\n        if(i < maxEvent){\n            try_res <- try({\n                gr <- gr1[i]\n                start(gr) <- start(gr) - 2* resolution\n                end(gr) <- end(gr) + 2* resolution\n                seqlevelsStyle(gr) <- seqlevelsStyle(txdb)[1]\n                seqlengths(gr) <- seql[seqlevels(gr)]\n                gr <- GenomicRanges::trim(gr)\n                chr_gr <- as(seqinfo(gr), \"GRanges\")[seqnames(gr)[1]]\n                bait <- subsetByOverlaps(grs, gr, use.region=\"both\")\n                ## merge the bait by ends\n                bait <- split(second(bait), as.character(first(bait)))\n                bait <- lapply(bait, range)\n                bait <- unlist(GRangesList(bait))\n                bait <- GInteractions(anchor1 = parse2GRanges(names(bait)),\n                                    anchor2 = unname(bait))\n                ## get the v4c\n                info <- readPairFile(pairfiles,\n                                    ranges = c(first(bait), second(bait)),\n                                    resolution=resolution)\n                h5closeAll()\n                total <- info\\$norm_factor\n                names(cools) <- sub(\"\\\\\\\\d+\\\\\\\\.mcool\\$\", \"\", basename(cools))\n                gis <- lapply(cools, importGInteractions,\n                            resolution = resolution,\n                            format=\"cool\",\n                            ranges=gr, out = \"GInteractions\")\n                gis <- mapply(gis, total[names(gis)], FUN=function(.ele, .total){\n                    .ele\\$score <- log2(.ele\\$score+1) * .total\n                    .ele\n                })\n                maxV <- max(sapply(gis, function(.ele) max(.ele\\$score, na.rm=TRUE)))\n                maxV <- prettyMax(maxV)\n                heat <- lapply(gis, function(.ele){\n                    .ele <- gi2track(.ele)\n                    setTrackStyleParam(.ele, \"breaks\", seq(from=0, to=maxV, by=maxV/10))\n                    setTrackStyleParam(.ele, \"color\", c(\"lightblue\", \"yellow\", \"red\"))\n                    #setTrackStyleParam(.ele, \"ylim\", c(0, .5))\n                    .ele\n                })\n                names(heat) <- paste(\"valid pairs\", names(heat))\n                get_v4c <- function(bait1, bait2){\n                    v4c <- lapply(seq_along(bait1), function(.e){\n                        vp1 <- mapply(info\\$gi, total[names(info\\$gi)], FUN=function(.ele, .total){\n                            .ele <- subsetByOverlaps(.ele, bait1[.e], use.region=\"both\")\n                            .ele <- GRanges(coverage(c(first(.ele), second(.ele))))\n                            .ele\\$score <- .ele\\$score*.total\n                            new(\"track\", dat=.ele,\n                                type=\"data\", format = \"BED\",\n                                name = paste0(seqnames(bait1[.e]), \":\",\n                                            start(bait1[.e]), \"-\",\n                                            end(bait1[.e])))\n                        })\n                        maxY <- sapply(vp1, FUN = function(.ele){\n                            max(subsetByOverlaps(.ele\\$dat, bait2[.e])\\$score)\n                        })\n                        maxY <- prettyMax(max(maxY))\n                        vp1 <- lapply(vp1, function(.ele){\n                            setTrackStyleParam(.ele, \"ylim\", c(0, maxY))\n                            setTrackStyleParam(.ele, \"color\", \"gray80\")\n                            .ele\n                        })\n                    })\n                    names(v4c) <- paste0(seqnames(bait1), \":\",\n                                        start(bait1), \"-\",\n                                        end(bait1))\n                    v4c <- unlist(v4c, recursive = FALSE)\n                }\n                v4c_first <- get_v4c(first(bait), second(bait))\n                v4c_second <- get_v4c(second(bait), first(bait))\n                ids <- getGeneIDsFromTxDb(gr, txdb)\n                if(length(ids)){\n                    genes <- geneTrack(ids, txdb, map[ids], asList=FALSE)\n                    tL <- trackList(genes, link, heat, v4c_first, v4c_second,\n                                    heightDist = c(1, 1, 2*length(heat),\n                                    2*length(v4c_first), 2*length(v4c_second)))\n                    names(tL)[2] <- \"called links\"\n                }else{\n                    tL <- trackList(link, heat, v4c_first, v4c_second,\n                                    heightDist = c(1, 2*length(heat),\n                                    2*length(v4c_first), 2*length(v4c_second)))\n                    names(tL)[1] <- \"called links\"\n                }\n                pdf(file.path(output, paste0(\"event_\", i, \"_\", seqnames(gr), \":\", start(gr), \"-\", end(gr), \".pdf\")),\n                    width = 9, height = length(tL))\n                viewTracks(tL, gr=gr, autoOptimizeStyle = TRUE)\n                dev.off()\n            })\n            if(inherits(try_res, \"try-error\")){\n                message(try_res)\n            }\n        }\n    }\n    \"\"\"\n}",
        "nb_lignes_process": 333,
        "string_script": "    prefix   = task.ext.prefix ?: \"diffhic_bin${bin_size}\"\n    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on Aug. 24, 2021 for trackViewer parser\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n\n    pkgs <- c(\"trackViewer\", \"GenomicFeatures\", \"InteractionSet\", \"rtracklayer\", \"rhdf5\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    # Options\n    ## make_option(c(\"-g\", \"--gtf\"), type=\"character\", default=NULL, help=\"filename of gtf\", metavar=\"string\")\n    ## make_option(c(\"-s\", \"--chromsize\"), type=\"character\", default=NULL, help=\"filename of chrome size\", metavar=\"string\")\n    ## make_option(c(\"-x\", \"--restrict\"), type=\"character\", default=NULL, help=\"filename of restrict cut\", metavar=\"string\")\n    ## make_option(c(\"-r\", \"--resolution\"), type=\"integer\", default=NULL, help=\"resolution\", metavar=\"integer\")\n    ## make_option(c(\"-d\", \"--gap\"), type=\"integer\", default=NULL, help=\"gap, default 2*resolution\", metavar=\"integer\")\n    ## make_option(c(\"-l\", \"--readlength\"), type=\"integer\", default=NULL, help=\"reads length used to strengthen the signal, default resolution/10\", metavar=\"integer\")\n    ## make_option(c(\"-m\", \"--maxevents\"), type=\"integer\", default=25, help=\"max events to plot\", metavar=\"integer\")\n    ## make_option(c(\"-o\", \"--output\"), type=\"character\", default=\".\", help=\"output folder\", metavar=\"string\")\n    ## make_option(c(\"-c\", \"--cores\"), type=\"integer\", default=1, help=\"Number of cores\", metavar=\"integer\")\n    ## make_option(c(\"-e\", \"--events\"), type=\"character\", default=NULL, help=\"given events csv file, must be ginteractions file\", metavar=\"string\")\n    maxRegionWidth <- 1e6\n    maxEvent <- 25\n    args <- strsplit(\"${args}\", \"\\\\\\\\s+\")[[1]]\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"gap\"=c(\"--gap\", \"-d\", \"integer\"),\n                        \"readlength\"=c(\"--readlength\", \"-l\", \"integer\"),\n                        \"maxevents\"=c(\"--maxevents\", \"-m\", \"integer\"))\n    opt <- parse_args(option_list, args)\n\n    gtf <- \"${gtf}\"\n    resolution <- ${bin_size}\n    gap <- 2 * resolution\n    readwidth <- ceiling(resolution/10)\n    output <- \"${prefix}\"\n    chrom_size <- \"${chrom_sizes}\"\n    restrict_cut <- \"${restrict}\"\n    if(!is.null(opt\\$gap)){\n        gap <- as.numeric(opt\\$gap)\n    }\n    if(!is.null(opt\\$readlength)){\n        readwidth <- as.numeric(opt\\$readlength)\n    }\n    if(!is.null(opt\\$maxevents)){\n        maxEvent <- as.numeric(opt\\$maxevents)\n    }\n\n    # read the signals\n    cools <- dir(\".\", \"mcool\\$\", full.names = TRUE, recursive = TRUE)\n    pairfiles <- dir(\".\", \".h5\\$\", full.names = TRUE, recursive = TRUE)\n    if(!is.null(opt\\$events)){\n        evts <- read.csv(opt\\$events)\n        if(!\"fdr\" %in% colnames(evts)){\n            evts\\$fdr <- rep(1, nrow(evts))\n        }\n    }else{\n        evts <- dir(\".\", \".anno.csv\\$\", full.names = TRUE, recursive = TRUE)\n        if(length(evts)<1) stop(\"No events file.\")\n        evts <- lapply(evts, read.csv)\n        evts <- do.call(rbind, evts)\n    }\n    stopifnot(nrow(evts)>0)\n    colnames(evts) <- tolower(colnames(evts))\n    stopifnot(all(c(\"chr1\", \"chr2\", \"start1\", \"start2\", \"end1\", \"end2\", \"fdr\") %in%\n        colnames(evts)))\n    evts <- evts[order(evts\\$fdr), , drop=FALSE]\n    if(nrow(evts)<1) stop(\"No events in files.\")\n    dir.create(output, recursive = TRUE, showWarnings = FALSE)\n\n    ## get events ranges\n    restrict_pos <- import(restrict_cut, format = \"BED\")\n    seqlen <- read.delim(chrom_size, header=FALSE, row.names=1)\n    seql <- as.numeric(seqlen[, 1])\n    names(seql) <- rownames(seqlen)\n    txdb <- makeTxDbFromGFF(gtf)\n    gtf <- import(gtf, format = \"GTF\")\n    map <- gtf\\$gene_name\n    names(map) <- gtf\\$gene_id\n    map <- map[!duplicated(names(map))]\n    grs <- with(evts, GInteractions(anchor1 = GRanges(chr1, IRanges(start1, end1)),\n                                    anchor2 = GRanges(chr2, IRanges(start2, end2)),\n                                    mode = \"strict\",\n                                    score = fdr))\n    grs\\$score <- 1-grs\\$score\n    grs <- unique(grs)\n    grs_narrow <- grs\n    regions(grs_narrow) <-\n        resize(regions(grs), width = ceiling(width(regions(grs))/2), fix = \"center\")\n    link <- gi2track(grs_narrow)\n    setTrackStyleParam(link, \"tracktype\", \"link\")\n    setTrackStyleParam(link, \"color\", c(\"gray80\", \"yellow\", \"brown\"))\n    reg0 <- GRanges(seqnames(first(grs)),\n                            IRanges(start = start(first(grs)),\n                                    end = end(second(grs))))\n    reg0 <- reg0[width(reg0)<maxRegionWidth]\n    reg <- reduce(reg0, min.gapwidth = gap)\n    gr1 <- unique(subsetByOverlaps(reg, grs))\n    gr1 <- gr1[seq.int(min(maxEvent, length(gr1)))]\n    prettyMax <- function(x){\n        if(x<1) return(round(x, digits = 2))\n        if(x<=5) return(ceiling(x))\n        if(x<10) return(2*ceiling(x/2))\n        if(x<100) return(10*ceiling(x/10))\n        if(x<1000) return(50*ceiling(x/50))\n        n <- 10^ceiling(log10(x))\n        return(n*ceiling(x/n))\n    }\n\n    ## read file and summary counts\n    getIndex <- function(pos, tileWidth, ext=150){\n        A <- ceiling((start(pos)-ext)/rep(tileWidth, length(pos)))\n        B <- ceiling((end(pos)+ext)/rep(tileWidth, length(pos)))\n        out <- mapply(A, B, FUN=seq, SIMPLIFY=FALSE)\n        sort(unique(unlist(out)))\n    }\n    getPath <- function(root, ...){\n        paste(root, ..., sep=\"/\")\n    }\n    readPairs <- function(pair, chrom, range){\n        tileWidth <- h5read(pair, \"header/tile_width\")\n        idx <- getIndex(range, tileWidth)\n        idx <- expand.grid(idx, idx)\n        idx <- paste(idx[, 1], idx[, 2], sep=\"_\")\n        inf <- H5Fopen(pair, flags=\"H5F_ACC_RDONLY\")\n        on.exit(H5Fclose(inf))\n        pc <- lapply(idx, function(.ele){\n            n <- getPath(\"data\", chrom, chrom, .ele, \"position\")\n            if(H5Lexists(inf, n)){\n                h5read(inf, n)\n            }\n        })\n        pc <- do.call(rbind, pc)\n        strand <- lapply(idx, function(.ele){\n            n <- getPath(\"data\", chrom, chrom, .ele, \"strand\")\n            if(H5Lexists(inf, n)){\n                h5read(inf, n)\n            }\n        })\n        strand <- do.call(rbind, strand)\n        H5Fclose(inf)\n        h5closeAll()\n        on.exit()\n        GInteractions(anchor1 = GRanges(chrom, IRanges(pc[, 1], width=readwidth), strand = strand[, 1]),\n                    anchor2 = GRanges(chrom, IRanges(pc[, 2], width=readwidth), strand = strand[, 2]))\n    }\n    loadPairFile <- function(filenames, ranges, resolution){\n        stopifnot(is.character(filenames))\n        stopifnot(all(file.exists(filenames)))\n        start(ranges) <- start(ranges) - 2*resolution\n        end(ranges) <- end(ranges) + 2*resolution\n        names(filenames) <- sub(\".h5\\$\", \"\", basename(filenames))\n        total <- lapply(filenames, h5read, name=\"header/total\")\n        chrom <- as.character(seqnames(ranges)[1])\n        ranges <- ranges[seqnames(ranges)==chrom]\n        gi <- lapply(filenames, readPairs, chrom=chrom, range=ranges)\n        list(gi=gi, total=total)\n    }\n\n    readPairFile <- function(filenames, ranges, resolution){\n        stopifnot(is(ranges, \"GRanges\"))\n        chunks <- loadPairFile(filenames, ranges, resolution)\n        out <- list()\n        total <- list()\n        for(fn in names(chunks[[\"gi\"]])){\n            chunk <- chunks[[\"gi\"]][[fn]]\n            if(length(chunk)){\n                total[[fn]] <- chunks[[\"total\"]][[fn]]\n                out[[fn]] <-\n                    subsetByOverlaps(chunk, ranges = ranges, use.region=\"both\")\n            }\n            rm(chunk)\n        }\n        ## split by group\n        f <- sub(\"_REP\\\\\\\\d+\", \"\", names(out))\n        out <- split(out, f)\n        giRbind <- function(a, b){\n            GInteractions(anchor1 = c(first(a), first(b)),\n                        anchor2 = c(second(a), second(b)),\n                        regions=sort(unique(c(first(a), first(b), second(a), second(b)))))\n        }\n        out <- lapply(out, function(.ele){\n            Reduce(giRbind, .ele)\n        })\n        total <- split(unlist(total), f)\n        total <- sapply(total, sum)\n        total <- median(total)/total\n        return(list(gi=out, norm_factor=total))\n    }\n\n    for(i in seq_along(gr1)){\n        if(i < maxEvent){\n            try_res <- try({\n                gr <- gr1[i]\n                start(gr) <- start(gr) - 2* resolution\n                end(gr) <- end(gr) + 2* resolution\n                seqlevelsStyle(gr) <- seqlevelsStyle(txdb)[1]\n                seqlengths(gr) <- seql[seqlevels(gr)]\n                gr <- GenomicRanges::trim(gr)\n                chr_gr <- as(seqinfo(gr), \"GRanges\")[seqnames(gr)[1]]\n                bait <- subsetByOverlaps(grs, gr, use.region=\"both\")\n                ## merge the bait by ends\n                bait <- split(second(bait), as.character(first(bait)))\n                bait <- lapply(bait, range)\n                bait <- unlist(GRangesList(bait))\n                bait <- GInteractions(anchor1 = parse2GRanges(names(bait)),\n                                    anchor2 = unname(bait))\n                ## get the v4c\n                info <- readPairFile(pairfiles,\n                                    ranges = c(first(bait), second(bait)),\n                                    resolution=resolution)\n                h5closeAll()\n                total <- info\\$norm_factor\n                names(cools) <- sub(\"\\\\\\\\d+\\\\\\\\.mcool\\$\", \"\", basename(cools))\n                gis <- lapply(cools, importGInteractions,\n                            resolution = resolution,\n                            format=\"cool\",\n                            ranges=gr, out = \"GInteractions\")\n                gis <- mapply(gis, total[names(gis)], FUN=function(.ele, .total){\n                    .ele\\$score <- log2(.ele\\$score+1) * .total\n                    .ele\n                })\n                maxV <- max(sapply(gis, function(.ele) max(.ele\\$score, na.rm=TRUE)))\n                maxV <- prettyMax(maxV)\n                heat <- lapply(gis, function(.ele){\n                    .ele <- gi2track(.ele)\n                    setTrackStyleParam(.ele, \"breaks\", seq(from=0, to=maxV, by=maxV/10))\n                    setTrackStyleParam(.ele, \"color\", c(\"lightblue\", \"yellow\", \"red\"))\n                    #setTrackStyleParam(.ele, \"ylim\", c(0, .5))\n                    .ele\n                })\n                names(heat) <- paste(\"valid pairs\", names(heat))\n                get_v4c <- function(bait1, bait2){\n                    v4c <- lapply(seq_along(bait1), function(.e){\n                        vp1 <- mapply(info\\$gi, total[names(info\\$gi)], FUN=function(.ele, .total){\n                            .ele <- subsetByOverlaps(.ele, bait1[.e], use.region=\"both\")\n                            .ele <- GRanges(coverage(c(first(.ele), second(.ele))))\n                            .ele\\$score <- .ele\\$score*.total\n                            new(\"track\", dat=.ele,\n                                type=\"data\", format = \"BED\",\n                                name = paste0(seqnames(bait1[.e]), \":\",\n                                            start(bait1[.e]), \"-\",\n                                            end(bait1[.e])))\n                        })\n                        maxY <- sapply(vp1, FUN = function(.ele){\n                            max(subsetByOverlaps(.ele\\$dat, bait2[.e])\\$score)\n                        })\n                        maxY <- prettyMax(max(maxY))\n                        vp1 <- lapply(vp1, function(.ele){\n                            setTrackStyleParam(.ele, \"ylim\", c(0, maxY))\n                            setTrackStyleParam(.ele, \"color\", \"gray80\")\n                            .ele\n                        })\n                    })\n                    names(v4c) <- paste0(seqnames(bait1), \":\",\n                                        start(bait1), \"-\",\n                                        end(bait1))\n                    v4c <- unlist(v4c, recursive = FALSE)\n                }\n                v4c_first <- get_v4c(first(bait), second(bait))\n                v4c_second <- get_v4c(second(bait), first(bait))\n                ids <- getGeneIDsFromTxDb(gr, txdb)\n                if(length(ids)){\n                    genes <- geneTrack(ids, txdb, map[ids], asList=FALSE)\n                    tL <- trackList(genes, link, heat, v4c_first, v4c_second,\n                                    heightDist = c(1, 1, 2*length(heat),\n                                    2*length(v4c_first), 2*length(v4c_second)))\n                    names(tL)[2] <- \"called links\"\n                }else{\n                    tL <- trackList(link, heat, v4c_first, v4c_second,\n                                    heightDist = c(1, 2*length(heat),\n                                    2*length(v4c_first), 2*length(v4c_second)))\n                    names(tL)[1] <- \"called links\"\n                }\n                pdf(file.path(output, paste0(\"event_\", i, \"_\", seqnames(gr), \":\", start(gr), \"-\", end(gr), \".pdf\")),\n                    width = 9, height = length(tL))\n                viewTracks(tL, gr=gr, autoOptimizeStyle = TRUE)\n                dev.off()\n            })\n            if(inherits(try_res, \"try-error\")){\n                message(try_res)\n            }\n        }\n    }\n    \"\"\"",
        "nb_lignes_script": 309,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "bin_size",
            "events",
            "mcools",
            "raw_pairs",
            "gtf",
            "chrom_sizes",
            "restrict"
        ],
        "nb_inputs": 7,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$bin_size\"",
            "label 'process_high'",
            "label 'process_long'",
            "label 'error_ignore'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' : 'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "PAIR2BAM": {
        "name_process": "PAIR2BAM",
        "string_process": "process PAIR2BAM {\n    tag \"$meta.id\"\n    label 'process_high'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(peak), path(pairs)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), path(\"*.bam.bai\")    , emit: bam\n    path \"versions.yml\"                                  , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on Oct. 2021 convert pairs.gz to bam file for visualization\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"Rsamtools\", \"InteractionSet\", \"rhdf5\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    peaks <- \"$peak\"\n    pairs <- dir(\".\", \"h5\\$\")\n\n    ## load header\n    getHeader <- function(file){\n        header <- h5read(file, \"header/header\")\n        header <- header[grepl(\"#samheader: @SQ\", header)]\n        header <- sub(\"#samheader: \", \"\", header)\n    }\n    ## loading data\n    getPath <- function(root, ...){\n        paste(root, ..., sep=\"/\")\n    }\n    createReadsName <- function(ids, width=6, prefix=\"r\"){\n        paste0(prefix, formatC(ids, width=width, flag=\"0\"))\n    }\n    filterByPeak <- function(pos, strand, chr, peaks){\n        gi <- GInteractions(anchor1=GRanges(chr, IRanges(pos[, 1], width=150)),\n                            anchor2=GRanges(chr, IRanges(pos[, 2], width=150)))\n        ol <- findOverlaps(gi, peaks)\n        keep <- sort(unique(queryHits(ol)))\n        list(pos=pos[keep, , drop=FALSE], strand=strand[keep, , drop=FALSE])\n    }\n    createAlginment <- function(file, p, idx, width, peaks){\n        chr_ <- strsplit(p, \"/\")[[1]]\n        chr_id <- which(chr_==\"data\")[1]\n        chr1 <- chr_[chr_id+1]\n        chr2 <- chr_[chr_id+2]\n        if(chr1!=chr2){\n            return(NULL)\n        }\n        pos <- h5read(file, getPath(p, \"position\"))\n        strand <- h5read(file, getPath(p, \"strand\"))\n        fil <- filterByPeak(pos, strand, chr1, peaks)\n        pos <- fil[[\"pos\"]]\n        strand <- fil[[\"strand\"]]\n        if(nrow(pos)){\n            name <- createReadsName(idx+seq.int(nrow(pos)), width=width)\n            flag <- ifelse(strand[, 1]==\"-\", 16, 0)\n            posL <- rowMins(pos)\n            isize <- abs(pos[, 1] - pos[, 2])\n            cigar <- paste0(\"100M\", isize, \"N100M\")\n            aln <- paste(name, flag, chr1, posL, \"50\", cigar, \"*\", 0, isize+201, \"*\", \"*\", sep = \"\\\\t\")\n        }else{\n            return(NULL)\n        }\n    }\n    exportBamFile <- function(file, peaks){\n        con <- sub(\".h5\\$\", \"\", file)\n        sam_path <- sub(\"h5\\$\", \"sam\", file, ignore.case = TRUE)\n        if(sam_path==con){\n            sam_path <- paste0(con, \".sam\")\n        }\n        sam_con <- file(sam_path, \"w\")\n        on.exit(close(sam_con))\n        ## write header\n        header <- getHeader(file)\n        writeLines(header, sam_con)\n        ## write data\n        total <- h5read(file, \"header/total\")\n        total_n <- nchar(total)\n        h5content <- h5ls(file)\n        h5content <- h5content[, \"group\"]\n        h5content <- h5content[grepl(\"data.*\\\\\\\\d+_\\\\\\\\d+\", h5content)]\n        idx <- 0\n        for(p in h5content){\n            aln <- createAlginment(file, p, idx, total_n, peaks)\n            if(length(aln)){\n                writeLines(aln, sam_con)\n                idx <- idx + length(aln)\n            }\n        }\n        close(sam_con)\n        h5closeAll()\n        on.exit()\n        si <- do.call(rbind, strsplit(header, \"\\\\\\\\t\"))\n        si <- as.numeric(sub(\"LN:\", \"\", si[, 3]))\n        si <- si[!is.na(si)]\n        if(length(si)){\n            si <- any(si>536870912)\n        }else{\n            si <- TRUE\n        }\n        if(si){\n            bam <- asBam(sam_path, con,\n                        overwrite = TRUE, indexDestination = FALSE)\n        }else{\n            bam <- asBam(sam_path, con,\n                        overwrite = TRUE, indexDestination = TRUE)\n        }\n        unlink(sam_path)\n        invisible(bam)\n    }\n\n    peaks <- read.csv(peaks)\n    peaks <- with(peaks, GInteractions(GRanges(chr1, IRanges(start1, end1)),\n                                        GRanges(chr2, IRanges(start2, end2))))\n    # output\n    null <- lapply(pairs, exportBamFile, peaks=peaks)\n    \"\"\"\n}",
        "nb_lignes_process": 138,
        "string_script": "    def prefix   = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on Oct. 2021 convert pairs.gz to bam file for visualization\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"Rsamtools\", \"InteractionSet\", \"rhdf5\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    peaks <- \"$peak\"\n    pairs <- dir(\".\", \"h5\\$\")\n\n    ## load header\n    getHeader <- function(file){\n        header <- h5read(file, \"header/header\")\n        header <- header[grepl(\"#samheader: @SQ\", header)]\n        header <- sub(\"#samheader: \", \"\", header)\n    }\n    ## loading data\n    getPath <- function(root, ...){\n        paste(root, ..., sep=\"/\")\n    }\n    createReadsName <- function(ids, width=6, prefix=\"r\"){\n        paste0(prefix, formatC(ids, width=width, flag=\"0\"))\n    }\n    filterByPeak <- function(pos, strand, chr, peaks){\n        gi <- GInteractions(anchor1=GRanges(chr, IRanges(pos[, 1], width=150)),\n                            anchor2=GRanges(chr, IRanges(pos[, 2], width=150)))\n        ol <- findOverlaps(gi, peaks)\n        keep <- sort(unique(queryHits(ol)))\n        list(pos=pos[keep, , drop=FALSE], strand=strand[keep, , drop=FALSE])\n    }\n    createAlginment <- function(file, p, idx, width, peaks){\n        chr_ <- strsplit(p, \"/\")[[1]]\n        chr_id <- which(chr_==\"data\")[1]\n        chr1 <- chr_[chr_id+1]\n        chr2 <- chr_[chr_id+2]\n        if(chr1!=chr2){\n            return(NULL)\n        }\n        pos <- h5read(file, getPath(p, \"position\"))\n        strand <- h5read(file, getPath(p, \"strand\"))\n        fil <- filterByPeak(pos, strand, chr1, peaks)\n        pos <- fil[[\"pos\"]]\n        strand <- fil[[\"strand\"]]\n        if(nrow(pos)){\n            name <- createReadsName(idx+seq.int(nrow(pos)), width=width)\n            flag <- ifelse(strand[, 1]==\"-\", 16, 0)\n            posL <- rowMins(pos)\n            isize <- abs(pos[, 1] - pos[, 2])\n            cigar <- paste0(\"100M\", isize, \"N100M\")\n            aln <- paste(name, flag, chr1, posL, \"50\", cigar, \"*\", 0, isize+201, \"*\", \"*\", sep = \"\\\\t\")\n        }else{\n            return(NULL)\n        }\n    }\n    exportBamFile <- function(file, peaks){\n        con <- sub(\".h5\\$\", \"\", file)\n        sam_path <- sub(\"h5\\$\", \"sam\", file, ignore.case = TRUE)\n        if(sam_path==con){\n            sam_path <- paste0(con, \".sam\")\n        }\n        sam_con <- file(sam_path, \"w\")\n        on.exit(close(sam_con))\n        ## write header\n        header <- getHeader(file)\n        writeLines(header, sam_con)\n        ## write data\n        total <- h5read(file, \"header/total\")\n        total_n <- nchar(total)\n        h5content <- h5ls(file)\n        h5content <- h5content[, \"group\"]\n        h5content <- h5content[grepl(\"data.*\\\\\\\\d+_\\\\\\\\d+\", h5content)]\n        idx <- 0\n        for(p in h5content){\n            aln <- createAlginment(file, p, idx, total_n, peaks)\n            if(length(aln)){\n                writeLines(aln, sam_con)\n                idx <- idx + length(aln)\n            }\n        }\n        close(sam_con)\n        h5closeAll()\n        on.exit()\n        si <- do.call(rbind, strsplit(header, \"\\\\\\\\t\"))\n        si <- as.numeric(sub(\"LN:\", \"\", si[, 3]))\n        si <- si[!is.na(si)]\n        if(length(si)){\n            si <- any(si>536870912)\n        }else{\n            si <- TRUE\n        }\n        if(si){\n            bam <- asBam(sam_path, con,\n                        overwrite = TRUE, indexDestination = FALSE)\n        }else{\n            bam <- asBam(sam_path, con,\n                        overwrite = TRUE, indexDestination = TRUE)\n        }\n        unlink(sam_path)\n        invisible(bam)\n    }\n\n    peaks <- read.csv(peaks)\n    peaks <- with(peaks, GInteractions(GRanges(chr1, IRanges(start1, end1)),\n                                        GRanges(chr2, IRanges(start2, end2))))\n    # output\n    null <- lapply(pairs, exportBamFile, peaks=peaks)\n    \"\"\"",
        "nb_lignes_script": 119,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "peak",
            "pairs"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "label 'error_ignore'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' : 'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MAPS_FEND": {
        "name_process": "MAPS_FEND",
        "string_process": "process MAPS_FEND {\n    tag \"$bin_size\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    tuple val(bin_size), path(cut)\n    path chrom_sizes\n\n    output:\n    tuple val(bin_size), path(\"*.bed\")      , emit: bed\n    path \"versions.yml\"                     , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    awk -vOFS=\"\\t\" '{print \\$3,\\$4,\\$4,\\$3\"_\"\\$1,\"0\",\\$2}' $cut | \\\\\n        bedtools slop $args \\\\\n            -r $bin_size -g $chrom_sizes > \\\\\n            ${cut}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(echo \\$(bedtools --version) | sed -e \"s/bedtools v//g\")\n        MAPS: 1.1.0\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    awk -vOFS=\"\\t\" '{print \\$3,\\$4,\\$4,\\$3\"_\"\\$1,\"0\",\\$2}' $cut | \\\\\n        bedtools slop $args \\\\\n            -r $bin_size -g $chrom_sizes > \\\\\n            ${cut}.bed\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bedtools: \\$(echo \\$(bedtools --version) | sed -e \"s/bedtools v//g\")\n        MAPS: 1.1.0\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "BEDTools"
        ],
        "tools_url": [
            "https://bio.tools/bedtools"
        ],
        "tools_dico": [
            {
                "name": "BEDTools",
                "uri": "https://bio.tools/bedtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Cartography"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "BEDTools is an extensive suite of utilities for comparing genomic features in BED format.",
                "homepage": "https://github.com/arq5x/bedtools2"
            }
        ],
        "inputs": [
            "bin_size",
            "cut",
            "chrom_sizes"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$bin_size\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' : 'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "READS_STAT": {
        "name_process": "READS_STAT",
        "string_process": "process READS_STAT {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"r::r-magrittr=1.5\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/r-magrittr:1.5--r3.2.2_0' :\n        'quay.io/biocontainers/r-magrittr:1.5--r3.2.2_0' }\"\n\n    input:\n    tuple val(meta), path(raw), path(dedup)\n\n    output:\n    tuple val(meta), path(\"*.csv\"), emit: stat\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    ## generate the statistis for each samples\n    versions <- c(\"${task.process}:\",\n        paste0(\"    R:\", paste(R.version\\$major, R.version\\$minor, sep=\".\")))\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    raw = \"$raw\"\n    dedup = \"$dedup\"\n    out = \"${prefix}.reads_stats.csv\"\n\n    sample_name <- sub(\".raw.pairsam.stat\", \"\", basename(raw))\n    getDat <- function(f){\n        dat <- read.delim(f, header=FALSE)\n        res <- dat[, 2]\n        names(res) <- dat[, 1]\n        return(res)\n    }\n    all_pairs <- getDat(raw)\n    dep_pairs <- getDat(dedup)\n\n    df <- data.frame(sample=sample_name,\n            total=all_pairs[\"total\"],\n            duplicate=dep_pairs['total_dups'],\n            non_duplicated=dep_pairs['total_nodups'],\n            duplication_rate=round(100*dep_pairs['total_dups']/all_pairs[\"total\"],2),\n            trans=dep_pairs['trans'],\n            cis=dep_pairs['cis'],\n            longRange=dep_pairs['cis_20kb+'])\n    write.csv(df, out, row.names=FALSE)\n    \"\"\"\n}",
        "nb_lignes_process": 49,
        "string_script": "    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    ## generate the statistis for each samples\n    versions <- c(\"${task.process}:\",\n        paste0(\"    R:\", paste(R.version\\$major, R.version\\$minor, sep=\".\")))\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    raw = \"$raw\"\n    dedup = \"$dedup\"\n    out = \"${prefix}.reads_stats.csv\"\n\n    sample_name <- sub(\".raw.pairsam.stat\", \"\", basename(raw))\n    getDat <- function(f){\n        dat <- read.delim(f, header=FALSE)\n        res <- dat[, 2]\n        names(res) <- dat[, 1]\n        return(res)\n    }\n    all_pairs <- getDat(raw)\n    dep_pairs <- getDat(dedup)\n\n    df <- data.frame(sample=sample_name,\n            total=all_pairs[\"total\"],\n            duplicate=dep_pairs['total_dups'],\n            non_duplicated=dep_pairs['total_nodups'],\n            duplication_rate=round(100*dep_pairs['total_dups']/all_pairs[\"total\"],2),\n            trans=dep_pairs['trans'],\n            cis=dep_pairs['cis'],\n            longRange=dep_pairs['cis_20kb+'])\n    write.csv(df, out, row.names=FALSE)\n    \"\"\"",
        "nb_lignes_script": 31,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "raw",
            "dedup"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"r::r-magrittr=1.5\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/r-magrittr:1.5--r3.2.2_0' : 'quay.io/biocontainers/r-magrittr:1.5--r3.2.2_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "UCSC_BIGWIGAVERAGEOVERBED": {
        "name_process": "UCSC_BIGWIGAVERAGEOVERBED",
        "string_process": "\nprocess UCSC_BIGWIGAVERAGEOVERBED {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bigwigaverageoverbed=377\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ucsc-bigwigaverageoverbed:377--h0b8a92a_2' :\n        'quay.io/biocontainers/ucsc-bigwigaverageoverbed:377--h0b8a92a_2' }\"\n\n    input:\n    tuple val(meta), path(bed)\n    path bigwig\n\n    output:\n    tuple val(meta), path(\"*.tab\"), emit: tab\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n                                                                      \n    \"\"\"\n    bigWigAverageOverBed \\\\\n        $args \\\\\n        $bigwig \\\\\n        $bed \\\\\n        ${prefix}.tab\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 36,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n                                                                      \n    \"\"\"\n    bigWigAverageOverBed \\\\\n        $args \\\\\n        $bigwig \\\\\n        $bed \\\\\n        ${prefix}.tab\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "bed",
            "bigwig"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::ucsc-bigwigaverageoverbed=377\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/ucsc-bigwigaverageoverbed:377--h0b8a92a_2' : 'quay.io/biocontainers/ucsc-bigwigaverageoverbed:377--h0b8a92a_2' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "SAMTOOLS_STATS": {
        "name_process": "SAMTOOLS_STATS",
        "string_process": "process SAMTOOLS_STATS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(input), path(input_index)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.stats\"), emit: stats\n    path  \"versions.yml\"            , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        stats \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${input} \\\\\n        > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 35,
        "string_script": "    def args = task.ext.args ?: ''\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    \"\"\"\n    samtools \\\\\n        stats \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${input} \\\\\n        > ${input}.stats\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [
            "SAMtools",
            "GOstats"
        ],
        "tools_url": [
            "https://bio.tools/samtools",
            "https://bio.tools/gostats"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            },
            {
                "name": "GOstats",
                "uri": "https://bio.tools/gostats",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0089",
                            "term": "Ontology and terminology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2436",
                                    "term": "Gene-set enrichment analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2436",
                                    "term": "GSEA"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2436",
                                    "term": "Functional enrichment analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2436",
                                    "term": "Gene-set over-represenation analysis"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2603",
                                "term": "Expression data"
                            },
                            {
                                "uri": "http://edamontology.org/data_0582",
                                "term": "Ontology"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2884",
                                "term": "Plot"
                            },
                            {
                                "uri": "http://edamontology.org/data_2048",
                                "term": "Report"
                            },
                            {
                                "uri": "http://edamontology.org/data_1772",
                                "term": "Score"
                            }
                        ]
                    }
                ],
                "description": "A set of tools for interacting with GO and microarray data. A variety of basic manipulation tools for graphs, hypothesis testing and other simple calculations.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/GOstats.html"
            }
        ],
        "inputs": [
            "meta",
            "input",
            "input_index",
            "fasta"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' : 'quay.io/biocontainers/samtools:1.15--h1170115_1' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "DUMP_READS": {
        "name_process": "DUMP_READS",
        "string_process": "process DUMP_READS {\n    tag \"${meta.id}\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"anaconda::gawk=5.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gawk:5.1.0' :\n        'quay.io/biocontainers/gawk:5.1.0' }\"\n\n    input:\n    tuple val(meta), path(bed)\n\n    output:\n    tuple val(meta), path(\"*.shrt.vip.bed\") , emit: peak\n    path \"versions.yml\"                     , emit: versions\n\n    script:\n    def software = \"awk\"\n    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    \"\"\"\n    gunzip -c $bed | \\\\\n        awk -F \"\\t\" 'BEGIN { OFS=FS } {print \\$1,\\$2,\\$3 > \"${prefix}.\"\\$1\".shrt.vip.bed\"}'\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        awk: \\$(echo \\$(awk --version 2>&1) | sed -e \"s/GNU Awk //g; s/, API.*\\$//\")\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "    def software = \"awk\"\n    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    \"\"\"\n    gunzip -c $bed | \\\\\n        awk -F \"\\t\" 'BEGIN { OFS=FS } {print \\$1,\\$2,\\$3 > \"${prefix}.\"\\$1\".shrt.vip.bed\"}'\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        awk: \\$(echo \\$(awk --version 2>&1) | sed -e \"s/GNU Awk //g; s/, API.*\\$//\")\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "bed"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"${meta.id}\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"anaconda::gawk=5.1.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/gawk:5.1.0' : 'quay.io/biocontainers/gawk:5.1.0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "R1READS": {
        "name_process": "R1READS",
        "string_process": "process R1READS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"anaconda::gawk=5.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gawk:5.1.0' :\n        'quay.io/biocontainers/gawk:5.1.0' }\"\n\n    input:\n    tuple val(meta), path(pair)\n\n    output:\n    tuple val(meta), path(\"*.bed.gz\"), emit: bed\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    \"\"\"\n    gunzip -c $pair | \\\\\n        awk 'BEGIN {OFS=\"\\t\"};  /^[^#]/ { print \\$2, \\$3, \\$3+1, \"*\", \"*\", \\$6}' | \\\\\n        sort -k1,1 -k2,2n | \\\\\n        gzip -nc > ${prefix}.R1.distal.bed.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        awk: \\$(echo \\$(awk --version 2>&1 || awk -W version 2>&1) | sed 's/[[:alpha:]|(|)|[:space:]]//g; s/,.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    \"\"\"\n    gunzip -c $pair | \\\\\n        awk 'BEGIN {OFS=\"\\t\"};  /^[^#]/ { print \\$2, \\$3, \\$3+1, \"*\", \"*\", \\$6}' | \\\\\n        sort -k1,1 -k2,2n | \\\\\n        gzip -nc > ${prefix}.R1.distal.bed.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        awk: \\$(echo \\$(awk --version 2>&1 || awk -W version 2>&1) | sed 's/[[:alpha:]|(|)|[:space:]]//g; s/,.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "pair"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"anaconda::gawk=5.1.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/gawk:5.1.0' : 'quay.io/biocontainers/gawk:5.1.0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MAPS_REFORMAT": {
        "name_process": "MAPS_REFORMAT",
        "string_process": "process MAPS_REFORMAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"conda-forge::r-data.table=1.12.2\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/r-data.table:1.12.2' :\n        'quay.io/biocontainers/r-data.table:1.12.2' }\"\n\n    input:\n    tuple val(meta), val(bin_size), path(peak)\n\n    output:\n    tuple val(meta), val(bin_size), path(\"*.sig3Dinteractions.bedpe\"), emit: bedpe\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #########################################\n    # Author: [Ivan Juric](https://github.com/ijuric)\n    # File: MAPS_peak_formatting.r\n    # Source: https://github.com/ijuric/MAPS/blob/master/bin/MAPS/MAPS_peak_formatting.r\n    # Source+commit: https://raw.githubusercontent.com/ijuric/MAPS/46f92a6c7965a3b855ba7558c50d6c021b1d677c/bin/MAPS/MAPS_peak_formatting.r\n    # Data: 11/08/2021, commit:46f92a6\n    # modified by Jianhong:\n    # ## 1. set the Rscript environment.\n    # ## 2. simplify the input and output parameters by input filename and output filename\n    # ## 3. handle multiple input files\n    # ## 4. handle the empty input file\n    #########################################\n    versions <- c(\"${task.process}:\", \"    MAPS: 1.1.0\")\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    options(\"scipen\"=999)\n    library(data.table)\n    RESOLUTION = as.numeric($bin_size)\n    infs = strsplit(\"${peak}\", \"\\\\\\\\s+\")[[1]]\n\n    for(inf in infs){\n        peaks_raw = read.table(inf,header=TRUE, stringsAsFactors=FALSE)\n\n        peaks = as.data.table(subset(peaks_raw, ClusterType != 'Singleton' | (ClusterType == 'Singleton' & fdr < 1e-4))) # remove singletons\n        if(nrow(peaks)==0){\n            peaks\\$bin1_end <- peaks\\$bin2_end <- peaks\\$summit <- numeric(0)\n        }else{\n            peaks[, summit := 1*(fdr == min(fdr)), by = lab]\n            peaks\\$summit[ peaks\\$ClusterType == 'Singleton'] = 1\n\n            singleton_labs = peaks\\$lab[ peaks\\$ClusterType == 'Singleton']\n            peaks\\$lab[ peaks\\$ClusterType == 'Singleton'] = paste(singleton_labs, 1:length(singleton_labs),sep='')\n\n            peaks\\$bin1_end = peaks\\$bin1_mid + RESOLUTION\n            peaks\\$bin2_end = peaks\\$bin2_mid + RESOLUTION\n        }\n        peaks_final = subset(peaks, select = c(\"chr\", \"bin1_mid\", \"bin1_end\", \"chr\", \"bin2_mid\", \"bin2_end\", \"count\", \"expected2\", \"fdr\", \"lab\", \"ClusterSize\", \"ClusterType\", \"NegLog10P\", \"summit\"))\n        colnames(peaks_final) = c('chr1', 'start1', 'end1', 'chr2', 'start2', 'end2', 'count', 'expected', 'fdr', 'ClusterLabel', 'ClusterSize', 'ClusterType', 'ClusterNegLog10P', 'ClusterSummit')\n        fout = sub(\".peaks\",'.sig3Dinteractions.bedpe',inf)\n        write.table(peaks_final, fout, row.names = FALSE, col.names = TRUE, quote=FALSE, sep='\\t')\n    }\n    \"\"\"\n}",
        "nb_lignes_process": 62,
        "string_script": "    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #########################################\n    # Author: [Ivan Juric](https://github.com/ijuric)\n    # File: MAPS_peak_formatting.r\n    # Source: https://github.com/ijuric/MAPS/blob/master/bin/MAPS/MAPS_peak_formatting.r\n    # Source+commit: https://raw.githubusercontent.com/ijuric/MAPS/46f92a6c7965a3b855ba7558c50d6c021b1d677c/bin/MAPS/MAPS_peak_formatting.r\n    # Data: 11/08/2021, commit:46f92a6\n    # modified by Jianhong:\n    # ## 1. set the Rscript environment.\n    # ## 2. simplify the input and output parameters by input filename and output filename\n    # ## 3. handle multiple input files\n    # ## 4. handle the empty input file\n    #########################################\n    versions <- c(\"${task.process}:\", \"    MAPS: 1.1.0\")\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    options(\"scipen\"=999)\n    library(data.table)\n    RESOLUTION = as.numeric($bin_size)\n    infs = strsplit(\"${peak}\", \"\\\\\\\\s+\")[[1]]\n\n    for(inf in infs){\n        peaks_raw = read.table(inf,header=TRUE, stringsAsFactors=FALSE)\n\n        peaks = as.data.table(subset(peaks_raw, ClusterType != 'Singleton' | (ClusterType == 'Singleton' & fdr < 1e-4))) # remove singletons\n        if(nrow(peaks)==0){\n            peaks\\$bin1_end <- peaks\\$bin2_end <- peaks\\$summit <- numeric(0)\n        }else{\n            peaks[, summit := 1*(fdr == min(fdr)), by = lab]\n            peaks\\$summit[ peaks\\$ClusterType == 'Singleton'] = 1\n\n            singleton_labs = peaks\\$lab[ peaks\\$ClusterType == 'Singleton']\n            peaks\\$lab[ peaks\\$ClusterType == 'Singleton'] = paste(singleton_labs, 1:length(singleton_labs),sep='')\n\n            peaks\\$bin1_end = peaks\\$bin1_mid + RESOLUTION\n            peaks\\$bin2_end = peaks\\$bin2_mid + RESOLUTION\n        }\n        peaks_final = subset(peaks, select = c(\"chr\", \"bin1_mid\", \"bin1_end\", \"chr\", \"bin2_mid\", \"bin2_end\", \"count\", \"expected2\", \"fdr\", \"lab\", \"ClusterSize\", \"ClusterType\", \"NegLog10P\", \"summit\"))\n        colnames(peaks_final) = c('chr1', 'start1', 'end1', 'chr2', 'start2', 'end2', 'count', 'expected', 'fdr', 'ClusterLabel', 'ClusterSize', 'ClusterType', 'ClusterNegLog10P', 'ClusterSummit')\n        fout = sub(\".peaks\",'.sig3Dinteractions.bedpe',inf)\n        write.table(peaks_final, fout, row.names = FALSE, col.names = TRUE, quote=FALSE, sep='\\t')\n    }\n    \"\"\"",
        "nb_lignes_script": 44,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "bin_size",
            "peak"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"conda-forge::r-data.table=1.12.2\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/r-data.table:1.12.2' : 'quay.io/biocontainers/r-data.table:1.12.2' }\""
        ],
        "when": "",
        "stub": ""
    },
    "UCSC_BEDCLIP": {
        "name_process": "UCSC_BEDCLIP",
        "string_process": "\nprocess UCSC_BEDCLIP {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bedclip=377\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ucsc-bedclip:377--h0b8a92a_2' :\n        'quay.io/biocontainers/ucsc-bedclip:377--h0b8a92a_2' }\"\n\n    input:\n    tuple val(meta), path(bedgraph)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bedGraph\"), emit: bedgraph\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedClip \\\\\n        $bedgraph \\\\\n        $sizes \\\\\n        ${prefix}.bedGraph\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedClip \\\\\n        $bedgraph \\\\\n        $sizes \\\\\n        ${prefix}.bedGraph\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "bedgraph",
            "sizes"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::ucsc-bedclip=377\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/ucsc-bedclip:377--h0b8a92a_2' : 'quay.io/biocontainers/ucsc-bedclip:377--h0b8a92a_2' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "SAMTOOLS_FLAGSTAT": {
        "name_process": "SAMTOOLS_FLAGSTAT",
        "string_process": "process SAMTOOLS_FLAGSTAT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"*.flagstat\"), emit: flagstat\n    path  \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        flagstat \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        > ${bam}.flagstat\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    samtools \\\\\n        flagstat \\\\\n        --threads ${task.cpus-1} \\\\\n        $bam \\\\\n        > ${bam}.flagstat\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "meta",
            "bam",
            "bai"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' : 'quay.io/biocontainers/samtools:1.15--h1170115_1' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "BIOC_ENRICH": {
        "name_process": "BIOC_ENRICH",
        "string_process": "process BIOC_ENRICH {\n    tag \"$bin_size\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-clusterprofiler=3.18.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-clusterprofiler:3.18.1--r40hdfd78af_0' :\n        'quay.io/biocontainers/bioconductor-clusterprofiler:3.18.1--r40hdfd78af_0' }\"\n\n    input:\n    tuple val(bin_size), path(diff)\n    val ucscname\n\n    output:\n    tuple val(bin_size), path(\"${prefix}/enrichment/*\"), emit: enrichment\n    path \"versions.yml\"                                , emit: versions\n\n    script:\n    prefix   = task.ext.prefix ?: \"diffhic_bin${bin_size}\"\n    \"\"\"\n    enrich.r -s ${ucscname} -o \"${prefix}/enrichment\" $options.args\n\n    # *.version.txt files will be created in the rscripts\n    echo \"${task.process}:\" > versions.yml\n    for i in \\$(ls *.version.txt); do\n    echo \"    \\${i%.version.txt}: \\$(<\\$i)\" >> versions.yml\n    done\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "    prefix   = task.ext.prefix ?: \"diffhic_bin${bin_size}\"\n    \"\"\"\n    enrich.r -s ${ucscname} -o \"${prefix}/enrichment\" $options.args\n\n    # *.version.txt files will be created in the rscripts\n    echo \"${task.process}:\" > versions.yml\n    for i in \\$(ls *.version.txt); do\n    echo \"    \\${i%.version.txt}: \\$(<\\$i)\" >> versions.yml\n    done\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "bin_size",
            "diff",
            "ucscname"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$bin_size\"",
            "label 'process_medium'",
            "label 'error_ignore'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-clusterprofiler=3.18.1\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-clusterprofiler:3.18.1--r40hdfd78af_0' : 'quay.io/biocontainers/bioconductor-clusterprofiler:3.18.1--r40hdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "CIRCOS": {
        "name_process": "CIRCOS",
        "string_process": "process CIRCOS {\n    tag \"$meta.id\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::circos=0.69.8\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/circos:0.69.8--hdfd78af_1' :\n        'quay.io/biocontainers/circos:0.69.8--hdfd78af_1' }\"\n\n    input:\n    tuple val(meta), path(data)\n    path configfile\n\n    output:\n    path \"*.png\"                  , emit: circos\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    \"\"\"\n    circos\n    mv circos.png ${meta.id}.png\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        circos: \\$(echo \\$(circos -v 2>&1) | sed 's/circos.*v //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "    \"\"\"\n    circos\n    mv circos.png ${meta.id}.png\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        circos: \\$(echo \\$(circos -v 2>&1) | sed 's/circos.*v //; s/ .*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "Circos"
        ],
        "tools_url": [
            "https://bio.tools/circos"
        ],
        "tools_dico": [
            {
                "name": "Circos",
                "uri": "https://bio.tools/circos",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0797",
                            "term": "Comparative genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0006",
                                "term": "Data"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2968",
                                "term": "Image"
                            },
                            {
                                "uri": "http://edamontology.org/data_0006",
                                "term": "Data"
                            }
                        ]
                    }
                ],
                "description": "Circos is tool for visualizing data in a circular format. It was developed for genomic data but can work for many other kinds of data as well.",
                "homepage": "http://circos.ca/"
            }
        ],
        "inputs": [
            "meta",
            "data",
            "configfile"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "label 'error_ignore'",
            "conda (params.enable_conda ? \"bioconda::circos=0.69.8\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/circos:0.69.8--hdfd78af_1' : 'quay.io/biocontainers/circos:0.69.8--hdfd78af_1' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MERGE_READS": {
        "name_process": "MERGE_READS",
        "string_process": "process MERGE_READS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2' :\n        'quay.io/biocontainers/samtools:1.10--h9402c20_2' }\"\n\n    input:\n    tuple val(meta), path(bed)\n\n    output:\n    tuple val(meta), path(\"*.bed.gz\"), emit: bed\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    gunzip -c ${bed} | \\\\\n        sort -k1,1 -k2,2n | \\\\\n        gzip -nc > ${prefix}.bed.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gzip: \\$(echo \\$(gzip --version 2>&1) | sed 's/[[:alpha:]|(|[:space:]]//g')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "    def prefix   = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    gunzip -c ${bed} | \\\\\n        sort -k1,1 -k2,2n | \\\\\n        gzip -nc > ${prefix}.bed.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gzip: \\$(echo \\$(gzip --version 2>&1) | sed 's/[[:alpha:]|(|[:space:]]//g')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "bed"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::samtools=1.10\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/samtools:1.10--h9402c20_2' : 'quay.io/biocontainers/samtools:1.10--h9402c20_2' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MULTIQC": {
        "name_process": "MULTIQC",
        "string_process": "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "MultiQC"
        ],
        "tools_url": [
            "https://bio.tools/multiqc"
        ],
        "tools_dico": [
            {
                "name": "MultiQC",
                "uri": "https://bio.tools/multiqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0091",
                            "term": "Bioinformatics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2428",
                                    "term": "Validation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2048",
                                "term": "Report"
                            }
                        ]
                    }
                ],
                "description": "MultiQC aggregates results from multiple bioinformatics analyses across many samples into a single report. It searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.",
                "homepage": "http://multiqc.info/"
            }
        ],
        "inputs": [
            "multiqc_files"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "label 'process_medium'",
            "conda (params.enable_conda ? 'bioconda::multiqc=1.12' : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/multiqc:1.12--pyhdfd78af_0' : 'quay.io/biocontainers/multiqc:1.12--pyhdfd78af_0' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "CALL_HIPEAK": {
        "name_process": "CALL_HIPEAK",
        "string_process": "process CALL_HIPEAK {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-monocle=2.20.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-monocle:2.20.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-monocle:2.20.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(counts)\n\n    output:\n    tuple val(meta), path(\"${meta.id}.peaks.csv\"), emit: peak\n    path \"versions.yml\"                          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on Aug. 24, 2021 call peaks\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"VGAM\", \"MASS\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    options(warn=-1)\n    ### Options\n    ## make_option(c(\"-m\", \"--regression_type\"), type=\"character\", default='pospoisson', help=\"pospoisson for positive poisson regression, negbinom for negative binomial. default is pospoisson\", metavar=\"string\"),\n    ## make_option(c(\"-t\", \"--count\"), type=\"character\", default=NULL, help=\"count table output by prepare count\", metavar=\"string\"),\n    ## make_option(c(\"-o\", \"--output\"), type=\"character\", default=\"peaks.csv\", help=\"output folder\", metavar=\"string\")\n\n    OUTPUT = \"${meta.id}.peaks.csv\"\n    REG_TYPE = 'pospoisson'\n    mm <- read.csv(\"$counts\")\n    if(grepl(\"-m\", \"$args\") || grepl(\"--regression_type\", \"$args\")){\n        args <- strsplit(\"$args\", \"\\\\\\\\s+\")[[1]]\n        id <- args==\"-m\" | args==\"--regression_type\"\n        id <- which(id)[1]\n        if(args[id+1] %in% c(\"negbinom\", \"pospoisson\")){\n            REG_TYPE <- args[id+1]\n        }\n    }\n\n    if(!all(c(\"chr1\", \"start1\", \"end1\", \"width1\",\n            \"chr2\", \"start2\", \"end2\", 'width2',\n            \"count\", \"logl\", \"logn\", \"loggc\", \"logm\", \"logdist\", 'logShortCount') %in% colnames(mm))){\n        stop(\"count table is not in correct format.\")\n    }\n\n    ## doing statistics and resampling\n    getFormula <- function(mm){\n        coln <- c(\"logl\", \"loggc\", \"logm\", \"logdist\", \"logShortCount\", \"logn\")\n        ln <- vapply(coln, FUN=function(.ele){\n            length(unique(mm[, .ele]))>1\n        }, FUN.VALUE=logical(1))\n        list(\"formula\"=paste(\"count ~\", paste(coln[ln], collapse=\"+\")),\n            \"factor\"=coln[ln])\n    }\n    loglikelihood <- function (mu, y, w, residuals = FALSE, eta, extra = NULL, summation = TRUE) {\n        lambda <- eta2theta(eta, \"loglink\", earg = list(bvalue = NULL, inverse = FALSE, deriv = 0, short = TRUE,\n            tag = FALSE))\n        if (residuals) {\n            stop(\"loglikelihood residuals not implemented yet\")\n        }\n        else {\n            if(exists('dgaitdpois', where = 'package:VGAM', mode='function')){\n                dgaitpois <- dgaitdpois\n            }\n            ll.elts <- c(w) * dgaitpois(y, lambda, truncate = 0,\n                log = TRUE)\n            if (summation) {\n                sum(ll.elts[!is.infinite(ll.elts)])\n            }\n            else {\n                ll.elts\n            }\n        }\n    }\n    pospoisson_regression <- function(mm) {\n        dataset_length<- nrow(mm)\n        formula <- getFormula(mm)\n        fac <- formula[[\"factor\"]]\n        formula <- formula[[\"formula\"]]\n        family <- pospoisson()\n        family@loglikelihood <- loglikelihood\n        fit <- vglm(formula=as.formula(formula), family = family, data = mm)\n        mm\\$expected = fitted(fit)\n        mm\\$p_val = ppois(mm\\$count, mm\\$expected, lower.tail = FALSE, log.p = FALSE) / ppois(0, mm\\$expected, lower.tail = FALSE, log.p = FALSE)\n        m1 = mm[ mm\\$p_val > 1/length(mm\\$p_val),]\n        fit <- vglm(formula=as.formula(formula), family = family, data = m1)\n        coeff<-round(coef(fit),10)\n        mm\\$expected2 <- round(exp(coeff[1] + rowSums(t(coeff[-1]*t(mm[, fac])))), 10)\n        mm\\$expected2 <- mm\\$expected2 /(1-exp(-mm\\$expected2))\n        mm\\$ratio2 <- mm\\$count / mm\\$expected2\n        mm\\$p_val_reg2 = ppois(mm\\$count, mm\\$expected2, lower.tail = FALSE, log.p = FALSE) / ppois(0, mm\\$expected2, lower.tail = FALSE, log.p = FALSE)\n        mm\\$p_bonferroni = mm\\$p_val_reg2 * dataset_length\n        mm\\$fdr <- p.adjust(mm\\$p_val_reg2, method='fdr')\n        return(mm)\n    }\n\n    negbinom_regression <- function(mm) {\n        formula <- getFormula(mm)\n        fac <- formula[[\"factor\"]]\n        formula <- formula[[\"formula\"]]\n        fit <- glm.nb(formula=as.formula(formula), data = mm)\n        mm\\$expected = fitted(fit)\n        sze = fit\\$theta ##size parameter\n        mm\\$p_val = pnbinom(mm\\$count, mu = mm\\$expected, size = sze, lower.tail = FALSE)\n        m1 = mm[ mm\\$p_val > ( 1 / length(mm\\$p_val)),]\n        ## second regression\n        fit <- glm.nb(formula=as.formula(formula), data = m1)\n        coeff<-round(fit\\$coefficients,10)\n        sze = fit\\$theta\n        mm\\$expected2 <- round(exp(coeff[1] + rowSums(t(coeff[-1]*t(mm[, fac])))), 10) ## mu parameter\n        mm\\$ratio2 <- mm\\$count / mm\\$expected2\n        mm\\$p_val_reg2 = pnbinom(mm\\$count, mu = mm\\$expected2, size = sze, lower.tail = FALSE)\n        mm\\$p_bonferroni = mm\\$p_val_reg2 * nrow(mm)\n        mm\\$fdr <- p.adjust(mm\\$p_val_reg2, method='fdr')\n        return(mm)\n    }\n\n    mm <- switch(REG_TYPE,\n                \"pospoisson\"=pospoisson_regression(mm),\n                \"negbinom\"=negbinom_regression(mm))\n\n    write.csv(mm, OUTPUT, row.names=FALSE)\n    \"\"\"\n}",
        "nb_lignes_process": 139,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on Aug. 24, 2021 call peaks\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"VGAM\", \"MASS\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    options(warn=-1)\n    ### Options\n    ## make_option(c(\"-m\", \"--regression_type\"), type=\"character\", default='pospoisson', help=\"pospoisson for positive poisson regression, negbinom for negative binomial. default is pospoisson\", metavar=\"string\"),\n    ## make_option(c(\"-t\", \"--count\"), type=\"character\", default=NULL, help=\"count table output by prepare count\", metavar=\"string\"),\n    ## make_option(c(\"-o\", \"--output\"), type=\"character\", default=\"peaks.csv\", help=\"output folder\", metavar=\"string\")\n\n    OUTPUT = \"${meta.id}.peaks.csv\"\n    REG_TYPE = 'pospoisson'\n    mm <- read.csv(\"$counts\")\n    if(grepl(\"-m\", \"$args\") || grepl(\"--regression_type\", \"$args\")){\n        args <- strsplit(\"$args\", \"\\\\\\\\s+\")[[1]]\n        id <- args==\"-m\" | args==\"--regression_type\"\n        id <- which(id)[1]\n        if(args[id+1] %in% c(\"negbinom\", \"pospoisson\")){\n            REG_TYPE <- args[id+1]\n        }\n    }\n\n    if(!all(c(\"chr1\", \"start1\", \"end1\", \"width1\",\n            \"chr2\", \"start2\", \"end2\", 'width2',\n            \"count\", \"logl\", \"logn\", \"loggc\", \"logm\", \"logdist\", 'logShortCount') %in% colnames(mm))){\n        stop(\"count table is not in correct format.\")\n    }\n\n    ## doing statistics and resampling\n    getFormula <- function(mm){\n        coln <- c(\"logl\", \"loggc\", \"logm\", \"logdist\", \"logShortCount\", \"logn\")\n        ln <- vapply(coln, FUN=function(.ele){\n            length(unique(mm[, .ele]))>1\n        }, FUN.VALUE=logical(1))\n        list(\"formula\"=paste(\"count ~\", paste(coln[ln], collapse=\"+\")),\n            \"factor\"=coln[ln])\n    }\n    loglikelihood <- function (mu, y, w, residuals = FALSE, eta, extra = NULL, summation = TRUE) {\n        lambda <- eta2theta(eta, \"loglink\", earg = list(bvalue = NULL, inverse = FALSE, deriv = 0, short = TRUE,\n            tag = FALSE))\n        if (residuals) {\n            stop(\"loglikelihood residuals not implemented yet\")\n        }\n        else {\n            if(exists('dgaitdpois', where = 'package:VGAM', mode='function')){\n                dgaitpois <- dgaitdpois\n            }\n            ll.elts <- c(w) * dgaitpois(y, lambda, truncate = 0,\n                log = TRUE)\n            if (summation) {\n                sum(ll.elts[!is.infinite(ll.elts)])\n            }\n            else {\n                ll.elts\n            }\n        }\n    }\n    pospoisson_regression <- function(mm) {\n        dataset_length<- nrow(mm)\n        formula <- getFormula(mm)\n        fac <- formula[[\"factor\"]]\n        formula <- formula[[\"formula\"]]\n        family <- pospoisson()\n        family@loglikelihood <- loglikelihood\n        fit <- vglm(formula=as.formula(formula), family = family, data = mm)\n        mm\\$expected = fitted(fit)\n        mm\\$p_val = ppois(mm\\$count, mm\\$expected, lower.tail = FALSE, log.p = FALSE) / ppois(0, mm\\$expected, lower.tail = FALSE, log.p = FALSE)\n        m1 = mm[ mm\\$p_val > 1/length(mm\\$p_val),]\n        fit <- vglm(formula=as.formula(formula), family = family, data = m1)\n        coeff<-round(coef(fit),10)\n        mm\\$expected2 <- round(exp(coeff[1] + rowSums(t(coeff[-1]*t(mm[, fac])))), 10)\n        mm\\$expected2 <- mm\\$expected2 /(1-exp(-mm\\$expected2))\n        mm\\$ratio2 <- mm\\$count / mm\\$expected2\n        mm\\$p_val_reg2 = ppois(mm\\$count, mm\\$expected2, lower.tail = FALSE, log.p = FALSE) / ppois(0, mm\\$expected2, lower.tail = FALSE, log.p = FALSE)\n        mm\\$p_bonferroni = mm\\$p_val_reg2 * dataset_length\n        mm\\$fdr <- p.adjust(mm\\$p_val_reg2, method='fdr')\n        return(mm)\n    }\n\n    negbinom_regression <- function(mm) {\n        formula <- getFormula(mm)\n        fac <- formula[[\"factor\"]]\n        formula <- formula[[\"formula\"]]\n        fit <- glm.nb(formula=as.formula(formula), data = mm)\n        mm\\$expected = fitted(fit)\n        sze = fit\\$theta ##size parameter\n        mm\\$p_val = pnbinom(mm\\$count, mu = mm\\$expected, size = sze, lower.tail = FALSE)\n        m1 = mm[ mm\\$p_val > ( 1 / length(mm\\$p_val)),]\n        ## second regression\n        fit <- glm.nb(formula=as.formula(formula), data = m1)\n        coeff<-round(fit\\$coefficients,10)\n        sze = fit\\$theta\n        mm\\$expected2 <- round(exp(coeff[1] + rowSums(t(coeff[-1]*t(mm[, fac])))), 10) ## mu parameter\n        mm\\$ratio2 <- mm\\$count / mm\\$expected2\n        mm\\$p_val_reg2 = pnbinom(mm\\$count, mu = mm\\$expected2, size = sze, lower.tail = FALSE)\n        mm\\$p_bonferroni = mm\\$p_val_reg2 * nrow(mm)\n        mm\\$fdr <- p.adjust(mm\\$p_val_reg2, method='fdr')\n        return(mm)\n    }\n\n    mm <- switch(REG_TYPE,\n                \"pospoisson\"=pospoisson_regression(mm),\n                \"negbinom\"=negbinom_regression(mm))\n\n    write.csv(mm, OUTPUT, row.names=FALSE)\n    \"\"\"",
        "nb_lignes_script": 121,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "counts"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-monocle=2.20.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-monocle:2.20.0--r41h399db7b_0' : 'quay.io/biocontainers/bioconductor-monocle:2.20.0--r41h399db7b_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "COOLER_DIGEST": {
        "name_process": "COOLER_DIGEST",
        "string_process": "process COOLER_DIGEST {\n    tag \"$fasta\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::cooler=0.8.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cooler:0.8.11--pyh3252c3a_0' :\n        'quay.io/biocontainers/cooler:0.8.11--pyh3252c3a_0' }\"\n\n    input:\n    path fasta\n    path chromsizes\n    val  enzyme\n\n    output:\n    path \"*.bed\"                  , emit: bed\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    cooler digest \\\\\n        $args \\\\\n        -o \"${fasta.baseName}_${enzyme.replaceAll(/[^0-9a-zA-Z]+/, '_')}.bed\" \\\\\n        $chromsizes \\\\\n        $fasta \\\\\n        $enzyme\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cooler: \\$(cooler --version 2>&1 | sed 's/cooler, version //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 35,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    cooler digest \\\\\n        $args \\\\\n        -o \"${fasta.baseName}_${enzyme.replaceAll(/[^0-9a-zA-Z]+/, '_')}.bed\" \\\\\n        $chromsizes \\\\\n        $fasta \\\\\n        $enzyme\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cooler: \\$(cooler --version 2>&1 | sed 's/cooler, version //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fasta",
            "chromsizes",
            "enzyme"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$fasta\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::cooler=0.8.11\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/cooler:0.8.11--pyh3252c3a_0' : 'quay.io/biocontainers/cooler:0.8.11--pyh3252c3a_0' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "MAPS_CALLPEAK": {
        "name_process": "MAPS_CALLPEAK",
        "string_process": "process MAPS_CALLPEAK {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-monocle=2.20.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-monocle:2.20.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-monocle:2.20.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), val(bin_size), path(macs2), path(long_bedpe, stageAs: \"long/*\"), path(short_bed, stageAs: \"short/*\"), path(background), path(maps, stageAs: \"maps_out/*\")\n\n    output:\n    tuple val(meta), val(bin_size), path(\"${meta.id}_${bin_size}/summary.*.txt\"), optional: true, emit: summary\n    tuple val(meta), val(bin_size), path(\"${meta.id}_${bin_size}/*.peaks\"), optional: true, emit: peak\n    tuple val(meta), val(bin_size), path(\"${meta.id}_${bin_size}/reg_raw.*.MAPS2_*\"), optional: true, emit: signal\n    path \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mv maps_out ${meta.id}_${bin_size}\n    ## arguments:\n    ## INFDIR - dir with reg files, output folder of maps/maps.nf::MAP_MAPS\n    ## SET - dataset name, output of maps/maps.nf::MAP_MAPS. It is the postfix of the file name of reg files, eg: reg_raw.chr22.WT.5k.xor, SET should be WT.5k.\n    ## RESOLUTION - resolution (for example 5000 or 10000). In this pipeline, it is the bin_size.\n    ## COUNT_CUTOFF - count cutoff, default 12\n    ## RATIO_CUTOFF - ratio cutoff, default 2.0\n    ## FDR - -log10(fdr) cutoff, default 2\n    ## FILTER - file containing bins that need to be filtered out. Format: two columns \"chrom\", \"bin\". \"chrom\" contains 'chr1','chr2',.. \"bin\" is bin label\n    ## regresison_type - pospoisson for positive poisson regression, negbinom for negative binomial. default is pospoisson\n    MAPS_regression_and_peak_caller.r \"${meta.id}_${bin_size}/\" ${meta.id} $bin_size $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        MAPS: 1.1.0\n    END_VERSIONS\n    for i in \\$(ls *.version.txt); do\n    echo \"    \\${i%.version.txt}: \\$(<\\$i)\" >> versions.yml\n    done\n    \"\"\"\n}",
        "nb_lignes_process": 41,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    mv maps_out ${meta.id}_${bin_size}\n    ## arguments:\n    ## INFDIR - dir with reg files, output folder of maps/maps.nf::MAP_MAPS\n    ## SET - dataset name, output of maps/maps.nf::MAP_MAPS. It is the postfix of the file name of reg files, eg: reg_raw.chr22.WT.5k.xor, SET should be WT.5k.\n    ## RESOLUTION - resolution (for example 5000 or 10000). In this pipeline, it is the bin_size.\n    ## COUNT_CUTOFF - count cutoff, default 12\n    ## RATIO_CUTOFF - ratio cutoff, default 2.0\n    ## FDR - -log10(fdr) cutoff, default 2\n    ## FILTER - file containing bins that need to be filtered out. Format: two columns \"chrom\", \"bin\". \"chrom\" contains 'chr1','chr2',.. \"bin\" is bin label\n    ## regresison_type - pospoisson for positive poisson regression, negbinom for negative binomial. default is pospoisson\n    MAPS_regression_and_peak_caller.r \"${meta.id}_${bin_size}/\" ${meta.id} $bin_size $args\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        MAPS: 1.1.0\n    END_VERSIONS\n    for i in \\$(ls *.version.txt); do\n    echo \"    \\${i%.version.txt}: \\$(<\\$i)\" >> versions.yml\n    done\n    \"\"\"",
        "nb_lignes_script": 21,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "bin_size",
            "macs2",
            "long_bedpe",
            "short_bed",
            "background",
            "maps"
        ],
        "nb_inputs": 7,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-monocle=2.20.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-monocle:2.20.0--r41h399db7b_0' : 'quay.io/biocontainers/bioconductor-monocle:2.20.0--r41h399db7b_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "BWA_INDEX": {
        "name_process": "BWA_INDEX",
        "string_process": "process BWA_INDEX {\n    tag \"$fasta\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bwa:0.7.17--hed695b0_7' :\n        'quay.io/biocontainers/bwa:0.7.17--hed695b0_7' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"bwa\"         , emit: index\n    path \"versions.yml\", emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bwa\n    bwa \\\\\n        index \\\\\n        $args \\\\\n        -p bwa/${fasta.baseName} \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    mkdir bwa\n    bwa \\\\\n        index \\\\\n        $args \\\\\n        -p bwa/${fasta.baseName} \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        bwa: \\$(echo \\$(bwa 2>&1) | sed 's/^.*Version: //; s/Contact:.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "BWA",
            "CINdex"
        ],
        "tools_url": [
            "https://bio.tools/bwa",
            "https://bio.tools/cindex"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            },
            {
                "name": "CINdex",
                "uri": "https://bio.tools/cindex",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3233",
                                    "term": "Copy number estimation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3196",
                                    "term": "Genotyping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3233",
                                    "term": "Transcript copy number estimation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The package addresses important area of high-throughput genomic analysis. It allows the automated processing and analysis of the experimental DNA copy number data generated by Affymetrix SNP 6.0 arrays or similar. It calculates the chromosome instability index to quantitatively characterize genome-wide DNA copy number alterations. This package calculates not only overall genomic instability, but also instability in terms of copy number gains and losses at the chromosome and cytoband level.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/CINdex.html"
            }
        ],
        "inputs": [
            "fasta"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$fasta\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::bwa=0.7.17\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bwa:0.7.17--hed695b0_7' : 'quay.io/biocontainers/bwa:0.7.17--hed695b0_7' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "GFFREAD": {
        "name_process": "GFFREAD",
        "string_process": "process GFFREAD {\n    tag \"$gff\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::gffread=0.12.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gffread:0.12.1--h8b12597_0' :\n        'quay.io/biocontainers/gffread:0.12.1--h8b12597_0' }\"\n\n    input:\n    path gff\n\n    output:\n    path \"*.gtf\"        , emit: gtf\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: \"${gff.baseName}\"\n    \"\"\"\n    gffread \\\\\n        $gff \\\\\n        $args \\\\\n        -o ${prefix}.gtf\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gffread: \\$(gffread --version 2>&1)\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "    def args   = task.ext.args   ?: ''\n    def prefix = task.ext.prefix ?: \"${gff.baseName}\"\n    \"\"\"\n    gffread \\\\\n        $gff \\\\\n        $args \\\\\n        -o ${prefix}.gtf\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gffread: \\$(gffread --version 2>&1)\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "gffread"
        ],
        "tools_url": [
            "https://bio.tools/gffread"
        ],
        "tools_dico": [
            {
                "name": "gffread",
                "uri": "https://bio.tools/gffread",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acids"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0361",
                                    "term": "Sequence annotation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "program for filtering, converting and manipulating GFF files",
                "homepage": "https://ccb.jhu.edu/software/stringtie/gff.shtml"
            }
        ],
        "inputs": [
            "gff"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$gff\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::gffread=0.12.1\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/gffread:0.12.1--h8b12597_0' : 'quay.io/biocontainers/gffread:0.12.1--h8b12597_0' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "PREPARE_COUNTS": {
        "name_process": "PREPARE_COUNTS",
        "string_process": "process PREPARE_COUNTS {\n    tag \"$meta.id\"\n    label 'process_high'\n    label 'process_long'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(r2peak, stageAs:\"R2peak/*\"), path(r1peak, stageAs: \"R1peak/*\"), path(distalpair, stageAs: \"pairs/*\"), val(chrom1)\n\n    output:\n    tuple val(meta), path(\"*.rds\"), optional: true, emit: counts\n    path \"versions.yml\"                           , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on Aug. 24, 2021 count reads for peak filtering\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"rtracklayer\", \"InteractionSet\", \"rhdf5\", \"BiocParallel\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    ## options\n    ## make_option(c(\"-a\", \"--r1peak\"), type=\"character\", default=NULL, help=\"filename of r1 peak\", metavar=\"string\")\n    ## make_option(c(\"-b\", \"--r2peak\"), type=\"character\", default=NULL, help=\"filename of r2 peak\", metavar=\"string\")\n    ## make_option(c(\"-x\", \"--restrict\"), type=\"character\", default=NULL, help=\"filename of restrict cut\", metavar=\"string\")\n    ## make_option(c(\"-p\", \"--pairs\"), type=\"character\", default=NULL, help=\"folder of valid distal pairs\", metavar=\"string\")\n    ## make_option(c(\"-m\", \"--mappability\"), type=\"character\", default=NULL, help=\"mappability file\", metavar=\"string\")\n    ## make_option(c(\"-o\", \"--output\"), type=\"character\", default=\"counts.csv\", help=\"output folder\", metavar=\"string\")\n    ## make_option(c(\"-f\", \"--fasta\"), type=\"character\", default=NULL, help=\"genome fasta file\", metavar=\"string\")\n    ## make_option(c(\"-1\", \"--chrom1\"), type=\"character\", default=NULL, help=\"chromosome1\", metavar=\"string\")\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"peak_pair_block\"=c(\"--peak_pair_block\", \"-b\", \"integer\"),\n                        \"snow_type\"=c(\"--snow_type\", \"-t\", \"character\"))\n    opt <- parse_args(option_list, strsplit(\"$args\", \"\\\\\\\\s+\")[[1]])\n    CHROM1 <- \"$chrom1\"\n    OUTPUT <- \"counts.${meta.id}.${chrom1}.rds\"\n    NCORE <- as.numeric(\"$task.cpus\")\n    SNOW_TYPE <- \"SOCK\"\n    peak_pair_block <- 1e9\n    if(!is.null(opt\\$peak_pair_block)){\n        peak_pair_block <- opt\\$peak_pair_block\n    }\n    if(!is.null(opt\\$snow_type)){\n        SNOW_TYPE <- opt\\$snow_type\n    }\n    pattern <- \"h5\" ## h5 is postfix of output of pairtools pairs2hdf5\n    pairs <- dir(\"pairs\", paste0(pattern, \"\\$\"), full.names=TRUE)\n    names(pairs) <- sub(paste0(\"\\\\\\\\.\", pattern), \"\", basename(pairs))\n    R1PEAK <- import(\"$r1peak\")\n    R2PEAK <- import(\"$r2peak\")\n    mcols(R1PEAK) <- NULL\n    mcols(R2PEAK) <- NULL\n    ## split by chromsome\n    R1PEAK <- split(R1PEAK, seqnames(R1PEAK))\n    R2PEAK <- split(R2PEAK, seqnames(R2PEAK))\n    R1PEAK <- R1PEAK[lengths(R1PEAK)>0]\n    R2PEAK <- R2PEAK[lengths(R2PEAK)>0]\n    chromosomes <- intersect(names(R1PEAK), names(R2PEAK))\n    chromosomes <- chromosomes[!grepl(\"_\", chromosomes)]\n    chromosomes <- chromosomes[!grepl(\"M\", chromosomes)] ## remove chrM/chrMT\n    if(length(chromosomes)==0){\n        stop(\"no valid data in same chromosome.\")\n    }\n\n    ## loading data\n    readPairs <- function(pair, chrom1, chrom2){\n        h5content <- rhdf5::h5ls(pair)\n        h5content <- h5content[, \"group\"]\n        h5content <- h5content[grepl(\"data.*\\\\\\\\d+_\\\\\\\\d+\", h5content)]\n        h5content <- unique(h5content)\n        n <- h5content[grepl(paste0(\"data.\", chrom1, \".\", chrom2), h5content)]\n        n <- paste(n, \"position\", sep=\"/\")\n        inf <- rhdf5::H5Fopen(pair, flags=\"H5F_ACC_RDONLY\")\n        on.exit({rhdf5::H5Fclose(inf)})\n        pc <- lapply(n, function(.ele){\n            if(rhdf5::H5Lexists(inf, .ele)){\n                rhdf5::h5read(inf, .ele)\n            }\n        })\n        rhdf5::H5Fclose(inf)\n        rhdf5::h5closeAll()\n        on.exit()\n        pc <- do.call(rbind, pc)\n    }\n\n    ### load counts\n    gis <- NULL\n\n    if(CHROM1 %in% chromosomes){\n        gc(reset=TRUE)\n        if(SNOW_TYPE==\"FORK\"){\n            param <- MulticoreParam(workers = NCORE, progressbar = TRUE)\n        }else{\n            param <- SnowParam(workers = NCORE, progressbar = TRUE, type = SNOW_TYPE)\n        }\n        chrom1 <- CHROM1\n        parallel <- TRUE\n        for(chrom2 in chromosomes){\n            message(\"working on \", chrom1, \" and \", chrom2, \" from \", Sys.time())\n            r1peak <- R1PEAK[[chrom1]]\n            r2peak <- R2PEAK[[chrom2]]\n            message(\"read reads\")\n            if(parallel){\n                try_res <- try({reads <- bplapply(pairs, readPairs, chrom1=chrom1, chrom2=chrom2, BPPARAM = param)})\n                if(inherits(try_res, \"try-error\")){\n                    parallel <- FALSE\n                }\n            }\n            if(!parallel){\n                reads <- lapply(pairs, readPairs, chrom1=chrom1, chrom2=chrom2)\n            }\n            h5closeAll()\n            reads <- do.call(rbind, c(reads, make.row.names = FALSE))\n            if(length(reads) && length(r1peak) && length(r2peak)){\n                reads <- GInteractions(GRanges(chrom1, IRanges(reads[, 1], width=150)),\n                                        GRanges(chrom2, IRanges(reads[, 2], width=150)))\n                ## count twice,\n                ## first, merge the r1peak with gap 5k and filter the peak with 0 counts\n                ## second, count for filtered r1peak\n                r1peak_s <- reduce(r1peak, min.gapwidth=5000, with.revmap=TRUE)\n                countFUN <- function(peak_pair, reads, r1peak, r2peak){\n                    r1peak\\$revmap <- NULL\n                    .gi <- InteractionSet::GInteractions(r1peak[peak_pair[, 1]], r2peak[peak_pair[, 2]],\n                                                        p1=peak_pair[, 1], p2=peak_pair[, 2])\n                    reads <- IRanges::subsetByOverlaps(reads, InteractionSet::regions(.gi))\n                    ## remove the interactions with distance smaller than 1K\n                    .dist <- IRanges::distance(first(.gi), second(.gi))\n                    .dist[is.na(.dist)] <- 3e9\n                    S4Vectors::mcols(.gi)[, \"count\"] <- InteractionSet::countOverlaps(.gi, reads, use.region=\"both\")\n                    S4Vectors::mcols(.gi)[, \"shortCount\"] <- GenomicRanges::countOverlaps(S4Vectors::second(.gi), S4Vectors::second(reads))\n                    .gi[S4Vectors::mcols(.gi)[, \"count\"]>0 & S4Vectors::mcols(.gi)[, \"shortCount\"]>0 & .dist>1000]\n                }\n                countFUNbyPairs <- function(r1peak, r2peak, peak_pairs, reads, parallel){\n                    peak_pairs_group <- ceiling(nrow(peak_pairs)/peak_pair_block)\n                    if(peak_pairs_group>1){\n                        peak_pairs_group <- rep(seq.int(peak_pairs_group), each= peak_pair_block)\n                        peak_pairs <- split(peak_pairs,\n                                            peak_pairs_group[seq.int(nrow(peak_pairs))])\n                        if(parallel){\n                            try_res <- try({gi <- bplapply(peak_pairs, FUN=countFUN, reads=reads, r1peak=r1peak, r2peak=r2peak, BPPARAM = param)})\n                            if(inherits(try_res, \"try-error\")){\n                                parallel <- FALSE\n                            }\n                        }\n                        if(!parallel){\n                            gi <- lapply(peak_pairs, FUN=countFUN, reads=reads, r1peak=r1peak, r2peak=r2peak)\n                        }\n                        gi <- Reduce(c, gi)\n                    }else{\n                        gi <- countFUN(peak_pairs, reads, r1peak, r2peak)\n                    }\n                    gi\n                }\n                message(\"count reads\")\n                peak_pairs <- expand.grid(seq_along(r1peak_s), seq_along(r2peak))\n                gi <- countFUNbyPairs(r1peak_s, r2peak, peak_pairs, reads, parallel)\n                peak_pairs <- mcols(r1peak_s)[mcols(gi)[, \"p1\"], \"revmap\"]\n                peak_pairs <- data.frame(p1=unlist(peak_pairs),\n                                        p2=rep(mcols(gi)[, \"p2\"], lengths(peak_pairs)))\n                rm(gi)\n                gi <- countFUNbyPairs(r1peak, r2peak, peak_pairs, reads, parallel)\n                if(length(gi)>0){\n                    gis <- c(gis, gi)\n                }\n                rm(peak_pairs, gi)\n                gc(reset=TRUE)\n            }\n        }\n    }\n    if(is.list(gis)) gis <- do.call(c, gis)\n    saveRDS(gis, OUTPUT)\n    \"\"\"\n}",
        "nb_lignes_process": 202,
        "string_script": "    def args   = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on Aug. 24, 2021 count reads for peak filtering\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"rtracklayer\", \"InteractionSet\", \"rhdf5\", \"BiocParallel\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    ## options\n    ## make_option(c(\"-a\", \"--r1peak\"), type=\"character\", default=NULL, help=\"filename of r1 peak\", metavar=\"string\")\n    ## make_option(c(\"-b\", \"--r2peak\"), type=\"character\", default=NULL, help=\"filename of r2 peak\", metavar=\"string\")\n    ## make_option(c(\"-x\", \"--restrict\"), type=\"character\", default=NULL, help=\"filename of restrict cut\", metavar=\"string\")\n    ## make_option(c(\"-p\", \"--pairs\"), type=\"character\", default=NULL, help=\"folder of valid distal pairs\", metavar=\"string\")\n    ## make_option(c(\"-m\", \"--mappability\"), type=\"character\", default=NULL, help=\"mappability file\", metavar=\"string\")\n    ## make_option(c(\"-o\", \"--output\"), type=\"character\", default=\"counts.csv\", help=\"output folder\", metavar=\"string\")\n    ## make_option(c(\"-f\", \"--fasta\"), type=\"character\", default=NULL, help=\"genome fasta file\", metavar=\"string\")\n    ## make_option(c(\"-1\", \"--chrom1\"), type=\"character\", default=NULL, help=\"chromosome1\", metavar=\"string\")\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"peak_pair_block\"=c(\"--peak_pair_block\", \"-b\", \"integer\"),\n                        \"snow_type\"=c(\"--snow_type\", \"-t\", \"character\"))\n    opt <- parse_args(option_list, strsplit(\"$args\", \"\\\\\\\\s+\")[[1]])\n    CHROM1 <- \"$chrom1\"\n    OUTPUT <- \"counts.${meta.id}.${chrom1}.rds\"\n    NCORE <- as.numeric(\"$task.cpus\")\n    SNOW_TYPE <- \"SOCK\"\n    peak_pair_block <- 1e9\n    if(!is.null(opt\\$peak_pair_block)){\n        peak_pair_block <- opt\\$peak_pair_block\n    }\n    if(!is.null(opt\\$snow_type)){\n        SNOW_TYPE <- opt\\$snow_type\n    }\n    pattern <- \"h5\" ## h5 is postfix of output of pairtools pairs2hdf5\n    pairs <- dir(\"pairs\", paste0(pattern, \"\\$\"), full.names=TRUE)\n    names(pairs) <- sub(paste0(\"\\\\\\\\.\", pattern), \"\", basename(pairs))\n    R1PEAK <- import(\"$r1peak\")\n    R2PEAK <- import(\"$r2peak\")\n    mcols(R1PEAK) <- NULL\n    mcols(R2PEAK) <- NULL\n    ## split by chromsome\n    R1PEAK <- split(R1PEAK, seqnames(R1PEAK))\n    R2PEAK <- split(R2PEAK, seqnames(R2PEAK))\n    R1PEAK <- R1PEAK[lengths(R1PEAK)>0]\n    R2PEAK <- R2PEAK[lengths(R2PEAK)>0]\n    chromosomes <- intersect(names(R1PEAK), names(R2PEAK))\n    chromosomes <- chromosomes[!grepl(\"_\", chromosomes)]\n    chromosomes <- chromosomes[!grepl(\"M\", chromosomes)] ## remove chrM/chrMT\n    if(length(chromosomes)==0){\n        stop(\"no valid data in same chromosome.\")\n    }\n\n    ## loading data\n    readPairs <- function(pair, chrom1, chrom2){\n        h5content <- rhdf5::h5ls(pair)\n        h5content <- h5content[, \"group\"]\n        h5content <- h5content[grepl(\"data.*\\\\\\\\d+_\\\\\\\\d+\", h5content)]\n        h5content <- unique(h5content)\n        n <- h5content[grepl(paste0(\"data.\", chrom1, \".\", chrom2), h5content)]\n        n <- paste(n, \"position\", sep=\"/\")\n        inf <- rhdf5::H5Fopen(pair, flags=\"H5F_ACC_RDONLY\")\n        on.exit({rhdf5::H5Fclose(inf)})\n        pc <- lapply(n, function(.ele){\n            if(rhdf5::H5Lexists(inf, .ele)){\n                rhdf5::h5read(inf, .ele)\n            }\n        })\n        rhdf5::H5Fclose(inf)\n        rhdf5::h5closeAll()\n        on.exit()\n        pc <- do.call(rbind, pc)\n    }\n\n    ### load counts\n    gis <- NULL\n\n    if(CHROM1 %in% chromosomes){\n        gc(reset=TRUE)\n        if(SNOW_TYPE==\"FORK\"){\n            param <- MulticoreParam(workers = NCORE, progressbar = TRUE)\n        }else{\n            param <- SnowParam(workers = NCORE, progressbar = TRUE, type = SNOW_TYPE)\n        }\n        chrom1 <- CHROM1\n        parallel <- TRUE\n        for(chrom2 in chromosomes){\n            message(\"working on \", chrom1, \" and \", chrom2, \" from \", Sys.time())\n            r1peak <- R1PEAK[[chrom1]]\n            r2peak <- R2PEAK[[chrom2]]\n            message(\"read reads\")\n            if(parallel){\n                try_res <- try({reads <- bplapply(pairs, readPairs, chrom1=chrom1, chrom2=chrom2, BPPARAM = param)})\n                if(inherits(try_res, \"try-error\")){\n                    parallel <- FALSE\n                }\n            }\n            if(!parallel){\n                reads <- lapply(pairs, readPairs, chrom1=chrom1, chrom2=chrom2)\n            }\n            h5closeAll()\n            reads <- do.call(rbind, c(reads, make.row.names = FALSE))\n            if(length(reads) && length(r1peak) && length(r2peak)){\n                reads <- GInteractions(GRanges(chrom1, IRanges(reads[, 1], width=150)),\n                                        GRanges(chrom2, IRanges(reads[, 2], width=150)))\n                ## count twice,\n                ## first, merge the r1peak with gap 5k and filter the peak with 0 counts\n                ## second, count for filtered r1peak\n                r1peak_s <- reduce(r1peak, min.gapwidth=5000, with.revmap=TRUE)\n                countFUN <- function(peak_pair, reads, r1peak, r2peak){\n                    r1peak\\$revmap <- NULL\n                    .gi <- InteractionSet::GInteractions(r1peak[peak_pair[, 1]], r2peak[peak_pair[, 2]],\n                                                        p1=peak_pair[, 1], p2=peak_pair[, 2])\n                    reads <- IRanges::subsetByOverlaps(reads, InteractionSet::regions(.gi))\n                    ## remove the interactions with distance smaller than 1K\n                    .dist <- IRanges::distance(first(.gi), second(.gi))\n                    .dist[is.na(.dist)] <- 3e9\n                    S4Vectors::mcols(.gi)[, \"count\"] <- InteractionSet::countOverlaps(.gi, reads, use.region=\"both\")\n                    S4Vectors::mcols(.gi)[, \"shortCount\"] <- GenomicRanges::countOverlaps(S4Vectors::second(.gi), S4Vectors::second(reads))\n                    .gi[S4Vectors::mcols(.gi)[, \"count\"]>0 & S4Vectors::mcols(.gi)[, \"shortCount\"]>0 & .dist>1000]\n                }\n                countFUNbyPairs <- function(r1peak, r2peak, peak_pairs, reads, parallel){\n                    peak_pairs_group <- ceiling(nrow(peak_pairs)/peak_pair_block)\n                    if(peak_pairs_group>1){\n                        peak_pairs_group <- rep(seq.int(peak_pairs_group), each= peak_pair_block)\n                        peak_pairs <- split(peak_pairs,\n                                            peak_pairs_group[seq.int(nrow(peak_pairs))])\n                        if(parallel){\n                            try_res <- try({gi <- bplapply(peak_pairs, FUN=countFUN, reads=reads, r1peak=r1peak, r2peak=r2peak, BPPARAM = param)})\n                            if(inherits(try_res, \"try-error\")){\n                                parallel <- FALSE\n                            }\n                        }\n                        if(!parallel){\n                            gi <- lapply(peak_pairs, FUN=countFUN, reads=reads, r1peak=r1peak, r2peak=r2peak)\n                        }\n                        gi <- Reduce(c, gi)\n                    }else{\n                        gi <- countFUN(peak_pairs, reads, r1peak, r2peak)\n                    }\n                    gi\n                }\n                message(\"count reads\")\n                peak_pairs <- expand.grid(seq_along(r1peak_s), seq_along(r2peak))\n                gi <- countFUNbyPairs(r1peak_s, r2peak, peak_pairs, reads, parallel)\n                peak_pairs <- mcols(r1peak_s)[mcols(gi)[, \"p1\"], \"revmap\"]\n                peak_pairs <- data.frame(p1=unlist(peak_pairs),\n                                        p2=rep(mcols(gi)[, \"p2\"], lengths(peak_pairs)))\n                rm(gi)\n                gi <- countFUNbyPairs(r1peak, r2peak, peak_pairs, reads, parallel)\n                if(length(gi)>0){\n                    gis <- c(gis, gi)\n                }\n                rm(peak_pairs, gi)\n                gc(reset=TRUE)\n            }\n        }\n    }\n    if(is.list(gis)) gis <- do.call(c, gis)\n    saveRDS(gis, OUTPUT)\n    \"\"\"",
        "nb_lignes_script": 183,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "chrom1",
            "r2peak",
            "r1peak",
            "distalpair"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "label 'process_long'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' : 'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MAPS_CUT": {
        "name_process": "MAPS_CUT",
        "string_process": "process MAPS_CUT {\n    tag \"$bin_size\"\n    label 'process_high_cpus'\n    label 'process_long'\n    label 'error_retry'\n\n    conda (params.enable_conda ? \"conda-forge::biopython=1.70\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/biopython:1.70--np112py36_1' :\n        'quay.io/biocontainers/biopython:1.70--np112py36_1' }\"\n\n    input:\n    path fasta\n    val bin_size\n\n    output:\n    tuple val(bin_size), path('*.cut')        , emit: cut\n    path \"versions.yml\"                       , emit: versions\n\n    script:\n    def RE_cutsite = [\n        \"mboi\": [site:\"GATC\", pos:\"0\"],\n        \"ncoi\": [site:\"CCATGG\", pos:\"1\"],\n        \"dpnii\": [site:\"GATC\", pos:\"0\"],\n        \"bglii\": [site:\"AGATCT\", pos:\"1\"],\n        \"hindiii\": [site:\"AAGCTT\", pos:'1'],\n        \"cviqi\": [site:\"GTAC\", pos:\"1\"],\n        \"arima\": [site:\"GATC,GA.TC\", pos:\"0,1\"],\n        \"mnase\": [site:\"mnase\", pos:\"none\"]]\n    def enzyme = RE_cutsite[params.enzyme.toLowerCase()]?:[site:params.enzyme.replaceAll(\"^\", \"\"), pos:params.enzyme.indexOf('^')]\n    \"\"\"\n    restriction_cut_multipleenzyme.py \\\\\n        -f ${fasta} \\\\\n        -s ${enzyme.site} \\\\\n        -p ${enzyme.pos} \\\\\n        -b ${bin_size} \\\\\n        -o ${bin_size}_${params.enzyme}.cut \\\\\n        -c $task.cpus\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        MAPS: 1.1.0\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 44,
        "string_script": "    def RE_cutsite = [\n        \"mboi\": [site:\"GATC\", pos:\"0\"],\n        \"ncoi\": [site:\"CCATGG\", pos:\"1\"],\n        \"dpnii\": [site:\"GATC\", pos:\"0\"],\n        \"bglii\": [site:\"AGATCT\", pos:\"1\"],\n        \"hindiii\": [site:\"AAGCTT\", pos:'1'],\n        \"cviqi\": [site:\"GTAC\", pos:\"1\"],\n        \"arima\": [site:\"GATC,GA.TC\", pos:\"0,1\"],\n        \"mnase\": [site:\"mnase\", pos:\"none\"]]\n    def enzyme = RE_cutsite[params.enzyme.toLowerCase()]?:[site:params.enzyme.replaceAll(\"^\", \"\"), pos:params.enzyme.indexOf('^')]\n    \"\"\"\n    restriction_cut_multipleenzyme.py \\\\\n        -f ${fasta} \\\\\n        -s ${enzyme.site} \\\\\n        -p ${enzyme.pos} \\\\\n        -b ${bin_size} \\\\\n        -o ${bin_size}_${params.enzyme}.cut \\\\\n        -c $task.cpus\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        MAPS: 1.1.0\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 23,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fasta",
            "bin_size"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$bin_size\"",
            "label 'process_high_cpus'",
            "label 'process_long'",
            "label 'error_retry'",
            "conda (params.enable_conda ? \"conda-forge::biopython=1.70\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/biopython:1.70--np112py36_1' : 'quay.io/biocontainers/biopython:1.70--np112py36_1' }\""
        ],
        "when": "",
        "stub": ""
    },
    "PAIRTOOLS_SORT": {
        "name_process": "PAIRTOOLS_SORT",
        "string_process": "process PAIRTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' :\n        'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.pairs.gz\"), emit: sorted\n    path \"versions.yml\"                , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def mem      = task.memory.toString().replaceAll(/(\\s|\\.|B)+/, '')\n    \"\"\"\n    pairtools \\\\\n        sort \\\\\n        $args \\\\\n        --nproc $task.cpus \\\\\n        --memory \"$mem\" \\\\\n        -o ${prefix}.pairs.gz \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 36,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def mem      = task.memory.toString().replaceAll(/(\\s|\\.|B)+/, '')\n    \"\"\"\n    pairtools \\\\\n        sort \\\\\n        $args \\\\\n        --nproc $task.cpus \\\\\n        --memory \"$mem\" \\\\\n        -o ${prefix}.pairs.gz \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "input"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' : 'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "PAIRIX": {
        "name_process": "PAIRIX",
        "string_process": "process PAIRIX {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::pairix=0.3.7\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pairix:0.3.7--py36h30a8e3e_3' :\n        'quay.io/biocontainers/pairix:0.3.7--py36h30a8e3e_3' }\"\n\n    input:\n    tuple val(meta), path(pair)\n\n    output:\n    tuple val(meta), path(pair), path(\"*.px2\"), emit: index\n    path \"versions.yml\"                       , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    pairix \\\\\n        $args \\\\\n        $pair\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairix: \\$(echo \\$(pairix --help 2>&1) | sed 's/^.*Version: //; s/Usage.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    pairix \\\\\n        $args \\\\\n        $pair\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairix: \\$(echo \\$(pairix --help 2>&1) | sed 's/^.*Version: //; s/Usage.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "pair"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::pairix=0.3.7\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/pairix:0.3.7--py36h30a8e3e_3' : 'quay.io/biocontainers/pairix:0.3.7--py36h30a8e3e_3' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "CUTADAPT": {
        "name_process": "CUTADAPT",
        "string_process": "process CUTADAPT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::cutadapt=3.4' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cutadapt:3.4--py39h38f01e4_1' :\n        'quay.io/biocontainers/cutadapt:3.4--py39h38f01e4_1' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.log')          , emit: log\n    path \"versions.yml\"                     , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def trimmed  = meta.single_end ? \"-o ${prefix}.trim.fastq.gz\" : \"-o ${prefix}_1.trim.fastq.gz -p ${prefix}_2.trim.fastq.gz\"\n    \"\"\"\n    cutadapt \\\\\n        --cores $task.cpus \\\\\n        $args \\\\\n        $trimmed \\\\\n        $reads \\\\\n        > ${prefix}.cutadapt.log\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cutadapt: \\$(cutadapt --version)\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 35,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def trimmed  = meta.single_end ? \"-o ${prefix}.trim.fastq.gz\" : \"-o ${prefix}_1.trim.fastq.gz -p ${prefix}_2.trim.fastq.gz\"\n    \"\"\"\n    cutadapt \\\\\n        --cores $task.cpus \\\\\n        $args \\\\\n        $trimmed \\\\\n        $reads \\\\\n        > ${prefix}.cutadapt.log\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cutadapt: \\$(cutadapt --version)\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [
            "Cutadapt"
        ],
        "tools_url": [
            "https://bio.tools/cutadapt"
        ],
        "tools_dico": [
            {
                "name": "Cutadapt",
                "uri": "https://bio.tools/cutadapt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Sequence trimming"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Trimming"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3495",
                                "term": "RNA sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3495",
                                "term": "RNA sequence"
                            }
                        ]
                    }
                ],
                "description": "Find and remove adapter sequences, primers, poly-A tails and other types of unwanted sequence from your high-throughput sequencing reads.",
                "homepage": "https://pypi.python.org/pypi/cutadapt"
            }
        ],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? 'bioconda::cutadapt=3.4' : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/cutadapt:3.4--py39h38f01e4_1' : 'quay.io/biocontainers/cutadapt:3.4--py39h38f01e4_1' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "PAIRSPLOT": {
        "name_process": "PAIRSPLOT",
        "string_process": "process PAIRSPLOT {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-chipqc=1.28.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-chipqc:1.28.0--r41hdfd78af_0' :\n        'quay.io/biocontainers/bioconductor-chipqc:1.28.0--r41hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(qc, stageAs: \"pairsqc_report/*\")\n\n    output:\n    tuple val(meta), path(\"${meta.id}_report/*\"), emit: qc\n    tuple val(meta), path(\"${meta.id}_report/*.summary.out\"), emit: summary\n    tuple val(meta), path(\"${meta.id}_report/*.distance.vs.proportion.csv\"), emit: csv\n    path \"versions.yml\"                        , emit: versions\n\n    script:\n    def RE_cutsite = [\n        \"mboi\": 4,\n        \"ncoi\": 6,\n        \"dpnii\": 4,\n        \"bglii\": 6,\n        \"hindiii\": 6,\n        \"cviqi\": 4,\n        \"arima\": 4,\n        \"mnase\": 4]\n    def enzyme = RE_cutsite[params.enzyme.toLowerCase()]?:4\n    \"\"\"\n    mv pairsqc_report ${meta.id}_report\n    pairsqcplot.r $enzyme ${meta.id}_report\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairsqc: \"0.2.2\"\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "    def RE_cutsite = [\n        \"mboi\": 4,\n        \"ncoi\": 6,\n        \"dpnii\": 4,\n        \"bglii\": 6,\n        \"hindiii\": 6,\n        \"cviqi\": 4,\n        \"arima\": 4,\n        \"mnase\": 4]\n    def enzyme = RE_cutsite[params.enzyme.toLowerCase()]?:4\n    \"\"\"\n    mv pairsqc_report ${meta.id}_report\n    pairsqcplot.r $enzyme ${meta.id}_report\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairsqc: \"0.2.2\"\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "qc"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-chipqc=1.28.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-chipqc:1.28.0--r41hdfd78af_0' : 'quay.io/biocontainers/bioconductor-chipqc:1.28.0--r41hdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "SEQLEVELS_STYLE": {
        "name_process": "SEQLEVELS_STYLE",
        "string_process": "process SEQLEVELS_STYLE {\n    tag \"$bed\"\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-genomeinfodb=1.26.4\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-genomeinfodb:1.26.4--r40hdfd78af_0' :\n        'quay.io/biocontainers/bioconductor-genomeinfodb:1.26.4--r40hdfd78af_0' }\"\n\n    input:\n    path bed\n\n    output:\n    stdout emit: seqlevels_style\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(GenomeInfoDb)\n    versions <- c(\n        \"${task.process}:\",\n        paste(\"    GenomeInfoDb:\", as.character(packageVersion(\"GenomeInfoDb\"))))\n    writeLines(versions, \"versions.yml\")\n\n    inf = \"$bed\" ## input file must be a bed file\n\n    data <- read.table(inf, nrows=1000, header=FALSE, quote=NULL, comment.char=\"#\")\n\n    seqnames <- unique(as.character(data[, 1]))\n    seql <- \"UCSC\" %in% seqlevelsStyle(seqnames)\n\n    if(seql){\n        cat(\"UCSC\")\n    }else{\n        cat(\"NOT_UCSC\")\n    }\n    \"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(GenomeInfoDb)\n    versions <- c(\n        \"${task.process}:\",\n        paste(\"    GenomeInfoDb:\", as.character(packageVersion(\"GenomeInfoDb\"))))\n    writeLines(versions, \"versions.yml\")\n\n    inf = \"$bed\" ## input file must be a bed file\n\n    data <- read.table(inf, nrows=1000, header=FALSE, quote=NULL, comment.char=\"#\")\n\n    seqnames <- unique(as.character(data[, 1]))\n    seql <- \"UCSC\" %in% seqlevelsStyle(seqnames)\n\n    if(seql){\n        cat(\"UCSC\")\n    }else{\n        cat(\"NOT_UCSC\")\n    }\n    \"\"\"",
        "nb_lignes_script": 21,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "bed"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$bed\"",
            "conda (params.enable_conda ? \"bioconda::bioconductor-genomeinfodb=1.26.4\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-genomeinfodb:1.26.4--r40hdfd78af_0' : 'quay.io/biocontainers/bioconductor-genomeinfodb:1.26.4--r40hdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "COOLER_LOAD": {
        "name_process": "COOLER_LOAD",
        "string_process": "process COOLER_LOAD {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::cooler=0.8.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cooler:0.8.11--pyh3252c3a_0' :\n        'quay.io/biocontainers/cooler:0.8.11--pyh3252c3a_0' }\"\n\n    input:\n    tuple val(meta), val(cool_bin), path(pairs)\n    path chromsizes\n\n    output:\n    tuple val(meta), val(cool_bin), path(\"*.cool\"), emit: cool\n    path \"versions.yml\"                           , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def args     = task.ext.args ?: ''\n    \"\"\"\n    cooler load \\\\\n        $args \\\\\n        ${chromsizes}:${cool_bin} \\\\\n        $pairs \\\\\n        ${prefix}.${cool_bin}.cool\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cooler: \\$(echo \\$(cooler --version 2>&1) | sed 's/cooler, version //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "    def prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def args     = task.ext.args ?: ''\n    \"\"\"\n    cooler load \\\\\n        $args \\\\\n        ${chromsizes}:${cool_bin} \\\\\n        $pairs \\\\\n        ${prefix}.${cool_bin}.cool\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cooler: \\$(echo \\$(cooler --version 2>&1) | sed 's/cooler, version //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "cool_bin",
            "pairs",
            "chromsizes"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::cooler=0.8.11\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/cooler:0.8.11--pyh3252c3a_0' : 'quay.io/biocontainers/cooler:0.8.11--pyh3252c3a_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MAPS_MERGE": {
        "name_process": "MAPS_MERGE",
        "string_process": "process MAPS_MERGE {\n    tag \"$bin_size\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"pandas=1.1.5\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pandas:1.1.5' :\n        'quay.io/biocontainers/pandas:1.1.5' }\"\n\n    input:\n    tuple val(bin_size), path(cut), path(mappability)\n    path merge_map_py_source\n\n    output:\n    tuple val(bin_size), path(\"${cut.getSimpleName()}\")    , emit: map\n    path \"versions.yml\"                                    , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    python $merge_map_py_source \\\\\n        -c $cut \\\\\n        -m $mappability \\\\\n        -o tmp.map\n    awk \"\\\\\\$7>$args\" tmp.map > ${cut.getSimpleName()}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        MAPS: 1.1.0\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    python $merge_map_py_source \\\\\n        -c $cut \\\\\n        -m $mappability \\\\\n        -o tmp.map\n    awk \"\\\\\\$7>$args\" tmp.map > ${cut.getSimpleName()}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        MAPS: 1.1.0\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "bin_size",
            "cut",
            "mappability",
            "merge_map_py_source"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$bin_size\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"pandas=1.1.5\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/pandas:1.1.5' : 'quay.io/biocontainers/pandas:1.1.5' }\""
        ],
        "when": "",
        "stub": ""
    },
    "CUSTOM_DUMPSOFTWAREVERSIONS": {
        "name_process": "CUSTOM_DUMPSOFTWAREVERSIONS",
        "string_process": "process CUSTOM_DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n\n                                                                                                  \n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\"    , emit: yml\n    path \"software_versions_mqc.yml\", emit: mqc_yml\n    path \"versions.yml\"             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    template 'dumpsoftwareversions.py'\n}",
        "nb_lignes_process": 22,
        "string_script": "    def args = task.ext.args ?: ''\n    template 'dumpsoftwareversions.py'",
        "nb_lignes_script": 1,
        "language_script": "bash",
        "tools": [
            "docxtemplate"
        ],
        "tools_url": [
            "https://bio.tools/docxtemplate"
        ],
        "tools_dico": [
            {
                "name": "docxtemplate",
                "uri": "https://bio.tools/docxtemplate",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3314",
                            "term": "Chemistry"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0176",
                            "term": "Molecular dynamics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3489",
                            "term": "Database management"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3489",
                            "term": "Database administration"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0249",
                                    "term": "Protein geometry calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0322",
                                    "term": "Molecular model refinement"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Deposition"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data deposition"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> VERY_LOW CONFIDENCE! | > CORRECT NAME OF TOOL COULD ALSO BE 'Phenix', 'restraints', 'Amber', 'refinement' | Improved chemistry restraints for crystallographic refinement by integrating the Amber force field into Phenix | Word templates and tools for Windows | The IUCr Word templates utilize the content management features and document styles of Word to format your manuscript and to store essential details for submission of your manuscript",
                "homepage": "http://journals.iucr.org/services/docxtemplate/"
            }
        ],
        "inputs": [
            "versions"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' : 'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "GENMAP_INDEX": {
        "name_process": "GENMAP_INDEX",
        "string_process": "process GENMAP_INDEX {\n    tag '$fasta'\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::genmap=1.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/genmap:1.3.0--h1b792b2_1' :\n        'quay.io/biocontainers/genmap:1.3.0--h1b792b2_1' }\"\n\n    input:\n    path fasta\n\n    output:\n    path \"genmap\"       , emit: index\n    path \"versions.yml\" , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    genmap \\\\\n        index \\\\\n        -F $fasta \\\\\n        -I genmap\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        genmap: \\$(genmap --version 2>&1 | sed 's/GenMap version: //; s/SeqAn.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    genmap \\\\\n        index \\\\\n        -F $fasta \\\\\n        -I genmap\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        genmap: \\$(genmap --version 2>&1 | sed 's/GenMap version: //; s/SeqAn.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "GenMAPP",
            "CINdex"
        ],
        "tools_url": [
            "https://bio.tools/genmapp",
            "https://bio.tools/cindex"
        ],
        "tools_dico": [
            {
                "name": "GenMAPP",
                "uri": "https://bio.tools/genmapp",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarray experiment"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0602",
                            "term": "Molecular interactions, pathways and networks"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3518",
                            "term": "Microarrays"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0314",
                                    "term": "Gene expression profiling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0313",
                                    "term": "Expression profile clustering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0315",
                                    "term": "Expression profile comparison"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0314",
                                    "term": "Functional profiling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0314",
                                    "term": "Gene expression profile construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0314",
                                    "term": "Feature expression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0314",
                                    "term": "Gene transcription profiling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0314",
                                    "term": "Gene expression quantification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0314",
                                    "term": "Gene expression profile generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression data analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Microarray expression data visualization tool, allowing data to be viewed on maps representing gene groupings and biological pathways.",
                "homepage": "http://www.genmapp.org/"
            },
            {
                "name": "CINdex",
                "uri": "https://bio.tools/cindex",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3233",
                                    "term": "Copy number estimation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3196",
                                    "term": "Genotyping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3233",
                                    "term": "Transcript copy number estimation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "The package addresses important area of high-throughput genomic analysis. It allows the automated processing and analysis of the experimental DNA copy number data generated by Affymetrix SNP 6.0 arrays or similar. It calculates the chromosome instability index to quantitatively characterize genome-wide DNA copy number alterations. This package calculates not only overall genomic instability, but also instability in terms of copy number gains and losses at the chromosome and cytoband level.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/CINdex.html"
            }
        ],
        "inputs": [
            "fasta"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag '$fasta'",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::genmap=1.3.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/genmap:1.3.0--h1b792b2_1' : 'quay.io/biocontainers/genmap:1.3.0--h1b792b2_1' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "COOLER_DUMP": {
        "name_process": "COOLER_DUMP",
        "string_process": "process COOLER_DUMP {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::cooler=0.8.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cooler:0.8.11--pyh3252c3a_0' :\n        'quay.io/biocontainers/cooler:0.8.11--pyh3252c3a_0' }\"\n\n    input:\n    tuple val(meta), path(cool)\n    val resolution\n\n    output:\n    tuple val(meta), path(\"*.bedpe\"), emit: bedpe\n    path \"versions.yml\"             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def suffix   = resolution     ? \"::$resolution\"               : \"\"\n    \"\"\"\n    cooler dump \\\\\n        $args \\\\\n        -o ${prefix}.bedpe \\\\\n        $cool$suffix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cooler: \\$(cooler --version 2>&1 | sed 's/cooler, version //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def suffix   = resolution     ? \"::$resolution\"               : \"\"\n    \"\"\"\n    cooler dump \\\\\n        $args \\\\\n        -o ${prefix}.bedpe \\\\\n        $cool$suffix\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cooler: \\$(cooler --version 2>&1 | sed 's/cooler, version //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "cool",
            "resolution"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::cooler=0.8.11\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/cooler:0.8.11--pyh3252c3a_0' : 'quay.io/biocontainers/cooler:0.8.11--pyh3252c3a_0' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "PAIRTOOLS_SELECT": {
        "name_process": "PAIRTOOLS_SELECT",
        "string_process": "process PAIRTOOLS_SELECT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' :\n        'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.selected.pairs.gz\")  , emit: selected\n    tuple val(meta), path(\"*.unselected.pairs.gz\"), emit: unselected\n    path \"versions.yml\"                           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pairtools select \\\\\n        \"$args\" \\\\\n        -o ${prefix}.selected.pairs.gz \\\\\n        --output-rest ${prefix}.unselected.pairs.gz \\\\\n        ${input}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pairtools select \\\\\n        \"$args\" \\\\\n        -o ${prefix}.selected.pairs.gz \\\\\n        --output-rest ${prefix}.unselected.pairs.gz \\\\\n        ${input}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "input"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' : 'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "SAMTOOLS_VIEW": {
        "name_process": "SAMTOOLS_VIEW",
        "string_process": "process SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(input)\n    path fasta\n\n    output:\n    tuple val(meta), path(\"*.bam\") , emit: bam , optional: true\n    tuple val(meta), path(\"*.cram\"), emit: cram, optional: true\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--reference ${fasta} -C\" : \"\"\n    def file_type = input.getExtension()\n    if (\"$input\" == \"${prefix}.${file_type}\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        view \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        $args \\\\\n        $input \\\\\n        $args2 \\\\\n        > ${prefix}.${file_type}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 42,
        "string_script": "    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--reference ${fasta} -C\" : \"\"\n    def file_type = input.getExtension()\n    if (\"$input\" == \"${prefix}.${file_type}\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        view \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        $args \\\\\n        $input \\\\\n        $args2 \\\\\n        > ${prefix}.${file_type}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 20,
        "language_script": "bash",
        "tools": [
            "SAMtools",
            "iview"
        ],
        "tools_url": [
            "https://bio.tools/samtools",
            "https://bio.tools/iview"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            },
            {
                "name": "iview",
                "uri": "https://bio.tools/iview",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2275",
                            "term": "Molecular modelling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0128",
                            "term": "Protein interactions"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2814",
                            "term": "Protein structure analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_2814",
                            "term": "Protein structure"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0477",
                                    "term": "Protein modelling"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0477",
                                    "term": "Homology modelling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0477",
                                    "term": "Comparative modelling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0477",
                                    "term": "Protein structure comparative modelling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0477",
                                    "term": "Homology structure modelling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Interactive HTML5 visualizer of protein-ligand complex.",
                "homepage": "http://istar.cse.cuhk.edu.hk/iview/"
            }
        ],
        "inputs": [
            "meta",
            "input",
            "fasta"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' : 'quay.io/biocontainers/samtools:1.15--h1170115_1' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "PAIRTOOLS_PARSE": {
        "name_process": "PAIRTOOLS_PARSE",
        "string_process": "process PAIRTOOLS_PARSE {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' :\n        'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\"\n\n    input:\n    tuple val(meta), path(bam)\n    path chromsizes\n\n    output:\n    tuple val(meta), path(\"*.pairsam.gz\")  , emit: pairsam\n    tuple val(meta), path(\"*.pairsam.stat\"), emit: stat\n    path \"versions.yml\"                    , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pairtools \\\\\n        parse \\\\\n        -c $chromsizes \\\\\n        $args \\\\\n        --output-stats ${prefix}.pairsam.stat \\\\\n        -o ${prefix}.pairsam.gz \\\\\n        $bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 37,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pairtools \\\\\n        parse \\\\\n        -c $chromsizes \\\\\n        $args \\\\\n        --output-stats ${prefix}.pairsam.stat \\\\\n        -o ${prefix}.pairsam.gz \\\\\n        $bam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "Parseq"
        ],
        "tools_url": [
            "https://bio.tools/parseq"
        ],
        "tools_dico": [
            {
                "name": "Parseq",
                "uri": "https://bio.tools/parseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0438",
                                    "term": "Transcriptional regulatory element prediction"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0438",
                                    "term": "Regulatory element prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0438",
                                    "term": "Transcription regulatory element prediction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Statistical approach for transcription landscape reconstruction at a basepair resolution from RNA Seq read counts.",
                "homepage": "http://www.lcqb.upmc.fr/parseq/"
            }
        ],
        "inputs": [
            "meta",
            "bam",
            "chromsizes"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' : 'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "PAIRTOOLS_FLIP": {
        "name_process": "PAIRTOOLS_FLIP",
        "string_process": "process PAIRTOOLS_FLIP {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' :\n        'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\"\n\n    input:\n    tuple val(meta), path(sam)\n    path chromsizes\n\n    output:\n    tuple val(meta), path(\"*.flip.gz\"), emit: flip\n    path \"versions.yml\"               , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pairtools \\\\\n        flip \\\\\n        -c $chromsizes \\\\\n        $args \\\\\n        -o ${prefix}.flip.gz \\\\\n        $sam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 35,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pairtools \\\\\n        flip \\\\\n        -c $chromsizes \\\\\n        $args \\\\\n        -o ${prefix}.flip.gz \\\\\n        $sam\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [
            "Flipper"
        ],
        "tools_url": [
            "https://bio.tools/flipper"
        ],
        "tools_dico": [
            {
                "name": "Flipper",
                "uri": "https://bio.tools/flipper",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3070",
                            "term": "Biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3316",
                            "term": "Computer science"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3070",
                            "term": "Biological science"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3763",
                                    "term": "Service invocation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3436",
                                    "term": "Aggregation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Standalone client able to register Web-Services in different catalogues (e.g. BioMOBY, INB, etc.). In addition the tool is able to perform the deployment process of command line applications. The process first registers the service, then it creates the Web-Service implementation skeletons, and finally, it deploys the skeletons and the command-line application in a server.",
                "homepage": "http://bitlab-es.com/flipper/"
            }
        ],
        "inputs": [
            "meta",
            "sam",
            "chromsizes"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' : 'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "CHROMSIZES": {
        "name_process": "CHROMSIZES",
        "string_process": "process CHROMSIZES {\n    tag \"$fasta\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0' :\n        'quay.io/biocontainers/samtools:1.12--hd5e65b6_0' }\"\n\n    input:\n    path fasta\n\n    output:\n    path '*.sizes'      , emit: sizes\n    path '*.fai'        , emit: fai\n    path \"versions.yml\" , emit: versions\n\n    script:\n    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai | sort -k 1,1 > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "    \"\"\"\n    samtools faidx $fasta\n    cut -f 1,2 ${fasta}.fai | sort -k 1,1 > ${fasta}.sizes\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "fasta"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$fasta\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::samtools=1.12\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/samtools:1.12--hd5e65b6_0' : 'quay.io/biocontainers/samtools:1.12--hd5e65b6_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "GENOME_FILTER": {
        "name_process": "GENOME_FILTER",
        "string_process": "process GENOME_FILTER {\n    label 'process_low'\n\n    conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' :\n        'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\"\n\n    input:\n    path sizes\n    path blacklist\n\n    output:\n    path \"*.bed\",         emit: bed\n    path \"versions.yml\",  emit: versions\n\n    script:\n    def file_out = \"${sizes.simpleName}.include_regions.bed\"\n    if (blacklist) {\n        \"\"\"\n        sortBed -i $blacklist -g $sizes | complementBed -i stdin -g $sizes > $file_out\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bedtools: \\$(echo \\$(bedtools --version) | sed -e \"s/bedtools v//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        awk '{print \\$1, '0' , \\$2}' OFS='\\t' $sizes > $file_out\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            awk: \\$(echo \\$(awk --version 2>&1 || awk -W version 2>&1) | sed 's/[[:alpha:]|(|)|[:space:]]//g; s/,.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }\n}",
        "nb_lignes_process": 35,
        "string_script": "    def file_out = \"${sizes.simpleName}.include_regions.bed\"\n    if (blacklist) {\n        \"\"\"\n        sortBed -i $blacklist -g $sizes | complementBed -i stdin -g $sizes > $file_out\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            bedtools: \\$(echo \\$(bedtools --version) | sed -e \"s/bedtools v//g\")\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        awk '{print \\$1, '0' , \\$2}' OFS='\\t' $sizes > $file_out\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            awk: \\$(echo \\$(awk --version 2>&1 || awk -W version 2>&1) | sed 's/[[:alpha:]|(|)|[:space:]]//g; s/,.*\\$//')\n        END_VERSIONS\n        \"\"\"\n    }",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sizes",
            "blacklist"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::bedtools=2.30.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bedtools:2.30.0--hc088bd4_0' : 'quay.io/biocontainers/bedtools:2.30.0--hc088bd4_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "CALL_R1PEAK": {
        "name_process": "CALL_R1PEAK",
        "string_process": "process CALL_R1PEAK {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n    path cut\n    val pval\n\n    output:\n    tuple val(meta), path(\"*.narrowPeak\")            , emit: peak\n    path  \"versions.yml\"                             , emit: versions\n\n    tuple val(meta), path(\"*.bed\")                   , emit: bed\n    tuple val(meta), path(\"*.bdg\")                   , emit: bdg\n\n    script:\n    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on DEC. 13, 2021 call R1 peaks\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    options(scipen=10)\n    pkgs <- c(\"rtracklayer\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    ## options\n    R1READS <- \"$reads\"\n    CUT <- \"$cut\"\n    r1reads <- read.delim(R1READS, header=FALSE)\n    r1reads <- GRanges(r1reads[, 1], IRanges(r1reads[, 2], width=1), r1reads[, 6])\n    digest <- import(CUT, format=\"BED\")\n    width(digest) <- 1\n    ## peaks gap 5\n    peaks <- reduce(digest, min.gapwidth=5L)\n    mcols(peaks)[, \"signalValue\"] <- countOverlaps(peaks, r1reads, maxgap=5L)\n    lambda <- mean(mcols(peaks)[, \"signalValue\"])\n    p <- ppois(mcols(peaks)[, \"signalValue\"], lambda, lower.tail=FALSE)\n    mcols(peaks)[, \"pValue\"] <- -10*log10(p)\n    mcols(peaks)[, \"qValue\"] <- -10*log10(p.adjust(p, method=\"BH\"))\n    mcols(peaks)[, \"score\"] <- round((1-p)*100)\n    peaks <- peaks[p<$pval]\n    ## calculate the distance between digest sites\n    dist <- distanceToNearest(digest)\n    dist <- median(mcols(dist)[, \"distance\"])\n    peaks <- promoters(peaks, upstream=dist, downstream=dist)\n    peaks <- GenomicRanges::trim(peaks)\n    rd <- reduce(peaks, with.revmap=TRUE)\n    revmap <- mcols(rd)[, \"revmap\"]\n    l <- lengths(revmap)>1\n    if(any(l)){\n        rd_id <- unlist(revmap[l])\n        rd_gp <- rep(seq_along(revmap[l]), lengths(revmap[l]))\n        peaks_rd <- peaks[rd_id]\n        peaks_rd_data <- split(as.data.frame(mcols(peaks_rd)), rd_gp)\n        peaks_rd_data <- lapply(peaks_rd_data, FUN=colMeans)\n        peaks_rd_data <- peaks_rd_data[order(as.numeric(names(peaks_rd_data)))]\n        peaks_rd_data <- do.call(rbind, peaks_rd_data)\n        peaks_rd <- rd[l]\n        mcols(peaks_rd) <- peaks_rd_data\n        peaks <- c(peaks[!l], peaks_rd)\n        peaks <- sort(peaks)\n    }\n    if(any(start(peaks)<1)){\n        start(peaks[start(peaks)<1]) <- 1\n    }\n    export(peaks, \"${prefix}.bed\")\n    np <- paste(as.character(seqnames(peaks)), start(peaks)-1, end(peaks),\n                \".\", mcols(peaks)[, \"score\"],\n                \".\", mcols(peaks)[, \"signalValue\"],\n                mcols(peaks)[, \"pValue\"],\n                mcols(peaks)[, \"qValue\"],\n                dist, sep=\"\\t\")\n    writeLines(np, \"${prefix}.narrowPeak\")\n    r1reads <- promoters(r1reads, upstream=75, downstream=75)\n    cvg <- coverage(r1reads)\n    export(cvg, \"${prefix}_pileup.bdg\", format=\"bedgraph\")\n    \"\"\"\n}",
        "nb_lignes_process": 95,
        "string_script": "    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on DEC. 13, 2021 call R1 peaks\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    options(scipen=10)\n    pkgs <- c(\"rtracklayer\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    ## options\n    R1READS <- \"$reads\"\n    CUT <- \"$cut\"\n    r1reads <- read.delim(R1READS, header=FALSE)\n    r1reads <- GRanges(r1reads[, 1], IRanges(r1reads[, 2], width=1), r1reads[, 6])\n    digest <- import(CUT, format=\"BED\")\n    width(digest) <- 1\n    ## peaks gap 5\n    peaks <- reduce(digest, min.gapwidth=5L)\n    mcols(peaks)[, \"signalValue\"] <- countOverlaps(peaks, r1reads, maxgap=5L)\n    lambda <- mean(mcols(peaks)[, \"signalValue\"])\n    p <- ppois(mcols(peaks)[, \"signalValue\"], lambda, lower.tail=FALSE)\n    mcols(peaks)[, \"pValue\"] <- -10*log10(p)\n    mcols(peaks)[, \"qValue\"] <- -10*log10(p.adjust(p, method=\"BH\"))\n    mcols(peaks)[, \"score\"] <- round((1-p)*100)\n    peaks <- peaks[p<$pval]\n    ## calculate the distance between digest sites\n    dist <- distanceToNearest(digest)\n    dist <- median(mcols(dist)[, \"distance\"])\n    peaks <- promoters(peaks, upstream=dist, downstream=dist)\n    peaks <- GenomicRanges::trim(peaks)\n    rd <- reduce(peaks, with.revmap=TRUE)\n    revmap <- mcols(rd)[, \"revmap\"]\n    l <- lengths(revmap)>1\n    if(any(l)){\n        rd_id <- unlist(revmap[l])\n        rd_gp <- rep(seq_along(revmap[l]), lengths(revmap[l]))\n        peaks_rd <- peaks[rd_id]\n        peaks_rd_data <- split(as.data.frame(mcols(peaks_rd)), rd_gp)\n        peaks_rd_data <- lapply(peaks_rd_data, FUN=colMeans)\n        peaks_rd_data <- peaks_rd_data[order(as.numeric(names(peaks_rd_data)))]\n        peaks_rd_data <- do.call(rbind, peaks_rd_data)\n        peaks_rd <- rd[l]\n        mcols(peaks_rd) <- peaks_rd_data\n        peaks <- c(peaks[!l], peaks_rd)\n        peaks <- sort(peaks)\n    }\n    if(any(start(peaks)<1)){\n        start(peaks[start(peaks)<1]) <- 1\n    }\n    export(peaks, \"${prefix}.bed\")\n    np <- paste(as.character(seqnames(peaks)), start(peaks)-1, end(peaks),\n                \".\", mcols(peaks)[, \"score\"],\n                \".\", mcols(peaks)[, \"signalValue\"],\n                mcols(peaks)[, \"pValue\"],\n                mcols(peaks)[, \"qValue\"],\n                dist, sep=\"\\t\")\n    writeLines(np, \"${prefix}.narrowPeak\")\n    r1reads <- promoters(r1reads, upstream=75, downstream=75)\n    cvg <- coverage(r1reads)\n    export(cvg, \"${prefix}_pileup.bdg\", format=\"bedgraph\")\n    \"\"\"",
        "nb_lignes_script": 72,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads",
            "cut",
            "pval"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' : 'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "ATACQC": {
        "name_process": "ATACQC",
        "string_process": "process ATACQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-atacseqqc=1.16.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-atacseqqc:1.16.0--r41hdfd78af_0' :\n        'quay.io/biocontainers/bioconductor-atacseqqc:1.16.0--r41hdfd78af_0' }\"\n\n    input:\n    path peaks\n    path beds\n    path gtf\n\n    output:\n    path \"*.csv\"                   , emit: stats\n    path \"versions.yml\"            , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on Sept. 10, 2021 stats for ATAC reads\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"rtracklayer\", \"GenomicFeatures\", \"ATACseqQC\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n    args <- strsplit(\"${args}\", \"\\\\\\\\s+\")[[1]]\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"pattern\"=c(\"--pattern\", \"-p\", \"character\"))\n    opt <- parse_args(option_list, args)\n    pattern <- opt[['pattern']] #\"merged.ATAC.bed.gz\"\n    gtf <- \"$gtf\"\n\n    readsFiles <- dir(\".\", pattern) ## postfix from mergedreads.nf\n    peaksFiles <- dir(\".\", \"narrowPeak|broadPeak\") ## output from macs2\n\n    names(readsFiles) <- sub(pattern, \"\", readsFiles)\n    names(peaksFiles) <- sub(\"_peaks.*Peak\", \"\", peaksFiles)\n\n    N <- intersect(names(readsFiles), names(peaksFiles))\n    if(length(N)==0){\n        if(length(peaksFiles)==1){ # for user defined peaks.\n            peaksFiles <- rep(peaksFiles, length(readsFiles))\n            names(peaksFiles) <- names(readsFiles)\n            N <- intersect(names(readsFiles), names(peaksFiles))\n        }\n        stopifnot(\"no peak and signal pairs\"=length(N)>0)\n    }\n\n    peaksFiles <- peaksFiles[N]\n    readsFiles <- readsFiles[N]\n\n    ## import reads\n    readls <- lapply(readsFiles, function(f){\n        reads <- read.table(f, colClasses=c(\"character\", \"integer\", \"NULL\",\n                                \"NULL\", \"NULL\", \"character\"))\n        reads <- GRanges(reads[, 1],\n                        IRanges(as.numeric(reads[, 2])+1, as.numeric(reads[, 2])+150),\n                        strand = reads[, 3])\n    })\n\n    ## import peaks\n    peakls <- lapply(peaksFiles, import)\n\n    txdb <- makeTxDbFromGFF(gtf) #for TSS\n    txs <- exons(txdb)\n\n    stats <- mapply(peakls, readls, FUN = function(peaks, reads){\n        ## calculate FRiP score (fraction of reads in peaks), must over 1%\n        readsInPeaks <- countOverlaps(peaks, reads)\n        FRiP <- 100*sum(readsInPeaks)/length(reads)\n\n        reads <- as(reads, \"GAlignments\")\n        ## calculate Transcription Start Site Enrichment Score\n        tsse <- TSSEscore(reads, txs)\n\n        ## Promoter/Transcript body (PT) score\n        pt <- PTscore(reads, txs)\n        pt <- pt[!is.na(pt\\$promoter) & !is.na(pt\\$transcriptBody) & !is.na(pt\\$PT_score)]\n        pt <- pt[(pt\\$promoter>0 | pt\\$transcriptBody>0) & pt\\$PT_score!=0]\n        promoterEnriched <- table(pt\\$PT_score>0)\n        names(promoterEnriched) <-\n            c(\"FALSE\"=\"bodyEnrich\", \"TRUE\"=\"promoterEnrich\")[names(promoterEnriched)]\n        promoterEnriched <-\n            c(promoterEnriched,\n                prop.test=prop.test(cbind(table(pt\\$PT_score>0), c(50, 50)))\\$p.value)\n        list(tsse_FRiP = c(TSSEscore=tsse\\$TSSEscore, FRiP=FRiP, promoterEnriched),\n            tsseValues = tsse\\$values)\n    }, SIMPLIFY = FALSE)\n\n    ## FRiP, TSSEscore table\n    tsse_FRiP <- do.call(rbind, lapply(stats, function(.ele) .ele\\$tsse_FRiP))\n    write.csv(tsse_FRiP, \"TSSEscore_FRiP.csv\")\n\n    ## for TSSEscore to TSS plots\n    tsse <- do.call(rbind, lapply(stats, function(.ele) .ele\\$tsseValues))\n    if(ncol(tsse)==20){\n        colnames(tsse) <- 100*(-9:10-.5)\n        write.csv(tsse, \"aggregateTSSEscoreToTSS.csv\")\n    }\n    \"\"\"\n}",
        "nb_lignes_process": 126,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on Sept. 10, 2021 stats for ATAC reads\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"rtracklayer\", \"GenomicFeatures\", \"ATACseqQC\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n    args <- strsplit(\"${args}\", \"\\\\\\\\s+\")[[1]]\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"pattern\"=c(\"--pattern\", \"-p\", \"character\"))\n    opt <- parse_args(option_list, args)\n    pattern <- opt[['pattern']] #\"merged.ATAC.bed.gz\"\n    gtf <- \"$gtf\"\n\n    readsFiles <- dir(\".\", pattern) ## postfix from mergedreads.nf\n    peaksFiles <- dir(\".\", \"narrowPeak|broadPeak\") ## output from macs2\n\n    names(readsFiles) <- sub(pattern, \"\", readsFiles)\n    names(peaksFiles) <- sub(\"_peaks.*Peak\", \"\", peaksFiles)\n\n    N <- intersect(names(readsFiles), names(peaksFiles))\n    if(length(N)==0){\n        if(length(peaksFiles)==1){ # for user defined peaks.\n            peaksFiles <- rep(peaksFiles, length(readsFiles))\n            names(peaksFiles) <- names(readsFiles)\n            N <- intersect(names(readsFiles), names(peaksFiles))\n        }\n        stopifnot(\"no peak and signal pairs\"=length(N)>0)\n    }\n\n    peaksFiles <- peaksFiles[N]\n    readsFiles <- readsFiles[N]\n\n    ## import reads\n    readls <- lapply(readsFiles, function(f){\n        reads <- read.table(f, colClasses=c(\"character\", \"integer\", \"NULL\",\n                                \"NULL\", \"NULL\", \"character\"))\n        reads <- GRanges(reads[, 1],\n                        IRanges(as.numeric(reads[, 2])+1, as.numeric(reads[, 2])+150),\n                        strand = reads[, 3])\n    })\n\n    ## import peaks\n    peakls <- lapply(peaksFiles, import)\n\n    txdb <- makeTxDbFromGFF(gtf) #for TSS\n    txs <- exons(txdb)\n\n    stats <- mapply(peakls, readls, FUN = function(peaks, reads){\n        ## calculate FRiP score (fraction of reads in peaks), must over 1%\n        readsInPeaks <- countOverlaps(peaks, reads)\n        FRiP <- 100*sum(readsInPeaks)/length(reads)\n\n        reads <- as(reads, \"GAlignments\")\n        ## calculate Transcription Start Site Enrichment Score\n        tsse <- TSSEscore(reads, txs)\n\n        ## Promoter/Transcript body (PT) score\n        pt <- PTscore(reads, txs)\n        pt <- pt[!is.na(pt\\$promoter) & !is.na(pt\\$transcriptBody) & !is.na(pt\\$PT_score)]\n        pt <- pt[(pt\\$promoter>0 | pt\\$transcriptBody>0) & pt\\$PT_score!=0]\n        promoterEnriched <- table(pt\\$PT_score>0)\n        names(promoterEnriched) <-\n            c(\"FALSE\"=\"bodyEnrich\", \"TRUE\"=\"promoterEnrich\")[names(promoterEnriched)]\n        promoterEnriched <-\n            c(promoterEnriched,\n                prop.test=prop.test(cbind(table(pt\\$PT_score>0), c(50, 50)))\\$p.value)\n        list(tsse_FRiP = c(TSSEscore=tsse\\$TSSEscore, FRiP=FRiP, promoterEnriched),\n            tsseValues = tsse\\$values)\n    }, SIMPLIFY = FALSE)\n\n    ## FRiP, TSSEscore table\n    tsse_FRiP <- do.call(rbind, lapply(stats, function(.ele) .ele\\$tsse_FRiP))\n    write.csv(tsse_FRiP, \"TSSEscore_FRiP.csv\")\n\n    ## for TSSEscore to TSS plots\n    tsse <- do.call(rbind, lapply(stats, function(.ele) .ele\\$tsseValues))\n    if(ncol(tsse)==20){\n        colnames(tsse) <- 100*(-9:10-.5)\n        write.csv(tsse, \"aggregateTSSEscoreToTSS.csv\")\n    }\n    \"\"\"",
        "nb_lignes_script": 107,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "peaks",
            "beds",
            "gtf"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-atacseqqc=1.16.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-atacseqqc:1.16.0--r41hdfd78af_0' : 'quay.io/biocontainers/bioconductor-atacseqqc:1.16.0--r41hdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "BIOC_PAIRS2HDF5": {
        "name_process": "BIOC_PAIRS2HDF5",
        "string_process": "process BIOC_PAIRS2HDF5 {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(pairs)\n    path chromsizes\n\n    output:\n    tuple val(meta), path(\"*.h5\") , emit: hdf5\n    path \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: 'keep-dup'\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on Dec. 03, 2021 to convert pairs to hdf5\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    ## hdf5 format:\n    ## - header, including total reads, chromosome name and sizes, tileWidth\n    ##    * header/chrom_sizes COMPOUND\n    ##    * header/header      STRING\n    ##    * header/tile_width  INTEGER\n    ##    * header/total       INTEGER\n    ## - data, pairs in path data/chr1/chr2/tileIndex1_tileIndex2/\n    ##    * position, in path data/chr1/chr2/tileIndex1_tileIndex2/position\n    ##    * strand, in path data/chr1/chr2/tileIndex1_tileIndex2/strand\n    #######################################################################\n    #######################################################################\n\n    pkgs <- c(\"rhdf5\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    comment_char <- \"#\"\n    pattern <- \"unselected.pairs.gz\" ## this is from upstream output file.\n    tileWidth <- 1e7 # this will create about 200 groups for human data\n    block_size <- 1e7 # the size of block for tempfile\n    keepDup <- FALSE # remove duplicates or not\n    if(grepl(\"keep-dup\", \"$args\")){\n        keepDup <- TRUE\n    }\n    chrom_sizes <- read.delim(\"$chromsizes\", header=FALSE)\n    infs <- \"$pairs\"\n    infs <- strsplit(infs, \"\\\\\\\\s+\")[[1]]\n\n    getHeader <- function(inf){\n        f <- gzfile(inf, open = \"r\")\n        on.exit(close(f))\n        header <- c()\n        while(length(chunk <- readLines(f, n=1))){\n            if(substr(chunk, 1, 1) == comment_char){\n                header <- c(header, chunk)\n            }else{\n                break\n            }\n        }\n        close(f)\n        on.exit()\n        header\n    }\n    getData <- function(f, block_size, n=7){\n        if(n<0) return(data.frame())\n        pc <- try({read.table(f, nrow=block_size, comment.char = \"#\",\n                            colClasses=c(\n                                \"NULL\", # reads name, skip\n                                \"character\", # chrom1\n                                \"integer\", # start1\n                                \"character\", # chrom2\n                                \"integer\", # start2\n                                \"character\", #strand1\n                                \"character\", #strand2\n                                rep(\"NULL\", n)))}, silent = TRUE)\n        if(inherits(pc, \"try-error\")){\n            data.frame()\n        }else{\n            pc\n        }\n    }\n    getIndex <- function(pos, tileWidth){\n        ceiling(pos/tileWidth)\n    }\n    getPath <- function(root, ...){\n        paste(root, ..., sep=\"/\")\n    }\n    createGroup <- function(obj, path){\n        if(!H5Lexists(obj, path)){\n            h5createGroup(obj, path)\n        }\n    }\n    read_pair_write_tmp <- function(inf, out){\n        ## check ncol\n        h <- read.table(inf, nrow=1, comment.char = \"#\")\n        n <- ncol(h) - 7\n        f <- gzfile(inf, open = \"r\")\n        on.exit({\n            close(f)\n        })\n        filenames <- c()\n\n        while(nrow(pc <- getData(f, block_size, n))>0){\n            idx1 <- getIndex(pc[, 2], tileWidth)\n            idx2 <- getIndex(pc[, 4], tileWidth)\n            idx1_2 <- paste(pc[, 1], pc[, 3], paste(idx1, idx2, sep=\"_\"), sep=\"___\")\n            pc <- split(pc[, -c(1, 3)], f=idx1_2)\n            filenames <- unique(c(filenames, file.path(out, names(pc))))\n            mapply(pc, names(pc), FUN=function(.data, .name){\n                write.table(.data, file = file.path(out, .name),\n                            append = TRUE, sep=\"\\t\", quote=FALSE,\n                            col.names=FALSE, row.names=FALSE)\n            })\n        }\n\n        close(f)\n        on.exit()\n        sort(filenames)\n    }\n    rewrite_hd5 <- function(filenames, out_h5, keepDup, root){\n        total <- lapply(filenames, function(n){\n            if(file.exists(n)){\n                pc <- read.delim(n, header=FALSE)\n                if(!keepDup) pc <- unique(pc)\n                npc <- nrow(pc)\n                chunk_size <- ceiling(sqrt(npc)/1000)*1000\n                n <- getPath(root, gsub(\"___\", \"/\", basename(n)))\n                h5createGroup(out_h5, n)\n                pos <- getPath(n, \"position\")\n                if(npc>1000){\n                    h5createDataset(out_h5, pos, dims = dim(pc[, c(1, 2)]),\n                                    storage.mode = \"integer\",\n                                    chunk = c(chunk_size, 2))\n                }\n                h5write(as.matrix(pc[, c(1, 2)]), out_h5, pos)\n                strand <- getPath(n, \"strand\")\n                if(npc>1000){\n                    h5createDataset(out_h5, strand, dims = dim(pc[, c(3, 4)]),\n                                    storage.mode = \"character\", size = 1,\n                                    chunk = c(chunk_size, 2))\n                }\n                h5write(as.matrix(pc[, c(3, 4)]), out_h5, strand)\n                npc\n            }else{\n                0\n            }\n        })\n        sum(unlist(total))\n    }\n    root <- \"data\"\n    for(inf in infs){\n        out <- sub(pattern, \"h5\", basename(inf))\n        if(!h5testFileLocking(dirname(inf))){\n            h5disableFileLocking()\n        }\n        h5createFile(out)\n        #header start as comment char'#'\n        header <- getHeader(inf)\n        h5createGroup(out, \"header\")\n        h5write(header, out, \"header/header\")\n        h5write(chrom_sizes, out, \"header/chrom_sizes\")\n        h5write(as.integer(tileWidth), out, \"header/tile_width\")\n        h5createGroup(out, root)\n        # create groups for tiles\n        n_chrom <- nrow(chrom_sizes)\n        for(i in seq.int(n_chrom)){\n            h5createGroup(out, getPath(root, chrom_sizes[i, 1]))\n            for(j in seq.int(n_chrom)){\n                h5createGroup(out, getPath(root, chrom_sizes[i, 1],\n                                        chrom_sizes[j, 1]))\n            }\n        }\n\n        # read pairs\n        #columns: readID chrom1 pos1 chrom2 pos2 strand1 strand2 pair_type\n        tmp_dir <- \"tmp_files\"\n        dir.create(tmp_dir)\n        filenames <- try(read_pair_write_tmp(inf, tmp_dir))\n        #rewrite\n        total <- rewrite_hd5(filenames, out, keepDup, root)\n        h5write(total, out, \"header/total\")\n        h5closeAll()\n        unlink(tmp_dir, recursive=TRUE)\n    }\n    \"\"\"\n}",
        "nb_lignes_process": 201,
        "string_script": "    def args = task.ext.args ?: 'keep-dup'\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on Dec. 03, 2021 to convert pairs to hdf5\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    ## hdf5 format:\n    ## - header, including total reads, chromosome name and sizes, tileWidth\n    ##    * header/chrom_sizes COMPOUND\n    ##    * header/header      STRING\n    ##    * header/tile_width  INTEGER\n    ##    * header/total       INTEGER\n    ## - data, pairs in path data/chr1/chr2/tileIndex1_tileIndex2/\n    ##    * position, in path data/chr1/chr2/tileIndex1_tileIndex2/position\n    ##    * strand, in path data/chr1/chr2/tileIndex1_tileIndex2/strand\n    #######################################################################\n    #######################################################################\n\n    pkgs <- c(\"rhdf5\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # write versions.yml\n\n    comment_char <- \"#\"\n    pattern <- \"unselected.pairs.gz\" ## this is from upstream output file.\n    tileWidth <- 1e7 # this will create about 200 groups for human data\n    block_size <- 1e7 # the size of block for tempfile\n    keepDup <- FALSE # remove duplicates or not\n    if(grepl(\"keep-dup\", \"$args\")){\n        keepDup <- TRUE\n    }\n    chrom_sizes <- read.delim(\"$chromsizes\", header=FALSE)\n    infs <- \"$pairs\"\n    infs <- strsplit(infs, \"\\\\\\\\s+\")[[1]]\n\n    getHeader <- function(inf){\n        f <- gzfile(inf, open = \"r\")\n        on.exit(close(f))\n        header <- c()\n        while(length(chunk <- readLines(f, n=1))){\n            if(substr(chunk, 1, 1) == comment_char){\n                header <- c(header, chunk)\n            }else{\n                break\n            }\n        }\n        close(f)\n        on.exit()\n        header\n    }\n    getData <- function(f, block_size, n=7){\n        if(n<0) return(data.frame())\n        pc <- try({read.table(f, nrow=block_size, comment.char = \"#\",\n                            colClasses=c(\n                                \"NULL\", # reads name, skip\n                                \"character\", # chrom1\n                                \"integer\", # start1\n                                \"character\", # chrom2\n                                \"integer\", # start2\n                                \"character\", #strand1\n                                \"character\", #strand2\n                                rep(\"NULL\", n)))}, silent = TRUE)\n        if(inherits(pc, \"try-error\")){\n            data.frame()\n        }else{\n            pc\n        }\n    }\n    getIndex <- function(pos, tileWidth){\n        ceiling(pos/tileWidth)\n    }\n    getPath <- function(root, ...){\n        paste(root, ..., sep=\"/\")\n    }\n    createGroup <- function(obj, path){\n        if(!H5Lexists(obj, path)){\n            h5createGroup(obj, path)\n        }\n    }\n    read_pair_write_tmp <- function(inf, out){\n        ## check ncol\n        h <- read.table(inf, nrow=1, comment.char = \"#\")\n        n <- ncol(h) - 7\n        f <- gzfile(inf, open = \"r\")\n        on.exit({\n            close(f)\n        })\n        filenames <- c()\n\n        while(nrow(pc <- getData(f, block_size, n))>0){\n            idx1 <- getIndex(pc[, 2], tileWidth)\n            idx2 <- getIndex(pc[, 4], tileWidth)\n            idx1_2 <- paste(pc[, 1], pc[, 3], paste(idx1, idx2, sep=\"_\"), sep=\"___\")\n            pc <- split(pc[, -c(1, 3)], f=idx1_2)\n            filenames <- unique(c(filenames, file.path(out, names(pc))))\n            mapply(pc, names(pc), FUN=function(.data, .name){\n                write.table(.data, file = file.path(out, .name),\n                            append = TRUE, sep=\"\\t\", quote=FALSE,\n                            col.names=FALSE, row.names=FALSE)\n            })\n        }\n\n        close(f)\n        on.exit()\n        sort(filenames)\n    }\n    rewrite_hd5 <- function(filenames, out_h5, keepDup, root){\n        total <- lapply(filenames, function(n){\n            if(file.exists(n)){\n                pc <- read.delim(n, header=FALSE)\n                if(!keepDup) pc <- unique(pc)\n                npc <- nrow(pc)\n                chunk_size <- ceiling(sqrt(npc)/1000)*1000\n                n <- getPath(root, gsub(\"___\", \"/\", basename(n)))\n                h5createGroup(out_h5, n)\n                pos <- getPath(n, \"position\")\n                if(npc>1000){\n                    h5createDataset(out_h5, pos, dims = dim(pc[, c(1, 2)]),\n                                    storage.mode = \"integer\",\n                                    chunk = c(chunk_size, 2))\n                }\n                h5write(as.matrix(pc[, c(1, 2)]), out_h5, pos)\n                strand <- getPath(n, \"strand\")\n                if(npc>1000){\n                    h5createDataset(out_h5, strand, dims = dim(pc[, c(3, 4)]),\n                                    storage.mode = \"character\", size = 1,\n                                    chunk = c(chunk_size, 2))\n                }\n                h5write(as.matrix(pc[, c(3, 4)]), out_h5, strand)\n                npc\n            }else{\n                0\n            }\n        })\n        sum(unlist(total))\n    }\n    root <- \"data\"\n    for(inf in infs){\n        out <- sub(pattern, \"h5\", basename(inf))\n        if(!h5testFileLocking(dirname(inf))){\n            h5disableFileLocking()\n        }\n        h5createFile(out)\n        #header start as comment char'#'\n        header <- getHeader(inf)\n        h5createGroup(out, \"header\")\n        h5write(header, out, \"header/header\")\n        h5write(chrom_sizes, out, \"header/chrom_sizes\")\n        h5write(as.integer(tileWidth), out, \"header/tile_width\")\n        h5createGroup(out, root)\n        # create groups for tiles\n        n_chrom <- nrow(chrom_sizes)\n        for(i in seq.int(n_chrom)){\n            h5createGroup(out, getPath(root, chrom_sizes[i, 1]))\n            for(j in seq.int(n_chrom)){\n                h5createGroup(out, getPath(root, chrom_sizes[i, 1],\n                                        chrom_sizes[j, 1]))\n            }\n        }\n\n        # read pairs\n        #columns: readID chrom1 pos1 chrom2 pos2 strand1 strand2 pair_type\n        tmp_dir <- \"tmp_files\"\n        dir.create(tmp_dir)\n        filenames <- try(read_pair_write_tmp(inf, tmp_dir))\n        #rewrite\n        total <- rewrite_hd5(filenames, out, keepDup, root)\n        h5write(total, out, \"header/total\")\n        h5closeAll()\n        unlink(tmp_dir, recursive=TRUE)\n    }\n    \"\"\"",
        "nb_lignes_script": 179,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "pairs",
            "chromsizes"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' : 'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "COOLER_CLOAD": {
        "name_process": "COOLER_CLOAD",
        "string_process": "process COOLER_CLOAD {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::cooler=0.8.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/cooler:0.8.11--pyh3252c3a_0' :\n        'quay.io/biocontainers/cooler:0.8.11--pyh3252c3a_0' }\"\n\n    input:\n    tuple val(meta), path(pairs), path(index)\n    val cool_bin\n    path chromsizes\n\n    output:\n    tuple val(meta), val(cool_bin), path(\"*.cool\"), emit: cool\n    path \"versions.yml\"                           , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def args     = task.ext.args.tokenize()\n    def tool     = (task.ext.args.contains('hiclib')) ? \"hiclib\" :\n        (task.ext.args.contains('tabix')) ? 'tabix' :\n        (task.ext.args.contains('pairs')) ? 'pairs' : 'pairix'\n    def nproc    = tool in ['pairix','tabix'] ? \"--nproc ${task.cpus}\" : ''\n    args.removeIf { it.contains('hiclib') }\n    args.removeIf { it.contains('tabix') }\n    args.removeIf { it.contains('pairs') }\n    args.removeIf { it.contains('pairix') }\n\n    \"\"\"\n    cooler cload $tool \\\\\n        ${args.join(' ')} \\\\\n        $nproc \\\\\n        ${chromsizes}:${cool_bin} \\\\\n        $pairs \\\\\n        ${prefix}.${cool_bin}.cool\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cooler: \\$(echo \\$(cooler --version 2>&1) | sed 's/cooler, version //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 43,
        "string_script": "    def prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def args     = task.ext.args.tokenize()\n    def tool     = (task.ext.args.contains('hiclib')) ? \"hiclib\" :\n        (task.ext.args.contains('tabix')) ? 'tabix' :\n        (task.ext.args.contains('pairs')) ? 'pairs' : 'pairix'\n    def nproc    = tool in ['pairix','tabix'] ? \"--nproc ${task.cpus}\" : ''\n    args.removeIf { it.contains('hiclib') }\n    args.removeIf { it.contains('tabix') }\n    args.removeIf { it.contains('pairs') }\n    args.removeIf { it.contains('pairix') }\n\n    \"\"\"\n    cooler cload $tool \\\\\n        ${args.join(' ')} \\\\\n        $nproc \\\\\n        ${chromsizes}:${cool_bin} \\\\\n        $pairs \\\\\n        ${prefix}.${cool_bin}.cool\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        cooler: \\$(echo \\$(cooler --version 2>&1) | sed 's/cooler, version //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 23,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "pairs",
            "index",
            "cool_bin",
            "chromsizes"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::cooler=0.8.11\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/cooler:0.8.11--pyh3252c3a_0' : 'quay.io/biocontainers/cooler:0.8.11--pyh3252c3a_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "UCSC_BEDGRAPHTOBIGWIG": {
        "name_process": "UCSC_BEDGRAPHTOBIGWIG",
        "string_process": "\nprocess UCSC_BEDGRAPHTOBIGWIG {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1' :\n        'quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1' }\"\n\n    input:\n    tuple val(meta), path(bedgraph)\n    path  sizes\n\n    output:\n    tuple val(meta), path(\"*.bigWig\"), emit: bigwig\n    path \"versions.yml\"              , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedGraphToBigWig \\\\\n        $bedgraph \\\\\n        $sizes \\\\\n        ${prefix}.bigWig\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    bedGraphToBigWig \\\\\n        $bedgraph \\\\\n        $sizes \\\\\n        ${prefix}.bigWig\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        ucsc: $VERSION\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "bedGraphToBigWig"
        ],
        "tools_url": [
            "https://bio.tools/bedgraphtobigwig"
        ],
        "tools_dico": [
            {
                "name": "bedGraphToBigWig",
                "uri": "https://bio.tools/bedgraphtobigwig",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Biological databases"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Data management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Information systems"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Databases and information systems"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Convert bedGraph to bigWig file.",
                "homepage": "https://www.encodeproject.org/software/bedgraphtobigwig/"
            }
        ],
        "inputs": [
            "meta",
            "bedgraph",
            "sizes"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::ucsc-bedgraphtobigwig=377\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/ucsc-bedgraphtobigwig:377--h446ed27_1' : 'quay.io/biocontainers/ucsc-bedgraphtobigwig:377--h446ed27_1' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "DIFFHICAR": {
        "name_process": "DIFFHICAR",
        "string_process": "process DIFFHICAR {\n    tag \"$bin_size\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-edger=3.32.1\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-edger:3.32.1--r40h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-edger:3.32.1--r40h399db7b_0' }\"\n\n    input:\n    tuple val(bin_size), path(peaks, stageAs: \"peaks/*\"), path(long_bedpe, stageAs: \"long/*\")\n\n    output:\n    tuple val(bin_size), path(\"${prefix}/*\") , emit: diff\n    path \"${prefix}/*.qc.json\"               , emit: stats\n    path \"versions.yml\"                      , emit: versions\n\n    script:\n    prefix   = task.ext.prefix ?: \"diffhic_bin${bin_size}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on April. 29, 2021 call edgeR\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    library(edgeR)\n    versions <- c(\n        \"${task.process}:\",\n        paste(\"    edgeR:\", as.character(packageVersion(\"edgeR\"))))\n    writeLines(versions, \"versions.yml\")\n\n    binsize = \"$prefix\"\n\n    ## get peaks\n    pf <- dir(\"peaks\", \"bedpe\", full.names = TRUE)\n    peaks <- lapply(pf, read.delim)\n    ### reduce the peaks\n    peaks <- unique(do.call(rbind, peaks)[, c(\"chr1\", \"start1\", \"end1\",\n                                            \"chr2\", \"start2\", \"end2\")])\n\n    ## get counts\n    pc <- dir(\"long\", \"bedpe\", full.names = FALSE)\n    cnts <- lapply(file.path(\"long\", pc), read.table)\n    samples <- sub(\"(_REP\\\\\\\\d+)\\\\\\\\.(.*?)\\\\\\\\.long.intra.bedpe\", \"\\\\\\\\1\", pc)\n    cnts <- lapply(split(cnts, samples), do.call, what=rbind)\n    sizeFactor <- vapply(cnts, FUN=function(.ele) sum(.ele[, 7], na.rm = TRUE),\n                        FUN.VALUE = numeric(1))\n\n    getID <- function(mat) gsub(\"\\\\\\\\s+\", \"\", apply(mat[, seq.int(6)], 1, paste, collapse=\"_\"))\n    getID1 <- function(mat) gsub(\"\\\\\\\\s+\", \"\", apply(mat[, seq.int(3)], 1, paste, collapse=\"_\"))\n    getID2 <- function(mat) gsub(\"\\\\\\\\s+\", \"\", apply(mat[, 4:6], 1, paste, collapse=\"_\"))\n    ## prefilter, to decrease the memory cost\n    peaks_id1 <- getID1(peaks)\n    peaks_id2 <- getID2(peaks)\n    cnts <- lapply(cnts, function(.ele) .ele[getID1(.ele) %in% peaks_id1, , drop=FALSE])\n    cnts <- lapply(cnts, function(.ele) .ele[getID2(.ele) %in% peaks_id2, , drop=FALSE])\n    rm(peaks_id1, peaks_id2, getID1, getID2)\n    ## match all the counts for peaks\n    peaks_id <- getID(peaks)\n    cnts <- do.call(cbind, lapply(cnts, function(.ele){\n        .ele[match(peaks_id, getID(.ele)), 7]\n    }))\n    cnts[is.na(cnts)] <- 0\n    names(peaks_id) <- paste0(rep(\"p\", length(peaks_id)), seq_along(peaks_id))\n    rownames(cnts) <- names(peaks_id)\n\n    pf <- as.character(binsize)\n    dir.create(pf, showWarnings = FALSE, recursive=TRUE)\n\n    fname <- function(subf, ext, ...){ # create file name\n        pff <- ifelse(is.na(subf), pf, file.path(pf, subf))\n        dir.create(pff, showWarnings = FALSE, recursive = TRUE)\n        file.path(pff, paste(..., ext, sep=\".\"))\n    }\n\n    ## write counts\n    write.csv(cbind(peaks, cnts), fname(NA, \"csv\", \"raw.counts\"), row.names = FALSE)\n    ## write sizeFactors\n    write.csv(sizeFactor, fname(NA, \"csv\", \"library.size\"), row.names = TRUE)\n\n    ## coldata\n    sampleNames <- colnames(cnts)\n    condition <- make.names(sub(\"_REP.*\\$\", \"\", sampleNames), allow_=TRUE)\n    coldata <- data.frame(condition=factor(condition),\n                        row.names = sampleNames)\n    ## write designtable\n    write.csv(coldata, fname(NA, \"csv\", \"designTab\"), row.names = TRUE)\n\n    contrasts.lev <- levels(coldata\\$condition)\n\n    if(length(contrasts.lev)>1 && any(table(condition)>1)){\n        contrasts <- combn(contrasts.lev, 2, simplify = FALSE) ## pair all conditions\n        ## create DGEList\n        group <- coldata\\$condition\n        y <- DGEList(counts = cnts,\n                    lib.size = sizeFactor,\n                    group = group)\n\n        ## do differential analysis\n        names(contrasts) <- vapply(contrasts,\n                                    FUN=paste,\n                                    FUN.VALUE = character(1),\n                                    collapse = \"-\")\n        y <- calcNormFactors(y)\n        design <- model.matrix(~0+group)\n        colnames(design) <- levels(y\\$samples\\$group)\n        y <- estimateDisp(y,design)\n        fit <- glmQLFit(y, design)\n\n        ## PCA\n        pdf(fname(NA, \"pdf\", \"Multidimensional.scaling.plot-plot\"))\n        mds <- plotMDS(y)\n        dev.off()\n        ## PCA for multiQC\n        try_res <- try({ ## try to output PCA results for multiQC\n            json <- data.frame(x=mds\\$x, y=mds\\$y)\n            rownames(json) <- names(mds\\$x)\n            json <- split(json, coldata[rownames(json), \"condition\"])\n            json <- mapply(json, rainbow(n=length(json)), FUN=function(.ele, .color){\n                .ele <- cbind(.ele, \"name\"=rownames(.ele))\n                .ele <- apply(.ele, 1, function(.e){\n                    x <- names(.e)\n                    y <- .e\n                    .e <- paste0('{\"x\":', .e[1],\n                                ', \"y\":', .e[2],\n                                ', \"color\":\"', .color,\n                                '\", \"name\":\"', .e[3],\n                                '\"}')\n                })\n                .ele <- paste(.ele, collapse=\", \")\n                .ele <- paste(\"[\", .ele, \"]\")\n            })\n            json <- paste0('\"', names(json), '\" :', json)\n            json <- c(\n                    \"{\",\n                    '\"id\":\"sample_pca\",',\n                    '\"data\":{',\n                    paste(unlist(json), collapse=\", \"),\n                    \"}\",\n                    \"}\")\n            writeLines(json, fname(NA, \"json\", \"Multidimensional.scaling.qc\"))\n        })\n        if(inherits(try_res, \"try-error\")){\n            message(try_res)\n        }\n\n        ## plot dispersion\n        pdf(fname(NA, \"pdf\", \"DispersionEstimate-plot\"))\n        plotBCV(y)\n        dev.off()\n        ## plot QL dispersions\n        pdf(fname(NA, \"pdf\", \"Quasi-Likelihood-DispersionEstimate-plot\"))\n        plotQLDisp(fit)\n        dev.off()\n\n        res <- mapply(contrasts, names(contrasts), FUN = function(cont, name){\n            BvsA <- makeContrasts(contrasts = name, levels = design)\n            qlf <- glmQLFTest(fit, contrast = BvsA)\n            rs <- topTags(qlf, n = nrow(qlf), sort.by = \"none\")\n            ## MD-plot\n            pdf(fname(name, \"pdf\", \"Mean-Difference-plot\", name))\n            plotMD(qlf)\n            abline(h=0, col=\"red\", lty=2, lwd=2)\n            dev.off()\n            ## PValue distribution\n            pdf(fname(name, \"pdf\", \"PValue-distribution-plot\", name))\n            hist(rs\\$table\\$PValue, breaks = 20)\n            dev.off()\n            ## save res\n            res <- as.data.frame(rs)\n            res <- cbind(peaks, res[names(peaks_id), ])\n            write.csv(res, fname(name, \"csv\", \"edgeR.DEtable\", name), row.names = FALSE)\n            ## save metadata\n            elementMetadata <- do.call(rbind, lapply(c(\"adjust.method\",\"comparison\",\"test\"), function(.ele) rs[[.ele]]))\n            rownames(elementMetadata) <- c(\"adjust.method\",\"comparison\",\"test\")\n            colnames(elementMetadata)[1] <- \"value\"\n            write.csv(elementMetadata, fname(name, \"csv\", \"edgeR.metadata\", name), row.names = TRUE)\n            ## save subset results\n            res.s <- res[res\\$FDR<0.05 & abs(res\\$logFC)>1, ]\n            write.csv(res.s, fname(name, \"csv\", \"edgeR.DEtable\", name, \"padj0.05.lfc1\"), row.names = FALSE)\n            ## Volcano plot\n            res\\$qvalue <- -10*log10(res\\$PValue)\n            res.s\\$qvalue <- -10*log10(res.s\\$PValue)\n            pdf(fname(name, \"pdf\", \"Volcano-plot\", name))\n            plot(x=res\\$logFC, y=res\\$qvalue,\n                main = paste(\"Volcano plot for\", name),\n                xlab = \"log2 Fold Change\", ylab = \"-10*log10(P-value)\",\n                type = \"p\", col=NA)\n            res.1 <- res\n            if(nrow(res.1)>0) points(x=res.1\\$logFC, y=res.1\\$qvalue, pch = 20, cex=.5, col=\"gray80\")\n            if(nrow(res.s)>0) points(x=res.s\\$logFC, y=res.s\\$qvalue, pch = 19, cex=.5, col=ifelse(res.s\\$logFC>0, \"brown\", \"darkblue\"))\n            dev.off()\n            res\\$qvalue <- -10*log10(res\\$PValue)\n            png(fname(name, \"png\", \"Volcano-plot\", name))\n            plot(x=res\\$logFC, y=res\\$qvalue,\n                main = paste(\"Volcano plot for\", name),\n                xlab = \"log2 Fold Change\", ylab = \"-10*log10(P-value)\",\n                type = \"p\", col=NA)\n            res.1 <- res\n            if(nrow(res.1)>0) points(x=res.1\\$logFC, y=res.1\\$qvalue, pch = 20, cex=.5, col=\"gray80\")\n            if(nrow(res.s)>0) points(x=res.s\\$logFC, y=res.s\\$qvalue, pch = 19, cex=.5, col=ifelse(res.s\\$logFC>0, \"brown\", \"darkblue\"))\n            dev.off()\n        })\n    }\n    \"\"\"\n}",
        "nb_lignes_process": 208,
        "string_script": "    prefix   = task.ext.prefix ?: \"diffhic_bin${bin_size}\"\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on April. 29, 2021 call edgeR\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    library(edgeR)\n    versions <- c(\n        \"${task.process}:\",\n        paste(\"    edgeR:\", as.character(packageVersion(\"edgeR\"))))\n    writeLines(versions, \"versions.yml\")\n\n    binsize = \"$prefix\"\n\n    ## get peaks\n    pf <- dir(\"peaks\", \"bedpe\", full.names = TRUE)\n    peaks <- lapply(pf, read.delim)\n    ### reduce the peaks\n    peaks <- unique(do.call(rbind, peaks)[, c(\"chr1\", \"start1\", \"end1\",\n                                            \"chr2\", \"start2\", \"end2\")])\n\n    ## get counts\n    pc <- dir(\"long\", \"bedpe\", full.names = FALSE)\n    cnts <- lapply(file.path(\"long\", pc), read.table)\n    samples <- sub(\"(_REP\\\\\\\\d+)\\\\\\\\.(.*?)\\\\\\\\.long.intra.bedpe\", \"\\\\\\\\1\", pc)\n    cnts <- lapply(split(cnts, samples), do.call, what=rbind)\n    sizeFactor <- vapply(cnts, FUN=function(.ele) sum(.ele[, 7], na.rm = TRUE),\n                        FUN.VALUE = numeric(1))\n\n    getID <- function(mat) gsub(\"\\\\\\\\s+\", \"\", apply(mat[, seq.int(6)], 1, paste, collapse=\"_\"))\n    getID1 <- function(mat) gsub(\"\\\\\\\\s+\", \"\", apply(mat[, seq.int(3)], 1, paste, collapse=\"_\"))\n    getID2 <- function(mat) gsub(\"\\\\\\\\s+\", \"\", apply(mat[, 4:6], 1, paste, collapse=\"_\"))\n    ## prefilter, to decrease the memory cost\n    peaks_id1 <- getID1(peaks)\n    peaks_id2 <- getID2(peaks)\n    cnts <- lapply(cnts, function(.ele) .ele[getID1(.ele) %in% peaks_id1, , drop=FALSE])\n    cnts <- lapply(cnts, function(.ele) .ele[getID2(.ele) %in% peaks_id2, , drop=FALSE])\n    rm(peaks_id1, peaks_id2, getID1, getID2)\n    ## match all the counts for peaks\n    peaks_id <- getID(peaks)\n    cnts <- do.call(cbind, lapply(cnts, function(.ele){\n        .ele[match(peaks_id, getID(.ele)), 7]\n    }))\n    cnts[is.na(cnts)] <- 0\n    names(peaks_id) <- paste0(rep(\"p\", length(peaks_id)), seq_along(peaks_id))\n    rownames(cnts) <- names(peaks_id)\n\n    pf <- as.character(binsize)\n    dir.create(pf, showWarnings = FALSE, recursive=TRUE)\n\n    fname <- function(subf, ext, ...){ # create file name\n        pff <- ifelse(is.na(subf), pf, file.path(pf, subf))\n        dir.create(pff, showWarnings = FALSE, recursive = TRUE)\n        file.path(pff, paste(..., ext, sep=\".\"))\n    }\n\n    ## write counts\n    write.csv(cbind(peaks, cnts), fname(NA, \"csv\", \"raw.counts\"), row.names = FALSE)\n    ## write sizeFactors\n    write.csv(sizeFactor, fname(NA, \"csv\", \"library.size\"), row.names = TRUE)\n\n    ## coldata\n    sampleNames <- colnames(cnts)\n    condition <- make.names(sub(\"_REP.*\\$\", \"\", sampleNames), allow_=TRUE)\n    coldata <- data.frame(condition=factor(condition),\n                        row.names = sampleNames)\n    ## write designtable\n    write.csv(coldata, fname(NA, \"csv\", \"designTab\"), row.names = TRUE)\n\n    contrasts.lev <- levels(coldata\\$condition)\n\n    if(length(contrasts.lev)>1 && any(table(condition)>1)){\n        contrasts <- combn(contrasts.lev, 2, simplify = FALSE) ## pair all conditions\n        ## create DGEList\n        group <- coldata\\$condition\n        y <- DGEList(counts = cnts,\n                    lib.size = sizeFactor,\n                    group = group)\n\n        ## do differential analysis\n        names(contrasts) <- vapply(contrasts,\n                                    FUN=paste,\n                                    FUN.VALUE = character(1),\n                                    collapse = \"-\")\n        y <- calcNormFactors(y)\n        design <- model.matrix(~0+group)\n        colnames(design) <- levels(y\\$samples\\$group)\n        y <- estimateDisp(y,design)\n        fit <- glmQLFit(y, design)\n\n        ## PCA\n        pdf(fname(NA, \"pdf\", \"Multidimensional.scaling.plot-plot\"))\n        mds <- plotMDS(y)\n        dev.off()\n        ## PCA for multiQC\n        try_res <- try({ ## try to output PCA results for multiQC\n            json <- data.frame(x=mds\\$x, y=mds\\$y)\n            rownames(json) <- names(mds\\$x)\n            json <- split(json, coldata[rownames(json), \"condition\"])\n            json <- mapply(json, rainbow(n=length(json)), FUN=function(.ele, .color){\n                .ele <- cbind(.ele, \"name\"=rownames(.ele))\n                .ele <- apply(.ele, 1, function(.e){\n                    x <- names(.e)\n                    y <- .e\n                    .e <- paste0('{\"x\":', .e[1],\n                                ', \"y\":', .e[2],\n                                ', \"color\":\"', .color,\n                                '\", \"name\":\"', .e[3],\n                                '\"}')\n                })\n                .ele <- paste(.ele, collapse=\", \")\n                .ele <- paste(\"[\", .ele, \"]\")\n            })\n            json <- paste0('\"', names(json), '\" :', json)\n            json <- c(\n                    \"{\",\n                    '\"id\":\"sample_pca\",',\n                    '\"data\":{',\n                    paste(unlist(json), collapse=\", \"),\n                    \"}\",\n                    \"}\")\n            writeLines(json, fname(NA, \"json\", \"Multidimensional.scaling.qc\"))\n        })\n        if(inherits(try_res, \"try-error\")){\n            message(try_res)\n        }\n\n        ## plot dispersion\n        pdf(fname(NA, \"pdf\", \"DispersionEstimate-plot\"))\n        plotBCV(y)\n        dev.off()\n        ## plot QL dispersions\n        pdf(fname(NA, \"pdf\", \"Quasi-Likelihood-DispersionEstimate-plot\"))\n        plotQLDisp(fit)\n        dev.off()\n\n        res <- mapply(contrasts, names(contrasts), FUN = function(cont, name){\n            BvsA <- makeContrasts(contrasts = name, levels = design)\n            qlf <- glmQLFTest(fit, contrast = BvsA)\n            rs <- topTags(qlf, n = nrow(qlf), sort.by = \"none\")\n            ## MD-plot\n            pdf(fname(name, \"pdf\", \"Mean-Difference-plot\", name))\n            plotMD(qlf)\n            abline(h=0, col=\"red\", lty=2, lwd=2)\n            dev.off()\n            ## PValue distribution\n            pdf(fname(name, \"pdf\", \"PValue-distribution-plot\", name))\n            hist(rs\\$table\\$PValue, breaks = 20)\n            dev.off()\n            ## save res\n            res <- as.data.frame(rs)\n            res <- cbind(peaks, res[names(peaks_id), ])\n            write.csv(res, fname(name, \"csv\", \"edgeR.DEtable\", name), row.names = FALSE)\n            ## save metadata\n            elementMetadata <- do.call(rbind, lapply(c(\"adjust.method\",\"comparison\",\"test\"), function(.ele) rs[[.ele]]))\n            rownames(elementMetadata) <- c(\"adjust.method\",\"comparison\",\"test\")\n            colnames(elementMetadata)[1] <- \"value\"\n            write.csv(elementMetadata, fname(name, \"csv\", \"edgeR.metadata\", name), row.names = TRUE)\n            ## save subset results\n            res.s <- res[res\\$FDR<0.05 & abs(res\\$logFC)>1, ]\n            write.csv(res.s, fname(name, \"csv\", \"edgeR.DEtable\", name, \"padj0.05.lfc1\"), row.names = FALSE)\n            ## Volcano plot\n            res\\$qvalue <- -10*log10(res\\$PValue)\n            res.s\\$qvalue <- -10*log10(res.s\\$PValue)\n            pdf(fname(name, \"pdf\", \"Volcano-plot\", name))\n            plot(x=res\\$logFC, y=res\\$qvalue,\n                main = paste(\"Volcano plot for\", name),\n                xlab = \"log2 Fold Change\", ylab = \"-10*log10(P-value)\",\n                type = \"p\", col=NA)\n            res.1 <- res\n            if(nrow(res.1)>0) points(x=res.1\\$logFC, y=res.1\\$qvalue, pch = 20, cex=.5, col=\"gray80\")\n            if(nrow(res.s)>0) points(x=res.s\\$logFC, y=res.s\\$qvalue, pch = 19, cex=.5, col=ifelse(res.s\\$logFC>0, \"brown\", \"darkblue\"))\n            dev.off()\n            res\\$qvalue <- -10*log10(res\\$PValue)\n            png(fname(name, \"png\", \"Volcano-plot\", name))\n            plot(x=res\\$logFC, y=res\\$qvalue,\n                main = paste(\"Volcano plot for\", name),\n                xlab = \"log2 Fold Change\", ylab = \"-10*log10(P-value)\",\n                type = \"p\", col=NA)\n            res.1 <- res\n            if(nrow(res.1)>0) points(x=res.1\\$logFC, y=res.1\\$qvalue, pch = 20, cex=.5, col=\"gray80\")\n            if(nrow(res.s)>0) points(x=res.s\\$logFC, y=res.s\\$qvalue, pch = 19, cex=.5, col=ifelse(res.s\\$logFC>0, \"brown\", \"darkblue\"))\n            dev.off()\n        })\n    }\n    \"\"\"",
        "nb_lignes_script": 188,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "bin_size",
            "peaks",
            "long_bedpe"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$bin_size\"",
            "label 'process_medium'",
            "label 'error_ignore'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-edger=3.32.1\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-edger:3.32.1--r40h399db7b_0' : 'quay.io/biocontainers/bioconductor-edger:3.32.1--r40h399db7b_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "SHIFT_READS": {
        "name_process": "SHIFT_READS",
        "string_process": "process SHIFT_READS {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"anaconda::gawk=5.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gawk:5.1.0' :\n        'quay.io/biocontainers/gawk:5.1.0' }\"\n\n    input:\n    tuple val(meta), path(pair)\n    val do_shift\n\n    output:\n    tuple val(meta), path(\"*.bed.gz\"), emit: bed\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    def command  = do_shift ?\n        'BEGIN {OFS=\"\\t\"};  /^[^#]/ { if (\\$7 == \"+\") {\\$5 = \\$5 + 4} else if (\\$7 == \"-\") {\\$5 = \\$5 - 5};  print \\$4, \\$5, \\$5+1, \"*\", \"0\", \\$7}' :\n        'BEGIN {OFS=\"\\t\"};  /^[^#]/ { print \\$4, \\$5, \\$5+1, \"*\", \"0\", \\$7 }'\n    \"\"\"\n    gunzip -c $pair | \\\\\n        awk '$command' | \\\\\n        sort -k1,1 -k2,2n | \\\\\n        uniq | \\\\\n        gzip -nc > ${prefix}.R2.ATAC.bed.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        awk: \\$(echo \\$(awk --version 2>&1 || awk -W version 2>&1) | sed 's/[[:alpha:]|(|)|[:space:]]//g; s/,.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    def command  = do_shift ?\n        'BEGIN {OFS=\"\\t\"};  /^[^#]/ { if (\\$7 == \"+\") {\\$5 = \\$5 + 4} else if (\\$7 == \"-\") {\\$5 = \\$5 - 5};  print \\$4, \\$5, \\$5+1, \"*\", \"0\", \\$7}' :\n        'BEGIN {OFS=\"\\t\"};  /^[^#]/ { print \\$4, \\$5, \\$5+1, \"*\", \"0\", \\$7 }'\n    \"\"\"\n    gunzip -c $pair | \\\\\n        awk '$command' | \\\\\n        sort -k1,1 -k2,2n | \\\\\n        uniq | \\\\\n        gzip -nc > ${prefix}.R2.ATAC.bed.gz\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        awk: \\$(echo \\$(awk --version 2>&1 || awk -W version 2>&1) | sed 's/[[:alpha:]|(|)|[:space:]]//g; s/,.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "pair",
            "do_shift"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"anaconda::gawk=5.1.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/gawk:5.1.0' : 'quay.io/biocontainers/gawk:5.1.0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MAPS_MAPS": {
        "name_process": "MAPS_MAPS",
        "string_process": "process MAPS_MAPS{\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"pandas=1.1.5\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pandas:1.1.5' :\n        'quay.io/biocontainers/pandas:1.1.5' }\"\n\n    input:\n    tuple val(meta), val(bin_size), path(macs2), path(long_bedpe, stageAs: \"long/*\"), path(short_bed, stageAs: \"short/*\"), path(background)\n    path make_maps_runfile_source\n\n    output:\n    tuple val(meta), val(bin_size), path(macs2), path(long_bedpe), path(short_bed), path(background), path(\"${meta.id}_${bin_size}/*\"), emit: maps\n    path \"versions.yml\"          , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    ## 2 steps\n    ## step 1, prepare the config file for MAPS. The file will be used for multiple steps\n    mkdir -p \"${meta.id}_${bin_size}\"\n    python ${make_maps_runfile_source} \\\\\n        \"${meta.id}\" \\\\\n        \"${meta.id}_${bin_size}/\" \\\\\n        $macs2 \\\\\n        $background \\\\\n        \"long/\" \\\\\n        \"short/\" \\\\\n        $bin_size \\\\\n        0 \\\\\n        \"${meta.id}_${bin_size}/\" \\\\\n        $args\n    ## step 2, parse the signals into .xor and .and files, details please refer: doi:10.1371/journal.pcbi.1006982\n    ## by default, the sex chromosome will be excluded.\n    MAPS.py \"${meta.id}_${bin_size}/maps_${meta.id}.maps\"\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        MAPS: 1.1.0\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 43,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    ## 2 steps\n    ## step 1, prepare the config file for MAPS. The file will be used for multiple steps\n    mkdir -p \"${meta.id}_${bin_size}\"\n    python ${make_maps_runfile_source} \\\\\n        \"${meta.id}\" \\\\\n        \"${meta.id}_${bin_size}/\" \\\\\n        $macs2 \\\\\n        $background \\\\\n        \"long/\" \\\\\n        \"short/\" \\\\\n        $bin_size \\\\\n        0 \\\\\n        \"${meta.id}_${bin_size}/\" \\\\\n        $args\n    ## step 2, parse the signals into .xor and .and files, details please refer: doi:10.1371/journal.pcbi.1006982\n    ## by default, the sex chromosome will be excluded.\n    MAPS.py \"${meta.id}_${bin_size}/maps_${meta.id}.maps\"\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        MAPS: 1.1.0\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 24,
        "language_script": "bash",
        "tools": [
            "LONGO",
            "SHORTY"
        ],
        "tools_url": [
            "https://bio.tools/longo",
            "https://bio.tools/shorty"
        ],
        "tools_dico": [
            {
                "name": "LONGO",
                "uri": "https://bio.tools/longo",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2229",
                            "term": "Cell biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3304",
                            "term": "Neurobiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression data analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Gene Length-Dependent Expression Analysis Tool in Neuronal Cells.",
                "homepage": "https://github.com/biohpc/longo"
            },
            {
                "name": "SHORTY",
                "uri": "https://bio.tools/shorty",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0310",
                                    "term": "Sequence assembly"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Targetted for de novo assembly of microreads with mate pair information and sequencing errors. It has some novel approach and features in addressing the short read assembly problem.",
                "homepage": "http://www.cs.sunysb.edu/%7Eskiena/shorty/"
            }
        ],
        "inputs": [
            "meta",
            "bin_size",
            "macs2",
            "long_bedpe",
            "short_bed",
            "background",
            "make_maps_runfile_source"
        ],
        "nb_inputs": 7,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"pandas=1.1.5\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/pandas:1.1.5' : 'quay.io/biocontainers/pandas:1.1.5' }\""
        ],
        "when": "",
        "stub": ""
    },
    "PAIRTOOLS_DEDUP": {
        "name_process": "PAIRTOOLS_DEDUP",
        "string_process": "process PAIRTOOLS_DEDUP {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' :\n        'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\"\n\n    input:\n    tuple val(meta), path(input)\n\n    output:\n    tuple val(meta), path(\"*.pairs.gz\")  , emit: pairs\n    tuple val(meta), path(\"*.pairs.stat\"), emit: stat\n    path \"versions.yml\"                  , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pairtools dedup \\\\\n        $args \\\\\n        -o ${prefix}.pairs.gz \\\\\n        --output-stats ${prefix}.pairs.stat \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    pairtools dedup \\\\\n        $args \\\\\n        -o ${prefix}.pairs.gz \\\\\n        --output-stats ${prefix}.pairs.stat \\\\\n        $input\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairtools: \\$(pairtools --version 2>&1 | sed 's/pairtools.*version //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "input"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::pairtools=0.3.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/pairtools:0.3.0--py37hb9c2fc3_5' : 'quay.io/biocontainers/pairtools:0.3.0--py37hb9c2fc3_5' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "BEDFILES_SORT": {
        "name_process": "BEDFILES_SORT",
        "string_process": "process BEDFILES_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"conda-forge::coreutils=8.31\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/coreutils:8.31--h14c3975_0' :\n        'quay.io/biocontainers/coreutils:8.31--h14c3975_0' }\"\n\n    input:\n    tuple val(meta), path(intervals)\n    val   extension\n\n    output:\n    tuple val(meta), path(\"*.${extension}\"), emit: sorted\n    path  \"versions.yml\"                   , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def buffer   = task.memory.toGiga().intdiv(2)\n    \"\"\"\n    ## ref: https://www.biostars.org/p/66927/\n    LC_ALL=C sort \\\\\n        --parallel=$task.cpus \\\\\n        --buffer-size=${buffer}G \\\\\n        -k1,1 -k2,2n \\\\\n        $intervals \\\\\n        > ${prefix}.${extension}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        sort: \\$(sort --version | tr '\\\\n' ' ' | sed -e \"s/^[^0-9]*//; s/ Copyright.*\\$//\")\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def prefix   = task.ext.prefix ?: \"${meta.id}\"\n    def buffer   = task.memory.toGiga().intdiv(2)\n    \"\"\"\n    ## ref: https://www.biostars.org/p/66927/\n    LC_ALL=C sort \\\\\n        --parallel=$task.cpus \\\\\n        --buffer-size=${buffer}G \\\\\n        -k1,1 -k2,2n \\\\\n        $intervals \\\\\n        > ${prefix}.${extension}\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        sort: \\$(sort --version | tr '\\\\n' ' ' | sed -e \"s/^[^0-9]*//; s/ Copyright.*\\$//\")\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "intervals",
            "extension"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"conda-forge::coreutils=8.31\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/coreutils:8.31--h14c3975_0' : 'quay.io/biocontainers/coreutils:8.31--h14c3975_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "PAIRSQC": {
        "name_process": "PAIRSQC",
        "string_process": "process PAIRSQC {\n    tag \"$meta.id\"\n    label 'process_low'\n    errorStrategy { (task.exitStatus in 137..140 && task.attempt <= 3)  ? 'retry' : 'ignore' }\n\n    conda (params.enable_conda ? \"bioconda::pairix=0.3.7\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/pairix:0.3.7--py36h30a8e3e_3' :\n        'quay.io/biocontainers/pairix:0.3.7--py36h30a8e3e_3' }\"\n\n    input:\n    tuple val(meta), path(pair), path(index)\n    path chrom_sizes\n\n    output:\n    tuple val(meta), path(\"${meta.id}_report/*\"), emit: qc\n    path \"versions.yml\"                          , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    \"\"\"\n    MAX_LOGDISTANCE=\\$( cat ${chrom_sizes} | awk '{ sum += \\$2 } END { printf \"%.1f\", log(sum)/log(10) }' )\n    pairsqc.py \\\\\n        -p $pair \\\\\n        -c $chrom_sizes -t P \\\\\n        -O $prefix \\\\\n        -s $prefix \\\\\n        -M \\$MAX_LOGDISTANCE\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairsqc: \"0.2.2\"\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def prefix   = task.ext.prefix ? \"${meta.id}${task.ext.prefix}\" : \"${meta.id}\"\n    \"\"\"\n    MAX_LOGDISTANCE=\\$( cat ${chrom_sizes} | awk '{ sum += \\$2 } END { printf \"%.1f\", log(sum)/log(10) }' )\n    pairsqc.py \\\\\n        -p $pair \\\\\n        -c $chrom_sizes -t P \\\\\n        -O $prefix \\\\\n        -s $prefix \\\\\n        -M \\$MAX_LOGDISTANCE\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        pairsqc: \"0.2.2\"\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "pair",
            "index",
            "chrom_sizes"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "errorStrategy { (task.exitStatus in 137..140 && task.attempt <= 3) ? 'retry' : 'ignore' }",
            "conda (params.enable_conda ? \"bioconda::pairix=0.3.7\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/pairix:0.3.7--py36h30a8e3e_3' : 'quay.io/biocontainers/pairix:0.3.7--py36h30a8e3e_3' }\""
        ],
        "when": "",
        "stub": ""
    },
    "READS_SUMMARY": {
        "name_process": "READS_SUMMARY",
        "string_process": "process READS_SUMMARY {\n    label 'process_low'\n\n    conda (params.enable_conda ? \"r::r-magrittr=1.5\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/r-magrittr:1.5--r3.2.2_0' :\n        'quay.io/biocontainers/r-magrittr:1.5--r3.2.2_0' }\"\n\n    input:\n    path stat\n\n    output:\n    path \"*.{csv,json}\"           , emit: summary\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n    versions <- c(\"${task.process}:\",\n        paste0(\"    R:\", paste(R.version\\$major, R.version\\$minor, sep=\".\")))\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    fs <- dir(\".\", \"*.reads_stats.csv\")\n    if(length(fs)>0){\n        df <- do.call(rbind, lapply(fs, read.csv))\n        con <- file(\"reads_summary.csv\", open=\"wt\")\n        write.csv(df, con, row.names=FALSE)\n        close(con)\n    }\n\n    fs <- dir(\".\", \"*.summary.out\\$\")\n    if(length(fs)>0){\n        df <- t(do.call(cbind, lapply(fs, read.delim, header=FALSE, row.names=1)))\n        df <- gsub(\",\", \"\", df)\n        mode(df) <- \"numeric\"\n        df <- cbind(sample=sub(\".summary.out\", \"\", basename(fs)), df)\n        con <- file(\"pairsqc_summary_out.csv\", open=\"wt\")\n        write.csv(df, con, row.names=FALSE)\n        close(con)\n    }\n\n    fs <- dir(\".\", \"^summary\")\n    if(length(fs)>0){\n        df <- do.call(rbind, lapply(fs, read.delim, header=TRUE, sep=\" \"))\n        df[, 1] <- sub(\"summary.(.*?).txt\", \"\\\\\\\\1\", basename(fs))\n        write.csv(df, \"MAPS_summary_out.csv\", row.names=FALSE)\n    }\n\n    fs <- dir(\".\", \"*.distance.vs.proportion.csv\\$\")\n    if(length(fs)>0){\n        dfs <- lapply(fs, function(.f){\n            x <- read.csv(.f)[, -2, drop=FALSE]\n            colnames(x)[-1] <- paste0(sub(\"distance.vs.proportion.csv\", \"\", .f), colnames(x)[-1])\n            x\n        })\n\n        json <- lapply(dfs, function(.ele){\n            ## convert to list, named as colnames, for the vectors in list, named as distance\n            .df <- as.data.frame(.ele[, -1])\n            .df <- as.list(.df)\n            .df <- lapply(.df, function(.e){\n                names(.e) <- .ele[, 1]\n                as.list(.e)\n            })\n            .df\n        })\n        json <- mapply(json, rainbow(n=length(json)), FUN=function(.ele, .color){\n            .ele <- sapply(.ele, function(.e){\n                x <- names(.e)\n                y <- .e\n                .e <- paste('{ \"x\":', x, ', \"y\":', y, ', \"color\":\"', .color, '\"', \"}\")\n                .e <- paste(.e, collapse=\", \")\n                paste(\"[\", .e, \"]\")\n            })\n            .ele <- paste0('\"', names(.ele), '\" : ', .ele)\n            .ele <- paste(.ele, collapse=\", \")\n        })\n        json <- c(\n                \"{\",\n                '\"id\":\"pairs_reads_proportion\",',\n                '\"data\":{',\n                paste(unlist(json), collapse=\", \"),\n                \"}\",\n                \"}\")\n        writeLines(json, \"dist.prop.qc.json\")\n\n        ## merge by first columns\n        mg <- function(...){\n            merge(..., all = TRUE)\n        }\n        dfs <- Reduce(mg, dfs)\n        write.table(t(dfs), \"dist.prop.csv\", col.names=FALSE, quote=FALSE, sep=\",\", row.names=TRUE)\n    }\n    \"\"\"\n}",
        "nb_lignes_process": 94,
        "string_script": "    \"\"\"\n    #!/usr/bin/env Rscript\n    versions <- c(\"${task.process}:\",\n        paste0(\"    R:\", paste(R.version\\$major, R.version\\$minor, sep=\".\")))\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    fs <- dir(\".\", \"*.reads_stats.csv\")\n    if(length(fs)>0){\n        df <- do.call(rbind, lapply(fs, read.csv))\n        con <- file(\"reads_summary.csv\", open=\"wt\")\n        write.csv(df, con, row.names=FALSE)\n        close(con)\n    }\n\n    fs <- dir(\".\", \"*.summary.out\\$\")\n    if(length(fs)>0){\n        df <- t(do.call(cbind, lapply(fs, read.delim, header=FALSE, row.names=1)))\n        df <- gsub(\",\", \"\", df)\n        mode(df) <- \"numeric\"\n        df <- cbind(sample=sub(\".summary.out\", \"\", basename(fs)), df)\n        con <- file(\"pairsqc_summary_out.csv\", open=\"wt\")\n        write.csv(df, con, row.names=FALSE)\n        close(con)\n    }\n\n    fs <- dir(\".\", \"^summary\")\n    if(length(fs)>0){\n        df <- do.call(rbind, lapply(fs, read.delim, header=TRUE, sep=\" \"))\n        df[, 1] <- sub(\"summary.(.*?).txt\", \"\\\\\\\\1\", basename(fs))\n        write.csv(df, \"MAPS_summary_out.csv\", row.names=FALSE)\n    }\n\n    fs <- dir(\".\", \"*.distance.vs.proportion.csv\\$\")\n    if(length(fs)>0){\n        dfs <- lapply(fs, function(.f){\n            x <- read.csv(.f)[, -2, drop=FALSE]\n            colnames(x)[-1] <- paste0(sub(\"distance.vs.proportion.csv\", \"\", .f), colnames(x)[-1])\n            x\n        })\n\n        json <- lapply(dfs, function(.ele){\n            ## convert to list, named as colnames, for the vectors in list, named as distance\n            .df <- as.data.frame(.ele[, -1])\n            .df <- as.list(.df)\n            .df <- lapply(.df, function(.e){\n                names(.e) <- .ele[, 1]\n                as.list(.e)\n            })\n            .df\n        })\n        json <- mapply(json, rainbow(n=length(json)), FUN=function(.ele, .color){\n            .ele <- sapply(.ele, function(.e){\n                x <- names(.e)\n                y <- .e\n                .e <- paste('{ \"x\":', x, ', \"y\":', y, ', \"color\":\"', .color, '\"', \"}\")\n                .e <- paste(.e, collapse=\", \")\n                paste(\"[\", .e, \"]\")\n            })\n            .ele <- paste0('\"', names(.ele), '\" : ', .ele)\n            .ele <- paste(.ele, collapse=\", \")\n        })\n        json <- c(\n                \"{\",\n                '\"id\":\"pairs_reads_proportion\",',\n                '\"data\":{',\n                paste(unlist(json), collapse=\", \"),\n                \"}\",\n                \"}\")\n        writeLines(json, \"dist.prop.qc.json\")\n\n        ## merge by first columns\n        mg <- function(...){\n            merge(..., all = TRUE)\n        }\n        dfs <- Reduce(mg, dfs)\n        write.table(t(dfs), \"dist.prop.csv\", col.names=FALSE, quote=FALSE, sep=\",\", row.names=TRUE)\n    }\n    \"\"\"",
        "nb_lignes_script": 77,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "stat"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "label 'process_low'",
            "conda (params.enable_conda ? \"r::r-magrittr=1.5\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/r-magrittr:1.5--r3.2.2_0' : 'quay.io/biocontainers/r-magrittr:1.5--r3.2.2_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "POST_COUNTS": {
        "name_process": "POST_COUNTS",
        "string_process": "process POST_COUNTS {\n    tag \"$meta.id\"\n    label 'process_high'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' :\n        'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\"\n\n    input:\n    tuple val(meta), path(counts), path(mappability), path(fasta), path(cut)\n\n    output:\n    tuple val(meta), path(\"*.csv\"), emit: counts\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    def args   = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on Aug. 24, 2021 count reads for peak filtering\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"rtracklayer\", \"InteractionSet\", \"Biostrings\", \"Rsamtools\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    ## options\n    ## make_option(c(\"-a\", \"--r1peak\"), type=\"character\", default=NULL, help=\"filename of r1 peak\", metavar=\"string\")\n    ## make_option(c(\"-b\", \"--r2peak\"), type=\"character\", default=NULL, help=\"filename of r2 peak\", metavar=\"string\")\n    ## make_option(c(\"-x\", \"--restrict\"), type=\"character\", default=NULL, help=\"filename of restrict cut\", metavar=\"string\")\n    ## make_option(c(\"-p\", \"--pairs\"), type=\"character\", default=NULL, help=\"folder of valid distal pairs\", metavar=\"string\")\n    ## make_option(c(\"-m\", \"--mappability\"), type=\"character\", default=NULL, help=\"mappability file\", metavar=\"string\")\n    ## make_option(c(\"-o\", \"--output\"), type=\"character\", default=\"counts.csv\", help=\"output folder\", metavar=\"string\")\n    ## make_option(c(\"-f\", \"--fasta\"), type=\"character\", default=NULL, help=\"genome fasta file\", metavar=\"string\")\n    ## make_option(c(\"-1\", \"--chrom1\"), type=\"character\", default=NULL, help=\"chromosome1\", metavar=\"string\")\n    ## make_option(c(\"-2\", \"--chrom2\"), type=\"character\", default=NULL, help=\"chromosome1\", metavar=\"string\")\n    OUTPUT <- \"counts.${meta.id}.csv\"\n    FASTA <- \"$fasta\"\n    CUT <- \"$cut\"\n    MAPPABILITY <- \"$mappability\"\n\n    getMscore <- function(gr){\n        gr1 <- split(unique(gr), as.character(seqnames(unique(gr))))\n        gr0 <- lapply(gr1, function(.ele) .ele[seq.int(min(50, length(.ele)))])\n        available_chr <- vapply(gr0, FUN=function(chr){\n            out <- try(ms <- import(MAPPABILITY, which=chr))\n            if(inherits(out, \"try-error\")) return(FALSE)\n            return(length(ms)>0)\n        }, FUN.VALUE=logical(1))\n        available_chr <- names(available_chr)[available_chr]\n        mscore <- import(MAPPABILITY, which=gr[seqnames(gr) %in% available_chr], as=\"RleList\")\n        chr <- intersect(names(mscore), names(gr1))\n        vw <- Views(mscore[chr], gr1[chr])\n        sc <- viewMeans(vw)\n        vs <- ranges(vw)\n        vs <- as(vs, \"GRanges\")\n        vs\\$score <- unlist(sc, use.names=FALSE)\n        ol <- findOverlaps(gr, vs, type=\"equal\")\n        ol <- ol[!duplicated(queryHits(ol))]\n        score <- vs[subjectHits(ol)]\\$score[match(seq_along(gr), queryHits(ol))]\n        score[is.na(score)] <- 0\n        score\n    }\n\n    ### load counts\n    gis_rds <- dir(\".\", pattern=\"counts.${meta.id}.*.rds\")\n    gis <- lapply(gis_rds, readRDS)\n    gis <- gis[lengths(gis)>0]\n    gis <- do.call(c, gis)\n    stopifnot(is(gis, \"GInteractions\"))\n    ### load gc content\n    fa_idx <- indexFa(file=FASTA)\n    fa <- FaFile(file=FASTA, index=fa_idx)\n    seqinfo(gis) <- seqinfo(fa)[seqlevels(gis)]\n    gis <- trim(gis)\n    gc1 <- letterFrequency(getSeq(fa, first(gis)), letters=\"CG\", as.prob=TRUE)\n    gc2 <- letterFrequency(getSeq(fa, second(gis)), letters=\"CG\", as.prob=TRUE)\n    gis\\$gc <- gc1 * gc2 + 1e-9\n    ### load enzyme cut number in R1\n    cut <- import(CUT)\n    start(cut) <- end(cut)\n    gis\\$cut <- countOverlaps(first(gis), cut)+0.1\n    ### load mapping score\n    m1 <- getMscore(first(gis))\n    m2 <- getMscore(second(gis))\n    gis\\$mappability <- m1*m2 + 1e-6\n    ### get distance of the anchors\n    gis\\$dist <- distance(first(gis), second(gis))+1\n    gis\\$dist[is.na(gis\\$dist)] <- max(gis\\$dist, na.rm=TRUE)*100 ##trans interactions\n    gis\\$length <- width(first(gis))*width(second(gis))\n\n    mm <- log(as.data.frame(mcols(gis)[, c(\"length\", \"cut\", \"gc\", \"mappability\", \"dist\", \"shortCount\")]))\n    mm <- cbind(as.data.frame(first(gis)), as.data.frame(second(gis)), gis\\$count, mm)\n    colnames(mm) <- c(\"chr1\", \"start1\", \"end1\", \"width1\", \"strand1\",\n                    \"chr2\", \"start2\", \"end2\", 'width2', \"strand2\",\n                    \"count\", \"logl\", \"logn\", \"loggc\", \"logm\", \"logdist\", 'logShortCount')\n    mm\\$strand1 <- NULL\n    mm\\$strand2 <- NULL\n    write.csv(mm, OUTPUT, row.names=FALSE)\n    \"\"\"\n}",
        "nb_lignes_process": 111,
        "string_script": "    def args   = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n    #######################################################################\n    #######################################################################\n    ## Created on Aug. 24, 2021 count reads for peak filtering\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    pkgs <- c(\"rtracklayer\", \"InteractionSet\", \"Biostrings\", \"Rsamtools\")\n    versions <- c(\"${task.process}:\")\n    for(pkg in pkgs){\n        # load library\n        library(pkg, character.only=TRUE)\n        # parepare for versions.yml\n        versions <- c(versions,\n            paste0(\"    \", pkg, \": \", as.character(packageVersion(pkg))))\n    }\n    writeLines(versions, \"versions.yml\") # wirte versions.yml\n\n    ## options\n    ## make_option(c(\"-a\", \"--r1peak\"), type=\"character\", default=NULL, help=\"filename of r1 peak\", metavar=\"string\")\n    ## make_option(c(\"-b\", \"--r2peak\"), type=\"character\", default=NULL, help=\"filename of r2 peak\", metavar=\"string\")\n    ## make_option(c(\"-x\", \"--restrict\"), type=\"character\", default=NULL, help=\"filename of restrict cut\", metavar=\"string\")\n    ## make_option(c(\"-p\", \"--pairs\"), type=\"character\", default=NULL, help=\"folder of valid distal pairs\", metavar=\"string\")\n    ## make_option(c(\"-m\", \"--mappability\"), type=\"character\", default=NULL, help=\"mappability file\", metavar=\"string\")\n    ## make_option(c(\"-o\", \"--output\"), type=\"character\", default=\"counts.csv\", help=\"output folder\", metavar=\"string\")\n    ## make_option(c(\"-f\", \"--fasta\"), type=\"character\", default=NULL, help=\"genome fasta file\", metavar=\"string\")\n    ## make_option(c(\"-1\", \"--chrom1\"), type=\"character\", default=NULL, help=\"chromosome1\", metavar=\"string\")\n    ## make_option(c(\"-2\", \"--chrom2\"), type=\"character\", default=NULL, help=\"chromosome1\", metavar=\"string\")\n    OUTPUT <- \"counts.${meta.id}.csv\"\n    FASTA <- \"$fasta\"\n    CUT <- \"$cut\"\n    MAPPABILITY <- \"$mappability\"\n\n    getMscore <- function(gr){\n        gr1 <- split(unique(gr), as.character(seqnames(unique(gr))))\n        gr0 <- lapply(gr1, function(.ele) .ele[seq.int(min(50, length(.ele)))])\n        available_chr <- vapply(gr0, FUN=function(chr){\n            out <- try(ms <- import(MAPPABILITY, which=chr))\n            if(inherits(out, \"try-error\")) return(FALSE)\n            return(length(ms)>0)\n        }, FUN.VALUE=logical(1))\n        available_chr <- names(available_chr)[available_chr]\n        mscore <- import(MAPPABILITY, which=gr[seqnames(gr) %in% available_chr], as=\"RleList\")\n        chr <- intersect(names(mscore), names(gr1))\n        vw <- Views(mscore[chr], gr1[chr])\n        sc <- viewMeans(vw)\n        vs <- ranges(vw)\n        vs <- as(vs, \"GRanges\")\n        vs\\$score <- unlist(sc, use.names=FALSE)\n        ol <- findOverlaps(gr, vs, type=\"equal\")\n        ol <- ol[!duplicated(queryHits(ol))]\n        score <- vs[subjectHits(ol)]\\$score[match(seq_along(gr), queryHits(ol))]\n        score[is.na(score)] <- 0\n        score\n    }\n\n    ### load counts\n    gis_rds <- dir(\".\", pattern=\"counts.${meta.id}.*.rds\")\n    gis <- lapply(gis_rds, readRDS)\n    gis <- gis[lengths(gis)>0]\n    gis <- do.call(c, gis)\n    stopifnot(is(gis, \"GInteractions\"))\n    ### load gc content\n    fa_idx <- indexFa(file=FASTA)\n    fa <- FaFile(file=FASTA, index=fa_idx)\n    seqinfo(gis) <- seqinfo(fa)[seqlevels(gis)]\n    gis <- trim(gis)\n    gc1 <- letterFrequency(getSeq(fa, first(gis)), letters=\"CG\", as.prob=TRUE)\n    gc2 <- letterFrequency(getSeq(fa, second(gis)), letters=\"CG\", as.prob=TRUE)\n    gis\\$gc <- gc1 * gc2 + 1e-9\n    ### load enzyme cut number in R1\n    cut <- import(CUT)\n    start(cut) <- end(cut)\n    gis\\$cut <- countOverlaps(first(gis), cut)+0.1\n    ### load mapping score\n    m1 <- getMscore(first(gis))\n    m2 <- getMscore(second(gis))\n    gis\\$mappability <- m1*m2 + 1e-6\n    ### get distance of the anchors\n    gis\\$dist <- distance(first(gis), second(gis))+1\n    gis\\$dist[is.na(gis\\$dist)] <- max(gis\\$dist, na.rm=TRUE)*100 ##trans interactions\n    gis\\$length <- width(first(gis))*width(second(gis))\n\n    mm <- log(as.data.frame(mcols(gis)[, c(\"length\", \"cut\", \"gc\", \"mappability\", \"dist\", \"shortCount\")]))\n    mm <- cbind(as.data.frame(first(gis)), as.data.frame(second(gis)), gis\\$count, mm)\n    colnames(mm) <- c(\"chr1\", \"start1\", \"end1\", \"width1\", \"strand1\",\n                    \"chr2\", \"start2\", \"end2\", 'width2', \"strand2\",\n                    \"count\", \"logl\", \"logn\", \"loggc\", \"logm\", \"logdist\", 'logShortCount')\n    mm\\$strand1 <- NULL\n    mm\\$strand2 <- NULL\n    write.csv(mm, OUTPUT, row.names=FALSE)\n    \"\"\"",
        "nb_lignes_script": 93,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "counts",
            "mappability",
            "fasta",
            "cut"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-trackviewer=1.28.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-trackviewer:1.28.0--r41h399db7b_0' : 'quay.io/biocontainers/bioconductor-trackviewer:1.28.0--r41h399db7b_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "CIRCOS_PREPARE": {
        "name_process": "CIRCOS_PREPARE",
        "string_process": "process CIRCOS_PREPARE {\n    tag \"$meta.id\"\n    label 'process_medium'\n    label 'error_ignore'\n\n    conda (params.enable_conda ? \"bioconda::bioconductor-rtracklayer=1.50.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/bioconductor-rtracklayer:1.50.0--r40h7f5ccec_2' :\n        'quay.io/biocontainers/bioconductor-rtracklayer:1.50.0--r40h7f5ccec_2' }\"\n\n    input:\n    tuple val(meta), path(bedpe), val(ucscname), path(gtf), path(chromsize)\n\n    output:\n    tuple val(meta), path(\"${meta.id}/*\")           , emit: circos\n    path \"versions.yml\"                             , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on Oct. 16, 2021 prepare data for circos\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    options(scipen=10)\n    library(rtracklayer)\n    versions <- c(\n        \"${task.process}:\",\n        paste(\"    rtracklayer:\", as.character(packageVersion(\"rtracklayer\"))))\n    writeLines(versions, \"versions.yml\")\n\n    ## Options\n    ## make_option(c(\"-i\", \"--interaction\"), type=\"character\", default=NULL, help=\"interaction bedpe file\", metavar=\"string\")\n    ## make_option(c(\"-g\", \"--gtf\"), type=\"character\", default=NULL, help=\"annotation gtf file\", metavar=\"string\")\n    ## make_option(c(\"-c\", \"--chromsize\"), type=\"character\", default=NULL, help=\"filename of chromosome size\", metavar=\"string\")\n    ## make_option(c(\"-u\", \"--ucscname\"), type=\"character\", default=NULL, help=\"ucsc annotation name\", metavar=\"string\")\n    interaction <- \"$bedpe\"\n    chromsize <- \"$chromsize\"\n    gtf <- \"$gtf\"\n    ucscname <- \"$ucscname\"\n    outfolder <- \"${meta.id}\"\n    totalLinks <- 1e4\n\n    args <- strsplit(\"${args}\", \"\\\\\\\\s+\")[[1]]\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"pattern\"=c(\"--totalLinks\", \"-n\", \"numeric\"))\n    opt <- parse_args(option_list, args)\n    if(!is.null(opt[[\"totalLinks\"]])){\n        totalLinks <- opt[[\"totalLinks\"]]\n    }\n\n    dir.create(outfolder, showWarnings = FALSE)\n\n    headerline <- readLines(interaction, n=1)\n    if(grepl(\"start1\", headerline[1])){\n        ## output from MAPS\n        pe <- read.delim(interaction)\n        pe <- Pairs(GRanges(pe[, \"chr1\"], IRanges(pe[, \"start1\"]+1, pe[, \"end1\"])),\n                    GRanges(pe[, \"chr2\"], IRanges(pe[, \"start2\"]+1, pe[, \"end2\"])),\n                    score = pe[, \"ClusterNegLog10P\"])\n    }else{\n        pe <- import(interaction, format=\"BEDPE\")\n    }\n    seqlevelsStyle(first(pe)) <- seqlevelsStyle(second(pe)) <- \"UCSC\"\n    pes <- unique(pe[order(mcols(pe)\\$score, decreasing=TRUE)])\n    pes_cis <- pes[seqnames(first(pe))==seqnames(second(pe))]\n    pes_trans <- pes[seqnames(first(pe))!=seqnames(second(pe))]\n    if(length(pes_cis)>0){ # keep top events for plot, default 24K\n        pes <- pes_cis[seq.int(min(totalLinks, length(pes_cis)))]\n    }else{\n        stop(\"No data available for plot\")\n    }\n    if(length(pes_trans)>0){\n        ## keep top 24K links only. otherwise hard to plot.\n        pes <- sort(c(pes[seq.int(min(floor(totalLinks/2), length(pes_trans)))],\n                    pes_trans[seq.int(min(floor(totalLinks/2), length(pes_trans)))]))\n    }\n    out <- as.data.frame(pes)\n    scores <- sqrt(range(mcols(pe)\\$score)/10)\n    scores <- c(floor(scores[1]), ceiling(scores[2]))\n    cid <- cut(sqrt(mcols(pes)\\$score/10), breaks = seq(scores[1], scores[2]))\n    levels(cid) <- seq_along(levels(cid))\n    out <- cbind(out[, c(\"first.seqnames\", \"first.start\", \"first.end\",\n                        \"second.seqnames\", \"second.start\", \"second.end\")],\n                thickness=paste0(\"thickness=\", cid))\n\n    write.table(out, file.path(outfolder, \"link.txt\"),\n        quote=FALSE, col.names=FALSE, row.names=FALSE,\n        sep=\" \") ## output cis- and trans- interactions\n\n    ## create karyotype file\n    chromsize <- read.delim(chromsize, header=FALSE)\n    chromsize[, 1] <- as.character(chromsize[, 1])\n    seqlevelsStyle(chromsize[, 1]) <- \"UCSC\"\n    chromsize <- cbind(\"chr\", \"-\", chromsize[, c(1, 1)], 0, chromsize[, 2],\n                        paste0(\"chr\", sub(\"^chr\", \"\", chromsize[, 1])))\n    colnames(chromsize) <- c(\"chr\", \"-\", \"chrname\", \"chrlabel\",\n                            0, \"chrlen\", \"chrcolor\")\n    chromsize <- chromsize[order(as.numeric(sub(\"chr\", \"\", chromsize[, \"chrname\"])),\n                            chromsize[, \"chrname\"]), , drop=FALSE]\n    write.table(chromsize, file.path(outfolder, \"karyotype.tab\"),\n                quote=FALSE, col.names=FALSE, row.names=FALSE, sep=\" \")\n\n    getScore <- function(seql, rg){\n        gtile <- tileGenome(seqlengths = seql, tilewidth = 1e5)\n        gtile <- unlist(gtile)\n        gtile <- split(gtile, seqnames(gtile))\n        sharedChr <- intersect(names(rg), names(gtile))\n        rg <- rg[sharedChr]\n        gtile <- gtile[names(rg)]\n        vw <- Views(rg, gtile)\n        vm <- viewSums(vw, na.rm = TRUE)\n        stopifnot(identical(names(vm), names(gtile)))\n        gtile <- unlist(gtile, use.names = FALSE)\n        gtile\\$score <- unlist(vm, use.names = FALSE)\n        gtile\n    }\n    ## create link desity file\n    rg <- coverage(c(first(pe), second(pe)))\n    seql <- chromsize\\$chrlen\n    names(seql) <- chromsize\\$chrname\n    gtile <- getScore(seql, rg)\n    rg <- as.data.frame(gtile)\n    rg <- rg[rg\\$score>0, c(\"seqnames\", \"start\", \"end\", \"score\"), drop=FALSE]\n    write.table(rg, file.path(outfolder, \"hist.link.txt\"),\n                quote=FALSE, col.names=FALSE, row.names=FALSE,\n                sep=\" \")\n    seqn <- sort(as.character(unique(rg\\$seqnames)))[1]\n    labelA <- labelB <- c(seqn, 0, 5000)\n    labelA <- c(labelA, \"interaction-density\")\n    labelB <- c(labelB, \"exon-density\")\n    writeLines(labelA, file.path(outfolder, \"labelA.txt\"), sep=\" \")\n    writeLines(labelB, file.path(outfolder, \"labelB.txt\"), sep=\" \")\n\n    ## create exon desity file\n    gtf <- import(gtf)\n    gtf <- gtf[gtf\\$type %in% \"exon\"]\n    seqlevelsStyle(gtf) <- \"UCSC\"\n    rg <- coverage(gtf)\n    gtile <- getScore(seql, rg)\n    rg <- as.data.frame(gtile)\n    rg <- rg[rg\\$score>0, c(\"seqnames\", \"start\", \"end\", \"score\"), drop=FALSE]\n    write.table(rg, file.path(outfolder, \"hist.exon.txt\"),\n                quote=FALSE, col.names=FALSE, row.names=FALSE,\n                sep=\" \")\n    try_res <- try({\n        session <- browserSession()\n        genome(session) <- ucscname\n        ideo <- getTable(ucscTableQuery(session, table=\"cytoBandIdeo\"))\n        ideo <- ideo[ideo\\$chrom %in% chromsize\\$chrname, , drop=FALSE]\n        ideo <- data.frame(\"band\", ideo\\$chrom, ideo\\$name, ideo\\$name,\n                    ideo\\$chromStart, ideo\\$chromEnd, ideo\\$gieStain)\n        colnames(ideo) <- colnames(chromsize)\n        ideo <- rbind(chromsize, ideo)\n        for(i in c(\"chrname\", \"chrlabel\")){\n            ideo[ideo[, i]==\"\", i] <- make.names(ideo[ideo[, i]==\"\", 2], unique=TRUE)\n        }\n        write.table(ideo, file.path(outfolder, \"karyotype.tab\"),\n                    quote=FALSE, col.names=FALSE, row.names=FALSE, sep=\" \")\n    })\n    if(inherits(try_res, \"try-error\")){\n        message(try_res)\n    }\n    \"\"\"\n}",
        "nb_lignes_process": 181,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    #######################################################################\n    #######################################################################\n    ## Created on Oct. 16, 2021 prepare data for circos\n    ## Copyright (c) 2021 Jianhong Ou (jianhong.ou@gmail.com)\n    #######################################################################\n    #######################################################################\n    options(scipen=10)\n    library(rtracklayer)\n    versions <- c(\n        \"${task.process}:\",\n        paste(\"    rtracklayer:\", as.character(packageVersion(\"rtracklayer\"))))\n    writeLines(versions, \"versions.yml\")\n\n    ## Options\n    ## make_option(c(\"-i\", \"--interaction\"), type=\"character\", default=NULL, help=\"interaction bedpe file\", metavar=\"string\")\n    ## make_option(c(\"-g\", \"--gtf\"), type=\"character\", default=NULL, help=\"annotation gtf file\", metavar=\"string\")\n    ## make_option(c(\"-c\", \"--chromsize\"), type=\"character\", default=NULL, help=\"filename of chromosome size\", metavar=\"string\")\n    ## make_option(c(\"-u\", \"--ucscname\"), type=\"character\", default=NULL, help=\"ucsc annotation name\", metavar=\"string\")\n    interaction <- \"$bedpe\"\n    chromsize <- \"$chromsize\"\n    gtf <- \"$gtf\"\n    ucscname <- \"$ucscname\"\n    outfolder <- \"${meta.id}\"\n    totalLinks <- 1e4\n\n    args <- strsplit(\"${args}\", \"\\\\\\\\s+\")[[1]]\n    parse_args <- function(options, args){\n        out <- lapply(options, function(.ele){\n            if(any(.ele[-3] %in% args)){\n                if(.ele[3]==\"logical\"){\n                    TRUE\n                }else{\n                    id <- which(args %in% .ele[-3])[1]\n                    x <- args[id+1]\n                    mode(x) <- .ele[3]\n                    x\n                }\n            }\n        })\n    }\n    option_list <- list(\"pattern\"=c(\"--totalLinks\", \"-n\", \"numeric\"))\n    opt <- parse_args(option_list, args)\n    if(!is.null(opt[[\"totalLinks\"]])){\n        totalLinks <- opt[[\"totalLinks\"]]\n    }\n\n    dir.create(outfolder, showWarnings = FALSE)\n\n    headerline <- readLines(interaction, n=1)\n    if(grepl(\"start1\", headerline[1])){\n        ## output from MAPS\n        pe <- read.delim(interaction)\n        pe <- Pairs(GRanges(pe[, \"chr1\"], IRanges(pe[, \"start1\"]+1, pe[, \"end1\"])),\n                    GRanges(pe[, \"chr2\"], IRanges(pe[, \"start2\"]+1, pe[, \"end2\"])),\n                    score = pe[, \"ClusterNegLog10P\"])\n    }else{\n        pe <- import(interaction, format=\"BEDPE\")\n    }\n    seqlevelsStyle(first(pe)) <- seqlevelsStyle(second(pe)) <- \"UCSC\"\n    pes <- unique(pe[order(mcols(pe)\\$score, decreasing=TRUE)])\n    pes_cis <- pes[seqnames(first(pe))==seqnames(second(pe))]\n    pes_trans <- pes[seqnames(first(pe))!=seqnames(second(pe))]\n    if(length(pes_cis)>0){ # keep top events for plot, default 24K\n        pes <- pes_cis[seq.int(min(totalLinks, length(pes_cis)))]\n    }else{\n        stop(\"No data available for plot\")\n    }\n    if(length(pes_trans)>0){\n        ## keep top 24K links only. otherwise hard to plot.\n        pes <- sort(c(pes[seq.int(min(floor(totalLinks/2), length(pes_trans)))],\n                    pes_trans[seq.int(min(floor(totalLinks/2), length(pes_trans)))]))\n    }\n    out <- as.data.frame(pes)\n    scores <- sqrt(range(mcols(pe)\\$score)/10)\n    scores <- c(floor(scores[1]), ceiling(scores[2]))\n    cid <- cut(sqrt(mcols(pes)\\$score/10), breaks = seq(scores[1], scores[2]))\n    levels(cid) <- seq_along(levels(cid))\n    out <- cbind(out[, c(\"first.seqnames\", \"first.start\", \"first.end\",\n                        \"second.seqnames\", \"second.start\", \"second.end\")],\n                thickness=paste0(\"thickness=\", cid))\n\n    write.table(out, file.path(outfolder, \"link.txt\"),\n        quote=FALSE, col.names=FALSE, row.names=FALSE,\n        sep=\" \") ## output cis- and trans- interactions\n\n    ## create karyotype file\n    chromsize <- read.delim(chromsize, header=FALSE)\n    chromsize[, 1] <- as.character(chromsize[, 1])\n    seqlevelsStyle(chromsize[, 1]) <- \"UCSC\"\n    chromsize <- cbind(\"chr\", \"-\", chromsize[, c(1, 1)], 0, chromsize[, 2],\n                        paste0(\"chr\", sub(\"^chr\", \"\", chromsize[, 1])))\n    colnames(chromsize) <- c(\"chr\", \"-\", \"chrname\", \"chrlabel\",\n                            0, \"chrlen\", \"chrcolor\")\n    chromsize <- chromsize[order(as.numeric(sub(\"chr\", \"\", chromsize[, \"chrname\"])),\n                            chromsize[, \"chrname\"]), , drop=FALSE]\n    write.table(chromsize, file.path(outfolder, \"karyotype.tab\"),\n                quote=FALSE, col.names=FALSE, row.names=FALSE, sep=\" \")\n\n    getScore <- function(seql, rg){\n        gtile <- tileGenome(seqlengths = seql, tilewidth = 1e5)\n        gtile <- unlist(gtile)\n        gtile <- split(gtile, seqnames(gtile))\n        sharedChr <- intersect(names(rg), names(gtile))\n        rg <- rg[sharedChr]\n        gtile <- gtile[names(rg)]\n        vw <- Views(rg, gtile)\n        vm <- viewSums(vw, na.rm = TRUE)\n        stopifnot(identical(names(vm), names(gtile)))\n        gtile <- unlist(gtile, use.names = FALSE)\n        gtile\\$score <- unlist(vm, use.names = FALSE)\n        gtile\n    }\n    ## create link desity file\n    rg <- coverage(c(first(pe), second(pe)))\n    seql <- chromsize\\$chrlen\n    names(seql) <- chromsize\\$chrname\n    gtile <- getScore(seql, rg)\n    rg <- as.data.frame(gtile)\n    rg <- rg[rg\\$score>0, c(\"seqnames\", \"start\", \"end\", \"score\"), drop=FALSE]\n    write.table(rg, file.path(outfolder, \"hist.link.txt\"),\n                quote=FALSE, col.names=FALSE, row.names=FALSE,\n                sep=\" \")\n    seqn <- sort(as.character(unique(rg\\$seqnames)))[1]\n    labelA <- labelB <- c(seqn, 0, 5000)\n    labelA <- c(labelA, \"interaction-density\")\n    labelB <- c(labelB, \"exon-density\")\n    writeLines(labelA, file.path(outfolder, \"labelA.txt\"), sep=\" \")\n    writeLines(labelB, file.path(outfolder, \"labelB.txt\"), sep=\" \")\n\n    ## create exon desity file\n    gtf <- import(gtf)\n    gtf <- gtf[gtf\\$type %in% \"exon\"]\n    seqlevelsStyle(gtf) <- \"UCSC\"\n    rg <- coverage(gtf)\n    gtile <- getScore(seql, rg)\n    rg <- as.data.frame(gtile)\n    rg <- rg[rg\\$score>0, c(\"seqnames\", \"start\", \"end\", \"score\"), drop=FALSE]\n    write.table(rg, file.path(outfolder, \"hist.exon.txt\"),\n                quote=FALSE, col.names=FALSE, row.names=FALSE,\n                sep=\" \")\n    try_res <- try({\n        session <- browserSession()\n        genome(session) <- ucscname\n        ideo <- getTable(ucscTableQuery(session, table=\"cytoBandIdeo\"))\n        ideo <- ideo[ideo\\$chrom %in% chromsize\\$chrname, , drop=FALSE]\n        ideo <- data.frame(\"band\", ideo\\$chrom, ideo\\$name, ideo\\$name,\n                    ideo\\$chromStart, ideo\\$chromEnd, ideo\\$gieStain)\n        colnames(ideo) <- colnames(chromsize)\n        ideo <- rbind(chromsize, ideo)\n        for(i in c(\"chrname\", \"chrlabel\")){\n            ideo[ideo[, i]==\"\", i] <- make.names(ideo[ideo[, i]==\"\", 2], unique=TRUE)\n        }\n        write.table(ideo, file.path(outfolder, \"karyotype.tab\"),\n                    quote=FALSE, col.names=FALSE, row.names=FALSE, sep=\" \")\n    })\n    if(inherits(try_res, \"try-error\")){\n        message(try_res)\n    }\n    \"\"\"",
        "nb_lignes_script": 162,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "ucscname",
            "bedpe",
            "gtf",
            "chromsize"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "label 'error_ignore'",
            "conda (params.enable_conda ? \"bioconda::bioconductor-rtracklayer=1.50.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/bioconductor-rtracklayer:1.50.0--r40h7f5ccec_2' : 'quay.io/biocontainers/bioconductor-rtracklayer:1.50.0--r40h7f5ccec_2' }\""
        ],
        "when": "",
        "stub": ""
    },
    "DUMPINTRAREADS": {
        "name_process": "DUMPINTRAREADS",
        "string_process": "process DUMPINTRAREADS {\n    tag \"${meta.id}\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"anaconda::gawk=5.1.0\" : null)\n    container \"${ workflow.containerEngine == 'singularity' &&\n                    !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/gawk:5.1.0' :\n        'quay.io/biocontainers/gawk:5.1.0' }\"\n\n    input:\n    tuple val(meta), path(bedpe)\n\n    output:\n    tuple val(meta), path(\"*.long.intra.bedpe\")            , emit: bedpe\n    tuple val(meta), path(\"*.ginteractions\")               , emit: gi\n    path  \"versions.yml\"                                   , emit: versions\n\n    script:\n    def prefix   = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    awk -F \"\\t\" \\\\\n        '{if(\\$1 == \\$4) {print > \"${prefix}.\"\\$1\".long.intra.bedpe\"} }' \\\\\n        $bedpe\n\n    awk -F \"\\t\" '{print 0, \\$1, \\$2, 0, 0, \\$4, \\$5, 1, \\$7}' $bedpe > \\\\\n        ${prefix}.${meta.bin}.ginteractions\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        awk: \\$(echo \\$(awk --version 2>&1) | sed -e \"s/GNU Awk //g; s/, API.*\\$//\")\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "    def prefix   = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    awk -F \"\\t\" \\\\\n        '{if(\\$1 == \\$4) {print > \"${prefix}.\"\\$1\".long.intra.bedpe\"} }' \\\\\n        $bedpe\n\n    awk -F \"\\t\" '{print 0, \\$1, \\$2, 0, 0, \\$4, \\$5, 1, \\$7}' $bedpe > \\\\\n        ${prefix}.${meta.bin}.ginteractions\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        awk: \\$(echo \\$(awk --version 2>&1) | sed -e \"s/GNU Awk //g; s/, API.*\\$//\")\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "bedpe"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"${meta.id}\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"anaconda::gawk=5.1.0\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/gawk:5.1.0' : 'quay.io/biocontainers/gawk:5.1.0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "FASTQC": {
        "name_process": "FASTQC",
        "string_process": "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}",
        "nb_lignes_process": 45,
        "string_script": "    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }",
        "nb_lignes_script": 24,
        "language_script": "bash",
        "tools": [
            "FastQC"
        ],
        "tools_url": [
            "https://bio.tools/fastqc"
        ],
        "tools_dico": [
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            }
        ],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' : 'quay.io/biocontainers/fastqc:0.11.9--0' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    },
    "SAMTOOLS_SORT": {
        "name_process": "SAMTOOLS_SORT",
        "string_process": "process SAMTOOLS_SORT {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' :\n        'quay.io/biocontainers/samtools:1.15--h1170115_1' }\"\n\n    input:\n    tuple val(meta), path(bam)\n\n    output:\n    tuple val(meta), path(\"*.bam\"), emit: bam\n    path  \"versions.yml\"          , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (\"$bam\" == \"${prefix}.bam\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools sort $args -@ $task.cpus -o ${prefix}.bam -T $prefix $bam\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2>&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "meta",
            "bam"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "jianhong__nf-core-hicar",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::samtools=1.15\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/samtools:1.15--h1170115_1' : 'quay.io/biocontainers/samtools:1.15--h1170115_1' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    }
}