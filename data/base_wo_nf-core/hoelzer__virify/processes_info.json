{
    "hmmscan": {
        "name_process": "hmmscan",
        "string_process": "process hmmscan {\n      publishDir \"${params.output}/${name}/${params.hmmerdir}/${params.db}\", mode: 'copy', pattern: \"${set_name}_${params.db}_hmmscan.tbl\"\n      label 'hmmscan'\n\n    input:\n      tuple val(name), val(set_name), file(faa) \n      file(db)\n    \n    output:\n      tuple val(name), val(set_name), file(\"${set_name}_${params.db}_hmmscan.tbl\"), file(faa)\n    \n    shell:\n    \"\"\"\n    if [[ ${params.db} == \"viphogs\" ]]; then\n      if [[ ${params.version} == \"v1\" ]]; then\n        hmmscan --cpu ${task.cpus} --noali -E \"0.001\" --domtblout ${set_name}_${params.db}_hmmscan.tbl ${db}/${db}.hmm ${faa}\n      else\n        hmmscan --cpu ${task.cpus} --noali --cut_ga --domtblout ${set_name}_${params.db}_hmmscan_cutga.tbl ${db}/${db}.hmm ${faa}\n        #filter evalue for models that dont have any GA cutoff\n        awk '{if(\\$1 ~ /^#/){print \\$0}else{if(\\$7<0.001){print \\$0}}}' ${set_name}_${params.db}_hmmscan_cutga.tbl > ${set_name}_${params.db}_hmmscan.tbl\n      fi\n    else\n      hmmscan --cpu ${task.cpus} --noali -E \"0.001\" --domtblout ${set_name}_${params.db}_hmmscan.tbl ${db}/${db}.hmm ${faa}\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 24,
        "string_script": "    \"\"\"\n    if [[ ${params.db} == \"viphogs\" ]]; then\n      if [[ ${params.version} == \"v1\" ]]; then\n        hmmscan --cpu ${task.cpus} --noali -E \"0.001\" --domtblout ${set_name}_${params.db}_hmmscan.tbl ${db}/${db}.hmm ${faa}\n      else\n        hmmscan --cpu ${task.cpus} --noali --cut_ga --domtblout ${set_name}_${params.db}_hmmscan_cutga.tbl ${db}/${db}.hmm ${faa}\n        #filter evalue for models that dont have any GA cutoff\n        awk '{if(\\$1 ~ /^#/){print \\$0}else{if(\\$7<0.001){print \\$0}}}' ${set_name}_${params.db}_hmmscan_cutga.tbl > ${set_name}_${params.db}_hmmscan.tbl\n      fi\n    else\n      hmmscan --cpu ${task.cpus} --noali -E \"0.001\" --domtblout ${set_name}_${params.db}_hmmscan.tbl ${db}/${db}.hmm ${faa}\n    fi\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "set_name",
            "faa",
            "db"
        ],
        "nb_inputs": 4,
        "outputs": [
            "set_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/${params.hmmerdir}/${params.db}\", mode: 'copy', pattern: \"${set_name}_${params.db}_hmmscan.tbl\"",
            "label 'hmmscan'"
        ],
        "when": "",
        "stub": ""
    },
    "restore": {
        "name_process": "restore",
        "string_process": "process restore {\n      publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"*_original.fasta\"\n      publishDir \"${params.output}/${name}/${params.finaldir}/contigs/\", mode: 'copy', pattern: \"*_original.fasta\"\n      label 'python3'\n\n    input:\n      tuple val(name), file(fasta), file(virsorter_meta), file(viruses_log), file(assembly), file(map) \n    \n    output:\n      tuple val(name), env(BN), file(\"*_original.fasta\")\n    \n    shell:\n    \"\"\"    \n    BN=\\$(basename ${fasta} .fna)\n    rename_fasta.py -i ${fasta} -m ${map} -o \\${BN}_original.fasta restore 2> /dev/null\n    \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "    \"\"\"    \n    BN=\\$(basename ${fasta} .fna)\n    rename_fasta.py -i ${fasta} -m ${map} -o \\${BN}_original.fasta restore 2> /dev/null\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "NullSeq"
        ],
        "tools_url": [
            "https://bio.tools/nullseq"
        ],
        "tools_dico": [
            {
                "name": "NullSeq",
                "uri": "https://bio.tools/nullseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0364",
                                    "term": "Random sequence generation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Creates Random Coding Sequences with specified GC content and Amino Acid usage.",
                "homepage": "https://github.com/amarallab/NullSeq"
            }
        ],
        "inputs": [
            "name",
            "fasta",
            "virsorter_meta",
            "viruses_log",
            "assembly",
            "map"
        ],
        "nb_inputs": 6,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"*_original.fasta\"",
            "publishDir \"${params.output}/${name}/${params.finaldir}/contigs/\", mode: 'copy', pattern: \"*_original.fasta\"",
            "label 'python3'"
        ],
        "when": "",
        "stub": ""
    },
    "spades": {
        "name_process": "spades",
        "string_process": "process spades {\n    label 'spades'  \n    publishDir \"${params.output}/${name}/${params.assemblydir}\", mode: 'copy', pattern: \"${name}.fasta\"\n  input:\n    tuple val(name), file(reads)\n  output:\n    tuple val(name), file(\"${name}.fasta\")\n  shell:\n    '''\n    spades.py --meta --only-assembler -1 !{reads[0]} -2 !{reads[1]} -t !{task.cpus} -o assembly\n    mv assembly/contigs.fasta !{name}.fasta\n    '''\n  }",
        "nb_lignes_process": 11,
        "string_script": "    '''\n    spades.py --meta --only-assembler -1 !{reads[0]} -2 !{reads[1]} -t !{task.cpus} -o assembly\n    mv assembly/contigs.fasta !{name}.fasta\n    '''",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'spades'",
            "publishDir \"${params.output}/${name}/${params.assemblydir}\", mode: 'copy', pattern: \"${name}.fasta\""
        ],
        "when": "",
        "stub": ""
    },
    "generate_krona_table": {
        "name_process": "generate_krona_table",
        "string_process": "process generate_krona_table {\n      publishDir \"${params.output}/${name}/${params.plotdir}\", mode: 'copy', pattern: \"*.krona.tsv\"\n      publishDir \"${params.output}/${name}/${params.finaldir}/krona/\", mode: 'copy', pattern: \"*.krona.tsv\"\n      label 'python3'\n\n    input:\n      tuple val(name), val(set_name), file(tbl)\n    \n    output:\n      tuple val(name), val(set_name), file(\"*.krona.tsv\")\n    \n    shell:\n    \"\"\"\n    if [[ \"${set_name}\" == \"all\" ]]; then\n      grep contig_ID *.tsv | awk 'BEGIN{FS=\":\"};{print \\$2}' | uniq > ${name}.tsv\n      grep -v \"contig_ID\" *.tsv | awk 'BEGIN{FS=\":\"};{print \\$2}' | uniq >> ${name}.tsv\n      generate_counts_table.py -f ${name}.tsv -o ${name}.krona.tsv\n    else\n      generate_counts_table.py -f ${tbl} -o ${set_name}.krona.tsv\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "    \"\"\"\n    if [[ \"${set_name}\" == \"all\" ]]; then\n      grep contig_ID *.tsv | awk 'BEGIN{FS=\":\"};{print \\$2}' | uniq > ${name}.tsv\n      grep -v \"contig_ID\" *.tsv | awk 'BEGIN{FS=\":\"};{print \\$2}' | uniq >> ${name}.tsv\n      generate_counts_table.py -f ${name}.tsv -o ${name}.krona.tsv\n    else\n      generate_counts_table.py -f ${tbl} -o ${set_name}.krona.tsv\n    fi\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "set_name",
            "tbl"
        ],
        "nb_inputs": 3,
        "outputs": [
            "set_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/${params.plotdir}\", mode: 'copy', pattern: \"*.krona.tsv\"",
            "publishDir \"${params.output}/${name}/${params.finaldir}/krona/\", mode: 'copy', pattern: \"*.krona.tsv\"",
            "label 'python3'"
        ],
        "when": "",
        "stub": ""
    },
    "krona": {
        "name_process": "krona",
        "string_process": "\nprocess krona {\n    publishDir \"${params.output}/${name}/${params.plotdir}/krona/\", mode: 'copy', pattern: \"*.krona.html\"\n    publishDir \"${params.output}/${name}/${params.finaldir}/krona/\", mode: 'copy', pattern: \"*.krona.html\"\n    label 'krona'  \n  input:\n    tuple val(name), val(set_name), file(krona_file)\n  output:\n    file(\"*.krona.html\")\n  script:\n    \"\"\"\n    if [[ ${set_name} == \"all\" ]]; then\n      ktImportText -o ${name}.krona.html ${krona_file}\n    else\n      ktImportText -o ${set_name}.krona.html ${krona_file}\n    fi\n    \"\"\"\n  }",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    if [[ ${set_name} == \"all\" ]]; then\n      ktImportText -o ${name}.krona.html ${krona_file}\n    else\n      ktImportText -o ${set_name}.krona.html ${krona_file}\n    fi\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "set_name",
            "krona_file"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/${params.plotdir}/krona/\", mode: 'copy', pattern: \"*.krona.html\"",
            "publishDir \"${params.output}/${name}/${params.finaldir}/krona/\", mode: 'copy', pattern: \"*.krona.html\"",
            "label 'krona'"
        ],
        "when": "",
        "stub": ""
    },
    "blast_filter": {
        "name_process": "blast_filter",
        "string_process": "process blast_filter {\n      publishDir \"${params.output}/${assembly_name}/${params.blastdir}/\", mode: 'copy', pattern: \"*.meta\"\n      publishDir \"${params.output}/${assembly_name}/${params.finaldir}/blast/\", mode: 'copy', pattern: \"*.meta\"\n      label 'ruby'\n\n      errorStrategy 'retry'\n      maxRetries 1\n\n    input:\n      tuple val(assembly_name), val(confidence_set_name), file(blast), file(blast_filtered)\n      file(db)\n    \n    output:\n      tuple val(assembly_name), val(confidence_set_name), file(\"*.meta\")\n    \n    shell:\n    if (task.attempt.toString() == '1')\n    \"\"\"\n      imgvr_merge.py -f ${blast_filtered} -d IMG_VR_2018-07-01_4/IMGVR_all_Sequence_information.tsv -o \\$(basename ${blast_filtered} .blast).meta\n    \"\"\"\n    else if (task.attempt.toString() == '2')\n    \"\"\"\n      imgvr_merge.py -f ${blast_filtered} -d ${db}/IMG_VR_2018-07-01_4/IMGVR_all_Sequence_information.tsv -o \\$(basename ${blast_filtered} .blast).meta\n    \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "    if (task.attempt.toString() == '1')\n    \"\"\"\n      imgvr_merge.py -f ${blast_filtered} -d IMG_VR_2018-07-01_4/IMGVR_all_Sequence_information.tsv -o \\$(basename ${blast_filtered} .blast).meta\n    \"\"\"\n    else if (task.attempt.toString() == '2')\n    \"\"\"\n      imgvr_merge.py -f ${blast_filtered} -d ${db}/IMG_VR_2018-07-01_4/IMGVR_all_Sequence_information.tsv -o \\$(basename ${blast_filtered} .blast).meta\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "assembly_name",
            "confidence_set_name",
            "blast",
            "blast_filtered",
            "db"
        ],
        "nb_inputs": 5,
        "outputs": [
            "confidence_set_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${assembly_name}/${params.blastdir}/\", mode: 'copy', pattern: \"*.meta\"",
            "publishDir \"${params.output}/${assembly_name}/${params.finaldir}/blast/\", mode: 'copy', pattern: \"*.meta\"",
            "label 'ruby'",
            "errorStrategy 'retry'",
            "maxRetries 1"
        ],
        "when": "",
        "stub": ""
    },
    "parse": {
        "name_process": "parse",
        "string_process": "process parse {\n      errorStrategy { task.exitStatus = 1 ? 'ignore' :  'terminate' }\n      publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"*.fna\"\n      publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"virsorter_metadata.tsv\"\n      publishDir \"${params.output}/${name}/${params.finaldir}/\", mode: 'copy', pattern: \"${name}_virus_predictions.log\"\n      label 'python3'\n\n    input:\n      tuple val(name), file(fasta), val(contig_number), file(virfinder), file(virsorter), file(pprmeta)\n\n    when: \n      contig_number.toInteger() > 0 \n\n    output:\n      tuple val(name), file(\"*.fna\"), file('virsorter_metadata.tsv'), file(\"${name}_virus_predictions.log\")\n    \n    shell:\n    \"\"\"\n    touch virsorter_metadata.tsv\n    parse_viral_pred.py -a ${fasta} -f ${virfinder} -s ${virsorter}/Predicted_viral_sequences/ -p ${pprmeta} &> ${name}_virus_predictions.log\n    \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "    \"\"\"\n    touch virsorter_metadata.tsv\n    parse_viral_pred.py -a ${fasta} -f ${virfinder} -s ${virsorter}/Predicted_viral_sequences/ -p ${pprmeta} &> ${name}_virus_predictions.log\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "contig_number",
            "fasta",
            "virfinder",
            "virsorter",
            "pprmeta"
        ],
        "nb_inputs": 6,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "errorStrategy { task.exitStatus = 1 ? 'ignore' : 'terminate' }",
            "publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"*.fna\"",
            "publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"virsorter_metadata.tsv\"",
            "publishDir \"${params.output}/${name}/${params.finaldir}/\", mode: 'copy', pattern: \"${name}_virus_predictions.log\"",
            "label 'python3'"
        ],
        "when": "contig_number.toInteger() > 0",
        "stub": ""
    },
    "phanotate": {
        "name_process": "phanotate",
        "string_process": "process phanotate {\n      publishDir \"${params.output}/${name}/${params.phanotatedir}\", mode: 'copy', pattern: \"*.faa\"\n      label 'phanotate'\n\n    input:\n      tuple val(name), file(fasta) \n    \n    output:\n      tuple val(name), stdout, file(\"*.faa\")\n    \n    shell:\n    \"\"\"\n    # this can be removed as soon as a conda recipe is available\n    git clone https://github.com/deprekate/PHANOTATE.git\n    cd PHANOTATE \n    git clone https://github.com/deprekate/fastpath.git\n    make\n\n    BN=\\$(basename ${fasta} .fna)\n    phanotate.py -f fasta -o \\${BN}_phanotate.fna ../${fasta} > /dev/null\n    transeq -sequence \\${BN}_phanotate.fna -outseq \\${BN}_phanotate.faa > /dev/null\n    cp \\${BN}_phanotate.faa ../\n    printf \"\\$BN\"\n    \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "    \"\"\"\n    # this can be removed as soon as a conda recipe is available\n    git clone https://github.com/deprekate/PHANOTATE.git\n    cd PHANOTATE \n    git clone https://github.com/deprekate/fastpath.git\n    make\n\n    BN=\\$(basename ${fasta} .fna)\n    phanotate.py -f fasta -o \\${BN}_phanotate.fna ../${fasta} > /dev/null\n    transeq -sequence \\${BN}_phanotate.fna -outseq \\${BN}_phanotate.faa > /dev/null\n    cp \\${BN}_phanotate.faa ../\n    printf \"\\$BN\"\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "NullSeq",
            "transeq"
        ],
        "tools_url": [
            "https://bio.tools/nullseq",
            "https://bio.tools/transeq"
        ],
        "tools_dico": [
            {
                "name": "NullSeq",
                "uri": "https://bio.tools/nullseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0364",
                                    "term": "Random sequence generation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Creates Random Coding Sequences with specified GC content and Amino Acid usage.",
                "homepage": "https://github.com/amarallab/NullSeq"
            },
            {
                "name": "transeq",
                "uri": "https://bio.tools/transeq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0371",
                                    "term": "DNA translation"
                                }
                            ],
                            []
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0849",
                                "term": "Sequence record"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2886",
                                "term": "Protein sequence record"
                            }
                        ]
                    }
                ],
                "description": "Translate nucleic acid sequences.",
                "homepage": "http://emboss.open-bio.org/rel/rel6/apps/transeq.html"
            }
        ],
        "inputs": [
            "name",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/${params.phanotatedir}\", mode: 'copy', pattern: \"*.faa\"",
            "label 'phanotate'"
        ],
        "when": "",
        "stub": ""
    },
    "filter_reads": {
        "name_process": "filter_reads",
        "string_process": "process filter_reads {\n      publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}.filtered.fastq\"\n      label 'ucsc'\n\n    input:\n      tuple val(name), file(kaiju_filtered), file(fastq) \n    \n    output:\n      tuple val(name), file(\"${name}.filtered.fastq\")\n      tuple val(name), file(\"${name}.filtered.fasta\")\n    \n    shell:\n    \"\"\"\n    sed '/^@/!d;s//>/;N' ${fastq} > ${name}.fasta\n    faSomeRecords ${name}.fasta ${kaiju_filtered} ${name}.filtered.fasta\n    faToFastq ${name}.filtered.fasta ${name}.filtered.fastq\n    rm -f ${name}.fasta\n    \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "    \"\"\"\n    sed '/^@/!d;s//>/;N' ${fastq} > ${name}.fasta\n    faSomeRecords ${name}.fasta ${kaiju_filtered} ${name}.filtered.fasta\n    faToFastq ${name}.filtered.fasta ${name}.filtered.fastq\n    rm -f ${name}.fasta\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "kaiju_filtered",
            "fastq"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name",
            "name"
        ],
        "nb_outputs": 2,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}.filtered.fastq\"",
            "label 'ucsc'"
        ],
        "when": "",
        "stub": ""
    },
    "blast": {
        "name_process": "blast",
        "string_process": "process blast {\n      publishDir \"${params.output}/${assembly_name}/${params.blastdir}/\", mode: 'copy', pattern: \"*.blast\"\n      publishDir \"${params.output}/${assembly_name}/${params.finaldir}/blast/\", mode: 'copy', pattern: \"*.filtered.blast\"\n      label 'blast'\n\n      errorStrategy 'retry'\n      maxRetries 1\n\n    input:\n      tuple val(assembly_name), val(confidence_set_name), file(fasta) \n      file(db)\n    \n    output:\n      tuple val(assembly_name), val(confidence_set_name), file(\"${confidence_set_name}.blast\"), file(\"${confidence_set_name}.filtered.blast\")\n    \n    shell:\n    if (task.attempt.toString() == '1')\n    \"\"\"\n      HEADER_BLAST=\"qseqid\\\\tsseqid\\\\tpident\\\\tlength\\\\tmismatch\\\\tgapopen\\\\tqstart\\\\tqend\\\\tqlen\\\\tsstart\\\\tsend\\\\tevalue\\\\tbitscore\\\\tslen\"\n      printf \"\\$HEADER_BLAST\\\\n\" > ${confidence_set_name}.blast\n\n      blastn -task blastn -num_threads ${task.cpus} -query ${fasta} -db IMG_VR_2018-07-01_4/IMGVR_all_nucleotides.fna -evalue 1e-10 -outfmt \"6 qseqid sseqid pident length mismatch gapopen qstart qend qlen sstart send evalue bitscore slen\" >> ${confidence_set_name}.blast\n      awk '{if(\\$4>0.8*\\$9){print \\$0}}' ${confidence_set_name}.blast >> ${confidence_set_name}.filtered.blast\n    \"\"\"\n    else if (task.attempt.toString() == '2')\n    \"\"\"\n      HEADER_BLAST=\"qseqid\\\\tsseqid\\\\tpident\\\\tlength\\\\tmismatch\\\\tgapopen\\\\tqstart\\\\tqend\\\\tqlen\\\\tsstart\\\\tsend\\\\tevalue\\\\tbitscore\\\\tslen\"\n      printf \"\\$HEADER_BLAST\\\\n\" > ${confidence_set_name}.blast\n\n      blastn -task blastn -num_threads ${task.cpus} -query ${fasta} -db ${db}/IMG_VR_2018-07-01_4/IMGVR_all_nucleotides.fna -evalue 1e-10 -outfmt \"6 qseqid sseqid pident length mismatch gapopen qstart qend qlen sstart send evalue bitscore slen\" >> ${confidence_set_name}.blast\n      awk '{if(\\$4>0.8*\\$9){print \\$0}}' ${confidence_set_name}.blast >> ${confidence_set_name}.filtered.blast\n    \"\"\"\n\n}",
        "nb_lignes_process": 32,
        "string_script": "    if (task.attempt.toString() == '1')\n    \"\"\"\n      HEADER_BLAST=\"qseqid\\\\tsseqid\\\\tpident\\\\tlength\\\\tmismatch\\\\tgapopen\\\\tqstart\\\\tqend\\\\tqlen\\\\tsstart\\\\tsend\\\\tevalue\\\\tbitscore\\\\tslen\"\n      printf \"\\$HEADER_BLAST\\\\n\" > ${confidence_set_name}.blast\n\n      blastn -task blastn -num_threads ${task.cpus} -query ${fasta} -db IMG_VR_2018-07-01_4/IMGVR_all_nucleotides.fna -evalue 1e-10 -outfmt \"6 qseqid sseqid pident length mismatch gapopen qstart qend qlen sstart send evalue bitscore slen\" >> ${confidence_set_name}.blast\n      awk '{if(\\$4>0.8*\\$9){print \\$0}}' ${confidence_set_name}.blast >> ${confidence_set_name}.filtered.blast\n    \"\"\"\n    else if (task.attempt.toString() == '2')\n    \"\"\"\n      HEADER_BLAST=\"qseqid\\\\tsseqid\\\\tpident\\\\tlength\\\\tmismatch\\\\tgapopen\\\\tqstart\\\\tqend\\\\tqlen\\\\tsstart\\\\tsend\\\\tevalue\\\\tbitscore\\\\tslen\"\n      printf \"\\$HEADER_BLAST\\\\n\" > ${confidence_set_name}.blast\n\n      blastn -task blastn -num_threads ${task.cpus} -query ${fasta} -db ${db}/IMG_VR_2018-07-01_4/IMGVR_all_nucleotides.fna -evalue 1e-10 -outfmt \"6 qseqid sseqid pident length mismatch gapopen qstart qend qlen sstart send evalue bitscore slen\" >> ${confidence_set_name}.blast\n      awk '{if(\\$4>0.8*\\$9){print \\$0}}' ${confidence_set_name}.blast >> ${confidence_set_name}.filtered.blast\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "G-BLASTN"
        ],
        "tools_url": [
            "https://bio.tools/g-blastn"
        ],
        "tools_dico": [
            {
                "name": "G-BLASTN",
                "uri": "https://bio.tools/g-blastn",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acids"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0346",
                                    "term": "Sequence similarity search"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2976",
                                "term": "Protein sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0857",
                                "term": "Sequence search results"
                            }
                        ]
                    }
                ],
                "description": "GPU-accelerated nucleotide alignment tool based on the widely used NCBI-BLAST.",
                "homepage": "http://www.comp.hkbu.edu.hk/~chxw/software/G-BLASTN.html"
            }
        ],
        "inputs": [
            "assembly_name",
            "confidence_set_name",
            "fasta",
            "db"
        ],
        "nb_inputs": 4,
        "outputs": [
            "confidence_set_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${assembly_name}/${params.blastdir}/\", mode: 'copy', pattern: \"*.blast\"",
            "publishDir \"${params.output}/${assembly_name}/${params.finaldir}/blast/\", mode: 'copy', pattern: \"*.filtered.blast\"",
            "label 'blast'",
            "errorStrategy 'retry'",
            "maxRetries 1"
        ],
        "when": "",
        "stub": ""
    },
    "pvogsGetDB": {
        "name_process": "pvogsGetDB",
        "string_process": "process pvogsGetDB {\n  label 'noDocker'    \n  if (params.cloudProcess) { \n    publishDir \"${params.cloudDatabase}/\", mode: 'copy', pattern: \"pvogs\" \n  }\n  else { \n    storeDir \"nextflow-autodownload-databases/\" \n  }  \n\n  output:\n    file(\"pvogs\")\n\n  script:\n    \"\"\"\n    wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/hmmer_databases/pvogs.tar.gz && tar -zxvf pvogs.tar.gz\n    rm pvogs.tar.gz\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/hmmer_databases/pvogs.tar.gz && tar -zxvf pvogs.tar.gz\n    rm pvogs.tar.gz\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'noDocker' if (params.cloudProcess) { publishDir \"${params.cloudDatabase}/\", mode: 'copy', pattern: \"pvogs\" } else { storeDir \"nextflow-autodownload-databases/\" }"
        ],
        "when": "",
        "stub": ""
    },
    "generate_chromomap_table": {
        "name_process": "generate_chromomap_table",
        "string_process": "process generate_chromomap_table {\n      publishDir \"${params.output}/${name}/${params.finaldir}/chromomap/\", mode: 'copy', pattern: \"${id}.filtered-*.contigs.txt\"\n      label 'ruby'\n\n    input:\n      tuple val(name), val(set_name), file(assembly), file(annotation_table)\n    \n    output:\n      tuple val(name), val(set_name), file(\"${id}.filtered-*.contigs.txt\"), file(\"${id}.filtered-*.anno.txt\")\n    \n    shell:\n    id = set_name\n    if (set_name == \"all\") { id = name }\n    \"\"\"\n    # combine\n    if [[ ${set_name} == \"all\" ]]; then\n      cat *.tsv | grep -v Abs_Evalue_exp | sort | uniq > ${id}.tsv\n      cat *.fasta > ${id}.fasta\n    else\n      cp ${annotation_table} ${id}.tsv\n      cp ${assembly} ${id}.fasta\n    fi\n\n    # now we remove contigs shorter 1500 kb and very long ones because ChromoMap has an error when plotting to distinct lenghts\n    # we also split into multiple files when we have many contigs, chunk size default 30\n    filter_for_chromomap.rb ${id}.fasta ${id}.tsv ${id}.contigs.txt ${id}.anno.txt 1500\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "    id = set_name\n    if (set_name == \"all\") { id = name }\n    \"\"\"\n    # combine\n    if [[ ${set_name} == \"all\" ]]; then\n      cat *.tsv | grep -v Abs_Evalue_exp | sort | uniq > ${id}.tsv\n      cat *.fasta > ${id}.fasta\n    else\n      cp ${annotation_table} ${id}.tsv\n      cp ${assembly} ${id}.fasta\n    fi\n\n    # now we remove contigs shorter 1500 kb and very long ones because ChromoMap has an error when plotting to distinct lenghts\n    # we also split into multiple files when we have many contigs, chunk size default 30\n    filter_for_chromomap.rb ${id}.fasta ${id}.tsv ${id}.contigs.txt ${id}.anno.txt 1500\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "MID"
        ],
        "tools_url": [
            "https://bio.tools/mid"
        ],
        "tools_dico": [
            {
                "name": "MID",
                "uri": "https://bio.tools/mid",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2423",
                                    "term": "Prediction and recognition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Cartography"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "MID (Micro-Inversion Detector) is a tool to detect microinversions (MIs) by mapping initially unmapped short reads back onto reference genome sequence (i.e. human genome assebly hg19).",
                "homepage": "http://cqb.pku.edu.cn/ZhuLab/MID/index.html"
            }
        ],
        "inputs": [
            "name",
            "set_name",
            "assembly",
            "annotation_table"
        ],
        "nb_inputs": 4,
        "outputs": [
            "set_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/${params.finaldir}/chromomap/\", mode: 'copy', pattern: \"${id}.filtered-*.contigs.txt\"",
            "label 'ruby'"
        ],
        "when": "",
        "stub": ""
    },
    "chromomap": {
        "name_process": "chromomap",
        "string_process": "\nprocess chromomap {\n    errorStrategy { task.exitStatus = 1 ? 'ignore' :  'terminate' }\n    publishDir \"${params.output}/${name}/${params.finaldir}/chromomap/\", mode: 'copy', pattern: \"*.html\"\n    label 'chromomap'\n\n    input:\n      tuple val(name), val(set_name), file(contigs), file(annotations)\n    \n    output:\n      tuple val(name), val(set_name), file(\"*.html\")\n    \n    shell:\n    id = set_name\n    if (set_name == \"all\") { id = name }\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(chromoMap)\n    library(ggplot2)\n    library(plotly)\n\n    contigs <- list()\n    annos <- list()\n    contigs <- dir(pattern = \"*.contigs.txt\")\n    annos <- dir(pattern = \"*.anno.txt\")\n\n    for (k in 1:length(contigs)){\n      c = contigs[k]\n      a = annos[k]\n      p <-  chromoMap(c, a,\n        data_based_color_map = T,\n        data_type = \"categorical\",\n        data_colors = list(c(\"limegreen\", \"orange\",\"grey\")),\n        legend = T, lg_y = 400, lg_x = 100, segment_annotation = T,\n        left_margin = 100, canvas_width = 1000, chr_length = 8, ch_gap = 6)\n      htmlwidgets::saveWidget(as_widget(p), paste(\"${id}.chromomap-\", k, \".html\", sep=''))\n    }    \n    \"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "    id = set_name\n    if (set_name == \"all\") { id = name }\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(chromoMap)\n    library(ggplot2)\n    library(plotly)\n\n    contigs <- list()\n    annos <- list()\n    contigs <- dir(pattern = \"*.contigs.txt\")\n    annos <- dir(pattern = \"*.anno.txt\")\n\n    for (k in 1:length(contigs)){\n      c = contigs[k]\n      a = annos[k]\n      p <-  chromoMap(c, a,\n        data_based_color_map = T,\n        data_type = \"categorical\",\n        data_colors = list(c(\"limegreen\", \"orange\",\"grey\")),\n        legend = T, lg_y = 400, lg_x = 100, segment_annotation = T,\n        left_margin = 100, canvas_width = 1000, chr_length = 8, ch_gap = 6)\n      htmlwidgets::saveWidget(as_widget(p), paste(\"${id}.chromomap-\", k, \".html\", sep=''))\n    }    \n    \"\"\"",
        "nb_lignes_script": 25,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "set_name",
            "contigs",
            "annotations"
        ],
        "nb_inputs": 4,
        "outputs": [
            "set_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "errorStrategy { task.exitStatus = 1 ? 'ignore' : 'terminate' }",
            "publishDir \"${params.output}/${name}/${params.finaldir}/chromomap/\", mode: 'copy', pattern: \"*.html\"",
            "label 'chromomap'"
        ],
        "when": "",
        "stub": ""
    },
    "kaiju": {
        "name_process": "kaiju",
        "string_process": "process kaiju {\n      publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}.out\"\n      label 'kaiju'\n\n    input:\n      tuple val(name), file(fastq) \n      file(database) \n    \n    output:\n      tuple val(name), file(\"${name}.out\")\n      tuple val(name), file(\"${name}.out.krona\")\n    \n    shell:\n      if (params.illumina) {\n      '''\n      kaiju -z !{task.cpus} -t !{database}/nodes.dmp -f !{database}/!{database}/kaiju_db_!{database}.fmi -i !{fastq[0]} -j !{fastq[1]} -o !{name}.out\n      kaiju2krona -t !{database}/nodes.dmp -n !{database}/names.dmp -i !{name}.out -o !{name}.out.krona\n      '''\n      } \n      if (params.fasta) {\n      '''\n      kaiju -z !{task.cpus} -t !{database}/nodes.dmp -f !{database}/!{database}/kaiju_db_!{database}.fmi -i !{fastq} -o !{name}.out\n      kaiju2krona -t !{database}/nodes.dmp -n !{database}/names.dmp -i !{name}.out -o !{name}.out.krona\n      '''\n      }\n}",
        "nb_lignes_process": 24,
        "string_script": "      if (params.illumina) {\n      '''\n      kaiju -z !{task.cpus} -t !{database}/nodes.dmp -f !{database}/!{database}/kaiju_db_!{database}.fmi -i !{fastq[0]} -j !{fastq[1]} -o !{name}.out\n      kaiju2krona -t !{database}/nodes.dmp -n !{database}/names.dmp -i !{name}.out -o !{name}.out.krona\n      '''\n      } \n      if (params.fasta) {\n      '''\n      kaiju -z !{task.cpus} -t !{database}/nodes.dmp -f !{database}/!{database}/kaiju_db_!{database}.fmi -i !{fastq} -o !{name}.out\n      kaiju2krona -t !{database}/nodes.dmp -n !{database}/names.dmp -i !{name}.out -o !{name}.out.krona\n      '''\n      }",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "Kaiju"
        ],
        "tools_url": [
            "https://bio.tools/kaiju"
        ],
        "tools_dico": [
            {
                "name": "Kaiju",
                "uri": "https://bio.tools/kaiju",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2975",
                                "term": "Nucleic acid sequence (raw)"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3028",
                                "term": "Taxonomy"
                            }
                        ]
                    }
                ],
                "description": "Program for the taxonomic assignment of high-throughput sequencing reads, e.g., Illumina or Roche/454, from whole-genome sequencing of metagenomic DNA. Reads are directly assigned to taxa using the NCBI taxonomy and a reference database of protein sequences from Bacteria, Archaea, Fungi, microbial eukaryotes and viruses.",
                "homepage": "http://kaiju.binf.ku.dk"
            }
        ],
        "inputs": [
            "name",
            "fastq",
            "database"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name",
            "name"
        ],
        "nb_outputs": 2,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}.out\"",
            "label 'kaiju'"
        ],
        "when": "",
        "stub": ""
    },
    "virsorterGetDB": {
        "name_process": "virsorterGetDB",
        "string_process": "process virsorterGetDB {\n  label 'noDocker'    \n  if (params.cloudProcess) { \n    publishDir \"${params.cloudDatabase}/virsorter/\", mode: 'copy', pattern: \"virsorter-data\" \n  }\n  else { \n    storeDir \"nextflow-autodownload-databases/virsorter/\" \n  }  \n\n  output:\n    file(\"virsorter-data\")\n  script:\n    \"\"\"\n    wget ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/virsorter-data-v2.tar.gz \n    tar -xvzf virsorter-data-v2.tar.gz\n    rm virsorter-data-v2.tar.gz\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    wget ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/virsorter-data-v2.tar.gz \n    tar -xvzf virsorter-data-v2.tar.gz\n    rm virsorter-data-v2.tar.gz\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'noDocker' if (params.cloudProcess) { publishDir \"${params.cloudDatabase}/virsorter/\", mode: 'copy', pattern: \"virsorter-data\" } else { storeDir \"nextflow-autodownload-databases/virsorter/\" }"
        ],
        "when": "",
        "stub": ""
    },
    "assign": {
        "name_process": "assign",
        "string_process": "process assign {\n      publishDir \"${params.output}/${name}/${params.taxdir}\", mode: 'copy', pattern: \"*tax_assign.tsv\"\n      publishDir \"${params.output}/${name}/${params.finaldir}/taxonomy\", mode: 'copy', pattern: \"*tax_assign.tsv\"\n      label 'assign'\n\n    input:\n      tuple val(name), val(set_name), file(tab)\n      file(db)\n    \n    output:\n      tuple val(name), val(set_name), file(\"*tax_assign.tsv\")\n    \n    shell:\n    \"\"\"\n    contig_taxonomic_assign.py -i ${tab} -d ${db}\n    \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "    \"\"\"\n    contig_taxonomic_assign.py -i ${tab} -d ${db}\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "set_name",
            "tab",
            "db"
        ],
        "nb_inputs": 4,
        "outputs": [
            "set_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/${params.taxdir}\", mode: 'copy', pattern: \"*tax_assign.tsv\"",
            "publishDir \"${params.output}/${name}/${params.finaldir}/taxonomy\", mode: 'copy', pattern: \"*tax_assign.tsv\"",
            "label 'assign'"
        ],
        "when": "",
        "stub": ""
    },
    "viphogGetDB": {
        "name_process": "viphogGetDB",
        "string_process": "process viphogGetDB {\n  label 'noDocker'    \n  if (params.cloudProcess) { \n    publishDir \"${params.cloudDatabase}/\", mode: 'copy', pattern: \"vpHMM_database_${params.version}\" \n  }\n  else { \n    storeDir \"nextflow-autodownload-databases/\" \n  }  \n\n  output:\n    file(\"vpHMM_database_${params.version}\")\n\n  script:\n    \"\"\"\n    wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/hmmer_databases/vpHMM_database_${params.version}.tar.gz && tar -zxvf vpHMM_database_${params.version}.tar.gz\n    rm vpHMM_database_${params.version}.tar.gz\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/hmmer_databases/vpHMM_database_${params.version}.tar.gz && tar -zxvf vpHMM_database_${params.version}.tar.gz\n    rm vpHMM_database_${params.version}.tar.gz\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'noDocker' if (params.cloudProcess) { publishDir \"${params.cloudDatabase}/\", mode: 'copy', pattern: \"vpHMM_database_${params.version}\" } else { storeDir \"nextflow-autodownload-databases/\" }"
        ],
        "when": "",
        "stub": ""
    },
    "fastp": {
        "name_process": "fastp",
        "string_process": "process fastp {\n    label 'fastp'  \n                                                                                                \n  input:\n    tuple val(name), file(reads)\n  output:\n    tuple val(name), file(\"${name}*.fastp.fastq.gz\")\n  script:\n    \"\"\"\n    fastp -i ${reads[0]} -I ${reads[1]} --thread ${task.cpus} -o ${name}.R1.fastp.fastq.gz -O ${name}.R2.fastp.fastq.gz\n    \"\"\"\n  }",
        "nb_lignes_process": 10,
        "string_script": "    \"\"\"\n    fastp -i ${reads[0]} -I ${reads[1]} --thread ${task.cpus} -o ${name}.R1.fastp.fastq.gz -O ${name}.R2.fastp.fastq.gz\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "fastPHASE"
        ],
        "tools_url": [
            "https://bio.tools/fastphase"
        ],
        "tools_dico": [
            {
                "name": "fastPHASE",
                "uri": "https://bio.tools/fastphase",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3056",
                            "term": "Population genetics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3454",
                                    "term": "Phasing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Imputation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Data imputation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "fastPHASE is a program to estimate missing genotypes and unobserved haplotypes. It is an implementation of the model described in Scheet & Stephens (2006). This is a cluster-based model for haplotype variation, and gains its utility from implicitly modeling the genealogy of chromosomes in a random sample from a population as a tree but summarizing all haplotype variation in the \"tips\" of the trees.",
                "homepage": "http://scheet.org/software.html"
            }
        ],
        "inputs": [
            "name",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'fastp'"
        ],
        "when": "",
        "stub": ""
    },
    "kaijuGetDB": {
        "name_process": "kaijuGetDB",
        "string_process": "process kaijuGetDB {\n  label 'noDocker'    \n  if (params.cloudProcess) { \n    publishDir \"${params.cloudDatabase}/kaiju/\", mode: 'copy', pattern: \"viruses\"//pattern: \"nr_euk\" \n  }\n  else { \n    storeDir \"nextflow-autodownload-databases/kaiju/\" \n  }  \n\n  output:\n                    \n    file(\"viruses\")\n\n  script:\n    \"\"\"\n    #this is the full database\n    if [ 42 == 0 ]; then\n    mkdir -p nr_euk\n    cd nr_euk\n    wget http://kaiju.binf.ku.dk/database/kaiju_db_nr_euk_2019-06-25.tgz \n    tar -xvzf kaiju_db_nr_euk_2019-06-25.tgz\n    rm kaiju_db_nr_euk_2019-06-25.tgz\n    fi\n\n    # for testing purpose download a smaller one\n    if [ 42 == 42 ]; then\n    mkdir -p viruses\n    cd viruses\n    wget http://kaiju.binf.ku.dk/database/kaiju_db_viruses_2019-06-25.tgz\n    tar -xvzf kaiju_db_viruses_2019-06-25.tgz\n    rm kaiju_db_viruses_2019-06-25.tgz\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "    \"\"\"\n    #this is the full database\n    if [ 42 == 0 ]; then\n    mkdir -p nr_euk\n    cd nr_euk\n    wget http://kaiju.binf.ku.dk/database/kaiju_db_nr_euk_2019-06-25.tgz \n    tar -xvzf kaiju_db_nr_euk_2019-06-25.tgz\n    rm kaiju_db_nr_euk_2019-06-25.tgz\n    fi\n\n    # for testing purpose download a smaller one\n    if [ 42 == 42 ]; then\n    mkdir -p viruses\n    cd viruses\n    wget http://kaiju.binf.ku.dk/database/kaiju_db_viruses_2019-06-25.tgz\n    tar -xvzf kaiju_db_viruses_2019-06-25.tgz\n    rm kaiju_db_viruses_2019-06-25.tgz\n    fi\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'noDocker' if (params.cloudProcess) { publishDir \"${params.cloudDatabase}/kaiju/\", mode: 'copy', pattern: \"viruses\"//pattern: \"nr_euk\" } else { storeDir \"nextflow-autodownload-databases/kaiju/\" }"
        ],
        "when": "",
        "stub": ""
    },
    "prodigal": {
        "name_process": "prodigal",
        "string_process": "process prodigal {\n      publishDir \"${params.output}/${assembly_name}/${params.prodigaldir}\", mode: 'copy', pattern: \"*.faa\"\n      publishDir \"${params.output}/${assembly_name}/${params.finaldir}/cds/\", mode: 'copy', pattern: \"*.faa\"\n      label 'prodigal'\n\n    input:\n      tuple val(assembly_name), val(confidence_set_name), file(fasta) \n    \n    output:\n      tuple val(assembly_name), val(confidence_set_name), file(\"*.faa\")\n    \n    shell:\n    \"\"\"\n    prodigal -p \"meta\" -a ${confidence_set_name}_prodigal.faa -i ${fasta}\n    \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "    \"\"\"\n    prodigal -p \"meta\" -a ${confidence_set_name}_prodigal.faa -i ${fasta}\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "assembly_name",
            "confidence_set_name",
            "fasta"
        ],
        "nb_inputs": 3,
        "outputs": [
            "confidence_set_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${assembly_name}/${params.prodigaldir}\", mode: 'copy', pattern: \"*.faa\"",
            "publishDir \"${params.output}/${assembly_name}/${params.finaldir}/cds/\", mode: 'copy', pattern: \"*.faa\"",
            "label 'prodigal'"
        ],
        "when": "",
        "stub": ""
    },
    "generate_sankey_table": {
        "name_process": "generate_sankey_table",
        "string_process": "process generate_sankey_table {\n      publishDir \"${params.output}/${name}/${params.plotdir}\", mode: 'copy', pattern: \"${set_name}.sankey.*\"\n      publishDir \"${params.output}/${name}/${params.finaldir}/sankey/\", mode: 'copy', pattern: \"${set_name}.sankey.filtered-${params.sankey}.json\"\n      label 'ruby'\n\n    input:\n      tuple val(name), val(set_name), file(krona_table)\n    \n    output:\n      tuple val(name), val(set_name), file(\"${set_name}.sankey.filtered-${params.sankey}.json\"), file(\"${set_name}.sankey.tsv\")\n    \n    shell:\n    \"\"\"\n    krona_table_2_sankey_table.rb ${krona_table} ${set_name}.sankey.tsv\n    \n    # select the top ${params.sankey} hits with highest count because otherwise sankey gets messy\n    sort -k1,1nr ${set_name}.sankey.tsv | head -${params.sankey} > ${set_name}.sankey.filtered.tsv\n\n    tsv2json.rb ${set_name}.sankey.tsv ${set_name}.sankey.filtered-${params.sankey}.json\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "    \"\"\"\n    krona_table_2_sankey_table.rb ${krona_table} ${set_name}.sankey.tsv\n    \n    # select the top ${params.sankey} hits with highest count because otherwise sankey gets messy\n    sort -k1,1nr ${set_name}.sankey.tsv | head -${params.sankey} > ${set_name}.sankey.filtered.tsv\n\n    tsv2json.rb ${set_name}.sankey.tsv ${set_name}.sankey.filtered-${params.sankey}.json\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "set_name",
            "krona_table"
        ],
        "nb_inputs": 3,
        "outputs": [
            "set_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/${params.plotdir}\", mode: 'copy', pattern: \"${set_name}.sankey.*\"",
            "publishDir \"${params.output}/${name}/${params.finaldir}/sankey/\", mode: 'copy', pattern: \"${set_name}.sankey.filtered-${params.sankey}.json\"",
            "label 'ruby'"
        ],
        "when": "",
        "stub": ""
    },
    "sankey": {
        "name_process": "sankey",
        "string_process": "\nprocess sankey {\n    publishDir \"${params.output}/${name}/${params.plotdir}\", mode: 'copy', pattern: \"*.sankey.html\"\n    publishDir \"${params.output}/${name}/${params.finaldir}/sankey/\", mode: 'copy', pattern: \"*.sankey.html\"\n    label 'sankey'\n\n    input:\n      tuple val(name), val(set_name), file(json), file(tsv)\n    \n    output:\n      tuple val(name), val(set_name), file(\"*.sankey.html\")\n    \n    shell:\n    id = set_name\n    if (set_name == \"all\") { id = name }\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(sankeyD3)\n    library(magrittr)\n\n    Taxonomy <- jsonlite::fromJSON(\"${json}\")\n\n    # print to HTML file\n    sankey = sankeyNetwork(Links = Taxonomy\\$links, Nodes = Taxonomy\\$nodes, Source = \"source\", Target = \"target\", Value = \"value\", NodeID = \"name\", units = \"count\", fontSize = 22, nodeWidth = 30, nodeShadow = TRUE, nodePadding = 30, nodeStrokeWidth = 1, nodeCornerRadius = 10, dragY = TRUE, dragX = TRUE, numberFormat = \",.3g\", align = \"left\", orderByPath = TRUE)\n    saveNetwork(sankey, file = '${id}.sankey.html')\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "    id = set_name\n    if (set_name == \"all\") { id = name }\n    \"\"\"\n    #!/usr/bin/env Rscript\n\n    library(sankeyD3)\n    library(magrittr)\n\n    Taxonomy <- jsonlite::fromJSON(\"${json}\")\n\n    # print to HTML file\n    sankey = sankeyNetwork(Links = Taxonomy\\$links, Nodes = Taxonomy\\$nodes, Source = \"source\", Target = \"target\", Value = \"value\", NodeID = \"name\", units = \"count\", fontSize = 22, nodeWidth = 30, nodeShadow = TRUE, nodePadding = 30, nodeStrokeWidth = 1, nodeCornerRadius = 10, dragY = TRUE, dragX = TRUE, numberFormat = \",.3g\", align = \"left\", orderByPath = TRUE)\n    saveNetwork(sankey, file = '${id}.sankey.html')\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "set_name",
            "json",
            "tsv"
        ],
        "nb_inputs": 4,
        "outputs": [
            "set_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/${params.plotdir}\", mode: 'copy', pattern: \"*.sankey.html\"",
            "publishDir \"${params.output}/${name}/${params.finaldir}/sankey/\", mode: 'copy', pattern: \"*.sankey.html\"",
            "label 'sankey'"
        ],
        "when": "",
        "stub": ""
    },
    "vogdbGetDB": {
        "name_process": "vogdbGetDB",
        "string_process": "process vogdbGetDB {\n  label 'noDocker'    \n  if (params.cloudProcess) { \n    publishDir \"${params.cloudDatabase}/\", mode: 'copy', pattern: \"vogdb\" \n  }\n  else { \n    storeDir \"nextflow-autodownload-databases/\" \n  }  \n\n  output:\n    file(\"vogdb\")\n\n  script:\n    \"\"\"\n    wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/hmmer_databases/vogdb.tar.gz && tar -zxvf vogdb.tar.gz\n    rm vogdb.tar.gz\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/hmmer_databases/vogdb.tar.gz && tar -zxvf vogdb.tar.gz\n    rm vogdb.tar.gz\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'noDocker' if (params.cloudProcess) { publishDir \"${params.cloudDatabase}/\", mode: 'copy', pattern: \"vogdb\" } else { storeDir \"nextflow-autodownload-databases/\" }"
        ],
        "when": "",
        "stub": ""
    },
    "ratio_evalue": {
        "name_process": "ratio_evalue",
        "string_process": "process ratio_evalue {\n      errorStrategy { task.exitStatus = 1 ? 'ignore' :  'terminate' }\n      publishDir \"${params.output}/${name}/ratio_evalue_tables\", mode: 'copy', pattern: \"${set_name}_modified_informative.tsv\"\n      label 'ratio_evalue'\n\n    input:\n      tuple val(name), val(set_name), file(modified_table), file(faa)\n      file(model_metadata)\n    \n    output:\n      tuple val(name), val(set_name), file(\"${set_name}_modified_informative.tsv\"), file(faa)\n    \n    shell:\n    \"\"\"\n    [ -d \"models\" ] && cp models/* .\n    ratio_evalue_table.py -i ${modified_table} -t ${model_metadata} -o .\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    [ -d \"models\" ] && cp models/* .\n    ratio_evalue_table.py -i ${modified_table} -t ${model_metadata} -o .\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "set_name",
            "modified_table",
            "faa",
            "model_metadata"
        ],
        "nb_inputs": 5,
        "outputs": [
            "set_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "errorStrategy { task.exitStatus = 1 ? 'ignore' : 'terminate' }",
            "publishDir \"${params.output}/${name}/ratio_evalue_tables\", mode: 'copy', pattern: \"${set_name}_modified_informative.tsv\"",
            "label 'ratio_evalue'"
        ],
        "when": "",
        "stub": ""
    },
    "metaGetDB": {
        "name_process": "metaGetDB",
        "string_process": "\nprocess metaGetDB {\n  label 'ratio_evalue'    \n  if (params.cloudProcess) { \n    publishDir \"${params.cloudDatabase}/models\", mode: 'copy', pattern: \"Additional_data_vpHMMs.dict\" \n  }\n  else { \n    storeDir \"nextflow-autodownload-databases/models\" \n  }  \n    \n    output:\n      file(\"Additional_data_vpHMMs.dict\")\n    \n    shell:\n    \"\"\"\n    # v2 of metadata file\n#    wget ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/Additional_data_vpHMMs_v2.xlsx\n#    generate_vphmm_object.py -x Additional_data_vpHMMs_v2.xlsx -o Additional_data_vpHMMs.dict\n\n    # v1 of metadata file\n    wget ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/Additional_data_vpHMMs.xlsx\n    generate_vphmm_object.py -x Additional_data_vpHMMs.xlsx -o Additional_data_vpHMMs.dict\n    \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "    \"\"\"\n    # v2 of metadata file\n#    wget ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/Additional_data_vpHMMs_v2.xlsx\n#    generate_vphmm_object.py -x Additional_data_vpHMMs_v2.xlsx -o Additional_data_vpHMMs.dict\n\n    # v1 of metadata file\n    wget ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/Additional_data_vpHMMs.xlsx\n    generate_vphmm_object.py -x Additional_data_vpHMMs.xlsx -o Additional_data_vpHMMs.dict\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'ratio_evalue' if (params.cloudProcess) { publishDir \"${params.cloudDatabase}/models\", mode: 'copy', pattern: \"Additional_data_vpHMMs.dict\" } else { storeDir \"nextflow-autodownload-databases/models\" }"
        ],
        "when": "",
        "stub": ""
    },
    "imgvrGetDB": {
        "name_process": "imgvrGetDB",
        "string_process": "process imgvrGetDB {\n  label 'noDocker'    \n  if (params.cloudProcess) { \n    publishDir \"${params.cloudDatabase}/imgvr/\", mode: 'copy', pattern: \"IMG_VR_2018-07-01_4\" \n  }\n  else { \n    storeDir \"nextflow-autodownload-databases/imgvr/\" \n  }  \n\n  output:\n    file(\"IMG_VR_2018-07-01_4\")\n\n  script:\n    \"\"\"\n    wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/IMG_VR_2018-07-01_4.tar.gz && tar zxvf IMG_VR_2018-07-01_4.tar.gz\n    \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "    \"\"\"\n    wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/IMG_VR_2018-07-01_4.tar.gz && tar zxvf IMG_VR_2018-07-01_4.tar.gz\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'noDocker' if (params.cloudProcess) { publishDir \"${params.cloudDatabase}/imgvr/\", mode: 'copy', pattern: \"IMG_VR_2018-07-01_4\" } else { storeDir \"nextflow-autodownload-databases/imgvr/\" }"
        ],
        "when": "",
        "stub": ""
    },
    "multiqc": {
        "name_process": "multiqc",
        "string_process": "process multiqc {\n    publishDir \"${params.output}/${name}/${params.assemblydir}\", mode: 'copy', pattern: \"${name}_multiqc_report.html\"\n    label 'multiqc'  \n  input:\n    tuple val(name), file(fastqc)\n  output:\n    tuple val(name), file(\"${name}_multiqc_report.html\")\n  script:\n    \"\"\"\n    multiqc -i ${name} .\n    \"\"\"\n  }",
        "nb_lignes_process": 10,
        "string_script": "    \"\"\"\n    multiqc -i ${name} .\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "MultiQC"
        ],
        "tools_url": [
            "https://bio.tools/multiqc"
        ],
        "tools_dico": [
            {
                "name": "MultiQC",
                "uri": "https://bio.tools/multiqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0091",
                            "term": "Bioinformatics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2428",
                                    "term": "Validation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2048",
                                "term": "Report"
                            }
                        ]
                    }
                ],
                "description": "MultiQC aggregates results from multiple bioinformatics analyses across many samples into a single report. It searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.",
                "homepage": "http://multiqc.info/"
            }
        ],
        "inputs": [
            "name",
            "fastqc"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/${params.assemblydir}\", mode: 'copy', pattern: \"${name}_multiqc_report.html\"",
            "label 'multiqc'"
        ],
        "when": "",
        "stub": ""
    },
    "virsorter": {
        "name_process": "virsorter",
        "string_process": "process virsorter {\n      publishDir \"${params.output}/${name}/${params.virusdir}/\", mode: 'copy', pattern: \"*\"\n      label 'virsorter'\n\n    input:\n      tuple val(name), file(fasta), val(contig_number) \n      file(database) \n\n    when: \n      contig_number.toInteger() > 0 \n\n    output:\n      tuple val(name), file(\"*\")\n    \n    script:\n      \"\"\"\n      #perl /usr/local/bin/wrapper_phage_contigs_sorter_iPlant.pl -f ${fasta} --db 2 --wdir virsorter --ncpu ${task.cpus} --data-dir ${database}\n      if [[ \"${params.virome}\" == \"true\" ]]; then \n        wrapper_phage_contigs_sorter_iPlant.pl -f ${fasta} --db 2 --wdir virsorter --ncpu ${task.cpus} --data-dir ${database} --virome\n      else\n        wrapper_phage_contigs_sorter_iPlant.pl -f ${fasta} --db 2 --wdir virsorter --ncpu ${task.cpus} --data-dir ${database}\n      fi\n      \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "      \"\"\"\n      #perl /usr/local/bin/wrapper_phage_contigs_sorter_iPlant.pl -f ${fasta} --db 2 --wdir virsorter --ncpu ${task.cpus} --data-dir ${database}\n      if [[ \"${params.virome}\" == \"true\" ]]; then \n        wrapper_phage_contigs_sorter_iPlant.pl -f ${fasta} --db 2 --wdir virsorter --ncpu ${task.cpus} --data-dir ${database} --virome\n      else\n        wrapper_phage_contigs_sorter_iPlant.pl -f ${fasta} --db 2 --wdir virsorter --ncpu ${task.cpus} --data-dir ${database}\n      fi\n      \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "contig_number",
            "fasta",
            "database"
        ],
        "nb_inputs": 4,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/${params.virusdir}/\", mode: 'copy', pattern: \"*\"",
            "label 'virsorter'"
        ],
        "when": "contig_number.toInteger() > 0",
        "stub": ""
    },
    "annotation": {
        "name_process": "annotation",
        "string_process": "process annotation {\n                                                                                                               \n      label 'annotation'\n\n    input:\n      tuple val(name), val(set_name), file(tab), file(faa) \n    \n    output:\n      tuple val(name), val(set_name), file(\"*_prot_ann_table.tsv\")\n    \n    shell:\n    \"\"\"\n    viral_contigs_annotation.py -o . -p ${faa} -t ${tab}\n    \"\"\"\n}",
        "nb_lignes_process": 13,
        "string_script": "    \"\"\"\n    viral_contigs_annotation.py -o . -p ${faa} -t ${tab}\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "set_name",
            "tab",
            "faa"
        ],
        "nb_inputs": 4,
        "outputs": [
            "set_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'annotation'"
        ],
        "when": "",
        "stub": ""
    },
    "hmm_postprocessing": {
        "name_process": "hmm_postprocessing",
        "string_process": "process hmm_postprocessing {\n      publishDir \"${params.output}/${name}/${params.hmmerdir}/\", mode: 'copy', pattern: \"${set_name}_modified.tsv\"\n      label 'python3'\n\n    input:\n      tuple val(name), val(set_name), file(hmmer_tbl), file(faa) \n    \n    output:\n      tuple val(name), val(set_name), file(\"${set_name}_modified.tsv\"), file(faa)\n    \n    shell:\n    \"\"\"\n    hmmscan_format_table.py -t ${hmmer_tbl} -o ${set_name}_modified\n    \"\"\"\n}",
        "nb_lignes_process": 13,
        "string_script": "    \"\"\"\n    hmmscan_format_table.py -t ${hmmer_tbl} -o ${set_name}_modified\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "set_name",
            "hmmer_tbl",
            "faa"
        ],
        "nb_inputs": 4,
        "outputs": [
            "set_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/${params.hmmerdir}/\", mode: 'copy', pattern: \"${set_name}_modified.tsv\"",
            "label 'python3'"
        ],
        "when": "",
        "stub": ""
    },
    "plot_contig_map": {
        "name_process": "plot_contig_map",
        "string_process": "process plot_contig_map {\n      publishDir \"${params.output}/${name}/${params.plotdir}/\", mode: 'copy', pattern: \"${set_name}_mapping_results\"\n      publishDir \"${params.output}/${name}/${params.finaldir}/annotation/\", mode: 'copy', pattern: \"${set_name}_prot_ann_table_filtered.tsv\"\n      label 'plot_contig_map'\n\n    input:\n      tuple val(name), val(set_name), file(tab)\n    \n    output:\n      tuple val(name), val(set_name), file(\"${set_name}_mapping_results\"), file(\"${set_name}_prot_ann_table_filtered.tsv\")\n    \n    shell:\n    \"\"\"\n  \t# get only contig IDs that have at least one annotation hit \n\t  IDS=\\$(awk 'BEGIN{FS=\"\\\\t\"};{if(\\$6!=\"No hit\"){print \\$1}}' ${tab} | sort | uniq | grep -v Contig)\n\t  head -1 ${tab} > ${set_name}_prot_ann_table_filtered.tsv\n\t  for ID in \\$IDS; do\n\t\t  awk -v id=\"\\$ID\" '{if(id==\\$1){print \\$0}}' ${tab} >> ${set_name}_prot_ann_table_filtered.tsv\n\t  done\n    mkdir -p ${set_name}_mapping_results\n    cp ${set_name}_prot_ann_table_filtered.tsv ${set_name}_mapping_results/\n    make_viral_contig_map.R -o ${set_name}_mapping_results -t ${set_name}_prot_ann_table_filtered.tsv\n    \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "    \"\"\"\n  \t# get only contig IDs that have at least one annotation hit \n\t  IDS=\\$(awk 'BEGIN{FS=\"\\\\t\"};{if(\\$6!=\"No hit\"){print \\$1}}' ${tab} | sort | uniq | grep -v Contig)\n\t  head -1 ${tab} > ${set_name}_prot_ann_table_filtered.tsv\n\t  for ID in \\$IDS; do\n\t\t  awk -v id=\"\\$ID\" '{if(id==\\$1){print \\$0}}' ${tab} >> ${set_name}_prot_ann_table_filtered.tsv\n\t  done\n    mkdir -p ${set_name}_mapping_results\n    cp ${set_name}_prot_ann_table_filtered.tsv ${set_name}_mapping_results/\n    make_viral_contig_map.R -o ${set_name}_mapping_results -t ${set_name}_prot_ann_table_filtered.tsv\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "set_name",
            "tab"
        ],
        "nb_inputs": 3,
        "outputs": [
            "set_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/${params.plotdir}/\", mode: 'copy', pattern: \"${set_name}_mapping_results\"",
            "publishDir \"${params.output}/${name}/${params.finaldir}/annotation/\", mode: 'copy', pattern: \"${set_name}_prot_ann_table_filtered.tsv\"",
            "label 'plot_contig_map'"
        ],
        "when": "",
        "stub": ""
    },
    "virfinder": {
        "name_process": "virfinder",
        "string_process": "process virfinder {\n      errorStrategy { task.exitStatus = 1 ? 'ignore' :  'terminate' }\n      publishDir \"${params.output}/${name}/${params.virusdir}/virfinder\", mode: 'copy', pattern: \"${name}.txt\"\n      label 'virfinder'\n    \n    input:\n      tuple val(name), file(fasta), val(contig_number)\n    \n    when: \n      contig_number.toInteger() > 0 \n    \n    output:\n      tuple val(name), file(\"${name}.txt\")\n    \n    script:\n      \"\"\"\n      #Rscript /usr/local/bin/run_virfinder.Rscript ${fasta} ${name}.txt\n      \n      #run_virfinder_non_parallel.Rscript ${fasta} ${name}.txt\n\n      wget https://github.com/jessieren/VirFinder/raw/master/EPV/VF.modEPV_k8.rda\n      run_virfinder_modEPV.Rscript VF.modEPV_k8.rda ${fasta} .\n      awk '{print \\$1\"\\\\t\"\\$2\"\\\\t\"\\$3\"\\\\t\"\\$4}' ${name}*.txt > ${name}.txt\n      \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "      \"\"\"\n      #Rscript /usr/local/bin/run_virfinder.Rscript ${fasta} ${name}.txt\n      \n      #run_virfinder_non_parallel.Rscript ${fasta} ${name}.txt\n\n      wget https://github.com/jessieren/VirFinder/raw/master/EPV/VF.modEPV_k8.rda\n      run_virfinder_modEPV.Rscript VF.modEPV_k8.rda ${fasta} .\n      awk '{print \\$1\"\\\\t\"\\$2\"\\\\t\"\\$3\"\\\\t\"\\$4}' ${name}*.txt > ${name}.txt\n      \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "contig_number",
            "fasta"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "errorStrategy { task.exitStatus = 1 ? 'ignore' : 'terminate' }",
            "publishDir \"${params.output}/${name}/${params.virusdir}/virfinder\", mode: 'copy', pattern: \"${name}.txt\"",
            "label 'virfinder'"
        ],
        "when": "contig_number.toInteger() > 0",
        "stub": ""
    },
    "rvdbGetDB": {
        "name_process": "rvdbGetDB",
        "string_process": "process rvdbGetDB {\n  label 'noDocker'    \n  if (params.cloudProcess) { \n    publishDir \"${params.cloudDatabase}/\", mode: 'copy', pattern: \"rvdb\" \n  }\n  else { \n    storeDir \"nextflow-autodownload-databases/\" \n  }  \n\n  output:\n    file(\"rvdb\")\n\n  script:\n    \"\"\"\n    wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/hmmer_databases/rvdb.tar.gz && tar -zxvf rvdb.tar.gz\n    rm rvdb.tar.gz\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/hmmer_databases/rvdb.tar.gz && tar -zxvf rvdb.tar.gz\n    rm rvdb.tar.gz\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'noDocker' if (params.cloudProcess) { publishDir \"${params.cloudDatabase}/\", mode: 'copy', pattern: \"rvdb\" } else { storeDir \"nextflow-autodownload-databases/\" }"
        ],
        "when": "",
        "stub": ""
    },
    "rename": {
        "name_process": "rename",
        "string_process": "process rename {\n      publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}_renamed.fasta\"\n      label 'python3'\n\n    input:\n      tuple val(name), file(fasta) \n    \n    output:\n      tuple val(name), file(\"${name}_renamed.fasta\"), file(\"${name}_map.tsv\")\n    \n    shell:\n    \"\"\"    \n    if [[ ${fasta} =~ \\\\.gz\\$ ]]; then\n      zcat ${fasta} > tmp.fasta\n    else\n      cp ${fasta} tmp.fasta\n    fi\n    rename_fasta.py -i tmp.fasta -m ${name}_map.tsv -o ${name}_renamed.fasta rename\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "    \"\"\"    \n    if [[ ${fasta} =~ \\\\.gz\\$ ]]; then\n      zcat ${fasta} > tmp.fasta\n    else\n      cp ${fasta} tmp.fasta\n    fi\n    rename_fasta.py -i tmp.fasta -m ${name}_map.tsv -o ${name}_renamed.fasta rename\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "fasta"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}_renamed.fasta\"",
            "label 'python3'"
        ],
        "when": "",
        "stub": ""
    },
    "ncbiGetDB": {
        "name_process": "ncbiGetDB",
        "string_process": "process ncbiGetDB {\n  label 'noDocker'    \n  if (params.cloudProcess) { \n    publishDir \"${params.cloudDatabase}/ncbi/\", mode: 'copy', pattern: \"ete3_ncbi_tax.sqlite\" \n  }\n  else { \n    storeDir \"nextflow-autodownload-databases/ncbi/\" \n  }  \n\n  output:\n    file(\"ete3_ncbi_tax.sqlite\")\n\n  script:\n    \"\"\"\n    wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/ete3_ncbi_tax.sqlite.gz && gunzip -f ete3_ncbi_tax.sqlite.gz\n    \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "    \"\"\"\n    wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/ete3_ncbi_tax.sqlite.gz && gunzip -f ete3_ncbi_tax.sqlite.gz\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'noDocker' if (params.cloudProcess) { publishDir \"${params.cloudDatabase}/ncbi/\", mode: 'copy', pattern: \"ete3_ncbi_tax.sqlite\" } else { storeDir \"nextflow-autodownload-databases/ncbi/\" }"
        ],
        "when": "",
        "stub": ""
    },
    "pprmeta": {
        "name_process": "pprmeta",
        "string_process": "process pprmeta {\n      label 'pprmeta'\n      publishDir \"${params.output}/${name}/${params.virusdir}/pprmeta\", mode: 'copy', pattern: \"${name}_pprmeta.csv\"\n\n    input:\n      tuple val(name), file(fasta), val(contig_number)\n      path(pprmeta_git)\n    \n    when: \n      contig_number.toInteger() > 0 \n\n    output:\n      tuple val(name), file(\"${name}_pprmeta.csv\")\n\n    script:\n      \"\"\"\n      [ -d \"pprmeta\" ] && cp pprmeta/* .\n      ./PPR_Meta ${fasta} ${name}_pprmeta.csv\n      \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "      \"\"\"\n      [ -d \"pprmeta\" ] && cp pprmeta/* .\n      ./PPR_Meta ${fasta} ${name}_pprmeta.csv\n      \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "contig_number",
            "fasta",
            "pprmeta_git"
        ],
        "nb_inputs": 4,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'pprmeta'",
            "publishDir \"${params.output}/${name}/${params.virusdir}/pprmeta\", mode: 'copy', pattern: \"${name}_pprmeta.csv\""
        ],
        "when": "contig_number.toInteger() > 0",
        "stub": ""
    },
    "pprmetaGet": {
        "name_process": "pprmetaGet",
        "string_process": "\nprocess pprmetaGet {\n  label 'noDocker'    \n  if (params.cloudProcess) { \n    publishDir \"${params.cloudDatabase}/pprmeta\", mode: 'copy', pattern: \"*\" \n  }\n  else { \n    storeDir \"nextflow-autodownload-databases/pprmeta\" \n  }  \n\n  output:\n    path(\"*\")\n\n  script:\n    \"\"\"\n    git clone https://github.com/Stormrider935/PPR-Meta.git\n    mv PPR-Meta/* .  \n    rm -r PPR-Meta\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "    \"\"\"\n    git clone https://github.com/Stormrider935/PPR-Meta.git\n    mv PPR-Meta/* .  \n    rm -r PPR-Meta\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'noDocker' if (params.cloudProcess) { publishDir \"${params.cloudDatabase}/pprmeta\", mode: 'copy', pattern: \"*\" } else { storeDir \"nextflow-autodownload-databases/pprmeta\" }"
        ],
        "when": "",
        "stub": ""
    },
    "fastqc": {
        "name_process": "fastqc",
        "string_process": "process fastqc {\n    label 'fastqc'  \n  input:\n    tuple val(name), file(reads)\n  output:\n    tuple val(name), file(\"fastqc/${name}*fastqc*\")\n  script:\n    \"\"\"\n    mkdir fastqc\n    fastqc -t ${task.cpus} -o fastqc *.fastq.gz\n    \"\"\"\n  }",
        "nb_lignes_process": 10,
        "string_script": "    \"\"\"\n    mkdir fastqc\n    fastqc -t ${task.cpus} -o fastqc *.fastq.gz\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "FastQC"
        ],
        "tools_url": [
            "https://bio.tools/fastqc"
        ],
        "tools_dico": [
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            }
        ],
        "inputs": [
            "name",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'fastqc'"
        ],
        "when": "",
        "stub": ""
    },
    "vpfGetDB": {
        "name_process": "vpfGetDB",
        "string_process": "process vpfGetDB {\n  label 'noDocker'    \n  if (params.cloudProcess) { \n    publishDir \"${params.cloudDatabase}/\", mode: 'copy', pattern: \"vpf\" \n  }\n  else { \n    storeDir \"nextflow-autodownload-databases/\" \n  }  \n\n  output:\n    file(\"vpf\")\n\n  script:\n    \"\"\"\n    wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/hmmer_databases/vpf.tar.gz && tar -zxvf vpf.tar.gz\n    rm vpf.tar.gz\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    wget -nH ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/viral-pipeline/hmmer_databases/vpf.tar.gz && tar -zxvf vpf.tar.gz\n    rm vpf.tar.gz\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "label 'noDocker' if (params.cloudProcess) { publishDir \"${params.cloudDatabase}/\", mode: 'copy', pattern: \"vpf\" } else { storeDir \"nextflow-autodownload-databases/\" }"
        ],
        "when": "",
        "stub": ""
    },
    "length_filtering": {
        "name_process": "length_filtering",
        "string_process": "process length_filtering {\n      publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}*filt*.fasta\"\n      label 'python3'\n\n    input:\n      tuple val(name), file(fasta), file(map) \n    \n    output:\n      tuple val(name), file(\"${name}*filt*.fasta\"), env(CONTIGS)\n    \n    shell:\n    \"\"\"    \n      filter_contigs_len.py -f ${fasta} -l ${params.length} -o ./\n      CONTIGS=\\$(grep \">\" ${name}*filt*.fasta | wc -l)\n    \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "    \"\"\"    \n      filter_contigs_len.py -f ${fasta} -l ${params.length} -o ./\n      CONTIGS=\\$(grep \">\" ${name}*filt*.fasta | wc -l)\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "name",
            "fasta",
            "map"
        ],
        "nb_inputs": 3,
        "outputs": [
            "name"
        ],
        "nb_outputs": 1,
        "name_workflow": "hoelzer__virify",
        "directive": [
            "publishDir \"${params.output}/${name}/\", mode: 'copy', pattern: \"${name}*filt*.fasta\"",
            "label 'python3'"
        ],
        "when": "",
        "stub": ""
    }
}