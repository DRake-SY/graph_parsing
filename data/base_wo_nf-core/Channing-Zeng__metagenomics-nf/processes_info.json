{
    "AlignSV": {
        "name_process": "AlignSV",
        "string_process": "\nprocess AlignSV {\n    container = \"${container__infernal}\"\n    label = 'mem_veryhigh'\n\n    input:\n        path sv_fasta_f\n        path cm\n    \n    output:\n        path \"sv.aln.sto\"\n        path \"sv.aln.scores\"\n        \n    \n    \"\"\"\n    cmalign \\\n    --cpu ${task.cpus} --noprob --dnaout --mxsize ${params.cmalign_mxsize} \\\n    --sfile sv.aln.scores -o sv.aln.sto \\\n    ${cm} ${sv_fasta_f}\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "\"\"\"\n    cmalign \\\n    --cpu ${task.cpus} --noprob --dnaout --mxsize ${params.cmalign_mxsize} \\\n    --sfile sv.aln.scores -o sv.aln.sto \\\n    ${cm} ${sv_fasta_f}\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sv_fasta_f",
            "cm"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__infernal}\"",
            "label = 'mem_veryhigh'"
        ],
        "when": "",
        "stub": ""
    },
    "CombineAln_SV_refpkg": {
        "name_process": "CombineAln_SV_refpkg",
        "string_process": "\nprocess CombineAln_SV_refpkg {\n    container = \"${container__easel}\"\n    label = 'mem_veryhigh'\n\n    input:\n        file sv_aln_sto_f \n        file refpkg_aln_sto_f\n        \n    \n    output:\n        file \"sv_refpkg.aln.sto\"\n    \n    \"\"\"\n    esl-alimerge --dna \\\n     -o sv_refpkg.aln.sto \\\n     ${sv_aln_sto_f} ${refpkg_aln_sto_f}\n    \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "\"\"\"\n    esl-alimerge --dna \\\n     -o sv_refpkg.aln.sto \\\n     ${sv_aln_sto_f} ${refpkg_aln_sto_f}\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sv_aln_sto_f",
            "refpkg_aln_sto_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__easel}\"",
            "label = 'mem_veryhigh'"
        ],
        "when": "",
        "stub": ""
    },
    "ConvertAlnToFasta": {
        "name_process": "ConvertAlnToFasta",
        "string_process": "\nprocess ConvertAlnToFasta {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    errorStrategy \"retry\"\n\n    input: \n        file recruits_aln_sto_f\n    \n    output:\n        file \"recruits.aln.fasta\"\n    \n    \"\"\"\n    #!/usr/bin/env python\n    from Bio import AlignIO\n\n    with open('recruits.aln.fasta', 'wt') as out_h:\n        AlignIO.write(\n            AlignIO.read(\n                open('${recruits_aln_sto_f}', 'rt'),\n                'stockholm'\n            ),\n            out_h,\n            'fasta'\n        )\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n    #!/usr/bin/env python\n    from Bio import AlignIO\n\n    with open('recruits.aln.fasta', 'wt') as out_h:\n        AlignIO.write(\n            AlignIO.read(\n                open('${recruits_aln_sto_f}', 'rt'),\n                'stockholm'\n            ),\n            out_h,\n            'fasta'\n        )\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "recruits_aln_sto_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "errorStrategy \"retry\""
        ],
        "when": "",
        "stub": ""
    },
    "ExtractRefpkg": {
        "name_process": "ExtractRefpkg",
        "string_process": "\nprocess ExtractRefpkg {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    \n    input:\n        file refpkg_tgz_f\n\n    output:\n        path 'refpkg_tree.nwk', emit: tree\n        path 'refpkg.aln.fasta', emit: ref_aln_fasta\n        path 'refpkg.aln.sto', emit: ref_aln_sto\n        path 'model.txt', emit: model\n        path 'leaf_info.csv', emit: leaf_info\n        path 'taxonomy.csv', emit: taxonomy\n        path 'refpkg.cm', emit: cm\n\n\"\"\"\n#!/usr/bin/env python\nimport tarfile\nimport json\nimport os\nimport re\n\ntar_h = tarfile.open('${refpkg_tgz_f}')\ntar_contents_dict = {os.path.basename(f.name): f for f in tar_h.getmembers()}\ncontents = json.loads(\n    tar_h.extractfile(\n        tar_contents_dict['CONTENTS.json']\n    ).read().decode('utf-8')\n)\n\nwith open('refpkg.cm', 'wb') as cm_h:\n    cm_h.write(\n        tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('profile')]\n        ).read()\n    )\n\nwith open('refpkg_tree.nwk', 'wt') as tree_h:\n    tree_h.writelines(\n        tar_h.extractfile(\n                tar_contents_dict[contents['files'].get('tree')]\n            ).read().decode('utf-8')\n    )\n\naln_fasta_intgz = contents['files'].get('aln_fasta')\naln_sto_intgz = contents['files'].get('aln_sto')\n\nif aln_fasta_intgz and aln_sto_intgz:\n    # Both version of the alignment are in the refpkg\n    with open('refpkg.aln.fasta','w') as out_aln_fasta_h:\n        out_aln_fasta_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_fasta_intgz]\n            ).read().decode('utf-8')\n        )\n    with open('refpkg.aln.sto','w') as out_aln_sto_h:\n        out_aln_sto_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_sto_intgz]\n            ).read().decode('utf-8')\n        )\nelif aln_fasta_intgz:\n    # Only fasta exists\n    with open('refpkg.aln.fasta','w') as out_aln_fasta_h:\n        out_aln_fasta_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_fasta_intgz]\n            ).read().decode('utf-8')\n        )\n    # And convert to sto format\n    with open('refpkg.aln.sto','w') as out_aln_sto_h:\n        AlignIO.write(\n            AlignIO.read(\n                tar_h.extractfile(tar_contents_dict[aln_fasta_intgz]),\n                'fasta'),\n            out_aln_sto_h,\n            'stockholm'\n        )\nelif aln_sto_intgz:\n    # Only STO exists\n    with open('refpkg.aln.sto','w') as out_aln_sto_h:\n        out_aln_sto_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_sto_intgz]\n            ).read().decode('utf-8')\n        )\n    with sopen('refpkg.aln.fasta','w') as out_aln_fasta_h:\n        AlignIO.write(\n            AlignIO.read(\n                            tar_h.extractfile(tar_contents_dict[aln_sto_intgz]),\n                            'stockholm'),\n            out_aln_fasta_h,\n            'fasta'\n        )\n# Model\nif 'raxml_ng_model' in contents['files']:\n    with open('model.txt', 'wt') as out_h:\n        out_h.write(tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('raxml_ng_model')]\n        ).read().decode('utf-8'))\n\nelse:\n    phylo_model = json.loads(tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('phylo_model')]\n        ).read().decode('utf-8')\n    ).get('subs_rates')\n\n    re_basefreq = re.compile(r'Base frequencies: (?P<A>0\\\\.\\\\d+) (?P<C>0\\\\.\\\\d+) (?P<G>0\\\\.\\\\d+) (?P<T>0\\\\.\\\\d+)')\n    bf_m = re_basefreq.search(tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('tree_stats')]\n        ).read().decode('utf-8'))\n    with open('model.txt', 'wt') as model_h:\n        model_h.writelines( \n            \"GTR{\"+\n            \"{}/{}/{}/{}/{}/{}\".format(\n                phylo_model['ac'],\n                phylo_model['ag'],\n                phylo_model['at'],\n                phylo_model['cg'],\n                phylo_model['ct'],\n                phylo_model['gt'],\n            )\n            +\"}\"+\"+FU{\"+\n            \"{}/{}/{}/{}\".format(\n                bf_m['A'],\n                bf_m['C'],\n                bf_m['G'],\n                bf_m['T'],\n            )\n            +\"}\"\n        )\n\nwith open('leaf_info.csv', 'wt') as leaf_h:\n    leaf_h.write(tar_h.extractfile(\n        tar_contents_dict[contents['files'].get('seq_info')]\n    ).read().decode('utf-8'))\n\nwith open('taxonomy.csv', 'wt') as leaf_h:\n    leaf_h.write(tar_h.extractfile(\n        tar_contents_dict[contents['files'].get('taxonomy')]\n    ).read().decode('utf-8'))\n\"\"\"\n}",
        "nb_lignes_process": 143,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport tarfile\nimport json\nimport os\nimport re\n\ntar_h = tarfile.open('${refpkg_tgz_f}')\ntar_contents_dict = {os.path.basename(f.name): f for f in tar_h.getmembers()}\ncontents = json.loads(\n    tar_h.extractfile(\n        tar_contents_dict['CONTENTS.json']\n    ).read().decode('utf-8')\n)\n\nwith open('refpkg.cm', 'wb') as cm_h:\n    cm_h.write(\n        tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('profile')]\n        ).read()\n    )\n\nwith open('refpkg_tree.nwk', 'wt') as tree_h:\n    tree_h.writelines(\n        tar_h.extractfile(\n                tar_contents_dict[contents['files'].get('tree')]\n            ).read().decode('utf-8')\n    )\n\naln_fasta_intgz = contents['files'].get('aln_fasta')\naln_sto_intgz = contents['files'].get('aln_sto')\n\nif aln_fasta_intgz and aln_sto_intgz:\n    # Both version of the alignment are in the refpkg\n    with open('refpkg.aln.fasta','w') as out_aln_fasta_h:\n        out_aln_fasta_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_fasta_intgz]\n            ).read().decode('utf-8')\n        )\n    with open('refpkg.aln.sto','w') as out_aln_sto_h:\n        out_aln_sto_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_sto_intgz]\n            ).read().decode('utf-8')\n        )\nelif aln_fasta_intgz:\n    # Only fasta exists\n    with open('refpkg.aln.fasta','w') as out_aln_fasta_h:\n        out_aln_fasta_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_fasta_intgz]\n            ).read().decode('utf-8')\n        )\n    # And convert to sto format\n    with open('refpkg.aln.sto','w') as out_aln_sto_h:\n        AlignIO.write(\n            AlignIO.read(\n                tar_h.extractfile(tar_contents_dict[aln_fasta_intgz]),\n                'fasta'),\n            out_aln_sto_h,\n            'stockholm'\n        )\nelif aln_sto_intgz:\n    # Only STO exists\n    with open('refpkg.aln.sto','w') as out_aln_sto_h:\n        out_aln_sto_h.write(\n            tar_h.extractfile(\n                tar_contents_dict[aln_sto_intgz]\n            ).read().decode('utf-8')\n        )\n    with sopen('refpkg.aln.fasta','w') as out_aln_fasta_h:\n        AlignIO.write(\n            AlignIO.read(\n                            tar_h.extractfile(tar_contents_dict[aln_sto_intgz]),\n                            'stockholm'),\n            out_aln_fasta_h,\n            'fasta'\n        )\n# Model\nif 'raxml_ng_model' in contents['files']:\n    with open('model.txt', 'wt') as out_h:\n        out_h.write(tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('raxml_ng_model')]\n        ).read().decode('utf-8'))\n\nelse:\n    phylo_model = json.loads(tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('phylo_model')]\n        ).read().decode('utf-8')\n    ).get('subs_rates')\n\n    re_basefreq = re.compile(r'Base frequencies: (?P<A>0\\\\.\\\\d+) (?P<C>0\\\\.\\\\d+) (?P<G>0\\\\.\\\\d+) (?P<T>0\\\\.\\\\d+)')\n    bf_m = re_basefreq.search(tar_h.extractfile(\n            tar_contents_dict[contents['files'].get('tree_stats')]\n        ).read().decode('utf-8'))\n    with open('model.txt', 'wt') as model_h:\n        model_h.writelines( \n            \"GTR{\"+\n            \"{}/{}/{}/{}/{}/{}\".format(\n                phylo_model['ac'],\n                phylo_model['ag'],\n                phylo_model['at'],\n                phylo_model['cg'],\n                phylo_model['ct'],\n                phylo_model['gt'],\n            )\n            +\"}\"+\"+FU{\"+\n            \"{}/{}/{}/{}\".format(\n                bf_m['A'],\n                bf_m['C'],\n                bf_m['G'],\n                bf_m['T'],\n            )\n            +\"}\"\n        )\n\nwith open('leaf_info.csv', 'wt') as leaf_h:\n    leaf_h.write(tar_h.extractfile(\n        tar_contents_dict[contents['files'].get('seq_info')]\n    ).read().decode('utf-8'))\n\nwith open('taxonomy.csv', 'wt') as leaf_h:\n    leaf_h.write(tar_h.extractfile(\n        tar_contents_dict[contents['files'].get('taxonomy')]\n    ).read().decode('utf-8'))\n\"\"\"",
        "nb_lignes_script": 126,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "refpkg_tgz_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'"
        ],
        "when": "",
        "stub": ""
    },
    "EPAngPlacement": {
        "name_process": "EPAngPlacement",
        "string_process": "\nprocess EPAngPlacement {\n    container = \"${container__epang}\"\n    label = 'mem_veryhigh'\n    publishDir \"${params.output}/placement\", mode: 'copy'\n    input:\n        file refpkg_aln_fasta\n        file combined_aln_fasta\n        file model\n        file ref_tree\n\n    output:\n        file 'dedup.jplace'\n    \"\"\"\n    set -e\n\n    epa-ng --split ${refpkg_aln_fasta} ${combined_aln_fasta}\n    model=`cat ${model}`\n    \n    epa-ng -t ${ref_tree} \\\n    -s reference.fasta -q query.fasta \\\n    -m \\$model -T ${task.cpus} \\\n    --baseball-heur\n\n    mv epa_result.jplace dedup.jplace\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n    set -e\n\n    epa-ng --split ${refpkg_aln_fasta} ${combined_aln_fasta}\n    model=`cat ${model}`\n    \n    epa-ng -t ${ref_tree} \\\n    -s reference.fasta -q query.fasta \\\n    -m \\$model -T ${task.cpus} \\\n    --baseball-heur\n\n    mv epa_result.jplace dedup.jplace\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "EPA-ng"
        ],
        "tools_url": [
            "https://bio.tools/EPA-ng"
        ],
        "tools_dico": [
            {
                "name": "EPA-ng",
                "uri": "https://bio.tools/EPA-ng",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Microbial ecology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3697",
                            "term": "Environmental microbiology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0310",
                                    "term": "Sequence assembly"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0362",
                                    "term": "Genome annotation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression data analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Complete reimplementation of the evolutionary placement algorithm (EPA) that is substantially faster, offers a distributed memory parallelization, and integrates concepts from both, RAxML-EPA and PPLACER.",
                "homepage": "https://github.com/Pbdas/epa-ng"
            }
        ],
        "inputs": [
            "refpkg_aln_fasta",
            "combined_aln_fasta",
            "model",
            "ref_tree"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__epang}\"",
            "label = 'mem_veryhigh'",
            "publishDir \"${params.output}/placement\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "MakeEPAngTaxonomy": {
        "name_process": "MakeEPAngTaxonomy",
        "string_process": "\nprocess MakeEPAngTaxonomy {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/refpkg\", mode: 'copy'\n\n    input:\n        path leaf_info_f\n        path taxonomy_f\n\n    output:\n        path 'epang_taxon_file.tsv'\n\n\"\"\"\n#!/usr/bin/env python\nimport csv\n\ntax_dict = {\n    r['tax_id']: r for r in\n    csv.DictReader(open('${taxonomy_f}', 'rt'))\n}\ntax_names = {\n    tax_id: r['tax_name']\n    for tax_id, r in tax_dict.items()\n}\nRANKS = [\n    'superkingdom',\n    'phylum',\n    'class',\n    'order',\n    'family',\n    'genus',\n    'species',\n]\nwith open('epang_taxon_file.tsv', 'wt') as tf_h:\n    tf_w = csv.writer(tf_h, delimiter='\\\\t')\n    for row in csv.DictReader(open('${leaf_info_f}', 'rt')):\n        tax_id = row.get('tax_id', None)\n        if tax_id is None:\n            continue\n        # Implicit else\n        tax_lineage = tax_dict.get(tax_id, None)\n        if tax_lineage is None:\n            continue\n        lineage_str = \";\".join([\n            tax_names.get(tax_lineage.get(rank, \"\"), \"\")\n            for rank in RANKS\n        ])\n        tf_w.writerow([row['seqname'], lineage_str])\n\n\"\"\"\n\n}",
        "nb_lignes_process": 51,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport csv\n\ntax_dict = {\n    r['tax_id']: r for r in\n    csv.DictReader(open('${taxonomy_f}', 'rt'))\n}\ntax_names = {\n    tax_id: r['tax_name']\n    for tax_id, r in tax_dict.items()\n}\nRANKS = [\n    'superkingdom',\n    'phylum',\n    'class',\n    'order',\n    'family',\n    'genus',\n    'species',\n]\nwith open('epang_taxon_file.tsv', 'wt') as tf_h:\n    tf_w = csv.writer(tf_h, delimiter='\\\\t')\n    for row in csv.DictReader(open('${leaf_info_f}', 'rt')):\n        tax_id = row.get('tax_id', None)\n        if tax_id is None:\n            continue\n        # Implicit else\n        tax_lineage = tax_dict.get(tax_id, None)\n        if tax_lineage is None:\n            continue\n        lineage_str = \";\".join([\n            tax_names.get(tax_lineage.get(rank, \"\"), \"\")\n            for rank in RANKS\n        ])\n        tf_w.writerow([row['seqname'], lineage_str])\n\n\"\"\"",
        "nb_lignes_script": 37,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "leaf_info_f",
            "taxonomy_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/refpkg\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "Gappa_Classify": {
        "name_process": "Gappa_Classify",
        "string_process": "\nprocess Gappa_Classify {\n    container = \"${container__gappa}\"\n    label = 'mem_veryhigh'\n    publishDir \"${params.output}/classify\", mode: 'copy'\n    errorStrategy 'ignore'\n\n    input:\n        path dedup_jplace\n        path taxon_file\n    \n    output:\n        path 'per_query.tsv'\n\n    \"\"\"\n    set -e\n\n\n    gappa examine assign \\\n    --per-query-results \\\n    --verbose \\\n    --threads ${task.cpus} \\\n    --jplace-path ${dedup_jplace} \\\n    --taxon-file ${taxon_file} \\\n    \n    \"\"\"\n\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n    set -e\n\n\n    gappa examine assign \\\n    --per-query-results \\\n    --verbose \\\n    --threads ${task.cpus} \\\n    --jplace-path ${dedup_jplace} \\\n    --taxon-file ${taxon_file} \\\n    \n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "GAPPA"
        ],
        "tools_url": [
            "https://bio.tools/GAPPA"
        ],
        "tools_dico": [
            {
                "name": "GAPPA",
                "uri": "https://bio.tools/GAPPA",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0325",
                                    "term": "Phylogenetic tree comparison"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic tree reconstruction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Collection of commands for working with phylogenetic data.",
                "homepage": "http://github.com/lczech/gappa"
            }
        ],
        "inputs": [
            "dedup_jplace",
            "taxon_file"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__gappa}\"",
            "label = 'mem_veryhigh'",
            "publishDir \"${params.output}/classify\", mode: 'copy'",
            "errorStrategy 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "Make_Wide_Tax_Table": {
        "name_process": "Make_Wide_Tax_Table",
        "string_process": "\nprocess Make_Wide_Tax_Table {\n    container \"${container__dada2pplacer}\"\n    label 'io_mem'\n    publishDir \"${params.output}/classify\", mode: 'copy'\n                            \n\n    input:\n        path sv_long\n        path sv_taxonomy\n        val want_rank\n\n    output:\n        path \"tables/taxon_wide_ra.${want_rank}.csv\", emit: ra\n        path \"tables/taxon_wide_nreads.${want_rank}.csv\", emit: nreads\n\n\"\"\"\n#!/usr/bin/env python\nimport pandas as pd\nimport os\n\ntry:\n    os.makedirs('tables')\nexcept:\n    pass\n\nsv_long = pd.read_csv(\"${sv_long}\").rename({\n    'count': 'nreads'\n}, axis=1)\n# Add in rel abund\nfor sp, sp_sv in sv_long.groupby('specimen'):\n    sv_long.loc[sp_sv.index, 'fract'] = sp_sv.nreads / sp_sv.nreads.sum()\n\nsv_taxonomy = pd.read_csv('${sv_taxonomy}')\nsv_long_tax = pd.merge(\n    sv_long,\n    sv_taxonomy[sv_taxonomy.want_rank == '${want_rank}'],\n    on='sv',\n    how='left'\n)\n\nsp_tax = sv_long_tax.groupby(['specimen', 'tax_name']).sum().reset_index()[[\n    'specimen',\n    'tax_name',\n    'nreads',\n    'fract'\n]]\n\nsp_tax_wide_ra = sp_tax.pivot(\n    index='specimen',\n    columns='tax_name',\n    values='fract'\n).fillna(0)\n# Sort by mean RA\nsp_tax_wide_ra = sp_tax_wide_ra[sp_tax_wide_ra.mean().sort_values(ascending=False).index]\n\nsp_tax_wide_ra.to_csv(\"tables/taxon_wide_ra.${want_rank}.csv\")\n\nsp_tax_wide_nreads = sp_tax.pivot(\n    index='specimen',\n    columns='tax_name',\n    values='nreads'\n).fillna(0)\n# Sort by mean RA\nsp_tax_wide_nreads = sp_tax_wide_nreads[sp_tax_wide_ra.mean().sort_values(ascending=False).index].astype(int)\nsp_tax_wide_nreads.to_csv(\"tables/taxon_wide_nreads.${want_rank}.csv\")\n\n\"\"\"\n}",
        "nb_lignes_process": 67,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport pandas as pd\nimport os\n\ntry:\n    os.makedirs('tables')\nexcept:\n    pass\n\nsv_long = pd.read_csv(\"${sv_long}\").rename({\n    'count': 'nreads'\n}, axis=1)\n# Add in rel abund\nfor sp, sp_sv in sv_long.groupby('specimen'):\n    sv_long.loc[sp_sv.index, 'fract'] = sp_sv.nreads / sp_sv.nreads.sum()\n\nsv_taxonomy = pd.read_csv('${sv_taxonomy}')\nsv_long_tax = pd.merge(\n    sv_long,\n    sv_taxonomy[sv_taxonomy.want_rank == '${want_rank}'],\n    on='sv',\n    how='left'\n)\n\nsp_tax = sv_long_tax.groupby(['specimen', 'tax_name']).sum().reset_index()[[\n    'specimen',\n    'tax_name',\n    'nreads',\n    'fract'\n]]\n\nsp_tax_wide_ra = sp_tax.pivot(\n    index='specimen',\n    columns='tax_name',\n    values='fract'\n).fillna(0)\n# Sort by mean RA\nsp_tax_wide_ra = sp_tax_wide_ra[sp_tax_wide_ra.mean().sort_values(ascending=False).index]\n\nsp_tax_wide_ra.to_csv(\"tables/taxon_wide_ra.${want_rank}.csv\")\n\nsp_tax_wide_nreads = sp_tax.pivot(\n    index='specimen',\n    columns='tax_name',\n    values='nreads'\n).fillna(0)\n# Sort by mean RA\nsp_tax_wide_nreads = sp_tax_wide_nreads[sp_tax_wide_ra.mean().sort_values(ascending=False).index].astype(int)\nsp_tax_wide_nreads.to_csv(\"tables/taxon_wide_nreads.${want_rank}.csv\")\n\n\"\"\"",
        "nb_lignes_script": 51,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sv_long",
            "sv_taxonomy",
            "want_rank"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2pplacer}\"",
            "label 'io_mem'",
            "publishDir \"${params.output}/classify\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "Gappa_Extract_Taxonomy": {
        "name_process": "Gappa_Extract_Taxonomy",
        "string_process": "\nprocess Gappa_Extract_Taxonomy {\n    container \"${container__dada2pplacer}\"\n    label 'io_mem'\n    publishDir \"${params.output}/classify\", mode: 'copy'\n                            \n\n    input:\n        path gappa_taxonomy\n        path refpkg_taxtable\n\n    output:\n        path \"sv_taxonomy.csv\"\n        path refpkg_taxtable\n\n\"\"\"\n#!/usr/bin/env python\nimport pandas as pd\n\nMIN_AFRACT = 0\nRANKS = [\n    'superkingdom',\n    'phylum',\n    'class',\n    'order',\n    'family',\n    'genus',\n    'species',\n]\nRANK_DEPTH = {\n    i+1: r for (i, r) in enumerate(RANKS)\n}\nrefpkg_taxtable = pd.read_csv(\"${refpkg_taxtable}\")\ntax_name_to_id = {\n    row.tax_name: row.tax_id for\n    idx, row in refpkg_taxtable.iterrows()\n}\nepa_tax = pd.read_csv('${gappa_taxonomy}', sep='\\t')\nepa_tax['lineage']=epa_tax.taxopath.apply(lambda tp: tp.split(';'))\nepa_tax['rank_depth']=epa_tax.lineage.apply(len)\nsv_tax_list = []\nfor sv, sv_c in epa_tax[epa_tax.taxopath != 'DISTANT'].groupby('name'):\n    sv_tax = pd.DataFrame()\n    rank = None\n    tax_name = None\n    lineage = None\n    afract = None    \n    for rank_depth, want_rank in RANK_DEPTH.items():\n        sv_depth = sv_c[sv_c.rank_depth == rank_depth]\n        if len(sv_depth) > 0 and sv_depth.afract.sum() >= MIN_AFRACT:\n            # Something at this depth, and the cumulative fract likelihood is above our threshold\n            rank = want_rank\n            tax_name = \" / \".join(sv_depth.lineage.apply(lambda L: L[-1]))\n            ncbi_tax_id = \",\".join([str(tax_name_to_id.get(tn, -1)) for tn in sv_depth.lineage.apply(lambda L: L[-1])])\n            lineage = \";\".join(sv_depth.lineage.iloc[0][:-1] + [tax_name])\n            afract = sv_depth.afract.sum()\n            \n            \n        sv_tax.loc[rank, 'sv'] = sv\n        sv_tax.loc[rank, 'want_rank'] = want_rank\n        sv_tax.loc[rank, 'rank'] = rank\n        sv_tax.loc[rank, 'rank_depth'] = rank_depth\n        sv_tax.loc[rank, 'tax_name'] = tax_name\n        sv_tax.loc[rank, 'ncbi_tax_id'] = ncbi_tax_id\n        sv_tax.loc[rank, 'lineage'] = lineage\n        sv_tax.loc[rank, 'afract'] = afract\n        sv_tax.loc[rank, 'ambiguous'] = len(sv_depth) != 1\n    sv_tax_list.append(sv_tax)\n\n\nsv_taxonomy = pd.concat(sv_tax_list, ignore_index=True)\nsv_taxonomy['rank_depth'] = sv_taxonomy.rank_depth.astype(int)\nsv_taxonomy.to_csv('sv_taxonomy.csv', index=None)\n\n\"\"\"\n}",
        "nb_lignes_process": 74,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport pandas as pd\n\nMIN_AFRACT = 0\nRANKS = [\n    'superkingdom',\n    'phylum',\n    'class',\n    'order',\n    'family',\n    'genus',\n    'species',\n]\nRANK_DEPTH = {\n    i+1: r for (i, r) in enumerate(RANKS)\n}\nrefpkg_taxtable = pd.read_csv(\"${refpkg_taxtable}\")\ntax_name_to_id = {\n    row.tax_name: row.tax_id for\n    idx, row in refpkg_taxtable.iterrows()\n}\nepa_tax = pd.read_csv('${gappa_taxonomy}', sep='\\t')\nepa_tax['lineage']=epa_tax.taxopath.apply(lambda tp: tp.split(';'))\nepa_tax['rank_depth']=epa_tax.lineage.apply(len)\nsv_tax_list = []\nfor sv, sv_c in epa_tax[epa_tax.taxopath != 'DISTANT'].groupby('name'):\n    sv_tax = pd.DataFrame()\n    rank = None\n    tax_name = None\n    lineage = None\n    afract = None    \n    for rank_depth, want_rank in RANK_DEPTH.items():\n        sv_depth = sv_c[sv_c.rank_depth == rank_depth]\n        if len(sv_depth) > 0 and sv_depth.afract.sum() >= MIN_AFRACT:\n            # Something at this depth, and the cumulative fract likelihood is above our threshold\n            rank = want_rank\n            tax_name = \" / \".join(sv_depth.lineage.apply(lambda L: L[-1]))\n            ncbi_tax_id = \",\".join([str(tax_name_to_id.get(tn, -1)) for tn in sv_depth.lineage.apply(lambda L: L[-1])])\n            lineage = \";\".join(sv_depth.lineage.iloc[0][:-1] + [tax_name])\n            afract = sv_depth.afract.sum()\n            \n            \n        sv_tax.loc[rank, 'sv'] = sv\n        sv_tax.loc[rank, 'want_rank'] = want_rank\n        sv_tax.loc[rank, 'rank'] = rank\n        sv_tax.loc[rank, 'rank_depth'] = rank_depth\n        sv_tax.loc[rank, 'tax_name'] = tax_name\n        sv_tax.loc[rank, 'ncbi_tax_id'] = ncbi_tax_id\n        sv_tax.loc[rank, 'lineage'] = lineage\n        sv_tax.loc[rank, 'afract'] = afract\n        sv_tax.loc[rank, 'ambiguous'] = len(sv_depth) != 1\n    sv_tax_list.append(sv_tax)\n\n\nsv_taxonomy = pd.concat(sv_tax_list, ignore_index=True)\nsv_taxonomy['rank_depth'] = sv_taxonomy.rank_depth.astype(int)\nsv_taxonomy.to_csv('sv_taxonomy.csv', index=None)\n\n\"\"\"",
        "nb_lignes_script": 59,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "gappa_taxonomy",
            "refpkg_taxtable"
        ],
        "nb_inputs": 2,
        "outputs": [
            "refpkg_taxtable"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2pplacer}\"",
            "label 'io_mem'",
            "publishDir \"${params.output}/classify\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "EDPL": {
        "name_process": "EDPL",
        "string_process": "\nprocess EDPL {\n    container = \"${container__gappa}\"\n    label = 'multithread'\n    publishDir \"${params.output}/placement\", mode: 'copy'\n    errorStrategy 'ignore' \n\n    input:\n        path dedup_jplace\n    \n    output:\n        path 'edpl_list.csv'\n\n    \"\"\"\n    set -e\n\n\n    gappa examine edpl \\\n    --jplace-path ${dedup_jplace} \\\n    --verbose \\\n    --threads ${task.cpus}\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\n    set -e\n\n\n    gappa examine edpl \\\n    --jplace-path ${dedup_jplace} \\\n    --verbose \\\n    --threads ${task.cpus}\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "GAPPA"
        ],
        "tools_url": [
            "https://bio.tools/GAPPA"
        ],
        "tools_dico": [
            {
                "name": "GAPPA",
                "uri": "https://bio.tools/GAPPA",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0325",
                                    "term": "Phylogenetic tree comparison"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic tree reconstruction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Collection of commands for working with phylogenetic data.",
                "homepage": "http://github.com/lczech/gappa"
            }
        ],
        "inputs": [
            "dedup_jplace"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__gappa}\"",
            "label = 'multithread'",
            "publishDir \"${params.output}/placement\", mode: 'copy'",
            "errorStrategy 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "MakeSplit": {
        "name_process": "MakeSplit",
        "string_process": "\nprocess MakeSplit {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/sv\", mode: 'copy'\n\n    input:\n        path sv_long_f\n    output:\n        path 'sv_multiplicity.csv'\n\n\"\"\"\n#!/usr/bin/env python\nimport csv\n\nwith open('${sv_long_f}', 'rt') as in_h, open('sv_multiplicity.csv', 'wt') as out_h:\n    svl = csv.DictReader(in_h)\n    t_svl = csv.writer(\n        out_h,\n    )\n    for r in svl:\n        t_svl.writerow([\n            r['sv'],\n            r['specimen'],\n            r['count']\n        ])\n\"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport csv\n\nwith open('${sv_long_f}', 'rt') as in_h, open('sv_multiplicity.csv', 'wt') as out_h:\n    svl = csv.DictReader(in_h)\n    t_svl = csv.writer(\n        out_h,\n    )\n    for r in svl:\n        t_svl.writerow([\n            r['sv'],\n            r['specimen'],\n            r['count']\n        ])\n\"\"\"",
        "nb_lignes_script": 15,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sv_long_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/sv\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "GappaSplit": {
        "name_process": "GappaSplit",
        "string_process": "\nprocess GappaSplit {\n    container = \"${container__gappa}\"\n    label = 'multithread'\n    publishDir \"${params.output}/placement/\", mode: 'copy'\n\n    input:\n        path dedup_jplace\n        path split_csv\n    \n    output:\n        path 'specimen_jplace/*.jplace.gz'\n\n    \"\"\"\n    set -e\n\n    mkdir specimen_jplace\n\n    gappa edit split \\\n    --jplace-path ${dedup_jplace} \\\n    --split-file ${split_csv} \\\n    --compress \\\n    --verbose \\\n    --threads ${task.cpus} \\\n    --out-dir specimen_jplace\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n    set -e\n\n    mkdir specimen_jplace\n\n    gappa edit split \\\n    --jplace-path ${dedup_jplace} \\\n    --split-file ${split_csv} \\\n    --compress \\\n    --verbose \\\n    --threads ${task.cpus} \\\n    --out-dir specimen_jplace\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "GAPPA"
        ],
        "tools_url": [
            "https://bio.tools/GAPPA"
        ],
        "tools_dico": [
            {
                "name": "GAPPA",
                "uri": "https://bio.tools/GAPPA",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0325",
                                    "term": "Phylogenetic tree comparison"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic tree reconstruction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Collection of commands for working with phylogenetic data.",
                "homepage": "http://github.com/lczech/gappa"
            }
        ],
        "inputs": [
            "dedup_jplace",
            "split_csv"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__gappa}\"",
            "label = 'multithread'",
            "publishDir \"${params.output}/placement/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "Gappa_KRD": {
        "name_process": "Gappa_KRD",
        "string_process": "\nprocess Gappa_KRD {\n    container = \"${container__gappa}\"\n    label = 'mem_veryhigh'\n    publishDir \"${params.output}/placement/\", mode: 'copy'\n    errorStrategy 'ignore'\n\n    input:\n        path specimen_jplace\n    \n    output:\n        path 'krd/krd_matrix.csv.gz'\n\n    \"\"\"\n    set -e\n\n    gappa analyze krd \\\n    --jplace-path ${specimen_jplace} \\\n    --krd-out-dir krd/ \\\n    --krd-compress \\\n    --verbose \\\n    --threads ${task.cpus}\n\n    \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "\"\"\"\n    set -e\n\n    gappa analyze krd \\\n    --jplace-path ${specimen_jplace} \\\n    --krd-out-dir krd/ \\\n    --krd-compress \\\n    --verbose \\\n    --threads ${task.cpus}\n\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [
            "GAPPA"
        ],
        "tools_url": [
            "https://bio.tools/GAPPA"
        ],
        "tools_dico": [
            {
                "name": "GAPPA",
                "uri": "https://bio.tools/GAPPA",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0325",
                                    "term": "Phylogenetic tree comparison"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic tree reconstruction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Collection of commands for working with phylogenetic data.",
                "homepage": "http://github.com/lczech/gappa"
            }
        ],
        "inputs": [
            "specimen_jplace"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__gappa}\"",
            "label = 'mem_veryhigh'",
            "publishDir \"${params.output}/placement/\", mode: 'copy'",
            "errorStrategy 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "Gappa_ePCA": {
        "name_process": "Gappa_ePCA",
        "string_process": "\nprocess Gappa_ePCA {\n    container = \"${container__gappa}\"\n    label = 'mem_veryhigh'\n    publishDir \"${params.output}/placement/\", mode: 'copy'\n    errorStrategy 'ignore'\n\n    input:\n        path specimen_jplace\n    \n    output:\n        path 'ePCA/projection.csv'\n        path 'ePCA/transformation.csv'\n\n    \"\"\"\n    set -e\n\n    gappa analyze edgepca \\\n    --jplace-path ${specimen_jplace} \\\n    --out-dir ePCA/ \\\n    --verbose \\\n    --threads ${task.cpus}\n\n    ls -l ePCA\n\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n    set -e\n\n    gappa analyze edgepca \\\n    --jplace-path ${specimen_jplace} \\\n    --out-dir ePCA/ \\\n    --verbose \\\n    --threads ${task.cpus}\n\n    ls -l ePCA\n\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "GAPPA"
        ],
        "tools_url": [
            "https://bio.tools/GAPPA"
        ],
        "tools_dico": [
            {
                "name": "GAPPA",
                "uri": "https://bio.tools/GAPPA",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0325",
                                    "term": "Phylogenetic tree comparison"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0567",
                                    "term": "Phylogenetic tree rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic tree reconstruction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Collection of commands for working with phylogenetic data.",
                "homepage": "http://github.com/lczech/gappa"
            }
        ],
        "inputs": [
            "specimen_jplace"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__gappa}\"",
            "label = 'mem_veryhigh'",
            "publishDir \"${params.output}/placement/\", mode: 'copy'",
            "errorStrategy 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "PplacerADCL": {
        "name_process": "PplacerADCL",
        "string_process": "\nprocess PplacerADCL {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n\n    publishDir \"${params.output}/placement\", mode: 'copy'\n\n    input:\n        file dedup_jplace_f\n    output:\n        file 'adcl.csv.gz'\n    \n    \"\"\"\n    (echo name,adcl,weight && \n    guppy adcl --no-collapse ${dedup_jplace_f} -o /dev/stdout) | \n    gzip > adcl.csv.gz\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "\"\"\"\n    (echo name,adcl,weight && \n    guppy adcl --no-collapse ${dedup_jplace_f} -o /dev/stdout) | \n    gzip > adcl.csv.gz\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "GUPPY"
        ],
        "tools_url": [
            "https://bio.tools/guppy"
        ],
        "tools_dico": [
            {
                "name": "GUPPY",
                "uri": "https://bio.tools/guppy",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Program to visualize sequence annotation data of the genetic sequence data with graphical layout. This highly interactive program allows scrolling and zooming from the genomic landscape to discrete nucleic acid sequences. Its main function is quick rendering of data on a standalone personal computer to save the layout as a parsonal file. With optional link to the internet resources, and printing support, this program tries to make more greater use of computational media in research actitiviy.",
                "homepage": "http://staff.aist.go.jp/yutaka.ueno/guppy/"
            }
        ],
        "inputs": [
            "dedup_jplace_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/placement\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "PplacerEDPL": {
        "name_process": "PplacerEDPL",
        "string_process": "\nprocess PplacerEDPL {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n\n    publishDir \"${params.output}/placement\", mode: 'copy'\n\n    input:\n        file dedup_jplace_f\n    output:\n        file 'edpl.csv.gz'\n    \n    \"\"\"\n    (echo name,edpl && guppy edpl --csv ${dedup_jplace_f} -o /dev/stdout) | \n    gzip > edpl.csv.gz\n    \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "\"\"\"\n    (echo name,edpl && guppy edpl --csv ${dedup_jplace_f} -o /dev/stdout) | \n    gzip > edpl.csv.gz\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "dedup_jplace_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/placement\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "PplacerPCA": {
        "name_process": "PplacerPCA",
        "string_process": "\nprocess PplacerPCA {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n    afterScript \"rm -r refpkg/\"\n    publishDir \"${params.output}/placement\", mode: 'copy'\n    errorStrategy = 'ignore'\n\n    input:\n        file refpkg_tgz_f\n        file dedup_jplace_f\n        file sv_map_f\n    output:\n        file 'pca/epca.proj'\n        file 'pca/epca.xml'\n        file 'pca/epca.trans'\n        file 'pca/lpca.proj'\n        file 'pca/lpca.xml'\n        file 'pca/lpca.trans'\n    \n    \"\"\"\n    mkdir -p refpkg/ && mkdir -p pca/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/ &&\n    guppy epca ${dedup_jplace_f}:${sv_map_f} -c refpkg/ --out-dir pca/ --prefix epca &&\n    guppy lpca ${dedup_jplace_f}:${sv_map_f} -c refpkg/ --out-dir pca/ --prefix lpca\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n    mkdir -p refpkg/ && mkdir -p pca/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/ &&\n    guppy epca ${dedup_jplace_f}:${sv_map_f} -c refpkg/ --out-dir pca/ --prefix epca &&\n    guppy lpca ${dedup_jplace_f}:${sv_map_f} -c refpkg/ --out-dir pca/ --prefix lpca\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "GUPPY"
        ],
        "tools_url": [
            "https://bio.tools/guppy"
        ],
        "tools_dico": [
            {
                "name": "GUPPY",
                "uri": "https://bio.tools/guppy",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Program to visualize sequence annotation data of the genetic sequence data with graphical layout. This highly interactive program allows scrolling and zooming from the genomic landscape to discrete nucleic acid sequences. Its main function is quick rendering of data on a standalone personal computer to save the layout as a parsonal file. With optional link to the internet resources, and printing support, this program tries to make more greater use of computational media in research actitiviy.",
                "homepage": "http://staff.aist.go.jp/yutaka.ueno/guppy/"
            }
        ],
        "inputs": [
            "refpkg_tgz_f",
            "dedup_jplace_f",
            "sv_map_f"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "afterScript \"rm -r refpkg/\"",
            "publishDir \"${params.output}/placement\", mode: 'copy'",
            "errorStrategy = 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "PplacerAlphaDiversity": {
        "name_process": "PplacerAlphaDiversity",
        "string_process": "\nprocess PplacerAlphaDiversity {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n\n    publishDir \"${params.output}/placement\", mode: 'copy'\n\n    input:\n        file dedup_jplace_f\n        file sv_map_f\n    output:\n        file 'alpha_diversity.csv.gz'\n\n    \n    \"\"\"\n    guppy fpd --csv --include-pendant --chao-d 0,1,1.00001,2,3,4,5 \\\n    ${dedup_jplace_f}:${sv_map_f} |\n    gzip > alpha_diversity.csv.gz\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    guppy fpd --csv --include-pendant --chao-d 0,1,1.00001,2,3,4,5 \\\n    ${dedup_jplace_f}:${sv_map_f} |\n    gzip > alpha_diversity.csv.gz\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "GUPPY"
        ],
        "tools_url": [
            "https://bio.tools/guppy"
        ],
        "tools_dico": [
            {
                "name": "GUPPY",
                "uri": "https://bio.tools/guppy",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Program to visualize sequence annotation data of the genetic sequence data with graphical layout. This highly interactive program allows scrolling and zooming from the genomic landscape to discrete nucleic acid sequences. Its main function is quick rendering of data on a standalone personal computer to save the layout as a parsonal file. With optional link to the internet resources, and printing support, this program tries to make more greater use of computational media in research actitiviy.",
                "homepage": "http://staff.aist.go.jp/yutaka.ueno/guppy/"
            }
        ],
        "inputs": [
            "dedup_jplace_f",
            "sv_map_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/placement\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "CombineSpAd": {
        "name_process": "CombineSpAd",
        "string_process": "\nprocess CombineSpAd {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/placement\", mode: 'copy'\n\n    input:\n        path specimen_ad_csv\n    \n    output:\n        path \"alpha_diversity.csv.gz\"\n    \n\"\"\"\n#!/usr/bin/env python\nimport csv\nimport gzip\nfiles_to_combine = \"${specimen_ad_csv}\".split()\ncombined_data = [\n    row\n    for fn in files_to_combine\n    for row in csv.DictReader(\n        open(fn, 'rt')\n    )\n]\n\nwith gzip.open('alpha_diversity.csv.gz', 'wt') as out_h:\n    out_w = csv.DictWriter(out_h,fieldnames=combined_data[0].keys())\n    out_w.writeheader()\n    out_w.writerows(combined_data)\n\n\"\"\"\n\n}",
        "nb_lignes_process": 31,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport csv\nimport gzip\nfiles_to_combine = \"${specimen_ad_csv}\".split()\ncombined_data = [\n    row\n    for fn in files_to_combine\n    for row in csv.DictReader(\n        open(fn, 'rt')\n    )\n]\n\nwith gzip.open('alpha_diversity.csv.gz', 'wt') as out_h:\n    out_w = csv.DictWriter(out_h,fieldnames=combined_data[0].keys())\n    out_w.writeheader()\n    out_w.writerows(combined_data)\n\n\"\"\"",
        "nb_lignes_script": 18,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen_ad_csv"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/placement\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "PplacerKR": {
        "name_process": "PplacerKR",
        "string_process": "\nprocess PplacerKR {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n    afterScript \"rm -r refpkg/\"\n    publishDir \"${params.output}/placement\", mode: 'copy'\n\n    input:\n        file refpkg_tgz_f\n        file dedup_jplace_f\n        file sv_map_f\n    output:\n        file 'kr_distance.csv.gz'\n\n    \n    \"\"\"\n    mkdir -p refpkg/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/\n    guppy kr --list-out -c refpkg/ ${dedup_jplace_f}:${sv_map_f} |\n    gzip > kr_distance.csv.gz\n    \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "\"\"\"\n    mkdir -p refpkg/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/\n    guppy kr --list-out -c refpkg/ ${dedup_jplace_f}:${sv_map_f} |\n    gzip > kr_distance.csv.gz\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "GUPPY"
        ],
        "tools_url": [
            "https://bio.tools/guppy"
        ],
        "tools_dico": [
            {
                "name": "GUPPY",
                "uri": "https://bio.tools/guppy",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Program to visualize sequence annotation data of the genetic sequence data with graphical layout. This highly interactive program allows scrolling and zooming from the genomic landscape to discrete nucleic acid sequences. Its main function is quick rendering of data on a standalone personal computer to save the layout as a parsonal file. With optional link to the internet resources, and printing support, this program tries to make more greater use of computational media in research actitiviy.",
                "homepage": "http://staff.aist.go.jp/yutaka.ueno/guppy/"
            }
        ],
        "inputs": [
            "refpkg_tgz_f",
            "dedup_jplace_f",
            "sv_map_f"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "afterScript \"rm -r refpkg/\"",
            "publishDir \"${params.output}/placement\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "ClassifyDB_Prep": {
        "name_process": "ClassifyDB_Prep",
        "string_process": "\nprocess ClassifyDB_Prep {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n    afterScript \"rm -r refpkg/\"\n    cache = false\n\n    input:\n        file refpkg_tgz_f\n        file sv_map_f\n    \n    output:\n        file 'classify.prep.db'\n    \n\n    \"\"\"\n    mkdir -p refpkg/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/\n    rppr prep_db -c refpkg/ --sqlite classify.prep.db\n    (echo \"name,specimen\"; cat ${sv_map_f}) |\n    csvsql --table seq_info --insert --snifflimit 1000 --db sqlite:                   \n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\n    mkdir -p refpkg/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/\n    rppr prep_db -c refpkg/ --sqlite classify.prep.db\n    (echo \"name,specimen\"; cat ${sv_map_f}) |\n    csvsql --table seq_info --insert --snifflimit 1000 --db sqlite:                   \n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "refpkg_tgz_f",
            "sv_map_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "afterScript \"rm -r refpkg/\"",
            "cache = false"
        ],
        "when": "",
        "stub": ""
    },
    "ClassifySV": {
        "name_process": "ClassifySV",
        "string_process": "\nprocess ClassifySV {\n    container = \"${container__pplacer}\"\n    label = 'mem_veryhigh'\n    afterScript \"rm -r refpkg/\"\n    cache = false\n\n    input:\n        file refpkg_tgz_f\n        file classify_db_prepped\n        file dedup_jplace_f\n        file sv_refpkg_aln_sto_f\n    \n    output:\n        file 'classify.classified.db'\n\n    \"\"\"\n    mkdir -p refpkg/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/\n    guppy classify --pp \\\n    --classifier ${params.pp_classifer} \\\n    -j ${task.cpus} \\\n    -c refpkg/ \\\n    --nbc-sequences ${sv_refpkg_aln_sto_f} \\\n    --sqlite ${classify_db_prepped} \\\n    --seed ${params.pp_seed} \\\n    --cutoff ${params.pp_likelihood_cutoff} \\\n    --bayes-cutoff ${params.pp_bayes_cutoff} \\\n    --multiclass-min ${params.pp_multiclass_min} \\\n    --bootstrap-cutoff ${params.pp_bootstrap_cutoff} \\\n    --bootstrap-extension-cutoff ${params.pp_bootstrap_extension_cutoff} \\\n    --word-length ${params.pp_nbc_word_length} \\\n    --nbc-rank ${params.pp_nbc_target_rank} \\\n    --n-boot ${params.pp_nbc_boot} \\\n    ${dedup_jplace_f}\n    cp ${classify_db_prepped} classify.classified.db\n    \"\"\"\n}",
        "nb_lignes_process": 36,
        "string_script": "\"\"\"\n    mkdir -p refpkg/\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C refpkg/\n    guppy classify --pp \\\n    --classifier ${params.pp_classifer} \\\n    -j ${task.cpus} \\\n    -c refpkg/ \\\n    --nbc-sequences ${sv_refpkg_aln_sto_f} \\\n    --sqlite ${classify_db_prepped} \\\n    --seed ${params.pp_seed} \\\n    --cutoff ${params.pp_likelihood_cutoff} \\\n    --bayes-cutoff ${params.pp_bayes_cutoff} \\\n    --multiclass-min ${params.pp_multiclass_min} \\\n    --bootstrap-cutoff ${params.pp_bootstrap_cutoff} \\\n    --bootstrap-extension-cutoff ${params.pp_bootstrap_extension_cutoff} \\\n    --word-length ${params.pp_nbc_word_length} \\\n    --nbc-rank ${params.pp_nbc_target_rank} \\\n    --n-boot ${params.pp_nbc_boot} \\\n    ${dedup_jplace_f}\n    cp ${classify_db_prepped} classify.classified.db\n    \"\"\"",
        "nb_lignes_script": 20,
        "language_script": "bash",
        "tools": [
            "GUPPY"
        ],
        "tools_url": [
            "https://bio.tools/guppy"
        ],
        "tools_dico": [
            {
                "name": "GUPPY",
                "uri": "https://bio.tools/guppy",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Program to visualize sequence annotation data of the genetic sequence data with graphical layout. This highly interactive program allows scrolling and zooming from the genomic landscape to discrete nucleic acid sequences. Its main function is quick rendering of data on a standalone personal computer to save the layout as a parsonal file. With optional link to the internet resources, and printing support, this program tries to make more greater use of computational media in research actitiviy.",
                "homepage": "http://staff.aist.go.jp/yutaka.ueno/guppy/"
            }
        ],
        "inputs": [
            "refpkg_tgz_f",
            "classify_db_prepped",
            "dedup_jplace_f",
            "sv_refpkg_aln_sto_f"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'mem_veryhigh'",
            "afterScript \"rm -r refpkg/\"",
            "cache = false"
        ],
        "when": "",
        "stub": ""
    },
    "ClassifyMCC": {
        "name_process": "ClassifyMCC",
        "string_process": "\nprocess ClassifyMCC {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n    cache = false\n    publishDir \"${params.output}/classify\", mode: 'copy'\n\n    input:\n        file classifyDB_classified\n        file sv_weights_f\n\n    output:\n        file 'classify.mcc.db'\n\n    \"\"\"\n    multiclass_concat.py -k \\\n    --dedup-info ${sv_weights_f} ${classifyDB_classified}\n    cp ${classifyDB_classified} classify.mcc.db\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    multiclass_concat.py -k \\\n    --dedup-info ${sv_weights_f} ${classifyDB_classified}\n    cp ${classifyDB_classified} classify.mcc.db\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "classifyDB_classified",
            "sv_weights_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "cache = false",
            "publishDir \"${params.output}/classify\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "ClassifyTables": {
        "name_process": "ClassifyTables",
        "string_process": "\nprocess ClassifyTables {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/classify\", mode: 'copy'\n\n    input:\n        tuple val(rank), file(classifyDB_mcc), file(sv_map_for_tables_f)\n\n    output:\n        tuple val(rank), file(\"tables/by_specimen.${rank}.csv\"), file(\"tables/by_taxon.${rank}.csv\"), file(\"tables/tallies_wide.${rank}.csv\")\n\n    \"\"\"\n    mkdir -p tables/\n    classif_table.py ${classifyDB_mcc} \\\n    tables/by_taxon.${rank}.csv \\\n    --rank ${rank} \\\n    --specimen-map ${sv_map_for_tables_f} \\\n    --by-specimen tables/by_specimen.${rank}.csv \\\n    --tallies-wide tables/tallies_wide.${rank}.csv\n    \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "\"\"\"\n    mkdir -p tables/\n    classif_table.py ${classifyDB_mcc} \\\n    tables/by_taxon.${rank}.csv \\\n    --rank ${rank} \\\n    --specimen-map ${sv_map_for_tables_f} \\\n    --by-specimen tables/by_specimen.${rank}.csv \\\n    --tallies-wide tables/tallies_wide.${rank}.csv\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "rank",
            "classifyDB_mcc",
            "sv_map_for_tables_f"
        ],
        "nb_inputs": 3,
        "outputs": [
            "rank"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/classify\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "WeightMaptoLong": {
        "name_process": "WeightMaptoLong",
        "string_process": "\nprocess WeightMaptoLong {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/sv\", mode: 'copy'\n\n    input:\n        path (weight)\n        path (map)\n\n    output:\n        path (\"sp_sv_long.csv\")\n\n\"\"\"\n#!/usr/bin/env python\nimport csv\n\nspecimen_comSV = {\n    r[0]: r[1]\n    for r in \n    csv.reader(\n        open('${map}', 'rt')\n    )\n}\nwith open('${weight}', 'rt') as w_h, open(\"sp_sv_long.csv\", 'wt') as sv_long_h:\n    w_r = csv.reader(w_h)\n    svl_w = csv.writer(sv_long_h)\n    svl_w.writerow((\n        'specimen',\n        'sv',\n        'count'\n    ))    \n    for row in w_r:\n        svl_w.writerow((\n            specimen_comSV[row[1]],\n            row[0],\n            int(row[2])\n        ))\n    \n\"\"\"\n}",
        "nb_lignes_process": 39,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport csv\n\nspecimen_comSV = {\n    r[0]: r[1]\n    for r in \n    csv.reader(\n        open('${map}', 'rt')\n    )\n}\nwith open('${weight}', 'rt') as w_h, open(\"sp_sv_long.csv\", 'wt') as sv_long_h:\n    w_r = csv.reader(w_h)\n    svl_w = csv.writer(sv_long_h)\n    svl_w.writerow((\n        'specimen',\n        'sv',\n        'count'\n    ))    \n    for row in w_r:\n        svl_w.writerow((\n            specimen_comSV[row[1]],\n            row[0],\n            int(row[2])\n        ))\n    \n\"\"\"",
        "nb_lignes_script": 26,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "weight",
            "map"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/sv\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "SharetableToMapWeight": {
        "name_process": "SharetableToMapWeight",
        "string_process": "\nprocess SharetableToMapWeight {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/sv\", mode: 'copy'\n\n    input:\n        file (sharetable)\n\n    output:\n        file (\"sv_sp_map.csv\")\n        file (\"sv_weights.csv\")\n\n\"\"\"\n#!/usr/bin/env python\nimport csv\n\nsp_count = {}\nwith open('${sharetable}', 'rt') as st_h:\n    st_r = csv.reader(st_h, delimiter='\\\\t')\n    header = next(st_r)\n    sv_name = header[3:]\n    for r in st_r:\n        sp_count[r[0]] = [int(c) for c in r[3:]]\nweightsL = []\nmapL = []\nfor sv_i, sv in enumerate(sv_name):\n    sv_counts = [\n        (sp, c[sv_i]) for sp, c in sp_count.items()\n        if c[sv_i] > 0\n    ]\n    weightsL += [\n        (sv, \"{}__{}\".format(sv, sp), c)\n        for sp, c in \n        sv_counts\n    ]\n    mapL += [\n        (\"{}__{}\".format(sv, sp), sp)\n        for sp, c in \n        sv_counts\n    ]\nwith open(\"sv_sp_map.csv\", \"w\") as map_h:\n    map_w = csv.writer(map_h)\n    map_w.writerows(mapL)\nwith open(\"sv_weights.csv\", \"w\") as weights_h:\n    weights_w = csv.writer(weights_h)\n    weights_w.writerows(weightsL) \n\"\"\"\n}",
        "nb_lignes_process": 47,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport csv\n\nsp_count = {}\nwith open('${sharetable}', 'rt') as st_h:\n    st_r = csv.reader(st_h, delimiter='\\\\t')\n    header = next(st_r)\n    sv_name = header[3:]\n    for r in st_r:\n        sp_count[r[0]] = [int(c) for c in r[3:]]\nweightsL = []\nmapL = []\nfor sv_i, sv in enumerate(sv_name):\n    sv_counts = [\n        (sp, c[sv_i]) for sp, c in sp_count.items()\n        if c[sv_i] > 0\n    ]\n    weightsL += [\n        (sv, \"{}__{}\".format(sv, sp), c)\n        for sp, c in \n        sv_counts\n    ]\n    mapL += [\n        (\"{}__{}\".format(sv, sp), sp)\n        for sp, c in \n        sv_counts\n    ]\nwith open(\"sv_sp_map.csv\", \"w\") as map_h:\n    map_w = csv.writer(map_h)\n    map_w.writerows(mapL)\nwith open(\"sv_weights.csv\", \"w\") as weights_h:\n    weights_w = csv.writer(weights_h)\n    weights_w.writerows(weightsL) \n\"\"\"",
        "nb_lignes_script": 34,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sharetable"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/sv\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "Extract_Taxonomy": {
        "name_process": "Extract_Taxonomy",
        "string_process": "\nprocess Extract_Taxonomy {\n    container \"${container__dada2pplacer}\"\n    label 'io_mem'\n    publishDir \"${params.output}/classify\", mode: 'copy'\n    errorStrategy \"ignore\"\n\n    input:\n        file (weights_csv)\n        file (taxonomy_db)\n\n    output:\n        file \"sv_taxonomy.csv\"\n\n\"\"\"\n#!/usr/bin/env python\nimport csv\nimport pandas as pd\nimport sqlite3\n\nsv = {\n    r[0] for r in \n    csv.reader(open(\n        \"${weights_csv}\",\n        'rt'\n    ))\n}\n\ntax_db_conn = sqlite3.connect(\"${taxonomy_db}\")\ntax_db_cur = tax_db_conn.cursor()\ntax_db_cur.execute(\"CREATE INDEX IF NOT EXISTS idx_mcc_name_tax  ON multiclass_concat(name, tax_id)\")\n\nsv_classification = pd.concat([\n    pd.read_sql(\"SELECT name, want_rank, taxa.tax_id, taxa.tax_name, taxa.rank, likelihood FROM multiclass_concat JOIN taxa ON multiclass_concat.tax_id = taxa.tax_id WHERE name=(?)\",\n        con = tax_db_conn,\n        params=(\n            sv,\n        ))    \n    for sv in sv\n], ignore_index=True)\n\nsv_classification.to_csv(\"sv_taxonomy.csv\", index=False)\n\n\"\"\"\n}",
        "nb_lignes_process": 43,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport csv\nimport pandas as pd\nimport sqlite3\n\nsv = {\n    r[0] for r in \n    csv.reader(open(\n        \"${weights_csv}\",\n        'rt'\n    ))\n}\n\ntax_db_conn = sqlite3.connect(\"${taxonomy_db}\")\ntax_db_cur = tax_db_conn.cursor()\ntax_db_cur.execute(\"CREATE INDEX IF NOT EXISTS idx_mcc_name_tax  ON multiclass_concat(name, tax_id)\")\n\nsv_classification = pd.concat([\n    pd.read_sql(\"SELECT name, want_rank, taxa.tax_id, taxa.tax_name, taxa.rank, likelihood FROM multiclass_concat JOIN taxa ON multiclass_concat.tax_id = taxa.tax_id WHERE name=(?)\",\n        con = tax_db_conn,\n        params=(\n            sv,\n        ))    \n    for sv in sv\n], ignore_index=True)\n\nsv_classification.to_csv(\"sv_taxonomy.csv\", index=False)\n\n\"\"\"",
        "nb_lignes_script": 29,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "weights_csv",
            "taxonomy_db"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2pplacer}\"",
            "label 'io_mem'",
            "publishDir \"${params.output}/classify\", mode: 'copy'",
            "errorStrategy \"ignore\""
        ],
        "when": "",
        "stub": ""
    },
    "TrimGalore": {
        "name_process": "TrimGalore",
        "string_process": "\nprocess TrimGalore {\n    container \"${container__trimgalore}\"\n    label 'io_limited'\n    errorStrategy 'ignore'\n\n    input:\n    tuple val(specimen), val(batch), file(R1), file(R2)\n\n    output:\n    tuple val(specimen), val(batch), file(\"${specimen}.R1.tg.fastq.gz\"), file(\"${specimen}.R2.tg.fastq.gz\")\n\n    \"\"\"\n    set -e\n\n    cp ${R1} R1.fastq.gz\n    cp ${R2} R2.fastq.gz\n\n    trim_galore \\\n    --gzip \\\n    --cores ${task.cpus} \\\n    --paired \\\n    R1.fastq.gz R2.fastq.gz\n\n    rm R1.fastq.gz\n    rm R2.fastq.gz\n    mv R1_val_1.fq.gz \"${specimen}.R1.tg.fastq.gz\"\n    mv R2_val_2.fq.gz \"${specimen}.R2.tg.fastq.gz\"\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "\"\"\"\n    set -e\n\n    cp ${R1} R1.fastq.gz\n    cp ${R2} R2.fastq.gz\n\n    trim_galore \\\n    --gzip \\\n    --cores ${task.cpus} \\\n    --paired \\\n    R1.fastq.gz R2.fastq.gz\n\n    rm R1.fastq.gz\n    rm R2.fastq.gz\n    mv R1_val_1.fq.gz \"${specimen}.R1.tg.fastq.gz\"\n    mv R2_val_2.fq.gz \"${specimen}.R2.tg.fastq.gz\"\n    \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1",
            "R2"
        ],
        "nb_inputs": 4,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__trimgalore}\"",
            "label 'io_limited'",
            "errorStrategy 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "barcodecop": {
        "name_process": "barcodecop",
        "string_process": "\nprocess barcodecop {\n    container \"${container__barcodecop}\"\n    label 'io_limited'\n    errorStrategy \"ignore\"\n\n    input:\n    tuple val(specimen), val(batch), file(R1), file(R2), file(I1), file(I2)\n    \n    output:\n    tuple val(specimen), val(batch), file(\"${R1.getSimpleName()}.bcc.fq.gz\"), file(\"${R2.getSimpleName()}.bcc.fq.gz\")\n\n    \"\"\"\n    set -e\n\n    barcodecop \\\n    ${I1} ${I2} \\\n    --match-filter \\\n    -f ${R1} \\\n    -o ${R1.getSimpleName()}.bcc.fq.gz &&\n    barcodecop \\\n    ${I1} ${I2} \\\n    --match-filter \\\n    -f ${R2} \\\n    -o ${R2.getSimpleName()}.bcc.fq.gz\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n    set -e\n\n    barcodecop \\\n    ${I1} ${I2} \\\n    --match-filter \\\n    -f ${R1} \\\n    -o ${R1.getSimpleName()}.bcc.fq.gz &&\n    barcodecop \\\n    ${I1} ${I2} \\\n    --match-filter \\\n    -f ${R2} \\\n    -o ${R2.getSimpleName()}.bcc.fq.gz\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1",
            "R2",
            "I1",
            "I2"
        ],
        "nb_inputs": 6,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__barcodecop}\"",
            "label 'io_limited'",
            "errorStrategy \"ignore\""
        ],
        "when": "",
        "stub": ""
    },
    "TrimGaloreSE": {
        "name_process": "TrimGaloreSE",
        "string_process": "\nprocess TrimGaloreSE {\n    container \"${container__trimgalore}\"\n    label 'io_limited'\n    errorStrategy 'ignore'\n\n    input:\n    tuple val(specimen), val(batch), file(R1)\n\n    output:\n    tuple val(specimen), val(batch), file(\"${specimen}.R1.tg.fastq.gz\")\n\n    \"\"\"\n    set -e\n\n    cp ${R1} R1.fastq.gz\n\n    trim_galore \\\n    --gzip \\\n    --cores ${task.cpus} \\\n    R1.fastq.gz\n\n    rm R1.fastq.gz\n    mv R1_trimmed.fq.gz \"${specimen}.R1.tg.fastq.gz\"\n    \"\"\"\n}",
        "nb_lignes_process": 24,
        "string_script": "\"\"\"\n    set -e\n\n    cp ${R1} R1.fastq.gz\n\n    trim_galore \\\n    --gzip \\\n    --cores ${task.cpus} \\\n    R1.fastq.gz\n\n    rm R1.fastq.gz\n    mv R1_trimmed.fq.gz \"${specimen}.R1.tg.fastq.gz\"\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1"
        ],
        "nb_inputs": 3,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__trimgalore}\"",
            "label 'io_limited'",
            "errorStrategy 'ignore'"
        ],
        "when": "",
        "stub": ""
    },
    "output_failed": {
        "name_process": "output_failed",
        "string_process": "\nprocess output_failed {\n    container \"${container__barcodecop}\"\n    label 'io_limited'\n    publishDir \"${params.output}/sv/\", mode: 'copy'\n    errorStrategy 'retry'\n\n    input:\n        tuple val(specimens), val(reasons)\n    output:\n        file (\"failed_specimens.csv\")\n\n    \"\"\"\n    #!/usr/bin/env python\n    import csv\n    import re\n    specimens = re.sub(r'\\\\[|\\\\]', \"\", \"${specimens}\").split(',')\n    reasons = re.sub(r'\\\\[|\\\\]', \"\", \"${reasons}\").split(',')\n\n    with open(\"failed_specimens.csv\", 'wt') as out_h:\n        w = csv.writer(out_h)\n        w.writerow([\n            'specimen',\n            'reason'\n        ])\n        for sp, reason in zip(specimens, reasons):\n            w.writerow([sp.strip(), reason.strip()])\n    \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "\"\"\"\n    #!/usr/bin/env python\n    import csv\n    import re\n    specimens = re.sub(r'\\\\[|\\\\]', \"\", \"${specimens}\").split(',')\n    reasons = re.sub(r'\\\\[|\\\\]', \"\", \"${reasons}\").split(',')\n\n    with open(\"failed_specimens.csv\", 'wt') as out_h:\n        w = csv.writer(out_h)\n        w.writerow([\n            'specimen',\n            'reason'\n        ])\n        for sp, reason in zip(specimens, reasons):\n            w.writerow([sp.strip(), reason.strip()])\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimens",
            "reasons"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__barcodecop}\"",
            "label 'io_limited'",
            "publishDir \"${params.output}/sv/\", mode: 'copy'",
            "errorStrategy 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "parseSampleSheet": {
        "name_process": "parseSampleSheet",
        "string_process": "\nprocess parseSampleSheet {\n    container \"quay.io/fhcrc-microbiome/python-pandas:v0.24.2\"\n    label \"io_limited\"\n    \n    input:\n    file sample_sheet_csv from file(params.sample_sheet)\n    \n    output:\n    file \"${sample_sheet_csv}\" into sample_sheet_ch, sample_sheet_to_parse\n\n    \"\"\"#!/usr/bin/env python3\nimport pandas as pd\n\nprint(\"Reading in ${sample_sheet_csv}\")\n\ndf = pd.read_csv(\"${sample_sheet_csv}\", sep=\",\")\n\nprint(\"Read in %d rows and %d columns\" % (df.shape[0], df.shape[1]))\n\nfor k in [\"name\", \"genome\"]:\n    assert k in df.columns.values, \"Must provide a column '%s' in the sample sheet\" % k\n\n# Strip away all whitespace and carriage returns\ndf = df.applymap(str).applymap(lambda s: s.strip())\n\nprint(\"Writing out sanitized sample sheet\")\ndf.to_csv(\"${sample_sheet_csv}\", index=None, sep=\"\\\\t\")\nprint(\"Done\")\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "\"\"\"#!/usr/bin/env python3\nimport pandas as pd\n\nprint(\"Reading in ${sample_sheet_csv}\")\n\ndf = pd.read_csv(\"${sample_sheet_csv}\", sep=\",\")\n\nprint(\"Read in %d rows and %d columns\" % (df.shape[0], df.shape[1]))\n\nfor k in [\"name\", \"genome\"]:\n    assert k in df.columns.values, \"Must provide a column '%s' in the sample sheet\" % k\n\n# Strip away all whitespace and carriage returns\ndf = df.applymap(str).applymap(lambda s: s.strip())\n\nprint(\"Writing out sanitized sample sheet\")\ndf.to_csv(\"${sample_sheet_csv}\", index=None, sep=\"\\\\t\")\nprint(\"Done\")\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [
            "sample_sheet_ch",
            "sample_sheet_to_parse"
        ],
        "nb_outputs": 2,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/python-pandas:v0.24.2\"",
            "label \"io_limited\""
        ],
        "when": "",
        "stub": ""
    },
    "makeGenomeDB": {
        "name_process": "makeGenomeDB",
        "string_process": "\nprocess makeGenomeDB {\n    container \"${anvio_container}\"\n    label \"mem_medium\"\n    \n    input:\n    set name, file(fasta) from sample_sheet_ch.splitCsv(header:true, sep:\"\\t\").map { row -> tuple(row.name, file(row.genome)) }\n    \n    output:\n    set name, file(\"${name}.db\") into genomeDB_ch, nameDB_ch, aniDB_ch\n\n    \"\"\"\n#!/bin/bash\nfasta=${fasta}\n# Decompress the FASTA if it is compressed\ngzip -t \\$fasta && gunzip \\$fasta && fasta=\\$(echo \\$fasta | sed 's/.gz//')\n# The file ending must be \"fa\", \"fsa\", or \"fasta\"\nif [[ \\$fasta =~ \".fa\" ]] || [[ \\$fasta =~ \".fsa\" ]] || [[ \\$fasta =~ \".fasta\" ]]; then\n    pass\nelse\n    mv \\$fasta \\$fasta.fasta\n    fasta=\\$fasta.fasta\nfi\n\n# Reformat the FASTA to sanitize deflines\nanvi-script-reformat-fasta --simplify-names -l 0 -o \\$fasta.clean.fasta \\$fasta\n\n# Make the genome database\nanvi-gen-contigs-database -f \\$fasta.clean.fasta -n ${name} -o ${name}.db\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "\"\"\"\n#!/bin/bash\nfasta=${fasta}\n# Decompress the FASTA if it is compressed\ngzip -t \\$fasta && gunzip \\$fasta && fasta=\\$(echo \\$fasta | sed 's/.gz//')\n# The file ending must be \"fa\", \"fsa\", or \"fasta\"\nif [[ \\$fasta =~ \".fa\" ]] || [[ \\$fasta =~ \".fsa\" ]] || [[ \\$fasta =~ \".fasta\" ]]; then\n    pass\nelse\n    mv \\$fasta \\$fasta.fasta\n    fasta=\\$fasta.fasta\nfi\n\n# Reformat the FASTA to sanitize deflines\nanvi-script-reformat-fasta --simplify-names -l 0 -o \\$fasta.clean.fasta \\$fasta\n\n# Make the genome database\nanvi-gen-contigs-database -f \\$fasta.clean.fasta -n ${name} -o ${name}.db\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [
            "PASS"
        ],
        "tools_url": [
            "https://bio.tools/pass"
        ],
        "tools_dico": [
            {
                "name": "PASS",
                "uri": "https://bio.tools/pass",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "PASS: a program to align short sequences",
                "homepage": "http://pass.cribi.unipd.it/cgi-bin/pass.pl"
            }
        ],
        "inputs": [
            "sample_sheet_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "genomeDB_ch",
            "nameDB_ch",
            "aniDB_ch"
        ],
        "nb_outputs": 3,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${anvio_container}\"",
            "label \"mem_medium\""
        ],
        "when": "",
        "stub": ""
    },
    "setupNCBIcogs": {
        "name_process": "setupNCBIcogs",
        "string_process": "\nprocess setupNCBIcogs {\n    container \"${anvio_container}\"\n    label \"cpu_high\"\n    \n    output:\n    file \"COGS_DIR.tar\" into anvio_cogs_tar\n\n    \"\"\"\n#!/bin/bash\nanvi-setup-ncbi-cogs --num-threads ${task.cpus} --cog-data-dir COGS_DIR --just-do-it --reset\ntar cvf COGS_DIR.tar COGS_DIR\n    \"\"\"\n}",
        "nb_lignes_process": 12,
        "string_script": "\"\"\"\n#!/bin/bash\nanvi-setup-ncbi-cogs --num-threads ${task.cpus} --cog-data-dir COGS_DIR --just-do-it --reset\ntar cvf COGS_DIR.tar COGS_DIR\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [
            "anvio_cogs_tar"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${anvio_container}\"",
            "label \"cpu_high\""
        ],
        "when": "",
        "stub": ""
    },
    "annotateGenes": {
        "name_process": "annotateGenes",
        "string_process": "\nprocess annotateGenes {\n    container \"${anvio_container}\"\n    label \"cpu_high\"\n    \n    input:\n    set name, file(db) from genomeDB_ch\n    file anvio_cogs_tar\n    \n    output:\n    file \"${db}\" into annotatedDB\n\n    \"\"\"\n#!/bin/bash\ntar xvf ${anvio_cogs_tar}\nanvi-run-ncbi-cogs -c \"${db}\" --num-threads ${task.cpus} --cog-data-dir COGS_DIR\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "\"\"\"\n#!/bin/bash\ntar xvf ${anvio_cogs_tar}\nanvi-run-ncbi-cogs -c \"${db}\" --num-threads ${task.cpus} --cog-data-dir COGS_DIR\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "genomeDB_ch",
            "anvio_cogs_tar"
        ],
        "nb_inputs": 2,
        "outputs": [
            "annotatedDB"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${anvio_container}\"",
            "label \"cpu_high\""
        ],
        "when": "",
        "stub": ""
    },
    "linkGeneName": {
        "name_process": "linkGeneName",
        "string_process": "\nprocess linkGeneName {\n    container \"${anvio_container}\"\n    label \"mem_medium\"\n    \n    input:\n    set name, file(db) from nameDB_ch\n    \n    output:\n    file \"${db}.txt\" into layer_txt_for_combineGenomes\n\n    \"\"\"\n#!/bin/bash\n# Link the name to the database\necho -e ${name},${db} | tr ',' '\\\\t' > ${db}.txt\n    \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "\"\"\"\n#!/bin/bash\n# Link the name to the database\necho -e ${name},${db} | tr ',' '\\\\t' > ${db}.txt\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "nameDB_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "layer_txt_for_combineGenomes"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${anvio_container}\"",
            "label \"mem_medium\""
        ],
        "when": "",
        "stub": ""
    },
    "combineGenomes": {
        "name_process": "combineGenomes",
        "string_process": "\nprocess combineGenomes {\n    container \"${anvio_container}\"\n    label \"mem_medium\"\n    publishDir \"${params.output_folder}\"\n    \n    input:\n    file db_list from annotatedDB.collect()\n    file txt_list from layer_txt_for_combineGenomes.collect()\n    \n    output:\n    file \"${params.output_name}-GENOMES.db\" into combinedDB\n    file \"external-genomes.txt\" into external_genomes_for_ani\n\n    \"\"\"\n#!/bin/bash\necho -e \"name\\\\tcontigs_db_path\" > external-genomes.txt\nfor fp in ${txt_list}; do cat \\$fp; done >> external-genomes.txt\ncat external-genomes.txt\nanvi-gen-genomes-storage -e external-genomes.txt \\\n                         -o ${params.output_name}-GENOMES.db\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\n#!/bin/bash\necho -e \"name\\\\tcontigs_db_path\" > external-genomes.txt\nfor fp in ${txt_list}; do cat \\$fp; done >> external-genomes.txt\ncat external-genomes.txt\nanvi-gen-genomes-storage -e external-genomes.txt \\\n                         -o ${params.output_name}-GENOMES.db\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "annotatedDB",
            "layer_txt_for_combineGenomes"
        ],
        "nb_inputs": 2,
        "outputs": [
            "combinedDB",
            "external_genomes_for_ani"
        ],
        "nb_outputs": 2,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${anvio_container}\"",
            "label \"mem_medium\"",
            "publishDir \"${params.output_folder}\""
        ],
        "when": "",
        "stub": ""
    },
    "panGenomeAnalysis": {
        "name_process": "panGenomeAnalysis",
        "string_process": "\nprocess panGenomeAnalysis {\n    container \"${anvio_container}\"\n    label \"cpu_high\"\n    \n    input:\n    file combinedDB\n    val output_name from params.output_name\n    val min_occurrence from params.min_occurrence\n    val minbit from params.minbit\n    val distance from params.distance\n    val linkage from params.linkage\n    val mcl_inflation from params.mcl_inflation\n    \n    output:\n    file \"${params.output_name}-PAN.db\" into panGenome_for_addMetadata, panGenome_for_getSeqs\n\n    \"\"\"\n#!/bin/bash\nanvi-pan-genome -g ${combinedDB} \\\n                --project-name ${output_name} \\\n                --output-dir ./ \\\n                --num-threads ${task.cpus} \\\n                --use-ncbi-blast \\\n                --min-occurrence ${min_occurrence} \\\n                --minbit ${minbit} \\\n                --distance ${distance} \\\n                --linkage ${linkage} \\\n                --mcl-inflation ${mcl_inflation}\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "\"\"\"\n#!/bin/bash\nanvi-pan-genome -g ${combinedDB} \\\n                --project-name ${output_name} \\\n                --output-dir ./ \\\n                --num-threads ${task.cpus} \\\n                --use-ncbi-blast \\\n                --min-occurrence ${min_occurrence} \\\n                --minbit ${minbit} \\\n                --distance ${distance} \\\n                --linkage ${linkage} \\\n                --mcl-inflation ${mcl_inflation}\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "combinedDB",
            "params",
            "params",
            "params",
            "params",
            "params",
            "params"
        ],
        "nb_inputs": 7,
        "outputs": [
            "panGenome_for_addMetadata",
            "panGenome_for_getSeqs"
        ],
        "nb_outputs": 2,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${anvio_container}\"",
            "label \"cpu_high\""
        ],
        "when": "",
        "stub": ""
    },
    "getSequencesForGCs": {
        "name_process": "getSequencesForGCs",
        "string_process": "\nprocess getSequencesForGCs {\n    container \"${anvio_container}\"\n    label \"mem_medium\"\n    publishDir \"${params.output_folder}\"\n    \n    input:\n    file panGenome from panGenome_for_getSeqs\n    file combinedDB\n    val output_name from params.output_name\n    \n    output:\n    file \"${output_name}.gene_clusters.fastp\"\n    file \"${output_name}.gene_clusters.fasta\"\n\n    \"\"\"\n#!/bin/bash\n\nset -e\n    \nanvi-get-sequences-for-gene-clusters \\\n    -p ${panGenome} \\\n    -g ${combinedDB} \\\n    -o ${output_name}.gene_clusters.fastp \\\n    --just-do-it\n\nanvi-get-sequences-for-gene-clusters \\\n    -p ${panGenome} \\\n    -g ${combinedDB} \\\n    -o ${output_name}.gene_clusters.fasta \\\n    --report-DNA-sequences \\\n    --just-do-it\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n    \nanvi-get-sequences-for-gene-clusters \\\n    -p ${panGenome} \\\n    -g ${combinedDB} \\\n    -o ${output_name}.gene_clusters.fastp \\\n    --just-do-it\n\nanvi-get-sequences-for-gene-clusters \\\n    -p ${panGenome} \\\n    -g ${combinedDB} \\\n    -o ${output_name}.gene_clusters.fasta \\\n    --report-DNA-sequences \\\n    --just-do-it\n    \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "panGenome_for_getSeqs",
            "combinedDB",
            "params"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${anvio_container}\"",
            "label \"mem_medium\"",
            "publishDir \"${params.output_folder}\""
        ],
        "when": "",
        "stub": ""
    },
    "addMetadata": {
        "name_process": "addMetadata",
        "string_process": "\nprocess addMetadata {\n    container \"${anvio_container}\"\n    label \"io_limited\"\n    publishDir \"${params.output_folder}\"\n    \n    input:\n    file panGenome from panGenome_for_addMetadata\n    file combinedDB\n    file sample_sheet from sample_sheet_to_parse\n    \n    output:\n    file \"${panGenome}\" into panGenome_for_enrichFunctions, panGenome_for_ani\n\n    \"\"\"\n#!/bin/bash\n\n# Strip out the genome file path\ncat ${sample_sheet} | cut -f 2- > TEMP && mv TEMP ${sample_sheet}\necho \"Printing the reformatted sample sheet:\"\ncat ${sample_sheet}\n\nif (( \\$(head -1 ${sample_sheet} | tr \"\\\\t\" \"\\\\n\" | wc -l ) > 1 )); then\n    echo \"\"\n    echo \"Adding metadata\"\n    anvi-import-misc-data ${sample_sheet} \\\n                          -p ${panGenome} \\\n                          --target-data-table layers\nelse\n    echo \"\"\n    echo \"Not adding any metadata, none provided\"\n\nfi\n    \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "\"\"\"\n#!/bin/bash\n\n# Strip out the genome file path\ncat ${sample_sheet} | cut -f 2- > TEMP && mv TEMP ${sample_sheet}\necho \"Printing the reformatted sample sheet:\"\ncat ${sample_sheet}\n\nif (( \\$(head -1 ${sample_sheet} | tr \"\\\\t\" \"\\\\n\" | wc -l ) > 1 )); then\n    echo \"\"\n    echo \"Adding metadata\"\n    anvi-import-misc-data ${sample_sheet} \\\n                          -p ${panGenome} \\\n                          --target-data-table layers\nelse\n    echo \"\"\n    echo \"Not adding any metadata, none provided\"\n\nfi\n    \"\"\"",
        "nb_lignes_script": 19,
        "language_script": "bash",
        "tools": [
            "TEMP"
        ],
        "tools_url": [
            "https://bio.tools/temp"
        ],
        "tools_dico": [
            {
                "name": "TEMP",
                "uri": "https://bio.tools/temp",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2885",
                            "term": "DNA polymorphism"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0427",
                                    "term": "Transposon prediction"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A software package for detecting transposable elements (TEs) insertions and excisions from pooled high-throughput sequencing data.",
                "homepage": "https://github.com/JialiUMassWengLab/TEMP"
            }
        ],
        "inputs": [
            "panGenome_for_addMetadata",
            "combinedDB",
            "sample_sheet_to_parse"
        ],
        "nb_inputs": 3,
        "outputs": [
            "panGenome_for_enrichFunctions",
            "panGenome_for_ani"
        ],
        "nb_outputs": 2,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${anvio_container}\"",
            "label \"io_limited\"",
            "publishDir \"${params.output_folder}\""
        ],
        "when": "",
        "stub": ""
    },
    "enrichFunctions": {
        "name_process": "enrichFunctions",
        "string_process": " process enrichFunctions{\n        container \"${anvio_container}\"\n        label \"mem_medium\"\n        publishDir \"${params.output_folder}\"\n        \n        input:\n        file panGenome from panGenome_for_enrichFunctions\n        file combinedDB\n        val output_name from params.output_name\n        val category_name from Channel.of(params.category_name.split(\",\"))\n        \n        output:\n        file \"${output_name}-enriched-functions-${category_name}.txt\"\n        file \"${output_name}-functions-occurrence.txt\"\n\n        script:\n\n        if (params.gene_enrichment == false)\n            \"\"\"#!/bin/bash\n            anvi-compute-functional-enrichment -p ${panGenome} \\\n                                                -g ${combinedDB} \\\n                                                --category-variable ${category_name} \\\n                                                --annotation-source COG20_FUNCTION \\\n                                                -o \"${output_name}-enriched-functions-${category_name}.txt\" \\\n                                                --functional-occurrence-table-output \"${output_name}-functions-occurrence.txt\"\n        \"\"\"\n\n        else\n            \"\"\"#!/bin/bash\n            anvi-compute-functional-enrichment -p ${panGenome} \\\n                                                -g ${combinedDB} \\\n                                                --category-variable ${category_name} \\\n                                                --annotation-source IDENTITY \\\n                                                --include-gc-identity-as-function \\\n                                                -o \"${output_name}-enriched-functions-${category_name}.txt\" \\\n                                                --functional-occurrence-table-output \"${output_name}-functions-occurrence.txt\"\n            \"\"\"\n    }",
        "nb_lignes_process": 36,
        "string_script": "        if (params.gene_enrichment == false)\n            \"\"\"#!/bin/bash\n            anvi-compute-functional-enrichment -p ${panGenome} \\\n                                                -g ${combinedDB} \\\n                                                --category-variable ${category_name} \\\n                                                --annotation-source COG20_FUNCTION \\\n                                                -o \"${output_name}-enriched-functions-${category_name}.txt\" \\\n                                                --functional-occurrence-table-output \"${output_name}-functions-occurrence.txt\"\n        \"\"\"\n\n        else\n            \"\"\"#!/bin/bash\n            anvi-compute-functional-enrichment -p ${panGenome} \\\n                                                -g ${combinedDB} \\\n                                                --category-variable ${category_name} \\\n                                                --annotation-source IDENTITY \\\n                                                --include-gc-identity-as-function \\\n                                                -o \"${output_name}-enriched-functions-${category_name}.txt\" \\\n                                                --functional-occurrence-table-output \"${output_name}-functions-occurrence.txt\"\n            \"\"\"",
        "nb_lignes_script": 19,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "panGenome_for_enrichFunctions",
            "combinedDB",
            "params"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${anvio_container}\"",
            "label \"mem_medium\"",
            "publishDir \"${params.output_folder}\""
        ],
        "when": "",
        "stub": ""
    },
    "computeANI": {
        "name_process": "computeANI",
        "string_process": " process computeANI {\n        container \"${anvio_container}\"\n        label \"cpu_high\"\n        publishDir \"${params.output_folder}\"\n        \n        input:\n        file panGenome from panGenome_for_ani\n        val output_name from params.output_name\n        val min_alignment_fraction from params.min_alignment_fraction\n        file combinedDB\n        file genome_db_list from aniDB_ch.collect()\n        file externalGenomes from external_genomes_for_ani\n        \n        output:\n        file \"${panGenome}\"\n        file \"ANI/*\"\n\n        \"\"\"\n    #!/bin/bash\n\n    set -e\n        \n    anvi-compute-genome-similarity \\\n        --external-genomes ${externalGenomes} \\\n        --min-alignment-fraction ${min_alignment_fraction} \\\n        --output-dir ANI \\\n        --num-threads ${task.cpus} \\\n        --pan-db ${panGenome} \\\n        --program ${params.ani_program} \\\n        -T ${task.cpus}\n        \"\"\"\n    }",
        "nb_lignes_process": 30,
        "string_script": "\"\"\"\n    #!/bin/bash\n\n    set -e\n        \n    anvi-compute-genome-similarity \\\n        --external-genomes ${externalGenomes} \\\n        --min-alignment-fraction ${min_alignment_fraction} \\\n        --output-dir ANI \\\n        --num-threads ${task.cpus} \\\n        --pan-db ${panGenome} \\\n        --program ${params.ani_program} \\\n        -T ${task.cpus}\n        \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "panGenome_for_ani",
            "params",
            "params",
            "combinedDB",
            "aniDB_ch",
            "external_genomes_for_ani"
        ],
        "nb_inputs": 6,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${anvio_container}\"",
            "label \"cpu_high\"",
            "publishDir \"${params.output_folder}\""
        ],
        "when": "",
        "stub": ""
    },
    "unicyclerShortReadsOnly": {
        "name_process": "unicyclerShortReadsOnly",
        "string_process": " process unicyclerShortReadsOnly {\n        container \"quay.io/biocontainers/unicycler:0.4.7--py37hdbcaa40_1\"\n        cpus 16\n        memory \"128 GB\"\n        errorStrategy \"retry\"\n        publishDir \"${params.output_folder}/${genome_name}/${read_type}/${mode}/\"\n\n        input:\n        val threads from 16\n        val read_type from \"short_reads\"\n        val min_fasta_length from params.min_fasta_length\n        set val(genome_name), file(short_R1), file(short_R2) from illumina_ch\n        each mode from unicycler_modes\n\n        output:\n        file \"${genome_name}/${genome_name}.${read_type}.${mode}.gfa\"\n        file \"${genome_name}/${genome_name}.${read_type}.${mode}.log\"\n        set val(\"${genome_name}\"), file(\"${genome_name}/${genome_name}.${read_type}.${mode}.fasta.gz\"), val(\"${mode}\"), val(\"short_reads\") into contigsShortReadsOnly\n        \n\n        afterScript \"rm -rf *\"\n\n    \"\"\"\n    set -e\n\n    mkdir ${genome_name}\n\n    unicycler \\\n        -1 ${short_R1} \\\n        -2 ${short_R2} \\\n        -o ${genome_name} \\\n        --min_fasta_length ${min_fasta_length} \\\n        --keep 0 \\\n        --mode ${mode} \\\n        -t ${threads}\n\n    mv ${genome_name}/assembly.gfa ${genome_name}/${genome_name}.${read_type}.${mode}.gfa\n    mv ${genome_name}/assembly.fasta ${genome_name}/${genome_name}.${read_type}.${mode}.fasta\n    mv ${genome_name}/unicycler.log ${genome_name}/${genome_name}.${read_type}.${mode}.log\n    gzip ${genome_name}/${genome_name}.${read_type}.${mode}.fasta\n    \"\"\"\n    }",
        "nb_lignes_process": 40,
        "string_script": "\"\"\"\n    set -e\n\n    mkdir ${genome_name}\n\n    unicycler \\\n        -1 ${short_R1} \\\n        -2 ${short_R2} \\\n        -o ${genome_name} \\\n        --min_fasta_length ${min_fasta_length} \\\n        --keep 0 \\\n        --mode ${mode} \\\n        -t ${threads}\n\n    mv ${genome_name}/assembly.gfa ${genome_name}/${genome_name}.${read_type}.${mode}.gfa\n    mv ${genome_name}/assembly.fasta ${genome_name}/${genome_name}.${read_type}.${mode}.fasta\n    mv ${genome_name}/unicycler.log ${genome_name}/${genome_name}.${read_type}.${mode}.log\n    gzip ${genome_name}/${genome_name}.${read_type}.${mode}.fasta\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [
            "Unicycler"
        ],
        "tools_url": [
            "https://bio.tools/unicycler"
        ],
        "tools_dico": [
            {
                "name": "Unicycler",
                "uri": "https://bio.tools/unicycler",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3436",
                                    "term": "Aggregation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0925",
                                "term": "Sequence assembly"
                            }
                        ]
                    }
                ],
                "description": "A tool for assembling bacterial genomes from a combination of short (2nd generation) and long (3rd generation) sequencing reads.",
                "homepage": "https://github.com/rrwick/Unicycler"
            }
        ],
        "inputs": [
            "16",
            "\"short_reads\"",
            "params",
            "illumina_ch",
            "unicycler_modes"
        ],
        "nb_inputs": 5,
        "outputs": [
            "contigsShortReadsOnly"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/biocontainers/unicycler:0.4.7--py37hdbcaa40_1\"",
            "cpus 16",
            "memory \"128 GB\"",
            "errorStrategy \"retry\"",
            "publishDir \"${params.output_folder}/${genome_name}/${read_type}/${mode}/\""
        ],
        "when": "",
        "stub": ""
    },
    "unicyclerLongReadsOnly": {
        "name_process": "unicyclerLongReadsOnly",
        "string_process": " process unicyclerLongReadsOnly {\n        container \"quay.io/biocontainers/unicycler:0.4.7--py37hdbcaa40_1\"\n        cpus 16\n        memory \"128 GB\"\n        errorStrategy \"retry\"\n        publishDir \"${params.output_folder}/${genome_name}/${read_type}/${mode}/\"\n\n        input:\n        val threads from 16\n        val read_type from \"long_reads\"\n        val min_fasta_length from params.min_fasta_length\n        set val(genome_name), file(long_reads) from long_read_ch\n        each mode from unicycler_modes\n\n        output:\n        file \"${genome_name}/${genome_name}.${read_type}.${mode}.gfa\"\n        set val(\"${genome_name}\"), file(\"${genome_name}/${genome_name}.${read_type}.${mode}.fasta.gz\"), val(\"${mode}\"), val(\"long_reads\") into contigsLongReadsOnly\n        file \"${genome_name}/${genome_name}.${read_type}.${mode}.log\"\n\n        afterScript \"rm -rf *\"\n\n    \"\"\"\n    set -e\n\n    mkdir ${genome_name}\n\n    unicycler \\\n        -l ${long_reads} \\\n        -o ${genome_name} \\\n        --min_fasta_length ${min_fasta_length} \\\n        --keep 0 \\\n        --mode ${mode} \\\n        -t ${threads}\n\n    mv ${genome_name}/assembly.gfa ${genome_name}/${genome_name}.${read_type}.${mode}.gfa\n    mv ${genome_name}/assembly.fasta ${genome_name}/${genome_name}.${read_type}.${mode}.fasta\n    mv ${genome_name}/unicycler.log ${genome_name}/${genome_name}.${read_type}.${mode}.log\n    gzip ${genome_name}/${genome_name}.${read_type}.${mode}.fasta\n    \"\"\"\n    }",
        "nb_lignes_process": 38,
        "string_script": "\"\"\"\n    set -e\n\n    mkdir ${genome_name}\n\n    unicycler \\\n        -l ${long_reads} \\\n        -o ${genome_name} \\\n        --min_fasta_length ${min_fasta_length} \\\n        --keep 0 \\\n        --mode ${mode} \\\n        -t ${threads}\n\n    mv ${genome_name}/assembly.gfa ${genome_name}/${genome_name}.${read_type}.${mode}.gfa\n    mv ${genome_name}/assembly.fasta ${genome_name}/${genome_name}.${read_type}.${mode}.fasta\n    mv ${genome_name}/unicycler.log ${genome_name}/${genome_name}.${read_type}.${mode}.log\n    gzip ${genome_name}/${genome_name}.${read_type}.${mode}.fasta\n    \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [
            "Unicycler"
        ],
        "tools_url": [
            "https://bio.tools/unicycler"
        ],
        "tools_dico": [
            {
                "name": "Unicycler",
                "uri": "https://bio.tools/unicycler",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3436",
                                    "term": "Aggregation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0925",
                                "term": "Sequence assembly"
                            }
                        ]
                    }
                ],
                "description": "A tool for assembling bacterial genomes from a combination of short (2nd generation) and long (3rd generation) sequencing reads.",
                "homepage": "https://github.com/rrwick/Unicycler"
            }
        ],
        "inputs": [
            "16",
            "\"long_reads\"",
            "params",
            "long_read_ch",
            "unicycler_modes"
        ],
        "nb_inputs": 5,
        "outputs": [
            "contigsLongReadsOnly"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/biocontainers/unicycler:0.4.7--py37hdbcaa40_1\"",
            "cpus 16",
            "memory \"128 GB\"",
            "errorStrategy \"retry\"",
            "publishDir \"${params.output_folder}/${genome_name}/${read_type}/${mode}/\""
        ],
        "when": "",
        "stub": ""
    },
    "unicyclerHybrid": {
        "name_process": "unicyclerHybrid",
        "string_process": " process unicyclerHybrid {\n            container \"quay.io/biocontainers/unicycler:0.4.7--py37hdbcaa40_1\"\n            cpus 16\n            memory \"128 GB\"\n            errorStrategy \"retry\"\n            publishDir \"${params.output_folder}/${genome_name}/${read_type}/${mode}/\"\n\n            input:\n            val threads from 16\n            val read_type from \"hybrid\"\n            val min_fasta_length from params.min_fasta_length\n            set val(genome_name), file(long_reads), file(short_R1), file(short_R2) from hybrid_ch\n            each mode from unicycler_modes\n\n            output:\n            file \"${genome_name}/${genome_name}.${read_type}.${mode}.gfa\"\n            set val(\"${genome_name}\"), file(\"${genome_name}/${genome_name}.${read_type}.${mode}.fasta.gz\"), val(\"${mode}\"), val(\"hybrid\") into contigsHybrid\n            file \"${genome_name}/${genome_name}.${read_type}.${mode}.log\"\n\n            afterScript \"rm -rf *\"\n\n        \"\"\"\n        set -e\n\n        mkdir ${genome_name}\n\n        unicycler \\\n            -1 ${short_R1} \\\n            -2 ${short_R2} \\\n            -l ${long_reads} \\\n            -o ${genome_name} \\\n            --min_fasta_length ${min_fasta_length} \\\n            --keep 0 \\\n            --mode ${mode} \\\n            -t ${threads}\n\n        mv ${genome_name}/assembly.gfa ${genome_name}/${genome_name}.${read_type}.${mode}.gfa\n        mv ${genome_name}/assembly.fasta ${genome_name}/${genome_name}.${read_type}.${mode}.fasta\n        mv ${genome_name}/unicycler.log ${genome_name}/${genome_name}.${read_type}.${mode}.log\n        gzip ${genome_name}/${genome_name}.${read_type}.${mode}.fasta\n        \"\"\"\n        }",
        "nb_lignes_process": 40,
        "string_script": "\"\"\"\n        set -e\n\n        mkdir ${genome_name}\n\n        unicycler \\\n            -1 ${short_R1} \\\n            -2 ${short_R2} \\\n            -l ${long_reads} \\\n            -o ${genome_name} \\\n            --min_fasta_length ${min_fasta_length} \\\n            --keep 0 \\\n            --mode ${mode} \\\n            -t ${threads}\n\n        mv ${genome_name}/assembly.gfa ${genome_name}/${genome_name}.${read_type}.${mode}.gfa\n        mv ${genome_name}/assembly.fasta ${genome_name}/${genome_name}.${read_type}.${mode}.fasta\n        mv ${genome_name}/unicycler.log ${genome_name}/${genome_name}.${read_type}.${mode}.log\n        gzip ${genome_name}/${genome_name}.${read_type}.${mode}.fasta\n        \"\"\"",
        "nb_lignes_script": 19,
        "language_script": "bash",
        "tools": [
            "Unicycler"
        ],
        "tools_url": [
            "https://bio.tools/unicycler"
        ],
        "tools_dico": [
            {
                "name": "Unicycler",
                "uri": "https://bio.tools/unicycler",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3436",
                                    "term": "Aggregation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0925",
                                "term": "Sequence assembly"
                            }
                        ]
                    }
                ],
                "description": "A tool for assembling bacterial genomes from a combination of short (2nd generation) and long (3rd generation) sequencing reads.",
                "homepage": "https://github.com/rrwick/Unicycler"
            }
        ],
        "inputs": [
            "16",
            "\"hybrid\"",
            "params",
            "hybrid_ch",
            "unicycler_modes"
        ],
        "nb_inputs": 5,
        "outputs": [
            "contigsHybrid"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/biocontainers/unicycler:0.4.7--py37hdbcaa40_1\"",
            "cpus 16",
            "memory \"128 GB\"",
            "errorStrategy \"retry\"",
            "publishDir \"${params.output_folder}/${genome_name}/${read_type}/${mode}/\""
        ],
        "when": "",
        "stub": ""
    },
    "summarizeAssemblies": {
        "name_process": "summarizeAssemblies",
        "string_process": "\nprocess summarizeAssemblies {\n    container \"quay.io/fhcrc-microbiome/python-pandas:latest\"\n    cpus 1\n    memory \"4 GB\"\n    errorStrategy \"retry\"\n    publishDir \"${params.output_folder}/${genome_name}/${read_type}/${mode}/\"\n\n    input:\n    set val(genome_name), file(contigs_fasta_gz), val(mode), val(read_type) from contigsForSummary\n\n    output:\n    file \"${genome_name}.${read_type}.${mode}.json\" into assembly_summaries_ch\n\n    afterScript \"rm -rf *\"\n\n\"\"\"\n#!/usr/bin/env python3\nimport gzip\nimport json\nimport pandas as pd\n\n# Count the size, depth, and circularity of each contig\ncontig_info = []\ncontig_lengths = dict()\nlength_buffer = 0\ncontig_name = None\nfor l in gzip.open(\"${contigs_fasta_gz}\", \"rt\"):\n    if l.startswith(\">\"):\n        if contig_name is not None:\n            contig_lengths[contig_name] = length_buffer\n            length_buffer = 0\n        if \" \" in l:\n            contig_name, contig_dict = l.lstrip(\">\").rstrip(\"\\\\n\").split(\" \", 1)\n            contig_dict = dict([\n                (i.split(\"=\")[0], i.split(\"=\")[1])\n                for i in contig_dict.split(\" \")\n            ])\n            contig_dict[\"name\"] = contig_name\n            contig_dict[\"circular\"] = contig_dict.get(\"circular\", \"false\")\n            contig_info.append(contig_dict)\n        else:\n            contig_name = l.lstrip(\">\").rstrip(\"\\\\n\")\n    else:\n        length_buffer += len(l.rstrip(\"\\\\n\"))\n# Add the final contig\ncontig_lengths[contig_name] = length_buffer\n\n# Make into a DataFrame\nif len(contig_info) > 0:\n    contig_info = pd.DataFrame(contig_info)\n    contig_info[\"length\"] = contig_info[\"length\"].apply(int)\n    contig_info[\"depth\"] = contig_info[\"depth\"].apply(lambda s: float(s.rstrip(\"x\")))\n    contig_info[\"circular\"] = contig_info[\"circular\"].fillna(\"false\") == \"true\"\nelse:\n    contig_info = pd.DataFrame(dict([(\"length\", contig_lengths)])).reset_index()\n    contig_info[\"depth\"] = 1\n    contig_info[\"circular\"] = False\ncontig_info.sort_values(by=\"length\", ascending=False, inplace=True)\n\n# Calculate N50\nrunning_total = 0\nn50 = None\nfor nbases in contig_info[\"length\"].values:\n    running_total += nbases\n    if running_total >= contig_info[\"length\"].sum() / 2.:\n        n50 = int(nbases)\n        break\nassert n50 is not None\n\n# Summarize these contigs\noutput = {\n    \"mode\": \"${mode}\",\n    \"read_type\": \"${read_type}\",\n    \"genome_name\": \"${genome_name}\",\n    \"num_contigs\": int(contig_info.shape[0]),\n    \"num_circular_contigs\": int(contig_info[\"circular\"].sum()),\n    \"circular_contig_lengths\": \", \".join(contig_info.query(\"circular\")[\"length\"].apply(str).tolist()),\n    \"linear_contig_lengths\": \", \".join(contig_info.query(\"circular == False\")[\"length\"].apply(str).tolist()),\n    \"num_bases\": int(contig_info[\"length\"].sum()),\n    \"longest_contig\": int(contig_info[\"length\"].max()),\n    \"num_over_1Mb\": int((contig_info[\"length\"] >= 1000000).sum()),\n    \"num_100kb_to_1Mb\": int(((contig_info[\"length\"] < 1000000) & (contig_info[\"length\"] >= 100000)).sum()),\n    \"num_10kb_to_100kb\": int(((contig_info[\"length\"] < 100000) & (contig_info[\"length\"] >= 10000)).sum()),\n    \"num_1kb_to_10kb\":    int(((contig_info[\"length\"] < 10000) & (contig_info[\"length\"] >= 1000)).sum()),\n    \"num_under_1kb\":        int((contig_info[\"length\"] < 1000).sum()),\n    \"N50\": n50,\n}\n\njson.dump(output, open(\"${genome_name}.${read_type}.${mode}.json\", \"wt\"), indent=4)\n\n\"\"\"\n}",
        "nb_lignes_process": 91,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport gzip\nimport json\nimport pandas as pd\n\n# Count the size, depth, and circularity of each contig\ncontig_info = []\ncontig_lengths = dict()\nlength_buffer = 0\ncontig_name = None\nfor l in gzip.open(\"${contigs_fasta_gz}\", \"rt\"):\n    if l.startswith(\">\"):\n        if contig_name is not None:\n            contig_lengths[contig_name] = length_buffer\n            length_buffer = 0\n        if \" \" in l:\n            contig_name, contig_dict = l.lstrip(\">\").rstrip(\"\\\\n\").split(\" \", 1)\n            contig_dict = dict([\n                (i.split(\"=\")[0], i.split(\"=\")[1])\n                for i in contig_dict.split(\" \")\n            ])\n            contig_dict[\"name\"] = contig_name\n            contig_dict[\"circular\"] = contig_dict.get(\"circular\", \"false\")\n            contig_info.append(contig_dict)\n        else:\n            contig_name = l.lstrip(\">\").rstrip(\"\\\\n\")\n    else:\n        length_buffer += len(l.rstrip(\"\\\\n\"))\n# Add the final contig\ncontig_lengths[contig_name] = length_buffer\n\n# Make into a DataFrame\nif len(contig_info) > 0:\n    contig_info = pd.DataFrame(contig_info)\n    contig_info[\"length\"] = contig_info[\"length\"].apply(int)\n    contig_info[\"depth\"] = contig_info[\"depth\"].apply(lambda s: float(s.rstrip(\"x\")))\n    contig_info[\"circular\"] = contig_info[\"circular\"].fillna(\"false\") == \"true\"\nelse:\n    contig_info = pd.DataFrame(dict([(\"length\", contig_lengths)])).reset_index()\n    contig_info[\"depth\"] = 1\n    contig_info[\"circular\"] = False\ncontig_info.sort_values(by=\"length\", ascending=False, inplace=True)\n\n# Calculate N50\nrunning_total = 0\nn50 = None\nfor nbases in contig_info[\"length\"].values:\n    running_total += nbases\n    if running_total >= contig_info[\"length\"].sum() / 2.:\n        n50 = int(nbases)\n        break\nassert n50 is not None\n\n# Summarize these contigs\noutput = {\n    \"mode\": \"${mode}\",\n    \"read_type\": \"${read_type}\",\n    \"genome_name\": \"${genome_name}\",\n    \"num_contigs\": int(contig_info.shape[0]),\n    \"num_circular_contigs\": int(contig_info[\"circular\"].sum()),\n    \"circular_contig_lengths\": \", \".join(contig_info.query(\"circular\")[\"length\"].apply(str).tolist()),\n    \"linear_contig_lengths\": \", \".join(contig_info.query(\"circular == False\")[\"length\"].apply(str).tolist()),\n    \"num_bases\": int(contig_info[\"length\"].sum()),\n    \"longest_contig\": int(contig_info[\"length\"].max()),\n    \"num_over_1Mb\": int((contig_info[\"length\"] >= 1000000).sum()),\n    \"num_100kb_to_1Mb\": int(((contig_info[\"length\"] < 1000000) & (contig_info[\"length\"] >= 100000)).sum()),\n    \"num_10kb_to_100kb\": int(((contig_info[\"length\"] < 100000) & (contig_info[\"length\"] >= 10000)).sum()),\n    \"num_1kb_to_10kb\":    int(((contig_info[\"length\"] < 10000) & (contig_info[\"length\"] >= 1000)).sum()),\n    \"num_under_1kb\":        int((contig_info[\"length\"] < 1000).sum()),\n    \"N50\": n50,\n}\n\njson.dump(output, open(\"${genome_name}.${read_type}.${mode}.json\", \"wt\"), indent=4)\n\n\"\"\"",
        "nb_lignes_script": 75,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "contigsForSummary"
        ],
        "nb_inputs": 1,
        "outputs": [
            "assembly_summaries_ch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/python-pandas:latest\"",
            "cpus 1",
            "memory \"4 GB\"",
            "errorStrategy \"retry\"",
            "publishDir \"${params.output_folder}/${genome_name}/${read_type}/${mode}/\""
        ],
        "when": "",
        "stub": ""
    },
    "prokkaAnnotate": {
        "name_process": "prokkaAnnotate",
        "string_process": " process prokkaAnnotate {\n        container 'ummidock/prokka:1.13.3-01'\n        cpus 16\n        memory \"120 GB\"\n        errorStrategy \"retry\"\n        publishDir \"${params.output_folder}/${genome_name}/${read_type}/${mode}/\"\n\n        input:\n        set val(genome_name), file(contigs_fasta_gz), val(mode), val(read_type) from contigsForProkka\n        val threads from 16\n        \n        output:\n        file \"prokka/${genome_name}.${read_type}.${mode}.faa.gz\"\n        file \"prokka/${genome_name}.${read_type}.${mode}.gff.gz\"\n        file \"prokka/${genome_name}.${read_type}.${mode}.tsv.gz\"\n        \n        \"\"\"\n    #!/bin/bash\n    set -e;\n\n    # Decompress the assembly\n    gunzip -c ${contigs_fasta_gz} > ${contigs_fasta_gz.simpleName}.fasta\n\n    prokka \\\n        --outdir prokka/ \\\n        --prefix ${genome_name}.${read_type}.${mode} \\\n        --cpus ${threads} \\\n        --compliant \\\n        ${contigs_fasta_gz.simpleName}.fasta\n\n\n    gzip prokka/${genome_name}.${read_type}.${mode}.faa &&\n    gzip prokka/${genome_name}.${read_type}.${mode}.gff &&\n    gzip prokka/${genome_name}.${read_type}.${mode}.tsv\n        \"\"\"\n    }",
        "nb_lignes_process": 34,
        "string_script": "\"\"\"\n    #!/bin/bash\n    set -e;\n\n    # Decompress the assembly\n    gunzip -c ${contigs_fasta_gz} > ${contigs_fasta_gz.simpleName}.fasta\n\n    prokka \\\n        --outdir prokka/ \\\n        --prefix ${genome_name}.${read_type}.${mode} \\\n        --cpus ${threads} \\\n        --compliant \\\n        ${contigs_fasta_gz.simpleName}.fasta\n\n\n    gzip prokka/${genome_name}.${read_type}.${mode}.faa &&\n    gzip prokka/${genome_name}.${read_type}.${mode}.gff &&\n    gzip prokka/${genome_name}.${read_type}.${mode}.tsv\n        \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [
            "Prokka"
        ],
        "tools_url": [
            "https://bio.tools/prokka"
        ],
        "tools_dico": [
            {
                "name": "Prokka",
                "uri": "https://bio.tools/prokka",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "Coding region prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0362",
                                    "term": "Genome annotation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "ORF prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "ORF finding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene finding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene calling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Software tool to annotate bacterial, archaeal and viral genomes quickly and produce standards-compliant output files.",
                "homepage": "https://github.com/tseemann/prokka"
            }
        ],
        "inputs": [
            "contigsForProkka",
            "16"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container 'ummidock/prokka:1.13.3-01'",
            "cpus 16",
            "memory \"120 GB\"",
            "errorStrategy \"retry\"",
            "publishDir \"${params.output_folder}/${genome_name}/${read_type}/${mode}/\""
        ],
        "when": "",
        "stub": ""
    },
    "summarizeAll": {
        "name_process": "summarizeAll",
        "string_process": "\nprocess summarizeAll {\n    container \"quay.io/fhcrc-microbiome/python-pandas:latest\"\n    cpus 1\n    memory \"4 GB\"\n    errorStrategy \"retry\"\n    publishDir \"${params.output_folder}/\"\n\n    input:\n    file all_summary_jsons from assembly_summaries_ch.collect()\n\n    output:\n    file \"assembly_summary_table.csv\"\n\n    afterScript \"rm -rf *\"\n\n\"\"\"\n#!/usr/bin/env python3\nimport gzip\nimport json\nimport pandas as pd\nimport os\n\nall_summary_jsons = \"${all_summary_jsons}\".split(\" \")\n\n# Make sure that the right columns exist\ncol_names = [\"genome_name\", \"read_type\", \"mode\", \"num_contigs\", \"num_bases\", \"longest_contig\"]\n\ndf = []\nfor fp in all_summary_jsons:\n    assert os.path.exists(fp), \"File not found in working directory (%s) -- try running again\" % (fp)\n\n    print(\"Reading in %s\" % (fp))\n    dat = json.load(open(fp, \"rt\"))\n    assert isinstance(dat, dict), \"%s is not the correct format\" % (fp)\n\n    for k in col_names:\n        assert k in dat, \"%s not found in summary (%s)\" % (k, fp)\n\n    df.append(dat)\n\nprint(\"Making a single summary table\")\ndf = pd.DataFrame(df).sort_values(by=[\"genome_name\", \"read_type\", \"mode\"])\n\ndf = df.reindex(columns=[\n    \"genome_name\",\n    \"read_type\",\n    \"mode\",\n    \"num_contigs\",\n    \"num_circular_contigs\",\n    \"num_bases\",\n    \"N50\",\n    \"circular_contig_lengths\",\n    \"linear_contig_lengths\",\n    \"longest_contig\",\n    \"num_over_1Mb\",\n    \"num_100kb_to_1Mb\",\n    \"num_10kb_to_100kb\",\n    \"num_1kb_to_10kb\",\n    \"num_under_1kb\",\n])\n\nprint(\"Writing out the summary table\")\ndf.to_csv(\"assembly_summary_table.csv\", index=None)\n\n\"\"\"\n}",
        "nb_lignes_process": 65,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport gzip\nimport json\nimport pandas as pd\nimport os\n\nall_summary_jsons = \"${all_summary_jsons}\".split(\" \")\n\n# Make sure that the right columns exist\ncol_names = [\"genome_name\", \"read_type\", \"mode\", \"num_contigs\", \"num_bases\", \"longest_contig\"]\n\ndf = []\nfor fp in all_summary_jsons:\n    assert os.path.exists(fp), \"File not found in working directory (%s) -- try running again\" % (fp)\n\n    print(\"Reading in %s\" % (fp))\n    dat = json.load(open(fp, \"rt\"))\n    assert isinstance(dat, dict), \"%s is not the correct format\" % (fp)\n\n    for k in col_names:\n        assert k in dat, \"%s not found in summary (%s)\" % (k, fp)\n\n    df.append(dat)\n\nprint(\"Making a single summary table\")\ndf = pd.DataFrame(df).sort_values(by=[\"genome_name\", \"read_type\", \"mode\"])\n\ndf = df.reindex(columns=[\n    \"genome_name\",\n    \"read_type\",\n    \"mode\",\n    \"num_contigs\",\n    \"num_circular_contigs\",\n    \"num_bases\",\n    \"N50\",\n    \"circular_contig_lengths\",\n    \"linear_contig_lengths\",\n    \"longest_contig\",\n    \"num_over_1Mb\",\n    \"num_100kb_to_1Mb\",\n    \"num_10kb_to_100kb\",\n    \"num_1kb_to_10kb\",\n    \"num_under_1kb\",\n])\n\nprint(\"Writing out the summary table\")\ndf.to_csv(\"assembly_summary_table.csv\", index=None)\n\n\"\"\"",
        "nb_lignes_script": 49,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "assembly_summaries_ch"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/python-pandas:latest\"",
            "cpus 1",
            "memory \"4 GB\"",
            "errorStrategy \"retry\"",
            "publishDir \"${params.output_folder}/\""
        ],
        "when": "",
        "stub": ""
    },
    "downloadGenome": {
        "name_process": "downloadGenome",
        "string_process": "\nprocess downloadGenome {\n  container \"quay.io/fhcrc-microbiome/wget@sha256:98b90e8bb8a171182505f1e255b0bd85cbbda68f08c08b4877c3fc48e63ac82f\"\n  cpus 1\n  memory \"4 GB\"\n  errorStrategy 'retry'\n  \n  input:\n  set organism_name, fasta_url from download_genome_ch\n  \n  output:\n  set organism_name, file(\"${organism_name}.fasta.gz\") into get_headers_ch, get_ribosome_fasta_ch\n  file \"${organism_name}.fasta.gz\" into get_genome_ch\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\nwget -O ${organism_name}.fasta.gz ${fasta_url}\n\ngzip -t ${organism_name}.fasta.gz\n\n  \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\nwget -O ${organism_name}.fasta.gz ${fasta_url}\n\ngzip -t ${organism_name}.fasta.gz\n\n  \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "download_genome_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "get_headers_ch",
            "get_ribosome_fasta_ch",
            "get_genome_ch"
        ],
        "nb_outputs": 3,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/wget@sha256:98b90e8bb8a171182505f1e255b0bd85cbbda68f08c08b4877c3fc48e63ac82f\"",
            "cpus 1",
            "memory \"4 GB\"",
            "errorStrategy 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "downloadGFF": {
        "name_process": "downloadGFF",
        "string_process": "\nprocess downloadGFF {\n  container \"quay.io/fhcrc-microbiome/wget@sha256:98b90e8bb8a171182505f1e255b0bd85cbbda68f08c08b4877c3fc48e63ac82f\"\n  cpus 1\n  memory \"4 GB\"\n  errorStrategy 'retry'\n  \n  input:\n  set organism_name, gff_url from download_gff_ch\n  \n  output:\n  set organism_name, file(\"${organism_name}.gff.gz\") into get_ribosome_gff_ch, all_gff_ch\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\nwget -O ${organism_name}.gff.gz ${gff_url}\n\ngzip -t ${organism_name}.gff.gz\n\n  \"\"\"\n}",
        "nb_lignes_process": 24,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\nwget -O ${organism_name}.gff.gz ${gff_url}\n\ngzip -t ${organism_name}.gff.gz\n\n  \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "download_gff_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "get_ribosome_gff_ch",
            "all_gff_ch afterScript"
        ],
        "nb_outputs": 2,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/wget@sha256:98b90e8bb8a171182505f1e255b0bd85cbbda68f08c08b4877c3fc48e63ac82f\"",
            "cpus 1",
            "memory \"4 GB\"",
            "errorStrategy 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "indexHost": {
        "name_process": "indexHost",
        "string_process": "\nprocess indexHost {\n\n  container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"\n  cpus 8\n  memory \"60 GB\"\n  publishDir \"${params.output_folder}\"\n\n  input:\n  file host_genome\n  \n  output:\n  file \"${host_genome}.tar\"\n  \n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\nbwa index ${host_genome}\ntar cvf ${host_genome}.tar ${host_genome}*\n  \"\"\"\n\n}",
        "nb_lignes_process": 24,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\nbwa index ${host_genome}\ntar cvf ${host_genome}.tar ${host_genome}*\n  \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [
            "BWA"
        ],
        "tools_url": [
            "https://bio.tools/bwa"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            }
        ],
        "inputs": [
            "host_genome"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"",
            "cpus 8",
            "memory \"60 GB\"",
            "publishDir \"${params.output_folder}\""
        ],
        "when": "",
        "stub": ""
    },
    "extractRibosomes": {
        "name_process": "extractRibosomes",
        "string_process": "\nprocess extractRibosomes {\n  container \"quay.io/biocontainers/biopython@sha256:1196016b05927094af161ccf2cd8371aafc2e3a8daa51c51ff023f5eb45a820f\"\n  cpus 1\n  memory \"4 GB\"\n\n  input:\n  set organism_name, file(fasta), file(gff3) from get_ribosome_fasta_ch.join(get_ribosome_gff_ch)\n  \n  output:\n  file \"${fasta}.ribosome.fasta\" into ribosome_fasta_ch\n  file \"${fasta}.ribosome.tsv\" into ribosome_tsv_ch\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/usr/bin/env python3\nimport gzip\nfrom Bio.SeqIO.FastaIO import SimpleFastaParser\nfrom Bio.Seq import Seq\n\ndef safe_open(fp, mode=\"rt\"):\n    if fp.endswith(\".gz\"):\n        return gzip.open(fp, mode=mode)\n    return open(fp, mode=mode)\n\n# Get the location of all ribosomes from the GFF3\nribosomes = []\nfor line in safe_open(\"${gff3}\"):\n    if line[0] == '#':\n        continue\n    line = line.split(\"\\\\t\")\n    if line[2] == \"rRNA\":\n        # Get the gene name\n        gene_desc = dict([\n            field.split(\"=\", 1)\n            for field in line[8].split(\";\")\n        ])\n        assert \"ID\" in gene_desc\n        # Header, start, end, strand\n        assert line[6] in [\"+\", \"-\"], line[6]\n        assert int(line[3]) < int(line[4])\n        ribosomes.append((line[0], int(line[3]), int(line[4]), line[6], gene_desc[\"ID\"]))\n\n# Make sure that the gene names are all unique\nn_unique = len(set([f[4] for f in ribosomes]))\nassert n_unique == len(ribosomes), (n_unique, len(ribosomes))\n\n# Extract the sequences from the FASTA\n# Write out a TSV linking each FASTA header to the organism\nn_written = 0\nwith open(\"${fasta}.ribosome.fasta\", \"wt\") as fasta_out, open(\"${fasta}.ribosome.tsv\", \"wt\") as tsv_out:\n    for h, s in SimpleFastaParser(safe_open(\"${fasta}\")):\n        h = h.split(\" \")[0].split(\"\\\\t\")[0]\n        for header, start, end, strand, gene_id in ribosomes:\n            if header == h:\n                \n                gene_sequence = s[start - 1: end]\n                if strand == \"-\":\n                    gene_sequence = str(Seq(gene_sequence).reverse_complement())\n\n                gene_name = header + \"_\" + gene_id\n                fasta_out.write(\">\" + gene_name + \"\\\\n\" + gene_sequence + \"\\\\n\")\n                tsv_out.write(\"${organism_name}\\\\t\" + gene_name + \"\\\\n\")\n                n_written += 1\n\nassert n_written == len(ribosomes), (n_written, len(ribosomes))\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 69,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport gzip\nfrom Bio.SeqIO.FastaIO import SimpleFastaParser\nfrom Bio.Seq import Seq\n\ndef safe_open(fp, mode=\"rt\"):\n    if fp.endswith(\".gz\"):\n        return gzip.open(fp, mode=mode)\n    return open(fp, mode=mode)\n\n# Get the location of all ribosomes from the GFF3\nribosomes = []\nfor line in safe_open(\"${gff3}\"):\n    if line[0] == '#':\n        continue\n    line = line.split(\"\\\\t\")\n    if line[2] == \"rRNA\":\n        # Get the gene name\n        gene_desc = dict([\n            field.split(\"=\", 1)\n            for field in line[8].split(\";\")\n        ])\n        assert \"ID\" in gene_desc\n        # Header, start, end, strand\n        assert line[6] in [\"+\", \"-\"], line[6]\n        assert int(line[3]) < int(line[4])\n        ribosomes.append((line[0], int(line[3]), int(line[4]), line[6], gene_desc[\"ID\"]))\n\n# Make sure that the gene names are all unique\nn_unique = len(set([f[4] for f in ribosomes]))\nassert n_unique == len(ribosomes), (n_unique, len(ribosomes))\n\n# Extract the sequences from the FASTA\n# Write out a TSV linking each FASTA header to the organism\nn_written = 0\nwith open(\"${fasta}.ribosome.fasta\", \"wt\") as fasta_out, open(\"${fasta}.ribosome.tsv\", \"wt\") as tsv_out:\n    for h, s in SimpleFastaParser(safe_open(\"${fasta}\")):\n        h = h.split(\" \")[0].split(\"\\\\t\")[0]\n        for header, start, end, strand, gene_id in ribosomes:\n            if header == h:\n                \n                gene_sequence = s[start - 1: end]\n                if strand == \"-\":\n                    gene_sequence = str(Seq(gene_sequence).reverse_complement())\n\n                gene_name = header + \"_\" + gene_id\n                fasta_out.write(\">\" + gene_name + \"\\\\n\" + gene_sequence + \"\\\\n\")\n                tsv_out.write(\"${organism_name}\\\\t\" + gene_name + \"\\\\n\")\n                n_written += 1\n\nassert n_written == len(ribosomes), (n_written, len(ribosomes))\n\n  \"\"\"",
        "nb_lignes_script": 53,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "get_ribosome_fasta_ch",
            "get_ribosome_gff_ch"
        ],
        "nb_inputs": 2,
        "outputs": [
            "ribosome_fasta_ch",
            "ribosome_tsv_ch"
        ],
        "nb_outputs": 2,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/biocontainers/biopython@sha256:1196016b05927094af161ccf2cd8371aafc2e3a8daa51c51ff023f5eb45a820f\"",
            "cpus 1",
            "memory \"4 GB\""
        ],
        "when": "",
        "stub": ""
    },
    "indexRibosomes": {
        "name_process": "indexRibosomes",
        "string_process": "\nprocess indexRibosomes {\n  container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"\n  cpus 8\n  memory \"60 GB\"\n  publishDir \"${params.output_folder}\"\n\n  input:\n  file ribosome_fasta from ribosome_fasta_ch.collect()\n  val database_prefix from params.database_prefix\n  \n  output:\n  file \"${database_prefix}.ribosomes.tar\"\n  \n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\n# Concatenate all FASTAs\nfor f in ${ribosome_fasta}; do\n    cat \\$f >> ${database_prefix}.ribosomes.fasta\ndone\n\n# Index the ribosomal sequences\nbwa index ${database_prefix}.ribosomes.fasta\n\n# Tar up the index\ntar cvf ${database_prefix}.ribosomes.tar ${database_prefix}.ribosomes.fasta*\n    \"\"\"\n\n}",
        "nb_lignes_process": 32,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\n# Concatenate all FASTAs\nfor f in ${ribosome_fasta}; do\n    cat \\$f >> ${database_prefix}.ribosomes.fasta\ndone\n\n# Index the ribosomal sequences\nbwa index ${database_prefix}.ribosomes.fasta\n\n# Tar up the index\ntar cvf ${database_prefix}.ribosomes.tar ${database_prefix}.ribosomes.fasta*\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "BWA"
        ],
        "tools_url": [
            "https://bio.tools/bwa"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            }
        ],
        "inputs": [
            "ribosome_fasta_ch",
            "params"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"",
            "cpus 8",
            "memory \"60 GB\"",
            "publishDir \"${params.output_folder}\""
        ],
        "when": "",
        "stub": ""
    },
    "collectRibosomeTSV": {
        "name_process": "collectRibosomeTSV",
        "string_process": "\nprocess collectRibosomeTSV {\n  container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"\n  cpus 8\n  memory \"60 GB\"\n  publishDir \"${params.output_folder}\"\n\n  input:\n  file ribosome_tsv from ribosome_tsv_ch.collect()\n  val database_prefix from params.database_prefix\n  \n  output:\n  file \"${database_prefix}.ribosomes.tsv\"\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\n# Concatenate all TSVs\nfor f in ${ribosome_tsv}; do\n    cat \\$f >> ${database_prefix}.ribosomes.tsv\ndone\n    \"\"\"\n\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\n# Concatenate all TSVs\nfor f in ${ribosome_tsv}; do\n    cat \\$f >> ${database_prefix}.ribosomes.tsv\ndone\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ribosome_tsv_ch",
            "params"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"",
            "cpus 8",
            "memory \"60 GB\"",
            "publishDir \"${params.output_folder}\""
        ],
        "when": "",
        "stub": ""
    },
    "genomeHeaders": {
        "name_process": "genomeHeaders",
        "string_process": "\nprocess genomeHeaders {\n  container \"quay.io/biocontainers/biopython@sha256:1196016b05927094af161ccf2cd8371aafc2e3a8daa51c51ff023f5eb45a820f\"\n  cpus 1\n  memory \"4 GB\"\n\n  input:\n  set organism_name, file(fasta) from get_headers_ch\n  \n  output:\n  file \"${organism_name}.headers.tsv.gz\" into genome_headers\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/usr/bin/env python3\nfrom Bio.SeqIO.FastaIO import SimpleFastaParser\nimport gzip\n\ndef safe_open(fp, mode=\"rt\"):\n    if fp.endswith(\".gz\"):\n        return gzip.open(fp, mode=mode)\n    return open(fp, mode=mode)\n\n# Extract the headers from the FASTA and write out to TSV\nwith gzip.open(\"${organism_name}.headers.tsv.gz\", \"wt\") as fo:\n    for header, seq in SimpleFastaParser(safe_open(\"${fasta}\")):\n        header = header.split(\" \")[0].split(\"\\\\t\")[0].rstrip(\"\\\\n\")\n        fo.write(\"${organism_name}\\\\t\" + header + \"\\\\n\")\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 31,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nfrom Bio.SeqIO.FastaIO import SimpleFastaParser\nimport gzip\n\ndef safe_open(fp, mode=\"rt\"):\n    if fp.endswith(\".gz\"):\n        return gzip.open(fp, mode=mode)\n    return open(fp, mode=mode)\n\n# Extract the headers from the FASTA and write out to TSV\nwith gzip.open(\"${organism_name}.headers.tsv.gz\", \"wt\") as fo:\n    for header, seq in SimpleFastaParser(safe_open(\"${fasta}\")):\n        header = header.split(\" \")[0].split(\"\\\\t\")[0].rstrip(\"\\\\n\")\n        fo.write(\"${organism_name}\\\\t\" + header + \"\\\\n\")\n\n  \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "get_headers_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "genome_headers"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/biocontainers/biopython@sha256:1196016b05927094af161ccf2cd8371aafc2e3a8daa51c51ff023f5eb45a820f\"",
            "cpus 1",
            "memory \"4 GB\""
        ],
        "when": "",
        "stub": ""
    },
    "concatGenomes": {
        "name_process": "concatGenomes",
        "string_process": "\nprocess concatGenomes {\n  container \"ubuntu:16.04\"\n  cpus 8\n  memory \"60 GB\"\n  publishDir \"${params.output_folder}\"\n  \n  input:\n  file \"*\" from get_genome_ch.collect()\n  val database_prefix from params.database_prefix\n  \n  output:\n  file \"${database_prefix}.fasta.gz\"\n  \n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\ncat *fasta.gz >> ${database_prefix}.fasta.gz\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 24,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\ncat *fasta.gz >> ${database_prefix}.fasta.gz\n\n  \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "get_genome_ch",
            "params"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"ubuntu:16.04\"",
            "cpus 8",
            "memory \"60 GB\"",
            "publishDir \"${params.output_folder}\""
        ],
        "when": "",
        "stub": ""
    },
    "concatHeaders": {
        "name_process": "concatHeaders",
        "string_process": "\nprocess concatHeaders {\n  container \"ubuntu:16.04\"\n  cpus 8\n  memory \"60 GB\"\n  publishDir \"${params.output_folder}\"\n  \n  input:\n  file \"*\" from genome_headers.collect()\n  val database_prefix from params.database_prefix\n  \n  output:\n  file \"${database_prefix}.tsv.gz\"\n\n  afterScript \"rm *\"\n\n  \"\"\"\n\n#!/bin/bash\n\nset -e\n\ncat *tsv.gz > ${database_prefix}.tsv.gz\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n\n#!/bin/bash\n\nset -e\n\ncat *tsv.gz > ${database_prefix}.tsv.gz\n\n  \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "genome_headers",
            "params"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"ubuntu:16.04\"",
            "cpus 8",
            "memory \"60 GB\"",
            "publishDir \"${params.output_folder}\""
        ],
        "when": "",
        "stub": ""
    },
    "preConcatGFF": {
        "name_process": "preConcatGFF",
        "string_process": "\nprocess preConcatGFF {\n  container \"ubuntu:16.04\"\n  cpus 8\n  memory \"60 GB\"\n  \n  input:\n  file all_gff from all_gff_ch.map{ i -> i[1] }.collate(100)\n  \n  output:\n  file \"*.gff.gz\" into grouped_gff_ch\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\nfor gff in *.gff.gz; do\n  cat \\$gff >> TEMP\n  rm \\$gff\ndone\n\nmv TEMP \\$(head /dev/urandom | tr -dc A-Za-z0-9 | head -c 16).gff.gz\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 27,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\nfor gff in *.gff.gz; do\n  cat \\$gff >> TEMP\n  rm \\$gff\ndone\n\nmv TEMP \\$(head /dev/urandom | tr -dc A-Za-z0-9 | head -c 16).gff.gz\n\n  \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "TEMP"
        ],
        "tools_url": [
            "https://bio.tools/temp"
        ],
        "tools_dico": [
            {
                "name": "TEMP",
                "uri": "https://bio.tools/temp",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2885",
                            "term": "DNA polymorphism"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0427",
                                    "term": "Transposon prediction"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A software package for detecting transposable elements (TEs) insertions and excisions from pooled high-throughput sequencing data.",
                "homepage": "https://github.com/JialiUMassWengLab/TEMP"
            }
        ],
        "inputs": [
            "all_gff_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "grouped_gff_ch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"ubuntu:16.04\"",
            "cpus 8",
            "memory \"60 GB\""
        ],
        "when": "",
        "stub": ""
    },
    "concatGFF": {
        "name_process": "concatGFF",
        "string_process": "\nprocess concatGFF {\n  container \"ubuntu:16.04\"\n  cpus 8\n  memory \"60 GB\"\n  publishDir \"${params.output_folder}\"\n  \n  input:\n  file all_gff from grouped_gff_ch.collect()\n  val database_prefix from params.database_prefix\n  \n  output:\n  file \"${database_prefix}.gff.gz\"\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\nfor gff in *.gff.gz; do\n  cat \\$gff >> ${database_prefix}.gff.gz\ndone\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\nfor gff in *.gff.gz; do\n  cat \\$gff >> ${database_prefix}.gff.gz\ndone\n\n  \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "grouped_gff_ch",
            "params"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"ubuntu:16.04\"",
            "cpus 8",
            "memory \"60 GB\"",
            "publishDir \"${params.output_folder}\""
        ],
        "when": "",
        "stub": ""
    },
    "Circlator": {
        "name_process": "Circlator",
        "string_process": "\nprocess Circlator {\n\n                            \n  container \"quay.io/fhcrc-microbiome/circlator:latest\"\n  label \"mem_medium\"\n  errorStrategy 'retry'\n  maxRetries 3\n\n                                                                                                                             \n  publishDir \"${params.output_folder}\" \n  \n  input:\n    tuple val(name), file(fasta), file(reads) from input_ch\n\n                                                                                                                                                        \n  output:\n  file \"${name}/*\"\n\n\"\"\"\n#!/bin/bash\n\nset -e\n\n# If the reads are in BAM format, convert to FASTQ\nif [[ \"${reads.name}\" == *bam ]]; then\n    samtools index ${reads}\n    circlator bam2reads ${reads} reads\n    READS=reads.fasta\nelse\n    READS=${reads}\nfi\n\ndf -h\necho \"\"\nls -lahtr\necho \"\"\necho \"STARTING CIRCLATOR\"\necho \"\"\n\ncirclator all \\\n    --threads ${task.cpus} \\\n    --merge_min_id ${params.merge_min_id} \\\n    --merge_breaklen ${params.merge_breaklen} \\\n    --b2r_length_cutoff ${params.b2r_length_cutoff} \\\n    --merge_min_length_merge ${params.merge_min_length_merge} \\\n    --merge_reassemble_end ${params.merge_reassemble_end} \\\n    --merge_ref_end ${params.merge_ref_end} \\\n    ${fasta} \\\n    \\$READS \\\n    ${name}\n\n\"\"\"\n\n}",
        "nb_lignes_process": 53,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\n# If the reads are in BAM format, convert to FASTQ\nif [[ \"${reads.name}\" == *bam ]]; then\n    samtools index ${reads}\n    circlator bam2reads ${reads} reads\n    READS=reads.fasta\nelse\n    READS=${reads}\nfi\n\ndf -h\necho \"\"\nls -lahtr\necho \"\"\necho \"STARTING CIRCLATOR\"\necho \"\"\n\ncirclator all \\\n    --threads ${task.cpus} \\\n    --merge_min_id ${params.merge_min_id} \\\n    --merge_breaklen ${params.merge_breaklen} \\\n    --b2r_length_cutoff ${params.b2r_length_cutoff} \\\n    --merge_min_length_merge ${params.merge_min_length_merge} \\\n    --merge_reassemble_end ${params.merge_reassemble_end} \\\n    --merge_ref_end ${params.merge_ref_end} \\\n    ${fasta} \\\n    \\$READS \\\n    ${name}\n\n\"\"\"",
        "nb_lignes_script": 33,
        "language_script": "bash",
        "tools": [
            "SAMtools",
            "DFP"
        ],
        "tools_url": [
            "https://bio.tools/samtools",
            "https://bio.tools/dfp"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            },
            {
                "name": "DFP",
                "uri": "https://bio.tools/dfp",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene expression profiling"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differentially expressed gene identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential expression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3223",
                                    "term": "Differential gene expression analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package provides a supervised technique able to identify differentially expressed genes, based on the construction of \\emph{Fuzzy Patterns} (FPs). The Fuzzy Patterns are built by means of applying 3 Membership Functions to discretized gene expression values.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/DFP.html"
            }
        ],
        "inputs": [
            "input_ch"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/circlator:latest\"",
            "label \"mem_medium\"",
            "errorStrategy 'retry'",
            "maxRetries 3",
            "publishDir \"${params.output_folder}\""
        ],
        "when": "",
        "stub": ""
    },
    "interleave": {
        "name_process": "interleave",
        "string_process": " process interleave {\n    container \"ubuntu:16.04\"\n    cpus 1\n    memory \"2 GB\"\n    errorStrategy \"retry\"\n\n    input:\n    set sample_name, file(fastq1), file(fastq2) from interleave_ch\n\n    output:\n    set sample_name, file(\"${fastq1}.interleaved.fastq.gz\") into concatenate_ch\n\n    \"\"\"\n    set -e\n\n    # Some basic checks that the files exist and the line numbers match\n    [[ -s \"${fastq1}\" ]]\n    [[ -s \"${fastq2}\" ]]\n    (( \\$(gunzip -c ${fastq1} | wc -l) == \\$(gunzip -c ${fastq2} | wc -l) ))\n\n    # Now interleave the files\n    paste <(gunzip -c ${fastq1}) <(gunzip -c ${fastq2}) | paste - - - - | awk -v OFS=\"\\\\n\" -v FS=\"\\\\t\" '{print(\\$1,\\$3,\\$5,\\$7,\\$2,\\$4,\\$6,\\$8)}' | gzip -c > \"${fastq1}.interleaved.fastq.gz\"\n    \"\"\"\n      \n  }",
        "nb_lignes_process": 23,
        "string_script": "\"\"\"\n    set -e\n\n    # Some basic checks that the files exist and the line numbers match\n    [[ -s \"${fastq1}\" ]]\n    [[ -s \"${fastq2}\" ]]\n    (( \\$(gunzip -c ${fastq1} | wc -l) == \\$(gunzip -c ${fastq2} | wc -l) ))\n\n    # Now interleave the files\n    paste <(gunzip -c ${fastq1}) <(gunzip -c ${fastq2}) | paste - - - - | awk -v OFS=\"\\\\n\" -v FS=\"\\\\t\" '{print(\\$1,\\$3,\\$5,\\$7,\\$2,\\$4,\\$6,\\$8)}' | gzip -c > \"${fastq1}.interleaved.fastq.gz\"\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "interleave_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "concatenate_ch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"ubuntu:16.04\"",
            "cpus 1",
            "memory \"2 GB\"",
            "errorStrategy \"retry\""
        ],
        "when": "",
        "stub": ""
    },
    "bamToFastq": {
        "name_process": "bamToFastq",
        "string_process": " process bamToFastq {\n    container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"\n    cpus 1\n    memory \"2 GB\"\n\n    input:\n    set sample_name, file(bam) from bam_ch\n    \n    output:\n    set sample_name, file(\"${bam}.fastq.gz\") into concatenate_ch\n\n    afterScript \"rm *\"\n\n    \"\"\"\n#!/bin/bash\n\nset -e\n\nsamtools fastq \"${bam}\" | gzip -c > \"${bam}.fastq.gz\"\n      \"\"\"\n\n  }",
        "nb_lignes_process": 20,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\nsamtools fastq \"${bam}\" | gzip -c > \"${bam}.fastq.gz\"\n      \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "bam_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "concatenate_ch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"",
            "cpus 1",
            "memory \"2 GB\""
        ],
        "when": "",
        "stub": ""
    },
    "concatenate": {
        "name_process": "concatenate",
        "string_process": "\nprocess concatenate {\n  container \"ubuntu:16.04\"\n  cpus 1\n  memory \"4 GB\"\n  \n  input:\n  set sample_name, file(fastq_list) from concatenate_ch.groupTuple()\n  \n  output:\n  set sample_name, file(\"${sample_name}.fastq.gz\") into count_reads, filter_host_ch\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\ncat ${fastq_list} > TEMP && mv TEMP ${sample_name}.fastq.gz\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 22,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\ncat ${fastq_list} > TEMP && mv TEMP ${sample_name}.fastq.gz\n\n  \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [
            "TEMP"
        ],
        "tools_url": [
            "https://bio.tools/temp"
        ],
        "tools_dico": [
            {
                "name": "TEMP",
                "uri": "https://bio.tools/temp",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2885",
                            "term": "DNA polymorphism"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0427",
                                    "term": "Transposon prediction"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A software package for detecting transposable elements (TEs) insertions and excisions from pooled high-throughput sequencing data.",
                "homepage": "https://github.com/JialiUMassWengLab/TEMP"
            }
        ],
        "inputs": [
            "concatenate_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "count_reads",
            "filter_host_ch afterScript"
        ],
        "nb_outputs": 2,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"ubuntu:16.04\"",
            "cpus 1",
            "memory \"4 GB\""
        ],
        "when": "",
        "stub": ""
    },
    "countReads": {
        "name_process": "countReads",
        "string_process": "\nprocess countReads {\n  container \"${container__ubuntu}\"\n  label 'io_limited'\n  errorStrategy 'retry'\n  \n  input:\n  tuple val(sample_name), file(fastq)\n  \n  output:\n  file \"${sample_name}.${params.count_reads_label}.counts.csv\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\necho \"Counting reads\"\nn=\\$(gunzip -c \"${fastq}\" | awk 'NR % 4 == 1' | wc -l)\n\necho \"Found \\$n reads\"\n(( \\$n > 0 ))\n\necho \"${sample_name},\\$n,${params.count_reads_label}\" > \"${sample_name}.${params.count_reads_label}.counts.csv\"\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\necho \"Counting reads\"\nn=\\$(gunzip -c \"${fastq}\" | awk 'NR % 4 == 1' | wc -l)\n\necho \"Found \\$n reads\"\n(( \\$n > 0 ))\n\necho \"${sample_name},\\$n,${params.count_reads_label}\" > \"${sample_name}.${params.count_reads_label}.counts.csv\"\n\n  \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_name",
            "fastq"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__ubuntu}\"",
            "label 'io_limited'",
            "errorStrategy 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "filterHostReads": {
        "name_process": "filterHostReads",
        "string_process": "\nprocess filterHostReads {\n  container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"\n  cpus 8\n  memory \"16 GB\"\n\n  input:\n  file host_genome_tar\n  set sample_name, file(fastq) from filter_host_ch\n  val min_qual from params.min_qual\n  val extra_bwa_flag\n  val samtools_filter_unmapped\n  val threads from 8\n  \n  output:\n  set sample_name, file(\"${sample_name}.filtered.fastq.gz\") into align_ribo_ch, align_genome_ch, count_nonhuman\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\n# Untar the host genome\ntar xvf ${host_genome_tar}\n\nhost_genome_name=\\$(echo ${host_genome_tar} | sed 's/.tar//')\n\n[[ -s \\$host_genome_name ]]\n\n# Align with BWA and save the unmapped BAM\nbwa mem -T ${min_qual} -t ${threads}${extra_bwa_flag}\\$host_genome_name ${fastq} > ${sample_name}.bam\necho \"Number of alignments: \\$(samtools view ${sample_name}.bam | wc -l)\"\n\ncat ${sample_name}.bam | samtools view ${samtools_filter_unmapped} > ${sample_name}.unmapped.bam\necho \"Number of unmapped alignments: \\$(samtools view ${sample_name}.unmapped.bam | wc -l)\"\n\nsamtools fastq ${sample_name}.unmapped.bam | \\\ngzip -c \\\n> ${sample_name}.filtered.fastq.gz\n\n    \"\"\"\n\n}",
        "nb_lignes_process": 43,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\n# Untar the host genome\ntar xvf ${host_genome_tar}\n\nhost_genome_name=\\$(echo ${host_genome_tar} | sed 's/.tar//')\n\n[[ -s \\$host_genome_name ]]\n\n# Align with BWA and save the unmapped BAM\nbwa mem -T ${min_qual} -t ${threads}${extra_bwa_flag}\\$host_genome_name ${fastq} > ${sample_name}.bam\necho \"Number of alignments: \\$(samtools view ${sample_name}.bam | wc -l)\"\n\ncat ${sample_name}.bam | samtools view ${samtools_filter_unmapped} > ${sample_name}.unmapped.bam\necho \"Number of unmapped alignments: \\$(samtools view ${sample_name}.unmapped.bam | wc -l)\"\n\nsamtools fastq ${sample_name}.unmapped.bam | \\\ngzip -c \\\n> ${sample_name}.filtered.fastq.gz\n\n    \"\"\"",
        "nb_lignes_script": 23,
        "language_script": "bash",
        "tools": [
            "BWA",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/bwa",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "host_genome_tar",
            "filter_host_ch",
            "params",
            "extra_bwa_flag",
            "samtools_filter_unmapped",
            "8"
        ],
        "nb_inputs": 6,
        "outputs": [
            "align_ribo_ch",
            "align_genome_ch",
            "count_nonhuman afterScript"
        ],
        "nb_outputs": 3,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"",
            "cpus 8",
            "memory \"16 GB\""
        ],
        "when": "",
        "stub": ""
    },
    "countNonhumanReads": {
        "name_process": "countNonhumanReads",
        "string_process": "\nprocess countNonhumanReads {\n  container \"ubuntu:16.04\"\n  cpus 1\n  memory \"4 GB\"\n  \n  input:\n  set sample_name, file(fastq) from count_nonhuman\n  \n  output:\n  file \"${sample_name}.countNonhumanReads.csv\" into nonhuman_counts\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\nn=\\$(gunzip -c \"${fastq}\" | awk 'NR % 4 == 1' | wc -l)\necho \"${sample_name},nonhuman_reads,\\$n\" > \"${sample_name}.countNonhumanReads.csv\"\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 23,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\nn=\\$(gunzip -c \"${fastq}\" | awk 'NR % 4 == 1' | wc -l)\necho \"${sample_name},nonhuman_reads,\\$n\" > \"${sample_name}.countNonhumanReads.csv\"\n\n  \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "count_nonhuman"
        ],
        "nb_inputs": 1,
        "outputs": [
            "nonhuman_counts"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"ubuntu:16.04\"",
            "cpus 1",
            "memory \"4 GB\""
        ],
        "when": "",
        "stub": ""
    },
    "alignRibosomes": {
        "name_process": "alignRibosomes",
        "string_process": "\nprocess alignRibosomes {\n  container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"\n  cpus 4\n  memory \"30 GB\"\n  \n  errorStrategy \"retry\"\n\n  input:\n  file ribosome_tar\n  set sample_name, file(input_fastq) from align_ribo_ch\n  val min_qual from params.min_qual\n  val samtools_filter_mapped\n  val threads from 4\n    \n  output:\n  set sample_name, file(\"${sample_name}.ribosome.bam\") into ribo_coverage_ch\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\n# Untar the indexed ribosome database\ntar xvf ${ribosome_tar}\n\n# Align with BWA and remove unmapped reads\nbwa mem -T ${min_qual} -a -t ${threads * task.attempt}${extra_bwa_flag}${params.database_prefix}.ribosomes.fasta ${input_fastq} | samtools view -b ${samtools_filter_mapped} - -o ${sample_name}.ribosome.bam\n\n    \"\"\"\n\n}",
        "nb_lignes_process": 32,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\n# Untar the indexed ribosome database\ntar xvf ${ribosome_tar}\n\n# Align with BWA and remove unmapped reads\nbwa mem -T ${min_qual} -a -t ${threads * task.attempt}${extra_bwa_flag}${params.database_prefix}.ribosomes.fasta ${input_fastq} | samtools view -b ${samtools_filter_mapped} - -o ${sample_name}.ribosome.bam\n\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "BWA",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/bwa",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "ribosome_tar",
            "align_ribo_ch",
            "params",
            "samtools_filter_mapped",
            "4"
        ],
        "nb_inputs": 5,
        "outputs": [
            "ribo_coverage_ch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"",
            "cpus 4",
            "memory \"30 GB\"",
            "errorStrategy \"retry\""
        ],
        "when": "",
        "stub": ""
    },
    "riboCoverage": {
        "name_process": "riboCoverage",
        "string_process": "\nprocess riboCoverage {\n  container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"\n  cpus 4\n  memory \"30 GB\"\n\n  input:\n  set sample_name, file(bam) from ribo_coverage_ch\n  \n  output:\n  set sample_name, file(\"${sample_name}.ribosome.pileup\"), file(\"${sample_name}.ribosome.idxstats\") into ribo_hits_ch\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\nsamtools sort -m 2G ${bam} > ${bam}.sorted\nsamtools index ${bam}.sorted\nsamtools mpileup ${bam}.sorted > ${sample_name}.ribosome.pileup\nsamtools idxstats ${bam}.sorted > ${sample_name}.ribosome.idxstats\nrm ${bam}.sorted ${bam}\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\nsamtools sort -m 2G ${bam} > ${bam}.sorted\nsamtools index ${bam}.sorted\nsamtools mpileup ${bam}.sorted > ${sample_name}.ribosome.pileup\nsamtools idxstats ${bam}.sorted > ${sample_name}.ribosome.idxstats\nrm ${bam}.sorted ${bam}\n\n  \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "ribo_coverage_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "ribo_hits_ch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"",
            "cpus 4",
            "memory \"30 GB\""
        ],
        "when": "",
        "stub": ""
    },
    "pickGenomes": {
        "name_process": "pickGenomes",
        "string_process": "\nprocess pickGenomes {\n  container \"quay.io/fhcrc-microbiome/python-pandas:v0.24.2\"\n  cpus 8\n  memory \"16 GB\"\n\n  input:\n  set sample_name, file(sample_pileup), file(sample_idxstats) from ribo_hits_ch\n  file ribosome_tsv\n  val min_cov_pct from params.min_cov_pct\n  \n  output:\n  file \"${sample_name}.genomes.txt\" into sample_genomes_txt\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/usr/bin/env python3\nimport os\nimport json\nimport pandas as pd\n\n# Read in a file with the length of each reference\nidxstats = pd.read_csv(\"${sample_idxstats}\", sep=\"\\\\t\", header=None)\nref_len = idxstats.set_index(0)[1].apply(int)\n\n# Read in a file with a list of the positions covered in the alignment\nif os.stat(\"${sample_pileup}\").st_size > 0:\n    pileup = pd.read_csv(\"${sample_pileup}\", sep=\"\\\\t\", header=None)\n\n    # Calculate the coverage as the number of covered bases divided by the length\n    ref_cov = pileup.groupby(0).apply(len) / ref_len\n    ref_cov = ref_cov.dropna()\n    ref_cov.sort_values(ascending=False, inplace=True)\n    \n    # Find those ribosomes with the minimum threshold met\n    detected_ribosomes = ref_cov.index.values[ref_cov >= (float(\"${min_cov_pct}\") / 100)]\n    print(\"\\\\nDetected ribosomes:\")\n    print(\"\\\\n\".join(detected_ribosomes))\n\n    # Read in a list of which genomes match which ribosomes\n    genome_list = pd.read_csv(\"${ribosome_tsv}\", sep=\"\\\\t\", header=None)\n    print(genome_list)\n\n    # Get those genomes containing the detected ribosomes\n    detected_genomes = list(set(genome_list.loc[genome_list[1].isin(detected_ribosomes), 0]))\n    print(\"\\\\nDetected genomes:\")\n    print(\"\\\\n\".join(detected_genomes))\n\nelse:\n    detected_genomes = []\n\n# Write out to a file\nwith open(\"${sample_name}.genomes.txt\", \"wt\") as fo:\n    fo.write(\"\\\\n\".join(detected_genomes))\n\"\"\"\n\n}",
        "nb_lignes_process": 56,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport os\nimport json\nimport pandas as pd\n\n# Read in a file with the length of each reference\nidxstats = pd.read_csv(\"${sample_idxstats}\", sep=\"\\\\t\", header=None)\nref_len = idxstats.set_index(0)[1].apply(int)\n\n# Read in a file with a list of the positions covered in the alignment\nif os.stat(\"${sample_pileup}\").st_size > 0:\n    pileup = pd.read_csv(\"${sample_pileup}\", sep=\"\\\\t\", header=None)\n\n    # Calculate the coverage as the number of covered bases divided by the length\n    ref_cov = pileup.groupby(0).apply(len) / ref_len\n    ref_cov = ref_cov.dropna()\n    ref_cov.sort_values(ascending=False, inplace=True)\n    \n    # Find those ribosomes with the minimum threshold met\n    detected_ribosomes = ref_cov.index.values[ref_cov >= (float(\"${min_cov_pct}\") / 100)]\n    print(\"\\\\nDetected ribosomes:\")\n    print(\"\\\\n\".join(detected_ribosomes))\n\n    # Read in a list of which genomes match which ribosomes\n    genome_list = pd.read_csv(\"${ribosome_tsv}\", sep=\"\\\\t\", header=None)\n    print(genome_list)\n\n    # Get those genomes containing the detected ribosomes\n    detected_genomes = list(set(genome_list.loc[genome_list[1].isin(detected_ribosomes), 0]))\n    print(\"\\\\nDetected genomes:\")\n    print(\"\\\\n\".join(detected_genomes))\n\nelse:\n    detected_genomes = []\n\n# Write out to a file\nwith open(\"${sample_name}.genomes.txt\", \"wt\") as fo:\n    fo.write(\"\\\\n\".join(detected_genomes))\n\"\"\"",
        "nb_lignes_script": 39,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ribo_hits_ch",
            "ribosome_tsv",
            "params"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sample_genomes_txt"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/python-pandas:v0.24.2\"",
            "cpus 8",
            "memory \"16 GB\""
        ],
        "when": "",
        "stub": ""
    },
    "aggregateGenomes": {
        "name_process": "aggregateGenomes",
        "string_process": "\nprocess aggregateGenomes {\n  container \"ubuntu:16.04\"\n  cpus 1\n  memory \"4 GB\"\n\n  input:\n  file txt from sample_genomes_txt.collect()\n  \n  output:\n  file \"detected.genomes.txt\" into genome_hits_ch, gff_hits_ch\n\n  afterScript \"rm *\"\n\n\"\"\"\n#!/bin/bash\n\nset -e\n\nfor fp in ${txt}; do cat \\$fp; echo; done | sort -u | sed '/^\\$/d' > TEMP && mv TEMP detected.genomes.txt\n\"\"\"\n\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\nfor fp in ${txt}; do cat \\$fp; echo; done | sort -u | sed '/^\\$/d' > TEMP && mv TEMP detected.genomes.txt\n\"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "TEMP"
        ],
        "tools_url": [
            "https://bio.tools/temp"
        ],
        "tools_dico": [
            {
                "name": "TEMP",
                "uri": "https://bio.tools/temp",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2885",
                            "term": "DNA polymorphism"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0427",
                                    "term": "Transposon prediction"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A software package for detecting transposable elements (TEs) insertions and excisions from pooled high-throughput sequencing data.",
                "homepage": "https://github.com/JialiUMassWengLab/TEMP"
            }
        ],
        "inputs": [
            "sample_genomes_txt"
        ],
        "nb_inputs": 1,
        "outputs": [
            "genome_hits_ch",
            "gff_hits_ch afterScript"
        ],
        "nb_outputs": 2,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"ubuntu:16.04\"",
            "cpus 1",
            "memory \"4 GB\""
        ],
        "when": "",
        "stub": ""
    },
    "filterGenomes": {
        "name_process": "filterGenomes",
        "string_process": "\nprocess filterGenomes {\n  container \"quay.io/biocontainers/biopython@sha256:1196016b05927094af161ccf2cd8371aafc2e3a8daa51c51ff023f5eb45a820f\"\n  cpus 2\n  memory \"8 GB\"\n\n  input:\n  file genome_fasta\n  file genome_tsv\n  file sample_genomes from genome_hits_ch\n  publishDir \"${params.output_folder}/ref\"\n  \n  output:\n  file \"filtered.ref.fasta\" into filtered_ref_fasta\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/usr/bin/env python3\nfrom Bio.SeqIO.FastaIO import SimpleFastaParser\nimport gzip\n\n# Read in the genomes needed for this sample\nsample_genomes = open(\"${sample_genomes}\").readlines()\nsample_genomes = [fp.rstrip(\"\\\\n\") for fp in sample_genomes]\n\n# Figure out which headers that corresponds to\ngenome_headers = dict()\nall_headers = set([])\nfor line in gzip.open(\"${genome_tsv}\", \"rt\").readlines():\n    line = line.rstrip(\"\\\\n\").split(\"\\\\t\")\n    if len(line) != 2:\n        continue\n    genome, header = line\n    assert header not in all_headers, \"Found duplicate header: \" + header\n    genome_headers[genome] = genome_headers.get(genome, [])\n    genome_headers[genome].append(header)\n\nsample_headers = set([\n    header\n    for genome in sample_genomes\n    for header in genome_headers[genome]\n])\n\n# Extract the sequences from the FASTA\nn_written = 0\nwith open(\"filtered.ref.fasta\", \"wt\") as fo:\n    for header, seq in SimpleFastaParser(gzip.open(\"${genome_fasta}\", \"rt\")):\n        header = header.split(\" \")[0].split(\"\\\\t\")[0]\n        if header in sample_headers:\n            fo.write(\">\" + header + \"\\\\n\" + seq + \"\\\\n\")\n            n_written += 1\nassert n_written == len(sample_headers), (n_written, len(sample_headers))\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 55,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nfrom Bio.SeqIO.FastaIO import SimpleFastaParser\nimport gzip\n\n# Read in the genomes needed for this sample\nsample_genomes = open(\"${sample_genomes}\").readlines()\nsample_genomes = [fp.rstrip(\"\\\\n\") for fp in sample_genomes]\n\n# Figure out which headers that corresponds to\ngenome_headers = dict()\nall_headers = set([])\nfor line in gzip.open(\"${genome_tsv}\", \"rt\").readlines():\n    line = line.rstrip(\"\\\\n\").split(\"\\\\t\")\n    if len(line) != 2:\n        continue\n    genome, header = line\n    assert header not in all_headers, \"Found duplicate header: \" + header\n    genome_headers[genome] = genome_headers.get(genome, [])\n    genome_headers[genome].append(header)\n\nsample_headers = set([\n    header\n    for genome in sample_genomes\n    for header in genome_headers[genome]\n])\n\n# Extract the sequences from the FASTA\nn_written = 0\nwith open(\"filtered.ref.fasta\", \"wt\") as fo:\n    for header, seq in SimpleFastaParser(gzip.open(\"${genome_fasta}\", \"rt\")):\n        header = header.split(\" \")[0].split(\"\\\\t\")[0]\n        if header in sample_headers:\n            fo.write(\">\" + header + \"\\\\n\" + seq + \"\\\\n\")\n            n_written += 1\nassert n_written == len(sample_headers), (n_written, len(sample_headers))\n\n  \"\"\"",
        "nb_lignes_script": 37,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "genome_fasta",
            "genome_tsv",
            "genome_hits_ch"
        ],
        "nb_inputs": 3,
        "outputs": [
            "filtered_ref_fasta"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/biocontainers/biopython@sha256:1196016b05927094af161ccf2cd8371aafc2e3a8daa51c51ff023f5eb45a820f\"",
            "cpus 2",
            "memory \"8 GB\""
        ],
        "when": "",
        "stub": ""
    },
    "filterGFF": {
        "name_process": "filterGFF",
        "string_process": "\nprocess filterGFF {\n  container \"quay.io/biocontainers/biopython@sha256:1196016b05927094af161ccf2cd8371aafc2e3a8daa51c51ff023f5eb45a820f\"\n  cpus 1\n  memory \"4 GB\"\n  publishDir \"${params.output_folder}/ref\"\n\n  input:\n  file all_gff\n  file genome_tsv\n  file sample_genomes from gff_hits_ch\n  val min_cov_pct from params.min_cov_pct\n  \n  output:\n  file \"filtered.ref.gff.gz\" into filtered_gff\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/usr/bin/env python3\nimport gzip\n\n# Read in the genomes needed for this sample\nsample_genomes = open(\"${sample_genomes}\").readlines()\nsample_genomes = [fp.rstrip(\"\\\\n\") for fp in sample_genomes]\nassert len(sample_genomes) > 0, \"No genomes passed the filtering threshold of %d percent\" % (\"${min_cov_pct}\")\n\n# Figure out which headers that corresponds to\ngenome_headers = dict()\nall_headers = set([])\nfor line in gzip.open(\"${genome_tsv}\", \"rt\").readlines():\n    line = line.rstrip(\"\\\\n\").split(\"\\\\t\")\n    if len(line) != 2:\n        continue\n    genome, header = line\n    assert header not in all_headers, \"Found duplicate header: \" + header\n    genome_headers[genome] = genome_headers.get(genome, [])\n    genome_headers[genome].append(header)\n\nsample_headers = set([\n    header\n    for genome in sample_genomes\n    for header in genome_headers[genome]\n])\n\n# Extract the sequences from the GFF\nn_written = 0\nwith gzip.open(\"filtered.ref.gff.gz\", \"wt\") as fo:\n    with gzip.open(\"${all_gff}\", \"rt\") as fi:\n        for line in fi:\n            if line[0] == '#':\n                continue\n            if line.split(\"\\\\t\")[0] in sample_headers:\n                fo.write(line)\n                n_written += 1\nassert n_written > 0, \"No annotations were found for the filtered genomes\"\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 58,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport gzip\n\n# Read in the genomes needed for this sample\nsample_genomes = open(\"${sample_genomes}\").readlines()\nsample_genomes = [fp.rstrip(\"\\\\n\") for fp in sample_genomes]\nassert len(sample_genomes) > 0, \"No genomes passed the filtering threshold of %d percent\" % (\"${min_cov_pct}\")\n\n# Figure out which headers that corresponds to\ngenome_headers = dict()\nall_headers = set([])\nfor line in gzip.open(\"${genome_tsv}\", \"rt\").readlines():\n    line = line.rstrip(\"\\\\n\").split(\"\\\\t\")\n    if len(line) != 2:\n        continue\n    genome, header = line\n    assert header not in all_headers, \"Found duplicate header: \" + header\n    genome_headers[genome] = genome_headers.get(genome, [])\n    genome_headers[genome].append(header)\n\nsample_headers = set([\n    header\n    for genome in sample_genomes\n    for header in genome_headers[genome]\n])\n\n# Extract the sequences from the GFF\nn_written = 0\nwith gzip.open(\"filtered.ref.gff.gz\", \"wt\") as fo:\n    with gzip.open(\"${all_gff}\", \"rt\") as fi:\n        for line in fi:\n            if line[0] == '#':\n                continue\n            if line.split(\"\\\\t\")[0] in sample_headers:\n                fo.write(line)\n                n_written += 1\nassert n_written > 0, \"No annotations were found for the filtered genomes\"\n\n  \"\"\"",
        "nb_lignes_script": 39,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "all_gff",
            "genome_tsv",
            "gff_hits_ch",
            "params"
        ],
        "nb_inputs": 4,
        "outputs": [
            "filtered_gff"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/biocontainers/biopython@sha256:1196016b05927094af161ccf2cd8371aafc2e3a8daa51c51ff023f5eb45a820f\"",
            "cpus 1",
            "memory \"4 GB\"",
            "publishDir \"${params.output_folder}/ref\""
        ],
        "when": "",
        "stub": ""
    },
    "indexGenomes": {
        "name_process": "indexGenomes",
        "string_process": "\nprocess indexGenomes {\n  container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"\n  cpus 4\n  memory \"8 GB\"\n  publishDir \"${params.output_folder}/ref\"\n\n  input:\n  file filtered_ref_fasta\n  \n  output:\n  file \"filtered.ref.fasta.tar\" into ref_fasta_tar\n  file \"${filtered_ref_fasta}.fai\"\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\nset -e\n\n# Index the selected genomes\nbwa index \"${filtered_ref_fasta}\"\nsamtools faidx \"${filtered_ref_fasta}\"\n\n# Tar up the index\ntar cvf filtered.ref.fasta.tar filtered.ref.fasta*\n    \"\"\"\n\n}",
        "nb_lignes_process": 27,
        "string_script": "\"\"\"\n#!/bin/bash\nset -e\n\n# Index the selected genomes\nbwa index \"${filtered_ref_fasta}\"\nsamtools faidx \"${filtered_ref_fasta}\"\n\n# Tar up the index\ntar cvf filtered.ref.fasta.tar filtered.ref.fasta*\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [
            "BWA",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/bwa",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "filtered_ref_fasta"
        ],
        "nb_inputs": 1,
        "outputs": [
            "ref_fasta_tar"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"",
            "cpus 4",
            "memory \"8 GB\"",
            "publishDir \"${params.output_folder}/ref\""
        ],
        "when": "",
        "stub": ""
    },
    "alignGenomes": {
        "name_process": "alignGenomes",
        "string_process": "\nprocess alignGenomes {\n  container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"\n  cpus 8\n  memory \"60 GB\"\n  publishDir \"${params.output_folder}/bam\"\n  errorStrategy \"retry\"\n\n  input:\n  set sample_name, file(input_fastq) from align_genome_ch\n  file ref_fasta_tar\n  val min_qual from params.min_qual\n  val extra_bwa_flag\n  val samtools_filter_unmapped\n  val threads from 8\n  \n  output:\n  set sample_name, file(\"${sample_name}.genomes.bam\") optional true into count_aligned\n  set sample_name, file(\"${sample_name}.genomes.pileup.gz\") optional true into genome_pileup\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\nset -e\n\n# Untar the indexed genome database\ntar xvf ${ref_fasta_tar}\n\n# Align with BWA and remove unmapped reads\nbwa mem -T ${min_qual} -a -t ${threads * task.attempt}${extra_bwa_flag}filtered.ref.fasta ${input_fastq} | samtools view -b ${samtools_filter_mapped} - -o ${sample_name}.genomes.bam\n\nsamtools sort ${sample_name}.genomes.bam > ${sample_name}.genomes.bam.sorted\nmv ${sample_name}.genomes.bam.sorted ${sample_name}.genomes.bam\nsamtools index ${sample_name}.genomes.bam\nsamtools mpileup ${sample_name}.genomes.bam | gzip -c > ${sample_name}.genomes.pileup.gz\n\necho \"Number of aligned bases: \\$(gunzip -c ${sample_name}.genomes.pileup.gz | wc -l)\"\n\n# If no reads were aligned, delete the pileup and BAM\n(( \\$(gunzip -c ${sample_name}.genomes.pileup.gz | wc -l) > 0 )) || \\\nrm ${sample_name}.genomes.pileup.gz ${sample_name}.genomes.bam\n\n    \"\"\"\n\n}",
        "nb_lignes_process": 44,
        "string_script": "\"\"\"\n#!/bin/bash\nset -e\n\n# Untar the indexed genome database\ntar xvf ${ref_fasta_tar}\n\n# Align with BWA and remove unmapped reads\nbwa mem -T ${min_qual} -a -t ${threads * task.attempt}${extra_bwa_flag}filtered.ref.fasta ${input_fastq} | samtools view -b ${samtools_filter_mapped} - -o ${sample_name}.genomes.bam\n\nsamtools sort ${sample_name}.genomes.bam > ${sample_name}.genomes.bam.sorted\nmv ${sample_name}.genomes.bam.sorted ${sample_name}.genomes.bam\nsamtools index ${sample_name}.genomes.bam\nsamtools mpileup ${sample_name}.genomes.bam | gzip -c > ${sample_name}.genomes.pileup.gz\n\necho \"Number of aligned bases: \\$(gunzip -c ${sample_name}.genomes.pileup.gz | wc -l)\"\n\n# If no reads were aligned, delete the pileup and BAM\n(( \\$(gunzip -c ${sample_name}.genomes.pileup.gz | wc -l) > 0 )) || \\\nrm ${sample_name}.genomes.pileup.gz ${sample_name}.genomes.bam\n\n    \"\"\"",
        "nb_lignes_script": 21,
        "language_script": "bash",
        "tools": [
            "BWA",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/bwa",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "align_genome_ch",
            "ref_fasta_tar",
            "params",
            "extra_bwa_flag",
            "samtools_filter_unmapped",
            "8"
        ],
        "nb_inputs": 6,
        "outputs": [
            "count_aligned",
            "genome_pileup"
        ],
        "nb_outputs": 2,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"",
            "cpus 8",
            "memory \"60 GB\"",
            "publishDir \"${params.output_folder}/bam\"",
            "errorStrategy \"retry\""
        ],
        "when": "",
        "stub": ""
    },
    "countAlignedReads": {
        "name_process": "countAlignedReads",
        "string_process": "\nprocess countAlignedReads {\n  container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"\n  cpus 4\n  memory \"8 GB\"\n  \n  input:\n  set sample_name, file(bam) from count_aligned\n  \n  output:\n  file \"${sample_name}.countMapped.csv\" into mapped_counts\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\nn=\\$(samtools view \"${bam}\" | cut -f 1 | sort -u | wc -l)\necho \"${sample_name},mapped_reads,\\$n\" > \"${sample_name}.countMapped.csv\"\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 23,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\nn=\\$(samtools view \"${bam}\" | cut -f 1 | sort -u | wc -l)\necho \"${sample_name},mapped_reads,\\$n\" > \"${sample_name}.countMapped.csv\"\n\n  \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "count_aligned"
        ],
        "nb_inputs": 1,
        "outputs": [
            "mapped_counts"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/bwa@sha256:2fc9c6c38521b04020a1e148ba042a2fccf8de6affebc530fbdd45abc14bf9e6\"",
            "cpus 4",
            "memory \"8 GB\""
        ],
        "when": "",
        "stub": ""
    },
    "summarizeAlignments": {
        "name_process": "summarizeAlignments",
        "string_process": "\nprocess summarizeAlignments {\n  container \"quay.io/fhcrc-microbiome/python-pandas:v0.24.2\"\n  cpus 16\n  memory \"120 GB\"\n\n  input:\n  set sample_name, file(sample_pileup) from genome_pileup\n  file filtered_gff\n  file genome_table\n  \n  output:\n  file \"${sample_name}.summary.csv\" into sample_results\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/usr/bin/env python3\nimport os\nimport json\nimport gzip\nimport pandas as pd\n\n# Read in the pileup\n# Calculate the depth per base\nprint(\"Reading in the pileup file and calculating base depth\")\nbase_depth = dict([\n    (reference, reference_pileup.set_index(1)[3].to_dict())\n    for reference, reference_pileup in pd.read_csv(\n        \"${sample_pileup}\", \n        header=None, \n        sep=\"\\\\t\", \n        usecols=[0, 1, 3]\n    ).groupby(0)\n])\n\n# Read in the list of organism names for each reference\nprint(\"Reading in the organism names\")\norg_names = pd.read_csv(\n    \"${genome_table}\", \n    sep=\"\\\\t\", \n    header=None\n).set_index(1)[0]\n\nall_references = set(org_names.index.values)\n\n# Read in the GFF annotations\nprint(\"Reading in the GFF annotations\")\nannot = []\nfor line in gzip.open(\"${filtered_gff}\", \"rt\"):\n    if line[0] == '#':\n        continue\n    line = line.split(\"\\\\t\")\n\n    if line[0] not in all_references:\n        continue\n    \n    # Get the gene name\n    gene_desc = dict([\n        field.split(\"=\", 1)\n        for field in line[8].split(\";\")\n        if len(field.split(\"=\", 1)) == 2\n    ])\n\n    if \"ID\" not in gene_desc:\n        continue\n    \n    annot.append(dict([\n        (\"type\", line[2]),\n        (\"reference\", line[0]),\n        (\"start\", int(line[3])),\n        (\"end\", int(line[4])),\n        (\"ID\", gene_desc[\"ID\"]),\n        (\"name\", gene_desc.get(\"Name\", gene_desc.get(\"name\"))),\n        (\"product\", gene_desc.get(\"product\"))\n    ]))\n\n# Format as a DataFrame\nannot = pd.DataFrame(annot)\n\n# Subset to a few types of features\nannot = annot.query(\n    \"type in ['CDS', 'tRNA', 'ncRNA', 'rRNA']\"\n)\n\n# Add the organism name\nannot[\"organism\"] = annot[\"reference\"].apply(org_names.get)\n\n# Precompute the length of each feature\nannot[\"length\"] = 1 + annot[\"end\"] - annot[\"start\"]\nassert (annot[\"length\"] > 0).all()\n\n# Compute the depth of sequencing for each feature\nprint(\"Computing depth of sequencing per gene\")\nannot[\"depth\"] = annot.apply(\n    lambda r: sum([\n        base_depth.get(r[\"reference\"], dict()).get(ix, 0)\n        for ix in range(r[\"start\"], r[\"end\"] + 1)\n    ])/ (r[\"length\"]),\n    axis=1\n)\n\n# Add the sample name\nannot[\"sample\"] = \"${sample_name}\"\n\n# Write out to a file\nprint(\"Writing out to a file\")\nannot.to_csv(\"${sample_name}.summary.csv\", sep=\",\", index=None)\nprint(\"Done\")\n\"\"\"\n\n}",
        "nb_lignes_process": 110,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport os\nimport json\nimport gzip\nimport pandas as pd\n\n# Read in the pileup\n# Calculate the depth per base\nprint(\"Reading in the pileup file and calculating base depth\")\nbase_depth = dict([\n    (reference, reference_pileup.set_index(1)[3].to_dict())\n    for reference, reference_pileup in pd.read_csv(\n        \"${sample_pileup}\", \n        header=None, \n        sep=\"\\\\t\", \n        usecols=[0, 1, 3]\n    ).groupby(0)\n])\n\n# Read in the list of organism names for each reference\nprint(\"Reading in the organism names\")\norg_names = pd.read_csv(\n    \"${genome_table}\", \n    sep=\"\\\\t\", \n    header=None\n).set_index(1)[0]\n\nall_references = set(org_names.index.values)\n\n# Read in the GFF annotations\nprint(\"Reading in the GFF annotations\")\nannot = []\nfor line in gzip.open(\"${filtered_gff}\", \"rt\"):\n    if line[0] == '#':\n        continue\n    line = line.split(\"\\\\t\")\n\n    if line[0] not in all_references:\n        continue\n    \n    # Get the gene name\n    gene_desc = dict([\n        field.split(\"=\", 1)\n        for field in line[8].split(\";\")\n        if len(field.split(\"=\", 1)) == 2\n    ])\n\n    if \"ID\" not in gene_desc:\n        continue\n    \n    annot.append(dict([\n        (\"type\", line[2]),\n        (\"reference\", line[0]),\n        (\"start\", int(line[3])),\n        (\"end\", int(line[4])),\n        (\"ID\", gene_desc[\"ID\"]),\n        (\"name\", gene_desc.get(\"Name\", gene_desc.get(\"name\"))),\n        (\"product\", gene_desc.get(\"product\"))\n    ]))\n\n# Format as a DataFrame\nannot = pd.DataFrame(annot)\n\n# Subset to a few types of features\nannot = annot.query(\n    \"type in ['CDS', 'tRNA', 'ncRNA', 'rRNA']\"\n)\n\n# Add the organism name\nannot[\"organism\"] = annot[\"reference\"].apply(org_names.get)\n\n# Precompute the length of each feature\nannot[\"length\"] = 1 + annot[\"end\"] - annot[\"start\"]\nassert (annot[\"length\"] > 0).all()\n\n# Compute the depth of sequencing for each feature\nprint(\"Computing depth of sequencing per gene\")\nannot[\"depth\"] = annot.apply(\n    lambda r: sum([\n        base_depth.get(r[\"reference\"], dict()).get(ix, 0)\n        for ix in range(r[\"start\"], r[\"end\"] + 1)\n    ])/ (r[\"length\"]),\n    axis=1\n)\n\n# Add the sample name\nannot[\"sample\"] = \"${sample_name}\"\n\n# Write out to a file\nprint(\"Writing out to a file\")\nannot.to_csv(\"${sample_name}.summary.csv\", sep=\",\", index=None)\nprint(\"Done\")\n\"\"\"",
        "nb_lignes_script": 93,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "genome_pileup",
            "filtered_gff",
            "genome_table"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sample_results"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/python-pandas:v0.24.2\"",
            "cpus 16",
            "memory \"120 GB\""
        ],
        "when": "",
        "stub": ""
    },
    "finalResults": {
        "name_process": "finalResults",
        "string_process": "\nprocess finalResults {\n  container \"quay.io/fhcrc-microbiome/python-pandas:v0.24.2\"\n  cpus 8\n  memory \"240 GB\"\n  publishDir \"${params.output_folder}\"\n\n  input:\n  file \"*\" from sample_results.collect()\n  \n  output:\n  file \"${params.output_prefix}.*.csv\"\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/usr/bin/env python3\nimport os\nimport pandas as pd\n\ndf = pd.concat([\n    pd.read_csv(fp)\n    for fp in os.listdir(\".\")\n    if fp.endswith(\".csv\")\n])\n\n# Write out the complete set of data\ndf.to_csv(\"${params.output_prefix}.all.csv\", index=None, sep=\",\")\n\n# Summarize for each organism\n# Calculate the weighted average depth for genes broken out by type\nsummary_df = []\nfor ix, sub_df in df.groupby([\"organism\", \"sample\"]):\n    i = dict([\n        (\"organism\", ix[0]),\n        (\"sample\", ix[1])\n    ])\n    for t, type_df in sub_df.groupby(\"type\"):\n        i[t] = (type_df[\"depth\"] * type_df[\"length\"]).sum() / type_df[\"length\"].sum()\n    summary_df.append(i)\nsummary_df = pd.DataFrame(summary_df).set_index([\"organism\", \"sample\"])\n\n# Calculate the ratio of rRNA to CDS for each organism & sample\nsummary_df[\"rRNA_CDS_ratio\"] = summary_df[\"rRNA\"] / summary_df[\"CDS\"]\n\nfor t in summary_df.columns.values:\n    summary_df.reset_index().pivot_table(\n        index=\"sample\",\n        columns=\"organism\",\n        values=t\n    ).reset_index().to_csv(\n        \"${params.output_prefix}.\" + t + \".csv\",\n        index=None,\n        sep=\",\"\n    )\n\n# Make a column with the annotation of each gene\ndf[\"annotation\"] = df.apply(\n    lambda r: \"%s: %s (%s)\" % (r[\"name\"], r[\"product\"], r[\"ID\"]),\n    axis=1\n)\n\n# For each organism, print out the depth of sequencing across all samples\nfor org, org_df in df.groupby(\"organism\"):\n    org_df.pivot_table(\n        index=\"sample\",\n        columns=\"annotation\",\n        values=\"depth\"\n    ).reset_index().to_csv(\n        \"${params.output_prefix}.\" + org + \".csv\",\n        index=None,\n        sep=\",\"\n    )\n\n\"\"\"\n\n}",
        "nb_lignes_process": 75,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport os\nimport pandas as pd\n\ndf = pd.concat([\n    pd.read_csv(fp)\n    for fp in os.listdir(\".\")\n    if fp.endswith(\".csv\")\n])\n\n# Write out the complete set of data\ndf.to_csv(\"${params.output_prefix}.all.csv\", index=None, sep=\",\")\n\n# Summarize for each organism\n# Calculate the weighted average depth for genes broken out by type\nsummary_df = []\nfor ix, sub_df in df.groupby([\"organism\", \"sample\"]):\n    i = dict([\n        (\"organism\", ix[0]),\n        (\"sample\", ix[1])\n    ])\n    for t, type_df in sub_df.groupby(\"type\"):\n        i[t] = (type_df[\"depth\"] * type_df[\"length\"]).sum() / type_df[\"length\"].sum()\n    summary_df.append(i)\nsummary_df = pd.DataFrame(summary_df).set_index([\"organism\", \"sample\"])\n\n# Calculate the ratio of rRNA to CDS for each organism & sample\nsummary_df[\"rRNA_CDS_ratio\"] = summary_df[\"rRNA\"] / summary_df[\"CDS\"]\n\nfor t in summary_df.columns.values:\n    summary_df.reset_index().pivot_table(\n        index=\"sample\",\n        columns=\"organism\",\n        values=t\n    ).reset_index().to_csv(\n        \"${params.output_prefix}.\" + t + \".csv\",\n        index=None,\n        sep=\",\"\n    )\n\n# Make a column with the annotation of each gene\ndf[\"annotation\"] = df.apply(\n    lambda r: \"%s: %s (%s)\" % (r[\"name\"], r[\"product\"], r[\"ID\"]),\n    axis=1\n)\n\n# For each organism, print out the depth of sequencing across all samples\nfor org, org_df in df.groupby(\"organism\"):\n    org_df.pivot_table(\n        index=\"sample\",\n        columns=\"annotation\",\n        values=\"depth\"\n    ).reset_index().to_csv(\n        \"${params.output_prefix}.\" + org + \".csv\",\n        index=None,\n        sep=\",\"\n    )\n\n\"\"\"",
        "nb_lignes_script": 59,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_results"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/python-pandas:v0.24.2\"",
            "cpus 8",
            "memory \"240 GB\"",
            "publishDir \"${params.output_folder}\""
        ],
        "when": "",
        "stub": ""
    },
    "mappingSummary": {
        "name_process": "mappingSummary",
        "string_process": "\nprocess mappingSummary {\n  container \"quay.io/fhcrc-microbiome/python-pandas:v0.24.2\"\n  cpus 4\n  memory \"8 GB\"\n  publishDir \"${params.output_folder}\"\n\n  input:\n  file \"*\" from total_counts.collect()\n  file \"*\" from nonhuman_counts.collect()\n  file \"*\" from mapped_counts.collect()\n  \n  output:\n  file \"${params.output_prefix}.mapping_summary.csv\"\n\n  afterScript \"rm *\"\n\n  \"\"\"\n#!/usr/bin/env python3\nimport os\nimport pandas as pd\n\ndf = pd.concat([\n    pd.read_csv(fp, header=None, names=[\"sample\", \"metric\", \"value\"])\n    for fp in os.listdir(\".\")\n    if fp.endswith(\".csv\")\n])\n\ndf = df.pivot_table(\n    index=\"sample\",\n    columns=\"metric\",\n    values=\"value\"\n)\ndf.reset_index().to_csv(\n    \"${params.output_prefix}.mapping_summary.csv\",\n    index=None,\n    sep=\",\"\n)\n\n\"\"\"\n\n}",
        "nb_lignes_process": 40,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport os\nimport pandas as pd\n\ndf = pd.concat([\n    pd.read_csv(fp, header=None, names=[\"sample\", \"metric\", \"value\"])\n    for fp in os.listdir(\".\")\n    if fp.endswith(\".csv\")\n])\n\ndf = df.pivot_table(\n    index=\"sample\",\n    columns=\"metric\",\n    values=\"value\"\n)\ndf.reset_index().to_csv(\n    \"${params.output_prefix}.mapping_summary.csv\",\n    index=None,\n    sep=\",\"\n)\n\n\"\"\"",
        "nb_lignes_script": 22,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "total_counts",
            "nonhuman_counts",
            "mapped_counts"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/python-pandas:v0.24.2\"",
            "cpus 4",
            "memory \"8 GB\"",
            "publishDir \"${params.output_folder}\""
        ],
        "when": "",
        "stub": ""
    },
    "PplacerPlacement": {
        "name_process": "PplacerPlacement",
        "string_process": "\nprocess PplacerPlacement {\n    container = \"${container__pplacer}\"\n    label = 'mem_veryhigh'\n\n    publishDir \"${params.output}/placement\", mode: 'copy'\n\n    input:\n        file sv_refpkg_aln_sto_f\n        file refpkg_tgz_f\n    output:\n        file 'dedup.jplace'\n    \n    afterScript \"rm -rf refpkg/\"\n    \"\"\"\n    mkdir -p refpkg/ &&\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C ./refpkg &&\n    pplacer -p -j ${task.cpus} \\\n    --inform-prior --prior-lower ${params.pplacer_prior_lower} --map-identity \\\n    -c refpkg/ ${sv_refpkg_aln_sto_f} \\\n    -o dedup.jplace\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\n    mkdir -p refpkg/ &&\n    tar xzvf ${refpkg_tgz_f} --no-overwrite-dir -C ./refpkg &&\n    pplacer -p -j ${task.cpus} \\\n    --inform-prior --prior-lower ${params.pplacer_prior_lower} --map-identity \\\n    -c refpkg/ ${sv_refpkg_aln_sto_f} \\\n    -o dedup.jplace\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sv_refpkg_aln_sto_f",
            "refpkg_tgz_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'mem_veryhigh'",
            "publishDir \"${params.output}/placement\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "PplacerReduplicate": {
        "name_process": "PplacerReduplicate",
        "string_process": "\nprocess PplacerReduplicate {\n    container = \"${container__pplacer}\"\n    label = 'io_limited'\n\n    publishDir \"${params.output}/placement\", mode: 'copy'\n\n    input:\n        file dedup_jplace_f\n        file sv_weights_f\n    output:\n        file 'redup.jplace.gz'\n    \n    \"\"\"\n    guppy redup -m \\\n    -o /dev/stdout \\\n    -d ${sv_weights_f} \\\n    ${dedup_jplace_f} \\\n    | gzip > redup.jplace.gz\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "\"\"\"\n    guppy redup -m \\\n    -o /dev/stdout \\\n    -d ${sv_weights_f} \\\n    ${dedup_jplace_f} \\\n    | gzip > redup.jplace.gz\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "GUPPY"
        ],
        "tools_url": [
            "https://bio.tools/guppy"
        ],
        "tools_dico": [
            {
                "name": "GUPPY",
                "uri": "https://bio.tools/guppy",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data visualisation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0092",
                            "term": "Data rendering"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0564",
                                    "term": "Sequence rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Program to visualize sequence annotation data of the genetic sequence data with graphical layout. This highly interactive program allows scrolling and zooming from the genomic landscape to discrete nucleic acid sequences. Its main function is quick rendering of data on a standalone personal computer to save the layout as a parsonal file. With optional link to the internet resources, and printing support, this program tries to make more greater use of computational media in research actitiviy.",
                "homepage": "http://staff.aist.go.jp/yutaka.ueno/guppy/"
            }
        ],
        "inputs": [
            "dedup_jplace_f",
            "sv_weights_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/placement\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "Dada2_convert_output": {
        "name_process": "Dada2_convert_output",
        "string_process": "\nprocess Dada2_convert_output {\n    container \"${container__dada2pplacer}\"\n    label 'io_mem'\n    publishDir \"${params.output}/sv/\", mode: 'copy'\n    errorStrategy \"retry\"\n\n    input:\n        file(final_seqtab_csv)\n\n    output:\n        file \"dada2.sv.fasta\"\n        file \"dada2.sv.map.csv\"\n        file \"dada2.sv.weights.csv\"\n\n    \"\"\"\n    dada2-seqtab-to-pplacer \\\n    -s ${final_seqtab_csv} \\\n    -f dada2.sv.fasta \\\n    -m dada2.sv.map.csv \\\n    -w dada2.sv.weights.csv \\\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\n    dada2-seqtab-to-pplacer \\\n    -s ${final_seqtab_csv} \\\n    -f dada2.sv.fasta \\\n    -m dada2.sv.map.csv \\\n    -w dada2.sv.weights.csv \\\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "final_seqtab_csv"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2pplacer}\"",
            "label 'io_mem'",
            "publishDir \"${params.output}/sv/\", mode: 'copy'",
            "errorStrategy \"retry\""
        ],
        "when": "",
        "stub": ""
    },
    "Gappa_KRD_1t1": {
        "name_process": "Gappa_KRD_1t1",
        "string_process": "\nprocess Gappa_KRD_1t1 {\n    container = \"${container__gappa}\"\n    label = 'io_limited'\n\n    input:\n        path new_jplace\n        path old_jplaces\n    output:\n        path \"v${new_jplace.name.replace('.jplace.gz', '')}.krd_long.csv\"\n\n    \n    \"\"\"\n    interval_krd.py \\\n    --new-jplace ${new_jplace} \\\n    --old-jplaces \"${old_jplaces}\"\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "\"\"\"\n    interval_krd.py \\\n    --new-jplace ${new_jplace} \\\n    --old-jplaces \"${old_jplaces}\"\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "new_jplace",
            "old_jplaces"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__gappa}\"",
            "label = 'io_limited'"
        ],
        "when": "",
        "stub": ""
    },
    "CombineKRDLong": {
        "name_process": "CombineKRDLong",
        "string_process": "\nprocess CombineKRDLong {\n    container = \"${container__gappa}\"\n    label = 'io_limited'\n    publishDir \"${params.output}/\", mode: 'copy'\n\n    input:\n        path krd_longs\n    output:\n        path \"interval_krd_long.csv.gz\"\n    \n\"\"\"\n#!/usr/bin/env python3\nimport csv\nimport gzip\n\nkrd_long_fn = \"${krd_longs}\".split()\n\nwith gzip.open(\"interval_krd_long.csv.gz\", 'wt') as out_h:\n    w = csv.DictWriter(out_h, fieldnames=['specimen_1', 'specimen_2', 'krd'])\n    w.writeheader()\n    for fn in krd_long_fn:\n        with open(fn, 'rt') as in_h:\n            r = csv.DictReader(in_h)\n            w.writerows([\n                row for row in r\n            ])\n\"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport csv\nimport gzip\n\nkrd_long_fn = \"${krd_longs}\".split()\n\nwith gzip.open(\"interval_krd_long.csv.gz\", 'wt') as out_h:\n    w = csv.DictWriter(out_h, fieldnames=['specimen_1', 'specimen_2', 'krd'])\n    w.writeheader()\n    for fn in krd_long_fn:\n        with open(fn, 'rt') as in_h:\n            r = csv.DictReader(in_h)\n            w.writerows([\n                row for row in r\n            ])\n\"\"\"",
        "nb_lignes_script": 16,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "krd_longs"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__gappa}\"",
            "label = 'io_limited'",
            "publishDir \"${params.output}/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "joinFastq": {
        "name_process": "joinFastq",
        "string_process": "\nprocess joinFastq {\n  container \"${container__ubuntu}\"\n  errorStrategy 'retry'\n  \n  input:\n  tuple val(specimen), path(fastq_list)\n  \n  output:\n  tuple val(specimen), path(\"${specimen}.fastq.gz\")\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\ncat ${fastq_list} > TEMP && mv TEMP \"${specimen}.fastq.gz\"\n  \"\"\"\n\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\ncat ${fastq_list} > TEMP && mv TEMP \"${specimen}.fastq.gz\"\n  \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "TEMP"
        ],
        "tools_url": [
            "https://bio.tools/temp"
        ],
        "tools_dico": [
            {
                "name": "TEMP",
                "uri": "https://bio.tools/temp",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2885",
                            "term": "DNA polymorphism"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0427",
                                    "term": "Transposon prediction"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A software package for detecting transposable elements (TEs) insertions and excisions from pooled high-throughput sequencing data.",
                "homepage": "https://github.com/JialiUMassWengLab/TEMP"
            }
        ],
        "inputs": [
            "specimen",
            "fastq_list"
        ],
        "nb_inputs": 2,
        "outputs": [
            "specimen"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__ubuntu}\"",
            "errorStrategy 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "cutadapt": {
        "name_process": "cutadapt",
        "string_process": "\nprocess cutadapt {\n    tag \"Trim adapters from WGS reads\"\n    container \"${container__cutadapt}\"\n    label 'mem_medium'\n    errorStrategy 'retry'\n    maxRetries 10\n\n    input:\n    tuple val(sample_name), file(FASTQ)\n\n    output:\n    tuple val(sample_name), file(\"${sample_name}.cutadapt.fq.gz\") optional true\n\n\"\"\"\nset -e \n\ncutadapt \\\n--minimum-length ${params.min_len} \\\n-j ${task.cpus} \\\n--cut ${params.bases_before_adapter} \\\n-g ^${params.adapter_F}...${params.adapter_R} \\\n-q ${params.qual_threshold},${params.qual_threshold} \\\n--discard-untrimmed \\\n--max-n ${params.max_n_prop} \\\n--times 3 \\\n--trim-n \\\n${FASTQ} | \\\nawk '{if(NR % 4 != 0){printf \\$1 \"\\t\"}else{print \\$1}}' | \\\ngrep -v ${params.adapter_F} | \\\ngrep -v ${params.adapter_R} | \\\ntr '\\t' '\\n' | gzip -c > \\\n${sample_name}.cutadapt.fq.gz\n\n# If the file is empty, delete it entirely\nif (( \\$(gunzip -c ${sample_name}.cutadapt.fq.gz | wc -l) == 0 )); then\n  rm ${sample_name}.cutadapt.fq.gz\nfi\n\"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "\"\"\"\nset -e \n\ncutadapt \\\n--minimum-length ${params.min_len} \\\n-j ${task.cpus} \\\n--cut ${params.bases_before_adapter} \\\n-g ^${params.adapter_F}...${params.adapter_R} \\\n-q ${params.qual_threshold},${params.qual_threshold} \\\n--discard-untrimmed \\\n--max-n ${params.max_n_prop} \\\n--times 3 \\\n--trim-n \\\n${FASTQ} | \\\nawk '{if(NR % 4 != 0){printf \\$1 \"\\t\"}else{print \\$1}}' | \\\ngrep -v ${params.adapter_F} | \\\ngrep -v ${params.adapter_R} | \\\ntr '\\t' '\\n' | gzip -c > \\\n${sample_name}.cutadapt.fq.gz\n\n# If the file is empty, delete it entirely\nif (( \\$(gunzip -c ${sample_name}.cutadapt.fq.gz | wc -l) == 0 )); then\n  rm ${sample_name}.cutadapt.fq.gz\nfi\n\"\"\"",
        "nb_lignes_script": 24,
        "language_script": "bash",
        "tools": [
            "Cutadapt"
        ],
        "tools_url": [
            "https://bio.tools/cutadapt"
        ],
        "tools_dico": [
            {
                "name": "Cutadapt",
                "uri": "https://bio.tools/cutadapt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0632",
                            "term": "Probes and primers"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Sequence trimming"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Trimming"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3495",
                                "term": "RNA sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3495",
                                "term": "RNA sequence"
                            }
                        ]
                    }
                ],
                "description": "Find and remove adapter sequences, primers, poly-A tails and other types of unwanted sequence from your high-throughput sequencing reads.",
                "homepage": "https://pypi.python.org/pypi/cutadapt"
            }
        ],
        "inputs": [
            "sample_name",
            "FASTQ"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sample_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "tag \"Trim adapters from WGS reads\"",
            "container \"${container__cutadapt}\"",
            "label 'mem_medium'",
            "errorStrategy 'retry'",
            "maxRetries 10"
        ],
        "when": "",
        "stub": ""
    },
    "remove_human": {
        "name_process": "remove_human",
        "string_process": "\nprocess remove_human {\n    tag \"Remove human reads\"\n    container \"${container__bwa}\"\n    errorStrategy 'retry'\n    label 'mem_veryhigh'\n\n\n    input:\n        tuple val(sample_name), file(\"${sample_name}.input.fq.gz\")\n        file hg_index_tgz\n\n    output:\n        tuple val(sample_name), file(\"${sample_name}.fq.gz\") optional true\n\n\"\"\"\n#!/bin/bash\n\nset -e\n\n# Get the index name from the first file in the tar\nbwa_index_prefix=\\$(tar -ztvf ${hg_index_tgz} | head -1 | sed \\'s/.* //\\')\n\n# Remove whichever ending this file has\nbwa_index_prefix=\\${bwa_index_prefix%.amb}\nbwa_index_prefix=\\${bwa_index_prefix%.ann}\nbwa_index_prefix=\\${bwa_index_prefix%.bwt}\nbwa_index_prefix=\\${bwa_index_prefix%.fai}\nbwa_index_prefix=\\${bwa_index_prefix%.pac}\nbwa_index_prefix=\\${bwa_index_prefix%.sa}\n\necho BWA index prefix is \\${bwa_index_prefix}\necho \"If this index prefix is not correct, consider remaking the tarball\"\necho \"so that it doesn't include anything other than the index files\"\n\necho Extracting BWA index\n\nmkdir -p hg_index/ \n\ntar -I pigz -xf ${hg_index_tgz} -C hg_index/\n\necho Files in index directory: \nls -l -h hg_index \n\n# Make sure that there are files in the index directory\n(( \\$(ls -l -h hg_index | wc -l) > 0 ))\n\necho Running BWA \n\nbwa mem -t ${task.cpus} \\\n-T ${params.min_hg_align_score} \\\n-o alignment.sam \\\nhg_index/\\$bwa_index_prefix \\\n${sample_name}.input.fq.gz\n\necho Checking if alignment is empty  \nif [[ -s alignment.sam ]]; then\n\n  echo Extracting Unaligned Pairs \n  samtools \\\n    fastq \\\n    alignment.sam \\\n    --threads ${task.cpus} \\\n    -f 4 \\\n    -n \\\n    | gzip -c > ${sample_name}.fq.gz\n\n  \n  echo \"Checking to see how many reads pass the human filtering\"\n\n  gunzip -c ${sample_name}.fq.gz | wc -l\n\n  if (( \\$(gunzip -c ${sample_name}.fq.gz | wc -l) == 0 )); then\n\n    echo \"Removing empty output file\"\n    rm ${sample_name}.fq.gz\n\n  fi\n\nelse\n\n  echo \"Alignment SAM file was empty\"\n\nfi\n\necho Done \n\"\"\"\n}",
        "nb_lignes_process": 86,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\n# Get the index name from the first file in the tar\nbwa_index_prefix=\\$(tar -ztvf ${hg_index_tgz} | head -1 | sed \\'s/.* //\\')\n\n# Remove whichever ending this file has\nbwa_index_prefix=\\${bwa_index_prefix%.amb}\nbwa_index_prefix=\\${bwa_index_prefix%.ann}\nbwa_index_prefix=\\${bwa_index_prefix%.bwt}\nbwa_index_prefix=\\${bwa_index_prefix%.fai}\nbwa_index_prefix=\\${bwa_index_prefix%.pac}\nbwa_index_prefix=\\${bwa_index_prefix%.sa}\n\necho BWA index prefix is \\${bwa_index_prefix}\necho \"If this index prefix is not correct, consider remaking the tarball\"\necho \"so that it doesn't include anything other than the index files\"\n\necho Extracting BWA index\n\nmkdir -p hg_index/ \n\ntar -I pigz -xf ${hg_index_tgz} -C hg_index/\n\necho Files in index directory: \nls -l -h hg_index \n\n# Make sure that there are files in the index directory\n(( \\$(ls -l -h hg_index | wc -l) > 0 ))\n\necho Running BWA \n\nbwa mem -t ${task.cpus} \\\n-T ${params.min_hg_align_score} \\\n-o alignment.sam \\\nhg_index/\\$bwa_index_prefix \\\n${sample_name}.input.fq.gz\n\necho Checking if alignment is empty  \nif [[ -s alignment.sam ]]; then\n\n  echo Extracting Unaligned Pairs \n  samtools \\\n    fastq \\\n    alignment.sam \\\n    --threads ${task.cpus} \\\n    -f 4 \\\n    -n \\\n    | gzip -c > ${sample_name}.fq.gz\n\n  \n  echo \"Checking to see how many reads pass the human filtering\"\n\n  gunzip -c ${sample_name}.fq.gz | wc -l\n\n  if (( \\$(gunzip -c ${sample_name}.fq.gz | wc -l) == 0 )); then\n\n    echo \"Removing empty output file\"\n    rm ${sample_name}.fq.gz\n\n  fi\n\nelse\n\n  echo \"Alignment SAM file was empty\"\n\nfi\n\necho Done \n\"\"\"",
        "nb_lignes_script": 71,
        "language_script": "bash",
        "tools": [
            "BWA",
            "SAMtools",
            "FastQC"
        ],
        "tools_url": [
            "https://bio.tools/bwa",
            "https://bio.tools/samtools",
            "https://bio.tools/fastqc"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            },
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            }
        ],
        "inputs": [
            "sample_name",
            "hg_index_tgz"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sample_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "tag \"Remove human reads\"",
            "container \"${container__bwa}\"",
            "errorStrategy 'retry'",
            "label 'mem_veryhigh'"
        ],
        "when": "",
        "stub": ""
    },
    "assemble": {
        "name_process": "assemble",
        "string_process": "\nprocess assemble {\n    tag \"De novo metagenomic assembly\"\n    container \"${container__assembler}\"\n    label 'mem_veryhigh'\n    errorStrategy \"retry\"\n    publishDir params.output_folder\n\n    input:\n        path fastq_list\n    \n    output:\n        file \"${params.output_prefix}.fasta.gz\"\n    \n\"\"\"\nset -e \n\necho -e \"Concatenating inputs\"\ncat ${fastq_list} > INPUT.fastq.gz\n\ndate\necho -e \"Running Megahit\\\\n\"\n\nmegahit \\\n    -r INPUT.fastq.gz \\\n    -o OUTPUT \\\n    -t ${task.cpus} \\\n    --k-min ${params.k_min} \\\n    --k-max ${params.k_max} \\\n    --k-step ${params.k_step} \\\n    --min-contig-len ${params.min_len} \\\n\ndate\necho -e \"\\\\nMaking sure output files are not empty\\\\n\"\n[[ \\$(cat OUTPUT/final.contigs.fa | wc -l) > 0 ]]\n\ndate\necho -e \"\\\\nRenaming output files\\\\n\"\n\n# Rename the output file\ncat OUTPUT/final.contigs.fa | gzip -c > ${params.output_prefix}.fasta.gz\n\ndate\necho -e \"\\\\nDone\\\\n\"\n\"\"\"\n}",
        "nb_lignes_process": 44,
        "string_script": "\"\"\"\nset -e \n\necho -e \"Concatenating inputs\"\ncat ${fastq_list} > INPUT.fastq.gz\n\ndate\necho -e \"Running Megahit\\\\n\"\n\nmegahit \\\n    -r INPUT.fastq.gz \\\n    -o OUTPUT \\\n    -t ${task.cpus} \\\n    --k-min ${params.k_min} \\\n    --k-max ${params.k_max} \\\n    --k-step ${params.k_step} \\\n    --min-contig-len ${params.min_len} \\\n\ndate\necho -e \"\\\\nMaking sure output files are not empty\\\\n\"\n[[ \\$(cat OUTPUT/final.contigs.fa | wc -l) > 0 ]]\n\ndate\necho -e \"\\\\nRenaming output files\\\\n\"\n\n# Rename the output file\ncat OUTPUT/final.contigs.fa | gzip -c > ${params.output_prefix}.fasta.gz\n\ndate\necho -e \"\\\\nDone\\\\n\"\n\"\"\"",
        "nb_lignes_script": 30,
        "language_script": "bash",
        "tools": [
            "datelife",
            "MEGAHIT"
        ],
        "tools_url": [
            "https://bio.tools/datelife",
            "https://bio.tools/megahit"
        ],
        "tools_dico": [
            {
                "name": "datelife",
                "uri": "https://bio.tools/datelife",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0084",
                            "term": "Phylogeny"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3068",
                            "term": "Literature and language"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0194",
                            "term": "Phylogenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3068",
                            "term": "Language"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3068",
                            "term": "Literature"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Query and retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Imputation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Database retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3478",
                                    "term": "Phylogenetic tree reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Data imputation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Leveraging databases and analytical tools to reveal the dated Tree of Life.\n\nR package containing datelife's core functionality.\n\nDashboard \u22c5 phylotastic/datelife.\n\nGet a phylogenetic tree with branch lengths proportional to geologic time (aka a chronogram) of any two or more lineages of interest to you: use this R package or go to www.datelife.org to make a query of chronograms available for your lineages in the Open Tree of Life\u2019s tree store.\n\nWelcome to the DateLife project.\n\nAn R package, datelife for doing the calculations.\n\nCode coverage done right",
                "homepage": "http://www.datelife.org/"
            },
            {
                "name": "MEGAHIT",
                "uri": "https://bio.tools/megahit",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0610",
                            "term": "Ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Single node assembler for large and complex metagenomics NGS reads, such as soil. It makes use of succinct de Bruijn graph to achieve low memory usage, whereas its goal is not to make memory usage as low as possible.",
                "homepage": "https://github.com/voutcn/megahit"
            }
        ],
        "inputs": [
            "fastq_list"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "tag \"De novo metagenomic assembly\"",
            "container \"${container__assembler}\"",
            "label 'mem_veryhigh'",
            "errorStrategy \"retry\"",
            "publishDir params.output_folder"
        ],
        "when": "",
        "stub": ""
    },
    "collectCountReads": {
        "name_process": "collectCountReads",
        "string_process": "\nprocess collectCountReads {\n  container \"${container__ubuntu}\"\n  label 'io_limited'\n  errorStrategy 'retry'\n  publishDir params.output_folder\n  \n  input:\n  file csv_list\n  \n  output:\n  file \"${params.output_prefix}.counts.csv\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\necho \"specimen,nreads,label\" > ${params.output_prefix}.counts.csv\ncat ${csv_list} >> ${params.output_prefix}.counts.csv\n  \"\"\"\n\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\necho \"specimen,nreads,label\" > ${params.output_prefix}.counts.csv\ncat ${csv_list} >> ${params.output_prefix}.counts.csv\n  \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "csv_list"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__ubuntu}\"",
            "label 'io_limited'",
            "errorStrategy 'retry'",
            "publishDir params.output_folder"
        ],
        "when": "",
        "stub": ""
    },
    "index": {
        "name_process": "index",
        "string_process": "\nprocess index {\n  container \"${container__bwa}\"\n  label \"mem_medium\"\n  errorStrategy 'retry'\n\n  input:\n  file genome_fasta\n  \n  output:\n  file \"${genome_fasta}.tar\"\n  \n  \"\"\"\n#!/bin/bash\nset -e\n\necho \"Indexing ${genome_fasta}\"\nbwa index ${genome_fasta}\n\necho \"Tarring up indexed genome\"\ntar cvf ${genome_fasta}.tar ${genome_fasta}*\necho \"Done\"\n  \"\"\"\n\n}",
        "nb_lignes_process": 23,
        "string_script": "\"\"\"\n#!/bin/bash\nset -e\n\necho \"Indexing ${genome_fasta}\"\nbwa index ${genome_fasta}\n\necho \"Tarring up indexed genome\"\ntar cvf ${genome_fasta}.tar ${genome_fasta}*\necho \"Done\"\n  \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [
            "BWA"
        ],
        "tools_url": [
            "https://bio.tools/bwa"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            }
        ],
        "inputs": [
            "genome_fasta"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__bwa}\"",
            "label \"mem_medium\"",
            "errorStrategy 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "align": {
        "name_process": "align",
        "string_process": "\nprocess align {\n  container \"${container__bwa}\"\n  label \"mem_veryhigh\"\n  errorStrategy 'retry'\n  publishDir \"${params.output_folder}/bam/\", mode: 'copy'\n\n  input:\n  tuple val(sample_name), file(input_fastq)\n  file genome_index\n  \n  output:\n  tuple val(sample_name), file(\"${sample_name}.bam\"), file(\"${sample_name}.bam.bai\")\n\n  \"\"\"\n#!/bin/bash\nset -e\n\necho Unpacking ${genome_index}\ntar xvf ${genome_index}\n\necho \"Running BWA\"\nbwa mem \\\n  -t ${task.cpus} \\\n  ${genome_index.name.replaceAll(/.tar/, \"\")} \\\n  ${input_fastq} \\\n  | samtools view -b -F 4 -o ${sample_name}.bam\n\n# Sort the BAM file\nsamtools sort -o ${sample_name}.SORTED.bam ${sample_name}.bam\nmv ${sample_name}.SORTED.bam ${sample_name}.bam\n\n# Index the BAM file\nsamtools index ${sample_name}.bam\n\necho Done aligning ${sample_name}.bam\n\n    \"\"\"\n\n}",
        "nb_lignes_process": 38,
        "string_script": "\"\"\"\n#!/bin/bash\nset -e\n\necho Unpacking ${genome_index}\ntar xvf ${genome_index}\n\necho \"Running BWA\"\nbwa mem \\\n  -t ${task.cpus} \\\n  ${genome_index.name.replaceAll(/.tar/, \"\")} \\\n  ${input_fastq} \\\n  | samtools view -b -F 4 -o ${sample_name}.bam\n\n# Sort the BAM file\nsamtools sort -o ${sample_name}.SORTED.bam ${sample_name}.bam\nmv ${sample_name}.SORTED.bam ${sample_name}.bam\n\n# Index the BAM file\nsamtools index ${sample_name}.bam\n\necho Done aligning ${sample_name}.bam\n\n    \"\"\"",
        "nb_lignes_script": 23,
        "language_script": "bash",
        "tools": [
            "BWA",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/bwa",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "sample_name",
            "input_fastq",
            "genome_index"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sample_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__bwa}\"",
            "label \"mem_veryhigh\"",
            "errorStrategy 'retry'",
            "publishDir \"${params.output_folder}/bam/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "faidx": {
        "name_process": "faidx",
        "string_process": "\nprocess faidx {\n  container \"${container__bwa}\"\n  label \"mem_medium\"\n  errorStrategy 'retry'\n  publishDir params.output_folder, mode: 'copy'\n\n  input:\n  file fasta\n  \n  output:\n  file \"${fasta.name.replaceAll(/.gz/, '')}.fai\"\n\n  \"\"\"\n#!/bin/bash\nset -e\n\necho \"Decompressing input\"\ngunzip ${fasta}\n\necho \"Running samtools faidx ${fasta.name.replaceAll(/.gz/, '')}\"\nsamtools faidx ${fasta.name.replaceAll(/.gz/, '')}\n    \"\"\"\n\n}",
        "nb_lignes_process": 23,
        "string_script": "\"\"\"\n#!/bin/bash\nset -e\n\necho \"Decompressing input\"\ngunzip ${fasta}\n\necho \"Running samtools faidx ${fasta.name.replaceAll(/.gz/, '')}\"\nsamtools faidx ${fasta.name.replaceAll(/.gz/, '')}\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "fasta"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__bwa}\"",
            "label \"mem_medium\"",
            "errorStrategy 'retry'",
            "publishDir params.output_folder, mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "calcStats": {
        "name_process": "calcStats",
        "string_process": "\nprocess calcStats {\n  container \"${container__bwa}\"\n  label \"io_limited\"\n  errorStrategy 'retry'\n\n  input:\n  tuple val(sample_name), file(bam), file(bai)\n  \n  output:\n  tuple val(sample_name), file(\"${sample_name}.idxstats\"), file(\"${sample_name}.stats\"), file(\"${sample_name}.pileup\"), file(\"${sample_name}.positions\")\n\n  \"\"\"\n#!/bin/bash\nset -e\n\nsamtools sort ${bam} > ${sample_name}.sorted\nsamtools stats ${sample_name}.sorted > ${sample_name}.stats\nsamtools index ${sample_name}.sorted\nsamtools idxstats ${sample_name}.sorted > ${sample_name}.idxstats\nsamtools mpileup ${sample_name}.sorted > ${sample_name}.pileup\n\n# Make a file with four columns, the contig, the bitwise flag, and the leftmost position, and the length of the mapped segment\nsamtools view ${sample_name}.sorted | awk '{print \\$3 \"\\\\t\" \\$2 \"\\\\t\" \\$4 \"\\\\t\" length(\\$10)}' > ${sample_name}.positions\n\nrm ${sample_name}.sorted\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 28,
        "string_script": "\"\"\"\n#!/bin/bash\nset -e\n\nsamtools sort ${bam} > ${sample_name}.sorted\nsamtools stats ${sample_name}.sorted > ${sample_name}.stats\nsamtools index ${sample_name}.sorted\nsamtools idxstats ${sample_name}.sorted > ${sample_name}.idxstats\nsamtools mpileup ${sample_name}.sorted > ${sample_name}.pileup\n\n# Make a file with four columns, the contig, the bitwise flag, and the leftmost position, and the length of the mapped segment\nsamtools view ${sample_name}.sorted | awk '{print \\$3 \"\\\\t\" \\$2 \"\\\\t\" \\$4 \"\\\\t\" length(\\$10)}' > ${sample_name}.positions\n\nrm ${sample_name}.sorted\n\n  \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "sample_name",
            "bam",
            "bai"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sample_name"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__bwa}\"",
            "label \"io_limited\"",
            "errorStrategy 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "summarize": {
        "name_process": "summarize",
        "string_process": "\nprocess summarize {\n  container \"${container__pandas}\"\n  label \"mem_medium\"\n  errorStrategy 'retry'\n\n  input:\n  tuple val(sample_name), file(idxstats), file(stats), file(pileup), file(positions)\n  \n  output:\n  file \"${sample_name}.csv.gz\"\n\n  \"\"\"\n#!/usr/bin/env python3\nimport logging\nimport os\nimport gzip\nimport json\nimport pandas as pd\nfrom math import log as ln\n\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [nf-viral-metagenomics] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\ndef read_line(fp, prefix):\n    with open(fp, 'rt') as f:\n        for line in f:\n            if line.startswith(prefix):\n                return line.replace(prefix, '').strip(\" \").strip(\"\\\\t\")\n\ndef parse_flags(int_flags):\n    output = dict()\n    for n, flag in [\n        (2048, \"supplementary\"),\n        (1024, \"duplicate\"),\n        (512, \"fail_filter\"),\n        (256, \"secondary\"),\n        (128, \"last\"),\n        (64, \"first\"),\n        (32, \"next_rc\"),\n        (16, \"rc\"),\n        (8, \"next_unmapped\"),\n        (4, \"unmapped\"),\n        (2, \"aligned_properly\"),\n        (1, \"multiple_segments\"),\n    ]:\n        if int_flags >= n:\n            output[flag] = True\n            int_flags = int_flags - n\n        else:\n            output[flag] = False\n    assert int_flags == 0, int_flags\n    return output\n\n\ndef shannon_divesity(counts):\n    # See https://gist.github.com/audy/783125\n    \n    def p(n, N):\n        # Relative abundance\n        if n is 0:\n            return 0\n        else:\n            return (float(n)/N) * ln(float(n)/N)\n            \n    N = sum(counts)\n    \n    return -sum(p(n, N) for n in counts if n is not 0)\n\n\n# Read in the summary of alignment positions\nlogging.info(\"Reading in ${positions}\")\npositions = pd.read_csv(\n  \"${positions}\", \n  sep=\"\\\\t\", \n  header=None,\n  names = [\n    \"contig\", \"flags\", \"pos\", \"len\"\n  ]\n)\n\n# Parse the flags columns from the positions file\nlogging.info(\"Parsing flags in ${positions}\")\npositions = pd.concat([positions, pd.DataFrame(map(parse_flags, positions[\"flags\"]))], axis=1)\n\n# Find the read start position\n# If the read is aligned in the forward direction, use the leftmost position, otherwise use the rightmost\npositions = positions.assign(\n  start_pos = positions.apply(\n    lambda r: r[\"pos\"] + r[\"len\"] if r[\"rc\"] else r[\"pos\"],\n    axis=1\n  )\n)\n\n# Calculate Shannon diversity per contig\nlogging.info(\"Calculating Shannon diversity per contig\")\nsdi = positions.groupby(\n  \"contig\"\n).apply(\n  lambda contig_positions: shannon_divesity(contig_positions[\"start_pos\"].value_counts().values)\n)\n\n# Read in the pileup\nlogging.info(\"Reading in ${pileup}\")\npileup = pd.read_csv(\n  \"${pileup}\", \n  sep=\"\\\\t\", \n  header=None,\n  names = [\"contig\", \"position\", \"base\", \"depth\", \"aligned_bases\", \"aligned_quality\"]\n)\n\n# Get the stats for each contig\nlogging.info(\"Reading in ${idxstats}\")\ncontig_stats = pd.read_csv(\n  \"${idxstats}\", \n  sep=\"\\\\t\", \n  header=None,\n  names = [\"contig\", \"len\", \"mapped\", \"unmapped\"]\n).set_index(\n  \"contig\"\n)\n\n# Number of positions with any aligned reads per contig\nlogging.info(\"Number of positions with any aligned reads per contig\")\ncovlen = pileup.query(\n  \"depth > 0\"\n)[\"contig\"].value_counts()\n\n# Error rate over the entire alignment\nlogging.info(\"Reading overall error rate\")\nnerror = float(read_line(\"${stats}\", \"SN\\\\terror rate:\").split(\"\\\\t\")[0])\n\n# Number of bases aligned per contig\nnbases = pileup.groupby(\n  \"contig\"\n).apply(\n  lambda df: df[\"depth\"].sum()\n)\n\n# Make a single output object\nlogging.info(\"Making an output object\")\noutput = dict([\n  (\"specimen\", \"${sample_name}\"),\n  (\"reads_aligned\", positions[\"contig\"].value_counts()),\n  (\"bases_aligned\", nbases),\n  (\"bases_covered\", covlen),\n  (\"contig_length\", contig_stats[\"len\"]),\n  (\"error\", nerror),\n  (\"entropy\", sdi),\n  (\"specimen_total_reads\", contig_stats.reindex(columns=[\"mapped\", \"unmapped\"]).sum().sum())\n])\n\n# Format as a DataFrame\nlogging.info(\"Formatting as DataFrame\")\noutput = pd.DataFrame(\n  output\n).drop(  # Drop the pseudo contig name used for unaligned reads\n  index = \"*\"\n)\n\n# Add more summary columns, and reset the index used for the contig name\nlogging.info(\"Adding summary stats\")\noutput = output.assign(\n  depth = output[\"bases_aligned\"] / output[\"contig_length\"],\n  coverage = output[\"bases_covered\"] / output[\"contig_length\"],\n  proportion_of_reads = output[\"reads_aligned\"] / output[\"specimen_total_reads\"]\n).reset_index(\n).rename(\n  columns = dict([(\"index\", \"contig\")])\n)\n\n# Write out as CSV\nlogging.info(\"Writing out to ${sample_name}.csv.gz\")\noutput.to_csv(\n  \"${sample_name}.csv.gz\", \n  index=None\n)\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 187,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport logging\nimport os\nimport gzip\nimport json\nimport pandas as pd\nfrom math import log as ln\n\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [nf-viral-metagenomics] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\ndef read_line(fp, prefix):\n    with open(fp, 'rt') as f:\n        for line in f:\n            if line.startswith(prefix):\n                return line.replace(prefix, '').strip(\" \").strip(\"\\\\t\")\n\ndef parse_flags(int_flags):\n    output = dict()\n    for n, flag in [\n        (2048, \"supplementary\"),\n        (1024, \"duplicate\"),\n        (512, \"fail_filter\"),\n        (256, \"secondary\"),\n        (128, \"last\"),\n        (64, \"first\"),\n        (32, \"next_rc\"),\n        (16, \"rc\"),\n        (8, \"next_unmapped\"),\n        (4, \"unmapped\"),\n        (2, \"aligned_properly\"),\n        (1, \"multiple_segments\"),\n    ]:\n        if int_flags >= n:\n            output[flag] = True\n            int_flags = int_flags - n\n        else:\n            output[flag] = False\n    assert int_flags == 0, int_flags\n    return output\n\n\ndef shannon_divesity(counts):\n    # See https://gist.github.com/audy/783125\n    \n    def p(n, N):\n        # Relative abundance\n        if n is 0:\n            return 0\n        else:\n            return (float(n)/N) * ln(float(n)/N)\n            \n    N = sum(counts)\n    \n    return -sum(p(n, N) for n in counts if n is not 0)\n\n\n# Read in the summary of alignment positions\nlogging.info(\"Reading in ${positions}\")\npositions = pd.read_csv(\n  \"${positions}\", \n  sep=\"\\\\t\", \n  header=None,\n  names = [\n    \"contig\", \"flags\", \"pos\", \"len\"\n  ]\n)\n\n# Parse the flags columns from the positions file\nlogging.info(\"Parsing flags in ${positions}\")\npositions = pd.concat([positions, pd.DataFrame(map(parse_flags, positions[\"flags\"]))], axis=1)\n\n# Find the read start position\n# If the read is aligned in the forward direction, use the leftmost position, otherwise use the rightmost\npositions = positions.assign(\n  start_pos = positions.apply(\n    lambda r: r[\"pos\"] + r[\"len\"] if r[\"rc\"] else r[\"pos\"],\n    axis=1\n  )\n)\n\n# Calculate Shannon diversity per contig\nlogging.info(\"Calculating Shannon diversity per contig\")\nsdi = positions.groupby(\n  \"contig\"\n).apply(\n  lambda contig_positions: shannon_divesity(contig_positions[\"start_pos\"].value_counts().values)\n)\n\n# Read in the pileup\nlogging.info(\"Reading in ${pileup}\")\npileup = pd.read_csv(\n  \"${pileup}\", \n  sep=\"\\\\t\", \n  header=None,\n  names = [\"contig\", \"position\", \"base\", \"depth\", \"aligned_bases\", \"aligned_quality\"]\n)\n\n# Get the stats for each contig\nlogging.info(\"Reading in ${idxstats}\")\ncontig_stats = pd.read_csv(\n  \"${idxstats}\", \n  sep=\"\\\\t\", \n  header=None,\n  names = [\"contig\", \"len\", \"mapped\", \"unmapped\"]\n).set_index(\n  \"contig\"\n)\n\n# Number of positions with any aligned reads per contig\nlogging.info(\"Number of positions with any aligned reads per contig\")\ncovlen = pileup.query(\n  \"depth > 0\"\n)[\"contig\"].value_counts()\n\n# Error rate over the entire alignment\nlogging.info(\"Reading overall error rate\")\nnerror = float(read_line(\"${stats}\", \"SN\\\\terror rate:\").split(\"\\\\t\")[0])\n\n# Number of bases aligned per contig\nnbases = pileup.groupby(\n  \"contig\"\n).apply(\n  lambda df: df[\"depth\"].sum()\n)\n\n# Make a single output object\nlogging.info(\"Making an output object\")\noutput = dict([\n  (\"specimen\", \"${sample_name}\"),\n  (\"reads_aligned\", positions[\"contig\"].value_counts()),\n  (\"bases_aligned\", nbases),\n  (\"bases_covered\", covlen),\n  (\"contig_length\", contig_stats[\"len\"]),\n  (\"error\", nerror),\n  (\"entropy\", sdi),\n  (\"specimen_total_reads\", contig_stats.reindex(columns=[\"mapped\", \"unmapped\"]).sum().sum())\n])\n\n# Format as a DataFrame\nlogging.info(\"Formatting as DataFrame\")\noutput = pd.DataFrame(\n  output\n).drop(  # Drop the pseudo contig name used for unaligned reads\n  index = \"*\"\n)\n\n# Add more summary columns, and reset the index used for the contig name\nlogging.info(\"Adding summary stats\")\noutput = output.assign(\n  depth = output[\"bases_aligned\"] / output[\"contig_length\"],\n  coverage = output[\"bases_covered\"] / output[\"contig_length\"],\n  proportion_of_reads = output[\"reads_aligned\"] / output[\"specimen_total_reads\"]\n).reset_index(\n).rename(\n  columns = dict([(\"index\", \"contig\")])\n)\n\n# Write out as CSV\nlogging.info(\"Writing out to ${sample_name}.csv.gz\")\noutput.to_csv(\n  \"${sample_name}.csv.gz\", \n  index=None\n)\n\n  \"\"\"",
        "nb_lignes_script": 174,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_name",
            "idxstats",
            "stats",
            "pileup",
            "positions"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__pandas}\"",
            "label \"mem_medium\"",
            "errorStrategy 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "collect": {
        "name_process": "collect",
        "string_process": "\nprocess collect {\n  container \"${container__pandas}\"\n  label \"io_limited\"\n  errorStrategy 'retry'\n  publishDir params.output_folder\n\n  input:\n  file summary_csv_list\n\n  output:\n  file \"${params.output_prefix}.summary.csv.gz\"\n\n  \"\"\"\n#!/usr/bin/env python3\nimport logging\nimport os\nimport gzip\nimport json\nimport pandas as pd\n\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [nf-viral-metagenomics] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\nsummary_csv_list = \"${summary_csv_list}\".split(\" \")\nlogging.info(\"Reading in a list of %d summary CSV files\" % len(summary_csv_list))\nsummary_df = pd.concat([\n  pd.read_csv(fp)\n  for fp in summary_csv_list\n])\n\nlogging.info(\"Saving to ${params.output_prefix}.summary.csv.gz\")\nsummary_df.to_csv(\"${params.output_prefix}.summary.csv.gz\", index=None)\n\nlogging.info(\"Done\")\n\n\"\"\"\n\n}",
        "nb_lignes_process": 45,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport logging\nimport os\nimport gzip\nimport json\nimport pandas as pd\n\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [nf-viral-metagenomics] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\nsummary_csv_list = \"${summary_csv_list}\".split(\" \")\nlogging.info(\"Reading in a list of %d summary CSV files\" % len(summary_csv_list))\nsummary_df = pd.concat([\n  pd.read_csv(fp)\n  for fp in summary_csv_list\n])\n\nlogging.info(\"Saving to ${params.output_prefix}.summary.csv.gz\")\nsummary_df.to_csv(\"${params.output_prefix}.summary.csv.gz\", index=None)\n\nlogging.info(\"Done\")\n\n\"\"\"",
        "nb_lignes_script": 31,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "summary_csv_list"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__pandas}\"",
            "label \"io_limited\"",
            "errorStrategy 'retry'",
            "publishDir params.output_folder"
        ],
        "when": "",
        "stub": ""
    },
    "collect_with_kraken": {
        "name_process": "collect_with_kraken",
        "string_process": "\nprocess collect_with_kraken {\n  container \"${container__pandas}\"\n  label \"io_limited\"\n  errorStrategy 'retry'\n  publishDir params.output_folder\n\n  input:\n  file summary_csv_list\n  file kraken2_tsv\n\n  output:\n  file \"${params.output_prefix}.summary.csv.gz\"\n\n  \"\"\"\n#!/usr/bin/env python3\nimport logging\nimport os\nimport gzip\nimport json\nimport pandas as pd\n\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [nf-viral-metagenomics] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\nsummary_csv_list = \"${summary_csv_list}\".split(\" \")\nlogging.info(\"Reading in a list of %d summary CSV files\" % len(summary_csv_list))\nsummary_df = pd.concat([\n  pd.read_csv(fp)\n  for fp in summary_csv_list\n])\n\n# Read in the Kraken2 results\nkraken_df = pd.read_csv(\n  \"${kraken2_tsv}\", \n  sep=\"\\\\t\", \n  header=None,\n  names = [\n    \"success\",\n    \"query\",\n    \"tax_id\",\n    \"query_len\",\n    \"hit_string\"\n  ]\n).query(\n  \"success == 'C'\"\n).drop(\n  columns=[\"success\", \"query_len\"]\n).set_index(\n  \"query\"\n)\n\n# Add the Kraken results to the summary DataFrame\nsummary_df = summary_df.assign(\n  tax_id = summary_df[\"contig\"].apply(\n    kraken_df[\"tax_id\"].get\n  ).fillna(\n    0\n  ).apply(\n    int\n  )\n)\nsummary_df = summary_df.assign(\n  hit_string = summary_df[\"contig\"].apply(\n    kraken_df[\"hit_string\"].get\n  )\n)\n\nlogging.info(\"Saving to ${params.output_prefix}.summary.csv.gz\")\nsummary_df.to_csv(\"${params.output_prefix}.summary.csv.gz\", index=None)\n\nlogging.info(\"Done\")\n\n\"\"\"\n\n}",
        "nb_lignes_process": 82,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\nimport logging\nimport os\nimport gzip\nimport json\nimport pandas as pd\n\nlogFormatter = logging.Formatter(\n    '%(asctime)s %(levelname)-8s [nf-viral-metagenomics] %(message)s'\n)\nrootLogger = logging.getLogger()\nrootLogger.setLevel(logging.INFO)\n\n# Write to STDOUT\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nrootLogger.addHandler(consoleHandler)\n\nsummary_csv_list = \"${summary_csv_list}\".split(\" \")\nlogging.info(\"Reading in a list of %d summary CSV files\" % len(summary_csv_list))\nsummary_df = pd.concat([\n  pd.read_csv(fp)\n  for fp in summary_csv_list\n])\n\n# Read in the Kraken2 results\nkraken_df = pd.read_csv(\n  \"${kraken2_tsv}\", \n  sep=\"\\\\t\", \n  header=None,\n  names = [\n    \"success\",\n    \"query\",\n    \"tax_id\",\n    \"query_len\",\n    \"hit_string\"\n  ]\n).query(\n  \"success == 'C'\"\n).drop(\n  columns=[\"success\", \"query_len\"]\n).set_index(\n  \"query\"\n)\n\n# Add the Kraken results to the summary DataFrame\nsummary_df = summary_df.assign(\n  tax_id = summary_df[\"contig\"].apply(\n    kraken_df[\"tax_id\"].get\n  ).fillna(\n    0\n  ).apply(\n    int\n  )\n)\nsummary_df = summary_df.assign(\n  hit_string = summary_df[\"contig\"].apply(\n    kraken_df[\"hit_string\"].get\n  )\n)\n\nlogging.info(\"Saving to ${params.output_prefix}.summary.csv.gz\")\nsummary_df.to_csv(\"${params.output_prefix}.summary.csv.gz\", index=None)\n\nlogging.info(\"Done\")\n\n\"\"\"",
        "nb_lignes_script": 67,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "summary_csv_list",
            "kraken2_tsv"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__pandas}\"",
            "label \"io_limited\"",
            "errorStrategy 'retry'",
            "publishDir params.output_folder"
        ],
        "when": "",
        "stub": ""
    },
    "kraken2": {
        "name_process": "kraken2",
        "string_process": "\nprocess kraken2 {\n  container \"${container__kraken2}\"\n  errorStrategy 'retry'\n  label \"mem_veryhigh\"\n\n  input:\n  file input_fasta\n  file \"DB/*\"\n  \n  output:\n  file \"${params.output_prefix}.kraken2.gz\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\nls -lahtr\n\nls -lahtr DB/\n\necho \"Running kraken2\"\n\nKRAKEN2_DB_PATH=\\$PWD kraken2 \\\n    --db DB \\\n    --threads ${task.cpus} \\\n    ${input_fasta} \\\n    | gzip -c > ${params.output_prefix}.kraken2.gz\n\n  \"\"\"\n\n}",
        "nb_lignes_process": 31,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\nls -lahtr\n\nls -lahtr DB/\n\necho \"Running kraken2\"\n\nKRAKEN2_DB_PATH=\\$PWD kraken2 \\\n    --db DB \\\n    --threads ${task.cpus} \\\n    ${input_fasta} \\\n    | gzip -c > ${params.output_prefix}.kraken2.gz\n\n  \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "input_fasta"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__kraken2}\"",
            "errorStrategy 'retry'",
            "label \"mem_veryhigh\""
        ],
        "when": "",
        "stub": ""
    },
    "preprocessFASTA": {
        "name_process": "preprocessFASTA",
        "string_process": "\nprocess preprocessFASTA {\n\n    container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"\n    cpus 1\n    memory \"2 GB\"\n    \n    input:\n    tuple file(fasta), file(yaml) from sample_sheet_ch\n\n    output:\n    tuple file(\"${fasta}\"), file(\"${yaml}\") into preprocessed_sample_sheet_ch\n\n    \"\"\"\n#!/usr/bin/env python3\n\n# Following criteria from https://github.com/ncbi/pgap/wiki/Input-Files\n\nfrom Bio.SeqIO.FastaIO import SimpleFastaParser\nimport re\n\n# Read in all of the genome\ngenome = dict([\n    (header, seq)\n    for header, seq in SimpleFastaParser(open(\"${fasta}\"))\n])\n\n# Sanitize and write out\nseen_headers = set([])\nwith open(\"${fasta}\", \"w\") as handle:\n    for header, seq in genome.items():\n\n        \n        # Make sure the sequence is >= 199 nucleotides\n        if len(seq) < 199:\n            continue\n\n        # Trim the header to 50 characters\n        if len(header) > 50:\n            header = header[:50]\n\n        # Only include letters, digits, hyphens (-), underscores (_), periods (.), colons (:), asterisks (*), and number signs (#)\n        header = re.sub('[^0-9a-zA-Z-.*#\\$_:]', '_', header)\n\n        # All headers are unique\n        assert header not in seen_headers\n        seen_headers.add(header)\n\n        # Make sure there are no N's at the beginning or end\n        assert seq[0] != \"#\"\n        assert seq[-1] != \"#\"\n\n        handle.write(\">%s\\\\n%s\\\\n\" % (header, seq))\n\n    \"\"\"\n\n}",
        "nb_lignes_process": 55,
        "string_script": "\"\"\"\n#!/usr/bin/env python3\n\n# Following criteria from https://github.com/ncbi/pgap/wiki/Input-Files\n\nfrom Bio.SeqIO.FastaIO import SimpleFastaParser\nimport re\n\n# Read in all of the genome\ngenome = dict([\n    (header, seq)\n    for header, seq in SimpleFastaParser(open(\"${fasta}\"))\n])\n\n# Sanitize and write out\nseen_headers = set([])\nwith open(\"${fasta}\", \"w\") as handle:\n    for header, seq in genome.items():\n\n        \n        # Make sure the sequence is >= 199 nucleotides\n        if len(seq) < 199:\n            continue\n\n        # Trim the header to 50 characters\n        if len(header) > 50:\n            header = header[:50]\n\n        # Only include letters, digits, hyphens (-), underscores (_), periods (.), colons (:), asterisks (*), and number signs (#)\n        header = re.sub('[^0-9a-zA-Z-.*#\\$_:]', '_', header)\n\n        # All headers are unique\n        assert header not in seen_headers\n        seen_headers.add(header)\n\n        # Make sure there are no N's at the beginning or end\n        assert seq[0] != \"#\"\n        assert seq[-1] != \"#\"\n\n        handle.write(\">%s\\\\n%s\\\\n\" % (header, seq))\n\n    \"\"\"",
        "nb_lignes_script": 41,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_sheet_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "preprocessed_sample_sheet_ch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"quay.io/fhcrc-microbiome/integrate-metagenomic-assemblies:v0.5\"",
            "cpus 1",
            "memory \"2 GB\""
        ],
        "when": "",
        "stub": ""
    },
    "run_PGAP": {
        "name_process": "run_PGAP",
        "string_process": "\nprocess run_PGAP {\n\n    container \"ncbi/pgap:${params.pgap_version}\"\n    cpus 16\n    memory \"30 GB\"\n    publishDir \"${params.output_folder}/\"\n    errorStrategy 'retry'\n    maxRetries 2\n\n    input:\n    tuple file(fasta), file(yaml) from preprocessed_sample_sheet_ch\n    file reference_tarball\n\n    output:\n    file \"${fasta}.fna.gz\"\n    file \"${fasta}.faa.gz\"\n    file \"${fasta}.gbk.gz\"\n    file \"${fasta}.gff.gz\"\n    file \"${fasta}.sqn.gz\"\n\n    \"\"\"\n#!/bin/bash\n\nset -euxo pipefail\n\n# Decompress the supplemental_data\ntar xzvf ${reference_tarball}\n\n# Make sure that the expected folder exists\n[[ -s input-${params.pgap_version} ]]\n\n# usage: pgap.cwl [-h] [--blast_hits_cache_data BLAST_HITS_CACHE_DATA]\n#                 [--blast_rules_db BLAST_RULES_DB] --fasta FASTA\n#                 [--gc_assm_name GC_ASSM_NAME] [--ignore_all_errors]\n#                 [--no_internet] --report_usage --submol SUBMOL\n#                 [--supplemental_data SUPPLEMENTAL_DATA]\n#                 [job_order]\n\n# positional arguments:\n#   job_order             Job input json file\n\n# optional arguments:\n#   -h, --help            show this help message and exit\n#   --blast_hits_cache_data BLAST_HITS_CACHE_DATA\n#   --blast_rules_db BLAST_RULES_DB\n#   --fasta FASTA\n#   --gc_assm_name GC_ASSM_NAME\n#   --ignore_all_errors\n#   --no_internet\n#   --report_usage\n#   --submol SUBMOL\n#   --supplemental_data SUPPLEMENTAL_DATA\n\ncwl-runner \\\n    /pgap/pgap.cwl \\\n    --fasta ${fasta} \\\n    --submol ${yaml} \\\n    --supplemental_data input-${params.pgap_version} \\\n    --report_usage 2>&1 | tail -n 120\n# I have to throw away all but the tail of the standard error for this process,\n# because it can easily take up gigabytes of logfile\n\n# Rename the input files\nfor suffix in fna faa gbk gff sqn; do\n\n    echo \"Checking to make sure that annot.\\$suffix exists\"\n    [[ -s annot.\\$suffix ]]\n\n    echo \"Renaming to ${fasta}.\\$suffix\"\n    mv annot.\\$suffix ${fasta}.\\$suffix\n    echo \"Compressing ${fasta}.\\$suffix\"\n    gzip ${fasta}.\\$suffix\n\ndone\n\n    \"\"\"\n\n}",
        "nb_lignes_process": 77,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -euxo pipefail\n\n# Decompress the supplemental_data\ntar xzvf ${reference_tarball}\n\n# Make sure that the expected folder exists\n[[ -s input-${params.pgap_version} ]]\n\n# usage: pgap.cwl [-h] [--blast_hits_cache_data BLAST_HITS_CACHE_DATA]\n#                 [--blast_rules_db BLAST_RULES_DB] --fasta FASTA\n#                 [--gc_assm_name GC_ASSM_NAME] [--ignore_all_errors]\n#                 [--no_internet] --report_usage --submol SUBMOL\n#                 [--supplemental_data SUPPLEMENTAL_DATA]\n#                 [job_order]\n\n# positional arguments:\n#   job_order             Job input json file\n\n# optional arguments:\n#   -h, --help            show this help message and exit\n#   --blast_hits_cache_data BLAST_HITS_CACHE_DATA\n#   --blast_rules_db BLAST_RULES_DB\n#   --fasta FASTA\n#   --gc_assm_name GC_ASSM_NAME\n#   --ignore_all_errors\n#   --no_internet\n#   --report_usage\n#   --submol SUBMOL\n#   --supplemental_data SUPPLEMENTAL_DATA\n\ncwl-runner \\\n    /pgap/pgap.cwl \\\n    --fasta ${fasta} \\\n    --submol ${yaml} \\\n    --supplemental_data input-${params.pgap_version} \\\n    --report_usage 2>&1 | tail -n 120\n# I have to throw away all but the tail of the standard error for this process,\n# because it can easily take up gigabytes of logfile\n\n# Rename the input files\nfor suffix in fna faa gbk gff sqn; do\n\n    echo \"Checking to make sure that annot.\\$suffix exists\"\n    [[ -s annot.\\$suffix ]]\n\n    echo \"Renaming to ${fasta}.\\$suffix\"\n    mv annot.\\$suffix ${fasta}.\\$suffix\n    echo \"Compressing ${fasta}.\\$suffix\"\n    gzip ${fasta}.\\$suffix\n\ndone\n\n    \"\"\"",
        "nb_lignes_script": 55,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "preprocessed_sample_sheet_ch",
            "reference_tarball"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"ncbi/pgap:${params.pgap_version}\"",
            "cpus 16",
            "memory \"30 GB\"",
            "publishDir \"${params.output_folder}/\"",
            "errorStrategy 'retry'",
            "maxRetries 2"
        ],
        "when": "",
        "stub": ""
    },
    "dada2_ft": {
        "name_process": "dada2_ft",
        "string_process": "\nprocess dada2_ft {\n    container \"${container__dada2}\"\n    label 'io_mem'\n                            \n    errorStrategy \"ignore\"\n\n    input:\n        tuple val(specimen), val(batch), file(R1), file(R2)\n    \n    output:\n        tuple val(specimen), val(batch), file(\"${R1.getSimpleName()}.R1.dada2.ft.fq.gz\"), file(\"${R2.getSimpleName()}.R2.dada2.ft.fq.gz\")\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2'); \n    filterAndTrim(\n        '${R1}', '${R1.getSimpleName()}.R1.dada2.ft.fq.gz',\n        '${R2}', '${R2.getSimpleName()}.R2.dada2.ft.fq.gz',\n        trimLeft = ${params.trimLeft},\n        maxN = ${params.maxN},\n        maxEE = ${params.maxEE},\n        truncLen = c(${params.truncLenF}, ${params.truncLenR}),\n        truncQ = ${params.truncQ},\n        compress = TRUE,\n        verbose = TRUE,\n        multithread = ${task.cpus}\n    )\n    \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2'); \n    filterAndTrim(\n        '${R1}', '${R1.getSimpleName()}.R1.dada2.ft.fq.gz',\n        '${R2}', '${R2.getSimpleName()}.R2.dada2.ft.fq.gz',\n        trimLeft = ${params.trimLeft},\n        maxN = ${params.maxN},\n        maxEE = ${params.maxEE},\n        truncLen = c(${params.truncLenF}, ${params.truncLenR}),\n        truncQ = ${params.truncQ},\n        compress = TRUE,\n        verbose = TRUE,\n        multithread = ${task.cpus}\n    )\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1",
            "R2"
        ],
        "nb_inputs": 4,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"ignore\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_ft_se": {
        "name_process": "dada2_ft_se",
        "string_process": "\nprocess dada2_ft_se {\n    container \"${container__dada2}\"\n    label 'io_mem'\n                            \n    errorStrategy \"ignore\"\n\n    input:\n        tuple val(specimen), val(batch), file(R1)\n    \n    output:\n        tuple val(specimen), val(batch), file(\"${R1.getSimpleName()}.dada2.ft.fq.gz\")\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2'); \n    filterAndTrim(\n        '${R1}', '${R1.getSimpleName()}.dada2.ft.fq.gz',\n        trimLeft = ${params.trimLeft},\n        maxN = ${params.maxN},\n        maxEE = ${params.maxEE},\n        truncLen = ${params.truncLenF_se},\n        truncQ = ${params.truncQ},\n        compress = TRUE,\n        verbose = TRUE,\n        multithread = ${task.cpus}\n    )\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2'); \n    filterAndTrim(\n        '${R1}', '${R1.getSimpleName()}.dada2.ft.fq.gz',\n        trimLeft = ${params.trimLeft},\n        maxN = ${params.maxN},\n        maxEE = ${params.maxEE},\n        truncLen = ${params.truncLenF_se},\n        truncQ = ${params.truncQ},\n        compress = TRUE,\n        verbose = TRUE,\n        multithread = ${task.cpus}\n    )\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1"
        ],
        "nb_inputs": 3,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"ignore\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_ft_pyro": {
        "name_process": "dada2_ft_pyro",
        "string_process": "\nprocess dada2_ft_pyro {\n    container \"${container__dada2}\"\n    label 'io_mem'\n                            \n    errorStrategy \"ignore\"\n\n    input:\n        tuple val(specimen), val(batch), file(R1)\n    \n    output:\n        tuple val(specimen), val(batch), file(\"${R1.getSimpleName()}.dada2.ft.fq.gz\")\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2'); \n    filterAndTrim(\n        '${R1}', '${R1.getSimpleName()}.dada2.ft.fq.gz',\n        truncLen = ${params.truncLenF_pyro},\n        maxLen = ${params.maxLenPyro},\n        maxN = ${params.maxN},\n        maxEE = ${params.maxEE},\n        truncQ = ${params.truncQ},\n        compress = TRUE,\n        verbose = TRUE,\n        multithread = ${task.cpus}\n    )\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2'); \n    filterAndTrim(\n        '${R1}', '${R1.getSimpleName()}.dada2.ft.fq.gz',\n        truncLen = ${params.truncLenF_pyro},\n        maxLen = ${params.maxLenPyro},\n        maxN = ${params.maxN},\n        maxEE = ${params.maxEE},\n        truncQ = ${params.truncQ},\n        compress = TRUE,\n        verbose = TRUE,\n        multithread = ${task.cpus}\n    )\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1"
        ],
        "nb_inputs": 3,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"ignore\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_derep": {
        "name_process": "dada2_derep",
        "string_process": "\nprocess dada2_derep {\n    container \"${container__dada2}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(specimen), val(batch), file(R1), file(R2)\n    \n    output:\n        tuple val(specimen), val(batch), file(\"${specimen}.R1.dada2.ft.derep.rds\"), file(\"${specimen}.R2.dada2.ft.derep.rds\")\n    \n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_1 <- derepFastq('${R1}');\n    saveRDS(derep_1, '${specimen}.R1.dada2.ft.derep.rds');\n    derep_2 <- derepFastq('${R2}');\n    saveRDS(derep_2, '${specimen}.R2.dada2.ft.derep.rds');\n    \"\"\" \n}",
        "nb_lignes_process": 19,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_1 <- derepFastq('${R1}');\n    saveRDS(derep_1, '${specimen}.R1.dada2.ft.derep.rds');\n    derep_2 <- derepFastq('${R2}');\n    saveRDS(derep_2, '${specimen}.R2.dada2.ft.derep.rds');\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1",
            "R2"
        ],
        "nb_inputs": 4,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_derep_se": {
        "name_process": "dada2_derep_se",
        "string_process": "\nprocess dada2_derep_se {\n    container \"${container__dada2}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(specimen), val(batch), file(R1)\n    \n    output:\n        tuple val(specimen), val(batch), file(\"${specimen}.dada2.ft.derep.rds\")\n    \n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_1 <- derepFastq('${R1}');\n    saveRDS(derep_1, '${specimen}.dada2.ft.derep.rds');\n    \"\"\" \n}",
        "nb_lignes_process": 17,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_1 <- derepFastq('${R1}');\n    saveRDS(derep_1, '${specimen}.dada2.ft.derep.rds');\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1"
        ],
        "nb_inputs": 3,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_derep_pyro": {
        "name_process": "dada2_derep_pyro",
        "string_process": "\nprocess dada2_derep_pyro {\n    container \"${container__dada2}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(specimen), val(batch), file(R1)\n    \n    output:\n        tuple val(specimen), val(batch), file(\"${specimen}.dada2.ft.derep.rds\")\n    \n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_1 <- derepFastq('${R1}');\n    saveRDS(derep_1, '${specimen}.dada2.ft.derep.rds');\n    \"\"\" \n}",
        "nb_lignes_process": 17,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_1 <- derepFastq('${R1}');\n    saveRDS(derep_1, '${specimen}.dada2.ft.derep.rds');\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch",
            "R1"
        ],
        "nb_inputs": 3,
        "outputs": [
            "batch"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_learn_error": {
        "name_process": "dada2_learn_error",
        "string_process": "\nprocess dada2_learn_error {\n    container \"${container__dada2}\"\n    label 'multithread'\n    errorStrategy \"finish\"\n    publishDir \"${params.output}/sv/errM/${batch}\", mode: 'copy'\n\n    input:\n        tuple val(batch_specimens), val(batch), file(reads), val(read_num)\n\n    output:\n        tuple val(batch), val(read_num), file(\"${batch}.${read_num}.errM.rds\"), file(\"${batch}.${read_num}.errM.csv\")\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    err <- learnErrors(\n        unlist(strsplit(\n            '${reads}',\n            ' ',\n            fixed=TRUE\n        )),\n        multithread=${task.cpus},\n        MAX_CONSIST=${params.errM_maxConsist},\n        randomize=${params.errM_randomize},\n        nbases=${params.errM_nbases},\n        verbose=TRUE\n    );\n    saveRDS(err, \"${batch}.${read_num}.errM.rds\"); \n    write.csv(err, \"${batch}.${read_num}.errM.csv\");\n    \"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    err <- learnErrors(\n        unlist(strsplit(\n            '${reads}',\n            ' ',\n            fixed=TRUE\n        )),\n        multithread=${task.cpus},\n        MAX_CONSIST=${params.errM_maxConsist},\n        randomize=${params.errM_randomize},\n        nbases=${params.errM_nbases},\n        verbose=TRUE\n    );\n    saveRDS(err, \"${batch}.${read_num}.errM.rds\"); \n    write.csv(err, \"${batch}.${read_num}.errM.csv\");\n    \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch_specimens",
            "batch",
            "read_num",
            "reads"
        ],
        "nb_inputs": 4,
        "outputs": [
            "read_num"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'multithread'",
            "errorStrategy \"finish\"",
            "publishDir \"${params.output}/sv/errM/${batch}\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "dada2_learn_error_pyro": {
        "name_process": "dada2_learn_error_pyro",
        "string_process": "\nprocess dada2_learn_error_pyro {\n    container \"${container__dada2}\"\n    label 'multithread'\n    errorStrategy \"finish\"\n    publishDir \"${params.output}/sv/errM/${batch}\", mode: 'copy'\n\n    input:\n        tuple val(batch_specimens), val(batch), file(reads), val(read_num)\n\n    output:\n        tuple val(batch), val(read_num), file(\"${batch}.${read_num}.errM.rds\"), file(\"${batch}.${read_num}.errM.csv\")\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    err <- learnErrors(\n        unlist(strsplit(\n            '${reads}',\n            ' ',\n            fixed=TRUE\n        )),\n        multithread=${task.cpus},\n        HOMOPOLYMER_GAP_PENALTY=-1, BAND_SIZE=32,\n        MAX_CONSIST=${params.errM_maxConsist},\n        randomize=${params.errM_randomize},\n        nbases=${params.errM_nbases},\n        verbose=TRUE\n    );\n    saveRDS(err, \"${batch}.${read_num}.errM.rds\"); \n    write.csv(err, \"${batch}.${read_num}.errM.csv\");\n    \"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    err <- learnErrors(\n        unlist(strsplit(\n            '${reads}',\n            ' ',\n            fixed=TRUE\n        )),\n        multithread=${task.cpus},\n        HOMOPOLYMER_GAP_PENALTY=-1, BAND_SIZE=32,\n        MAX_CONSIST=${params.errM_maxConsist},\n        randomize=${params.errM_randomize},\n        nbases=${params.errM_nbases},\n        verbose=TRUE\n    );\n    saveRDS(err, \"${batch}.${read_num}.errM.rds\"); \n    write.csv(err, \"${batch}.${read_num}.errM.csv\");\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch_specimens",
            "batch",
            "read_num",
            "reads"
        ],
        "nb_inputs": 4,
        "outputs": [
            "read_num"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'multithread'",
            "errorStrategy \"finish\"",
            "publishDir \"${params.output}/sv/errM/${batch}\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "dada2_derep_batches": {
        "name_process": "dada2_derep_batches",
        "string_process": "\nprocess dada2_derep_batches {\n    container \"${container__dada2}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(batch), val(read_num), val(specimens), file(dereps), val(errM), val(dada_fns)\n    \n    output:\n        tuple val(batch), val(read_num), val(specimens), file(\"${batch}_${read_num}_derep.rds\"), val(errM), val(dada_fns)\n    \n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_str <- trimws('${dereps}')\n    print(derep_str)\n    derep <- lapply(\n        unlist(strsplit(derep_str, ' ', fixed=TRUE)),\n        readRDS\n    );\n    saveRDS(derep, \"${batch}_${read_num}_derep.rds\")\n    \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep_str <- trimws('${dereps}')\n    print(derep_str)\n    derep <- lapply(\n        unlist(strsplit(derep_str, ' ', fixed=TRUE)),\n        readRDS\n    );\n    saveRDS(derep, \"${batch}_${read_num}_derep.rds\")\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch",
            "read_num",
            "specimens",
            "errM",
            "dada_fns",
            "dereps"
        ],
        "nb_inputs": 6,
        "outputs": [
            "dada_fns"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_derep_batches_pyro": {
        "name_process": "dada2_derep_batches_pyro",
        "string_process": "\nprocess dada2_derep_batches_pyro {\n    container \"${container__dada2}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(batch), val(read_num), val(specimens), file(dereps), val(errM), val(dada_fns)\n    \n    output:\n        tuple val(batch), val(read_num), val(specimens), file(\"${batch}_${read_num}_derep.rds\"), val(errM), val(dada_fns)\n    \n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep <- lapply(\n        unlist(strsplit('${dereps}', ' ', fixed=TRUE)),\n        readRDS\n    );\n    saveRDS(derep, \"${batch}_${read_num}_derep.rds\")\n    \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    derep <- lapply(\n        unlist(strsplit('${dereps}', ' ', fixed=TRUE)),\n        readRDS\n    );\n    saveRDS(derep, \"${batch}_${read_num}_derep.rds\")\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch",
            "read_num",
            "specimens",
            "errM",
            "dada_fns",
            "dereps"
        ],
        "nb_inputs": 6,
        "outputs": [
            "dada_fns"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_dada": {
        "name_process": "dada2_dada",
        "string_process": "\nprocess dada2_dada {\n    container \"${container__dada2}\"\n    label 'multithread'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(batch), val(read_num), val(specimens), file(derep), file(errM), val(dada_fns)\n\n    output:\n        tuple val(batch), val(read_num), val(specimens), file(\"${batch}_${read_num}_dada.rds\"), val(dada_fns)\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    errM <- readRDS('${errM}');\n    derep <- readRDS('${derep}');\n    dadaResult <- dada(derep, err=errM, multithread=${task.cpus}, verbose=TRUE, pool=\"pseudo\");\n    saveRDS(dadaResult, \"${batch}_${read_num}_dada.rds\");\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    errM <- readRDS('${errM}');\n    derep <- readRDS('${derep}');\n    dadaResult <- dada(derep, err=errM, multithread=${task.cpus}, verbose=TRUE, pool=\"pseudo\");\n    saveRDS(dadaResult, \"${batch}_${read_num}_dada.rds\");\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch",
            "read_num",
            "specimens",
            "dada_fns",
            "derep",
            "errM"
        ],
        "nb_inputs": 6,
        "outputs": [
            "dada_fns"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'multithread'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_dada_pyro": {
        "name_process": "dada2_dada_pyro",
        "string_process": "\nprocess dada2_dada_pyro {\n    container \"${container__dada2}\"\n    label 'multithread'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(batch), val(read_num), val(specimens), file(derep), file(errM), val(dada_fns)\n\n    output:\n        tuple val(batch), val(read_num), val(specimens), file(\"${batch}_${read_num}_dada.rds\"), val(dada_fns)\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    errM <- readRDS('${errM}');\n    derep <- readRDS('${derep}');\n    dadaResult <- dada(derep, err=errM, multithread=${task.cpus}, verbose=TRUE, pool=\"pseudo\",  HOMOPOLYMER_GAP_PENALTY=-1, BAND_SIZE=32);\n    saveRDS(dadaResult, \"${batch}_${read_num}_dada.rds\");\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    errM <- readRDS('${errM}');\n    derep <- readRDS('${derep}');\n    dadaResult <- dada(derep, err=errM, multithread=${task.cpus}, verbose=TRUE, pool=\"pseudo\",  HOMOPOLYMER_GAP_PENALTY=-1, BAND_SIZE=32);\n    saveRDS(dadaResult, \"${batch}_${read_num}_dada.rds\");\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch",
            "read_num",
            "specimens",
            "dada_fns",
            "derep",
            "errM"
        ],
        "nb_inputs": 6,
        "outputs": [
            "dada_fns"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'multithread'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_demultiplex_dada": {
        "name_process": "dada2_demultiplex_dada",
        "string_process": "\nprocess dada2_demultiplex_dada {\n    container \"${container__dada2}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(batch), val(read_num), val(specimens), file(dada), val(dada_fns)\n    \n    output:\n        tuple val(batch), val(read_num), val(specimens), file(dada_fns)\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    dadaResult <- readRDS('${dada}');\n    dada_names <- unlist(strsplit(\n        gsub('(\\\\\\\\[|\\\\\\\\])', \"\", \"${dada_fns}\"),\n        \", \"));\n    print(dada_names);\n    sapply(1:length(dadaResult), function(i) {\n        saveRDS(dadaResult[i], dada_names[i]);\n    });\n    \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    dadaResult <- readRDS('${dada}');\n    dada_names <- unlist(strsplit(\n        gsub('(\\\\\\\\[|\\\\\\\\])', \"\", \"${dada_fns}\"),\n        \", \"));\n    print(dada_names);\n    sapply(1:length(dadaResult), function(i) {\n        saveRDS(dadaResult[i], dada_names[i]);\n    });\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch",
            "read_num",
            "specimens",
            "dada_fns",
            "dada"
        ],
        "nb_inputs": 5,
        "outputs": [
            "specimens"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_merge": {
        "name_process": "dada2_merge",
        "string_process": "\nprocess dada2_merge {\n    container \"${container__dada2}\"\n    label 'multithread'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(specimen), val(batch), file(\"R1.dada2.rds\"), file(\"R2.dada2.rds\"), file(\"R1.derep.rds\"), file(\"R2.derep.rds\")\n\n    output:\n        tuple val(batch), val(specimen), file(\"${specimen}.dada2.merged.rds\")\n    \n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    dada_1 <- readRDS('R1.dada2.rds');\n    derep_1 <- readRDS('R1.derep.rds');\n    dada_2 <- readRDS('R2.dada2.rds');\n    derep_2 <- readRDS('R2.derep.rds');        \n    merger <- mergePairs(\n        dada_1, derep_1,\n        dada_2, derep_2,\n        verbose=TRUE,\n        trimOverhang=TRUE,\n        maxMismatch=${params.maxMismatch},\n        minOverlap=${params.minOverlap}\n    );\n    saveRDS(merger, \"${specimen}.dada2.merged.rds\");\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    dada_1 <- readRDS('R1.dada2.rds');\n    derep_1 <- readRDS('R1.derep.rds');\n    dada_2 <- readRDS('R2.dada2.rds');\n    derep_2 <- readRDS('R2.derep.rds');        \n    merger <- mergePairs(\n        dada_1, derep_1,\n        dada_2, derep_2,\n        verbose=TRUE,\n        trimOverhang=TRUE,\n        maxMismatch=${params.maxMismatch},\n        minOverlap=${params.minOverlap}\n    );\n    saveRDS(merger, \"${specimen}.dada2.merged.rds\");\n    \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "specimen",
            "batch"
        ],
        "nb_inputs": 2,
        "outputs": [
            "specimen"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'multithread'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_seqtab_sp": {
        "name_process": "dada2_seqtab_sp",
        "string_process": "\nprocess dada2_seqtab_sp {\n    container \"${container__dada2}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(batch), val(specimen), file(merged)\n\n    output:\n        tuple val(batch), val(specimen), file(\"${specimen}.dada2.seqtab.rds\")\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    merged <- readRDS('${merged}');\n    seqtab <- makeSequenceTable(merged);\n    rownames(seqtab) <- c('${specimen}');\n    saveRDS(seqtab, '${specimen}.dada2.seqtab.rds');\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    merged <- readRDS('${merged}');\n    seqtab <- makeSequenceTable(merged);\n    rownames(seqtab) <- c('${specimen}');\n    saveRDS(seqtab, '${specimen}.dada2.seqtab.rds');\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch",
            "specimen",
            "merged"
        ],
        "nb_inputs": 3,
        "outputs": [
            "specimen"
        ],
        "nb_outputs": 1,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_seqtab_combine_batch": {
        "name_process": "dada2_seqtab_combine_batch",
        "string_process": "\nprocess dada2_seqtab_combine_batch {\n    container \"${container__fastcombineseqtab}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        tuple val(batch), file(sp_seqtabs_rds)\n\n    output:\n        file(\"${batch}.dada2.seqtabs.rds\")\n\n    \"\"\"\n    set -e\n\n    combine_seqtab \\\n    --rds ${batch}.dada2.seqtabs.rds \\\n    --seqtabs ${sp_seqtabs_rds}\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    set -e\n\n    combine_seqtab \\\n    --rds ${batch}.dada2.seqtabs.rds \\\n    --seqtabs ${sp_seqtabs_rds}\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "batch",
            "sp_seqtabs_rds"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__fastcombineseqtab}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_seqtab_combine_all": {
        "name_process": "dada2_seqtab_combine_all",
        "string_process": "\nprocess dada2_seqtab_combine_all {\n    container \"${container__fastcombineseqtab}\"\n    label 'io_mem'\n    errorStrategy \"finish\"\n\n    input:\n        file(seqtabs_rds)\n\n    output:\n        file(\"combined.dada2.seqtabs.rds\")\n\n    \"\"\"\n    set -e\n\n    combine_seqtab \\\n    --rds combined.dada2.seqtabs.rds \\\n    --seqtabs ${seqtabs_rds}\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    set -e\n\n    combine_seqtab \\\n    --rds combined.dada2.seqtabs.rds \\\n    --seqtabs ${seqtabs_rds}\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "seqtabs_rds"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__fastcombineseqtab}\"",
            "label 'io_mem'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2_remove_bimera": {
        "name_process": "dada2_remove_bimera",
        "string_process": "\nprocess dada2_remove_bimera {\n    container \"${container__dada2}\"\n    label 'mem_veryhigh'\n    errorStrategy \"finish\"\n    publishDir \"${params.output}/sv/\", mode: 'copy'\n\n    input:\n        file(combined_seqtab)\n\n    output:\n        file(\"dada2.combined.seqtabs.nochimera.csv\")\n        file(\"dada2.combined.seqtabs.nochimera.rds\")\n\n    script:\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    seqtab <- readRDS('${combined_seqtab}');\n    seqtab_nochim <- removeBimeraDenovo(\n        seqtab,\n        method = '${params.chimera_method}',\n        multithread = ${task.cpus}\n    );\n    saveRDS(seqtab_nochim, 'dada2.combined.seqtabs.nochimera.rds'); \n    write.csv(seqtab_nochim, 'dada2.combined.seqtabs.nochimera.csv', na='');\n    print((sum(seqtab) - sum(seqtab_nochim)) / sum(seqtab));\n    \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "    \"\"\"\n    #!/usr/bin/env Rscript\n    library('dada2');\n    seqtab <- readRDS('${combined_seqtab}');\n    seqtab_nochim <- removeBimeraDenovo(\n        seqtab,\n        method = '${params.chimera_method}',\n        multithread = ${task.cpus}\n    );\n    saveRDS(seqtab_nochim, 'dada2.combined.seqtabs.nochimera.rds'); \n    write.csv(seqtab_nochim, 'dada2.combined.seqtabs.nochimera.csv', na='');\n    print((sum(seqtab) - sum(seqtab_nochim)) / sum(seqtab));\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "combined_seqtab"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__dada2}\"",
            "label 'mem_veryhigh'",
            "errorStrategy \"finish\"",
            "publishDir \"${params.output}/sv/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "goods_filter_seqtab": {
        "name_process": "goods_filter_seqtab",
        "string_process": "\nprocess goods_filter_seqtab {\n    container \"${container__goodsfilter}\"\n    label 'io_mem'\n    publishDir \"${params.output}/sv/\", mode: 'copy'\n    errorStrategy \"finish\"\n\n    input:\n        file(seqtab_csv)\n\n    output:\n        file \"${seqtab_csv.getSimpleName()}.goodsfiltered.csv\"\n        file \"${seqtab_csv.getSimpleName()}.goods_converged.csv\"\n        path \"curves/*_collector.csv\"\n\n\n    \"\"\"\n    set -e \n\n\n    goodsfilter \\\n    --seqtable ${seqtab_csv} \\\n    --seqtable_filtered ${seqtab_csv.getSimpleName()}.goodsfiltered.csv \\\n    --converged_file ${seqtab_csv.getSimpleName()}.goods_converged.csv \\\n    --iteration_cutoff ${params.goods_convergence} \\\n    --min_prev ${params.min_sv_prev} \\\n    --min_reads ${params.goods_min_reads} \\\n    --curves_path curves/\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "\"\"\"\n    set -e \n\n\n    goodsfilter \\\n    --seqtable ${seqtab_csv} \\\n    --seqtable_filtered ${seqtab_csv.getSimpleName()}.goodsfiltered.csv \\\n    --converged_file ${seqtab_csv.getSimpleName()}.goods_converged.csv \\\n    --iteration_cutoff ${params.goods_convergence} \\\n    --min_prev ${params.min_sv_prev} \\\n    --min_reads ${params.goods_min_reads} \\\n    --curves_path curves/\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "seqtab_csv"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__goodsfilter}\"",
            "label 'io_mem'",
            "publishDir \"${params.output}/sv/\", mode: 'copy'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "build_kraken2_db": {
        "name_process": "build_kraken2_db",
        "string_process": " process build_kraken2_db {\n    container \"${container__kraken2}\"\n    errorStrategy 'retry'\n    publishDir \"${params.output_folder}\"\n    memory 240.Gb\n    cpus 32\n    \n    output:\n    file \"${params.output_prefix}*\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\nkraken2-build \\\n    --standard \\\n    --threads ${task.cpus} \\\n    --db ${params.output_prefix} \\\n    --use-ftp\n\n  \"\"\"\n\n  }",
        "nb_lignes_process": 22,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\nkraken2-build \\\n    --standard \\\n    --threads ${task.cpus} \\\n    --db ${params.output_prefix} \\\n    --use-ftp\n\n  \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__kraken2}\"",
            "errorStrategy 'retry'",
            "publishDir \"${params.output_folder}\"",
            "memory 240.Gb",
            "cpus 32"
        ],
        "when": "",
        "stub": ""
    },
    "build_kraken2_db_protein": {
        "name_process": "build_kraken2_db_protein",
        "string_process": " process build_kraken2_db_protein {\n      container \"${container__kraken2}\"\n      errorStrategy 'retry'\n      publishDir \"${params.output_folder}\"\n      memory 240.Gb\n      cpus 32\n      \n      output:\n      file \"${params.output_prefix}*\"\n\n  \"\"\"\n#!/bin/bash\n\nset -e\n\nkraken2-build \\\n    --standard \\\n    --threads ${task.cpus} \\\n    --db ${params.output_prefix} \\\n    --protein \\\n    --use-ftp\n\n  \"\"\"\n\n    }",
        "nb_lignes_process": 23,
        "string_script": "\"\"\"\n#!/bin/bash\n\nset -e\n\nkraken2-build \\\n    --standard \\\n    --threads ${task.cpus} \\\n    --db ${params.output_prefix} \\\n    --protein \\\n    --use-ftp\n\n  \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__kraken2}\"",
            "errorStrategy 'retry'",
            "publishDir \"${params.output_folder}\"",
            "memory 240.Gb",
            "cpus 32"
        ],
        "when": "",
        "stub": ""
    },
    "RefpkgSearchRepo": {
        "name_process": "RefpkgSearchRepo",
        "string_process": "\nprocess RefpkgSearchRepo {\n    container \"${container__vsearch}\"\n    label = 'multithread'\n\n    input:\n        path(sv_fasta_f)\n        path(repo_fasta)\n    \n    output:\n        path \"${repo_fasta}.repo.recruits.fasta\", emit: recruits\n        path \"${repo_fasta}.uc\", emit: uc\n        path \"${repo_fasta}.sv.nohit.fasta\", emit: nohits\n        path \"${repo_fasta}.vsearch.log\", emit: log\n        \n\n    \"\"\"\n    vsearch \\\n    --threads=${task.cpus} \\\n    --usearch_global ${sv_fasta_f} \\\n    --db ${repo_fasta} \\\n    --id=${params.repo_min_id} \\\n    --strand both \\\n    --uc=${repo_fasta}.uc --uc_allhits \\\n    --notmatched=${repo_fasta}.sv.nohit.fasta \\\n    --dbmatched=${repo_fasta}.repo.recruits.fasta \\\n    --maxaccepts=${params.repo_max_accepts} \\\n    | tee -a ${repo_fasta}.vsearch.log\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "\"\"\"\n    vsearch \\\n    --threads=${task.cpus} \\\n    --usearch_global ${sv_fasta_f} \\\n    --db ${repo_fasta} \\\n    --id=${params.repo_min_id} \\\n    --strand both \\\n    --uc=${repo_fasta}.uc --uc_allhits \\\n    --notmatched=${repo_fasta}.sv.nohit.fasta \\\n    --dbmatched=${repo_fasta}.repo.recruits.fasta \\\n    --maxaccepts=${params.repo_max_accepts} \\\n    | tee -a ${repo_fasta}.vsearch.log\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "VSEARCH"
        ],
        "tools_url": [
            "https://bio.tools/vsearch"
        ],
        "tools_dico": [
            {
                "name": "VSEARCH",
                "uri": "https://bio.tools/vsearch",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2520",
                                    "term": "DNA mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0450",
                                    "term": "Chimera detection"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0450",
                                    "term": "Chimeric sequence detection"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2977",
                                "term": "Nucleic acid sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_0865",
                                "term": "Sequence similarity score"
                            }
                        ]
                    }
                ],
                "description": "High-throughput search and clustering sequence analysis tool. It supports de novo and reference based chimera detection, clustering, full-length and prefix dereplication, reverse complementation, masking, all-vs-all pairwise global alignment, exact and global alignment searching, shuffling, subsampling and sorting. It also supports FASTQ file analysis, filtering and conversion.",
                "homepage": "https://github.com/torognes/vsearch"
            }
        ],
        "inputs": [
            "sv_fasta_f",
            "repo_fasta"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container \"${container__vsearch}\"",
            "label = 'multithread'"
        ],
        "when": "",
        "stub": ""
    },
    "CombinedRefFilter": {
        "name_process": "CombinedRefFilter",
        "string_process": "\nprocess CombinedRefFilter {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n\n    input:\n        path(repo_recruit_f)\n        path(repo_recruit_uc)\n        path(taxdb)\n        path(seq_info)\n\n    output:\n        path \"references.fasta\", emit: recruit_seq\n        path \"references_seq_info.csv\", emit: recruit_si\n\n\n\"\"\"\n#!/usr/bin/env python\nimport fastalite\nimport csv\nimport sqlite3\nfrom collections import defaultdict\n\ndef get_lineage(tax_id, cursor):\n    cur_tax_id = tax_id\n    lineage = [cur_tax_id]\n    while cur_tax_id != '1':\n        cur_tax_id = cursor.execute(\"SELECT parent_id FROM nodes where tax_id=?\", (cur_tax_id,)).fetchone()[0]\n        lineage.append(cur_tax_id)\n    return lineage\n\n# Get the seq <-> ID linkage and ID <-> seq linkage\nseq_ids = defaultdict(set)\nid_seq = {}\nwith open('${repo_recruit_f}', 'rt') as recruit_h:\n    for sr in fastalite.fastalite(recruit_h):\n        seq_ids[sr.seq].add(sr.id)\n        id_seq[sr.id] = sr.seq\n\nall_refs = set(id_seq.keys())\n\n### Start with the UC of alignment SV <-> Repo\nwith open('${repo_recruit_uc}', 'rt') as in_uc:\n    uc_data = [\n        (\n            row[8], # SV\n            row[9], # Reference_id\n            float(row[3]) # Pct ID\n        )\n        for row in \n        csv.reader(in_uc, delimiter='\\\\t')\n        if row[3] != '*' and row[9] in all_refs\n    ]\nsv_max_pctid = defaultdict(float)\n# Figure out the best-pct-id for each SV\nfor sv, ref, pctid in uc_data:\n    sv_max_pctid[sv] = max([sv_max_pctid[sv], pctid])\n\n\n\n# Load in seq_info\nwith open('${seq_info}', 'rt') as sif:\n    si_r = csv.DictReader(sif)\n    seq_info = {\n        r['seqname']: r\n        for r in si_r\n    }\n\n# For each reference sequence, pick a *representitive* seq_id\ntax_db = sqlite3.connect('${taxdb}')\ntax_db_cur = tax_db.cursor()\n\nseq_rep_id = {}\nfor seq, ids in seq_ids.items():\n    if len(ids) == 1:\n        # If there is only one ID for a sequence it automatically passes!\n        seq_rep_id[seq] = list(ids)[0]\n        continue\n    tax_ids = {seq_info[i]['tax_id']: i for i in ids if i in seq_info}\n    if len(tax_ids) == 1:\n        # Only one tax id, pick a random one as our champion\n        seq_rep_id[seq] = list(ids)[0]\n        continue\n    # Implicit else multiple taxa...\n    # Get the lineages for these taxa to root\n    tax_lineages = {\n        tid: get_lineage(tid, tax_db_cur)\n        for tid in tax_ids\n    }\n    # And the depth of each lineage\n    lin_depth_tax = {\n        len(lineage): tid\n        for tid, lineage in\n        tax_lineages.items()\n    }\n    # Pick the seq from the deepest lineage to be the representitive\n    seq_rep_id[seq] = tax_ids[\n            lin_depth_tax[max(lin_depth_tax.keys())]\n        ]\n\n# Only keep ref seqs as good as the best hit for an SV\nbesthit_ref_seqs = [\n    (sv, id_seq[ref])\n    for sv, ref, pctid\n    in uc_data\n    if sv_max_pctid[sv] == pctid\n]\n\n# Refseq -> besthit SV\nrefseq_sv = defaultdict(set)\nfor sv, refseq in besthit_ref_seqs:\n    refseq_sv[refseq].add(sv)\n\n# How many SV does each ref cover?\nref_sv_cnt = {\n    k: len(v)\n    for k, v in refseq_sv.items()\n}\n\n# Get rid of ref sequences that only represent less than MIN_REF_SV\nMIN_REF_SV = 2\nfiltered_sv_ref = [\n    (sv, seq_rep_id.get(ref))\n    for ref, svs in refseq_sv.items()\n    for sv in svs\n    if len(svs) >= MIN_REF_SV\n]\n\n# For the sv post the shared ref filter who no longer have a best hit, see if there is any hit for them \n# e.g. some reference in the current set that there is some identity, even if not as good as the best hits\ncovered_sv = {sv for sv, ref in filtered_sv_ref}\nextant_refs = {ref for sv, ref in filtered_sv_ref}\naddbacksv_refs = defaultdict(list)\nfor sv, ref_id, pct_id in uc_data:\n    if sv not in covered_sv:\n        if ref_id in extant_refs:\n            addbacksv_refs[sv].append((\n                ref_id, # Ref_ID\n                pct_id, # Pct_ID\n                True # Already part of our reference?\n            ))\n        elif pct_id == sv_max_pctid[sv]:\n            addbacksv_refs[sv].append((\n                ref_id,\n                pct_id,\n                False\n            ))\n# Great, now use this dict to make a decision of which refs to add for each sv\nfor sv, svr in addbacksv_refs.items():\n    sv_seq_pctid = {\n        id_seq.get(r): pct_id\n        for r, pct_id, already_in in svr\n        if not already_in\n    }\n    # Was there totally no representation?\n    if len([r for r, pct_id, already_in in svr if already_in]) == 0:\n        # if not, add in everything\n        filtered_sv_ref += [\n            (sv, seq_rep_id.get(s))\n            for s in sv_seq_pctid.keys()\n        ]\n    else:\n        # It's not perfect, but given we have some representation, just pick the longest sequence\n        filtered_sv_ref.append(\n            (\n                sv,\n                seq_rep_id.get(sorted(sv_seq_pctid.keys(), key=lambda v: len(v))[-1])\n            )\n        )\n\nfiltered_ref = {\n    ref\n    for sv, ref\n    in filtered_sv_ref\n}\n\n# Use this to create out outputted final output\nwith open('references.fasta', 'wt') as ref_out:\n    for ref_id in filtered_ref:\n        ref_out.write(\">{}\\\\n{}\\\\n\".format(\n            ref_id,\n            id_seq.get(ref_id)\n        ))\n\nsi_columns = list(seq_info.values())[0].keys()\n\nwith open('references_seq_info.csv', 'wt') as si_out:\n    si_writer = csv.DictWriter(\n        si_out,\n        fieldnames=si_columns\n    )\n    si_writer.writeheader()\n    si_writer.writerows([\n        r for i, r in seq_info.items()\n        if i in filtered_ref\n    ])\n\"\"\"\n}",
        "nb_lignes_process": 196,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nimport fastalite\nimport csv\nimport sqlite3\nfrom collections import defaultdict\n\ndef get_lineage(tax_id, cursor):\n    cur_tax_id = tax_id\n    lineage = [cur_tax_id]\n    while cur_tax_id != '1':\n        cur_tax_id = cursor.execute(\"SELECT parent_id FROM nodes where tax_id=?\", (cur_tax_id,)).fetchone()[0]\n        lineage.append(cur_tax_id)\n    return lineage\n\n# Get the seq <-> ID linkage and ID <-> seq linkage\nseq_ids = defaultdict(set)\nid_seq = {}\nwith open('${repo_recruit_f}', 'rt') as recruit_h:\n    for sr in fastalite.fastalite(recruit_h):\n        seq_ids[sr.seq].add(sr.id)\n        id_seq[sr.id] = sr.seq\n\nall_refs = set(id_seq.keys())\n\n### Start with the UC of alignment SV <-> Repo\nwith open('${repo_recruit_uc}', 'rt') as in_uc:\n    uc_data = [\n        (\n            row[8], # SV\n            row[9], # Reference_id\n            float(row[3]) # Pct ID\n        )\n        for row in \n        csv.reader(in_uc, delimiter='\\\\t')\n        if row[3] != '*' and row[9] in all_refs\n    ]\nsv_max_pctid = defaultdict(float)\n# Figure out the best-pct-id for each SV\nfor sv, ref, pctid in uc_data:\n    sv_max_pctid[sv] = max([sv_max_pctid[sv], pctid])\n\n\n\n# Load in seq_info\nwith open('${seq_info}', 'rt') as sif:\n    si_r = csv.DictReader(sif)\n    seq_info = {\n        r['seqname']: r\n        for r in si_r\n    }\n\n# For each reference sequence, pick a *representitive* seq_id\ntax_db = sqlite3.connect('${taxdb}')\ntax_db_cur = tax_db.cursor()\n\nseq_rep_id = {}\nfor seq, ids in seq_ids.items():\n    if len(ids) == 1:\n        # If there is only one ID for a sequence it automatically passes!\n        seq_rep_id[seq] = list(ids)[0]\n        continue\n    tax_ids = {seq_info[i]['tax_id']: i for i in ids if i in seq_info}\n    if len(tax_ids) == 1:\n        # Only one tax id, pick a random one as our champion\n        seq_rep_id[seq] = list(ids)[0]\n        continue\n    # Implicit else multiple taxa...\n    # Get the lineages for these taxa to root\n    tax_lineages = {\n        tid: get_lineage(tid, tax_db_cur)\n        for tid in tax_ids\n    }\n    # And the depth of each lineage\n    lin_depth_tax = {\n        len(lineage): tid\n        for tid, lineage in\n        tax_lineages.items()\n    }\n    # Pick the seq from the deepest lineage to be the representitive\n    seq_rep_id[seq] = tax_ids[\n            lin_depth_tax[max(lin_depth_tax.keys())]\n        ]\n\n# Only keep ref seqs as good as the best hit for an SV\nbesthit_ref_seqs = [\n    (sv, id_seq[ref])\n    for sv, ref, pctid\n    in uc_data\n    if sv_max_pctid[sv] == pctid\n]\n\n# Refseq -> besthit SV\nrefseq_sv = defaultdict(set)\nfor sv, refseq in besthit_ref_seqs:\n    refseq_sv[refseq].add(sv)\n\n# How many SV does each ref cover?\nref_sv_cnt = {\n    k: len(v)\n    for k, v in refseq_sv.items()\n}\n\n# Get rid of ref sequences that only represent less than MIN_REF_SV\nMIN_REF_SV = 2\nfiltered_sv_ref = [\n    (sv, seq_rep_id.get(ref))\n    for ref, svs in refseq_sv.items()\n    for sv in svs\n    if len(svs) >= MIN_REF_SV\n]\n\n# For the sv post the shared ref filter who no longer have a best hit, see if there is any hit for them \n# e.g. some reference in the current set that there is some identity, even if not as good as the best hits\ncovered_sv = {sv for sv, ref in filtered_sv_ref}\nextant_refs = {ref for sv, ref in filtered_sv_ref}\naddbacksv_refs = defaultdict(list)\nfor sv, ref_id, pct_id in uc_data:\n    if sv not in covered_sv:\n        if ref_id in extant_refs:\n            addbacksv_refs[sv].append((\n                ref_id, # Ref_ID\n                pct_id, # Pct_ID\n                True # Already part of our reference?\n            ))\n        elif pct_id == sv_max_pctid[sv]:\n            addbacksv_refs[sv].append((\n                ref_id,\n                pct_id,\n                False\n            ))\n# Great, now use this dict to make a decision of which refs to add for each sv\nfor sv, svr in addbacksv_refs.items():\n    sv_seq_pctid = {\n        id_seq.get(r): pct_id\n        for r, pct_id, already_in in svr\n        if not already_in\n    }\n    # Was there totally no representation?\n    if len([r for r, pct_id, already_in in svr if already_in]) == 0:\n        # if not, add in everything\n        filtered_sv_ref += [\n            (sv, seq_rep_id.get(s))\n            for s in sv_seq_pctid.keys()\n        ]\n    else:\n        # It's not perfect, but given we have some representation, just pick the longest sequence\n        filtered_sv_ref.append(\n            (\n                sv,\n                seq_rep_id.get(sorted(sv_seq_pctid.keys(), key=lambda v: len(v))[-1])\n            )\n        )\n\nfiltered_ref = {\n    ref\n    for sv, ref\n    in filtered_sv_ref\n}\n\n# Use this to create out outputted final output\nwith open('references.fasta', 'wt') as ref_out:\n    for ref_id in filtered_ref:\n        ref_out.write(\">{}\\\\n{}\\\\n\".format(\n            ref_id,\n            id_seq.get(ref_id)\n        ))\n\nsi_columns = list(seq_info.values())[0].keys()\n\nwith open('references_seq_info.csv', 'wt') as si_out:\n    si_writer = csv.DictWriter(\n        si_out,\n        fieldnames=si_columns\n    )\n    si_writer.writeheader()\n    si_writer.writerows([\n        r for i, r in seq_info.items()\n        if i in filtered_ref\n    ])\n\"\"\"",
        "nb_lignes_script": 180,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "repo_recruit_f",
            "repo_recruit_uc",
            "taxdb",
            "seq_info"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'"
        ],
        "when": "",
        "stub": ""
    },
    "FilterSeqInfo": {
        "name_process": "FilterSeqInfo",
        "string_process": "\nprocess FilterSeqInfo {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n\n    input:\n        file (repo_recruits_f)\n        file (repo_si)\n    \n    output:\n        file('refpkg.seq_info.csv')\n\n    \"\"\"\n    #!/usr/bin/env python\n    import fastalite\n    import csv\n\n    with open('${repo_recruits_f}', 'rt') as fasta_in:\n        seq_ids = {sr.id for sr in fastalite.fastalite(fasta_in)}\n    with open('${repo_si}', 'rt') as si_in, open('refpkg.seq_info.csv', 'wt') as si_out:\n        si_reader = csv.DictReader(si_in)\n        si_writer = csv.DictWriter(si_out, si_reader.fieldnames)\n        si_writer.writeheader()\n        for r in si_reader:\n            if r['seqname'] in seq_ids:\n                si_writer.writerow(r)\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n    #!/usr/bin/env python\n    import fastalite\n    import csv\n\n    with open('${repo_recruits_f}', 'rt') as fasta_in:\n        seq_ids = {sr.id for sr in fastalite.fastalite(fasta_in)}\n    with open('${repo_si}', 'rt') as si_in, open('refpkg.seq_info.csv', 'wt') as si_out:\n        si_reader = csv.DictReader(si_in)\n        si_writer = csv.DictWriter(si_out, si_reader.fieldnames)\n        si_writer.writeheader()\n        for r in si_reader:\n            if r['seqname'] in seq_ids:\n                si_writer.writerow(r)\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "repo_recruits_f",
            "repo_si"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'"
        ],
        "when": "",
        "stub": ""
    },
    "RemoveDroppedRecruits": {
        "name_process": "RemoveDroppedRecruits",
        "string_process": "\nprocess RemoveDroppedRecruits{\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n\n    input:\n        path (recruits_fasta)\n        path (seq_info)\n    \n    output:\n        path('refpkg_recruits.fasta')\n    \n    \"\"\"\n    #!/usr/bin/env python\n    import fastalite\n    import csv\n\n    with open('${seq_info}') as si_in:\n        si_reader = csv.DictReader(si_in)\n        seqinfo_seqs = {\n            r['seqname'] for r in si_reader\n        }\n    with open('${recruits_fasta}', 'rt') as fasta_in, open('refpkg_recruits.fasta', 'wt') as fasta_out:\n        for sr in fastalite.fastalite(fasta_in):\n            if sr.id in seqinfo_seqs:\n                fasta_out.write(\">{} {}\\\\n{}\\\\n\".format(\n                    sr.id,\n                    sr.description,\n                    sr.seq\n                )) \n    \"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "\"\"\"\n    #!/usr/bin/env python\n    import fastalite\n    import csv\n\n    with open('${seq_info}') as si_in:\n        si_reader = csv.DictReader(si_in)\n        seqinfo_seqs = {\n            r['seqname'] for r in si_reader\n        }\n    with open('${recruits_fasta}', 'rt') as fasta_in, open('refpkg_recruits.fasta', 'wt') as fasta_out:\n        for sr in fastalite.fastalite(fasta_in):\n            if sr.id in seqinfo_seqs:\n                fasta_out.write(\">{} {}\\\\n{}\\\\n\".format(\n                    sr.id,\n                    sr.description,\n                    sr.seq\n                )) \n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "recruits_fasta",
            "seq_info"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'"
        ],
        "when": "",
        "stub": ""
    },
    "DlBuildTaxtasticDB": {
        "name_process": "DlBuildTaxtasticDB",
        "string_process": "\nprocess DlBuildTaxtasticDB {\n    container = \"${container__taxtastic}\"\n    label = 'io_net'\n    errorStrategy = 'finish'\n\n    output:\n        file \"taxonomy.db\"\n\n    afterScript \"rm -rf dl/\"\n\n\n    \"\"\"\n    set -e\n\n    mkdir -p dl/ && \\\n    taxit new_database taxonomy.db -u ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip -p dl/\n    \"\"\"\n\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    set -e\n\n    mkdir -p dl/ && \\\n    taxit new_database taxonomy.db -u ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip -p dl/\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "TaxIt"
        ],
        "tools_url": [
            "https://bio.tools/TaxIt"
        ],
        "tools_dico": [
            {
                "name": "TaxIt",
                "uri": "https://bio.tools/TaxIt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3520",
                            "term": "Proteomics experiment"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3649",
                                    "term": "Target-Decoy"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An iterative and automated computational pipeline for untargeted strain-level identification using MS/MS spectra from pathogenic samples.",
                "homepage": "https://gitlab.com/rki_bioinformatics/TaxIt"
            }
        ],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__taxtastic}\"",
            "label = 'io_net'",
            "errorStrategy = 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "BuildTaxtasticDB": {
        "name_process": "BuildTaxtasticDB",
        "string_process": "\nprocess BuildTaxtasticDB {\n    container = \"${container__taxtastic}\"\n    label = 'io_limited'\n    errorStrategy = 'finish'\n\n    input:\n        file taxdump_zip_f\n\n    output:\n        file \"taxonomy.db\"\n\n    \"\"\"\n    taxit new_database taxonomy.db -z ${taxdump_zip_f}\n    \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "\"\"\"\n    taxit new_database taxonomy.db -z ${taxdump_zip_f}\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "TaxIt"
        ],
        "tools_url": [
            "https://bio.tools/TaxIt"
        ],
        "tools_dico": [
            {
                "name": "TaxIt",
                "uri": "https://bio.tools/TaxIt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3520",
                            "term": "Proteomics experiment"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3649",
                                    "term": "Target-Decoy"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An iterative and automated computational pipeline for untargeted strain-level identification using MS/MS spectra from pathogenic samples.",
                "homepage": "https://gitlab.com/rki_bioinformatics/TaxIt"
            }
        ],
        "inputs": [
            "taxdump_zip_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__taxtastic}\"",
            "label = 'io_limited'",
            "errorStrategy = 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "ConfirmSI": {
        "name_process": "ConfirmSI",
        "string_process": "\nprocess ConfirmSI {\n    container = \"${container__taxtastic}\"\n    label = 'io_mem'\n\n    input:\n        file taxonomy_db_f\n        file refpkg_si_f\n    \n    output:\n        file \"${refpkg_si_f.baseName}.corr.csv\"\n    \n    \"\"\"\n    taxit update_taxids \\\n    ${refpkg_si_f} \\\n    ${taxonomy_db_f} \\\n    -o ${refpkg_si_f.baseName}.corr.csv \\\n    -a drop\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    taxit update_taxids \\\n    ${refpkg_si_f} \\\n    ${taxonomy_db_f} \\\n    -o ${refpkg_si_f.baseName}.corr.csv \\\n    -a drop\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "TaxIt"
        ],
        "tools_url": [
            "https://bio.tools/TaxIt"
        ],
        "tools_dico": [
            {
                "name": "TaxIt",
                "uri": "https://bio.tools/TaxIt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3520",
                            "term": "Proteomics experiment"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3649",
                                    "term": "Target-Decoy"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An iterative and automated computational pipeline for untargeted strain-level identification using MS/MS spectra from pathogenic samples.",
                "homepage": "https://gitlab.com/rki_bioinformatics/TaxIt"
            }
        ],
        "inputs": [
            "taxonomy_db_f",
            "refpkg_si_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__taxtastic}\"",
            "label = 'io_mem'"
        ],
        "when": "",
        "stub": ""
    },
    "AlignRepoRecruits": {
        "name_process": "AlignRepoRecruits",
        "string_process": "\nprocess AlignRepoRecruits {\n    container = \"${container__infernal}\"\n    label = 'mem_veryhigh'\n\n    input:\n        file repo_recruits_f\n        file cm\n    \n    output:\n        file \"recruits.aln.sto\"\n        file \"recruits.aln.scores\" \n    \n    \"\"\"\n    cmalign \\\n    --cpu ${task.cpus} --noprob --dnaout --mxsize ${params.cmalign_mxsize} \\\n    --sfile recruits.aln.scores -o recruits.aln.sto \\\n    ${cm} ${repo_recruits_f}\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    cmalign \\\n    --cpu ${task.cpus} --noprob --dnaout --mxsize ${params.cmalign_mxsize} \\\n    --sfile recruits.aln.scores -o recruits.aln.sto \\\n    ${cm} ${repo_recruits_f}\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "repo_recruits_f",
            "cm"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__infernal}\"",
            "label = 'mem_veryhigh'"
        ],
        "when": "",
        "stub": ""
    },
    "ConvertAlnToPhy": {
        "name_process": "ConvertAlnToPhy",
        "string_process": "\nprocess ConvertAlnToPhy {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    errorStrategy \"finish\"\n\n    input: \n        file recruits_aln_sto_f\n    \n    output:\n        file \"recruits.aln.phy\"\n    \n    \"\"\"\n    #!/usr/bin/env python\n    from Bio import AlignIO\n\n    with open('recruits.aln.phy', 'wt') as out_h:\n        AlignIO.write(\n            AlignIO.read(\n                open('${recruits_aln_sto_f}', 'rt'),\n                'stockholm'\n            ),\n            out_h,\n            'phylip-relaxed'\n        )\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n    #!/usr/bin/env python\n    from Bio import AlignIO\n\n    with open('recruits.aln.phy', 'wt') as out_h:\n        AlignIO.write(\n            AlignIO.read(\n                open('${recruits_aln_sto_f}', 'rt'),\n                'stockholm'\n            ),\n            out_h,\n            'phylip-relaxed'\n        )\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "recruits_aln_sto_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "errorStrategy \"finish\""
        ],
        "when": "",
        "stub": ""
    },
    "RaxmlTreeNG": {
        "name_process": "RaxmlTreeNG",
        "string_process": "\nprocess RaxmlTreeNG {\n    container = \"${container__raxmlng}\"\n    label = 'mem_veryhigh'\n    errorStrategy = 'finish'\n\n    input:\n        path recruits_aln_fasta_f\n    \n    output:\n        path \"refpkg.raxml.bestTree\", emit: tree\n        path \"refpkg.raxml.log\", emit: log\n        path \"refpkg.raxml.bestModel\", emit: model\n    \n    \"\"\"\n    raxml-ng \\\n    --parse \\\n    --model ${params.raxmlng_model} \\\n    --msa ${recruits_aln_fasta_f} \\\n    --seed ${params.raxmlng_seed}\n\n    raxml-ng \\\n    --prefix refpkg \\\n    --model ${params.raxmlng_model} \\\n    --msa ${recruits_aln_fasta_f}.raxml.rba \\\n    --tree pars{${params.raxmlng_parsimony_trees}},rand{${params.raxmlng_random_trees}} \\\n    --bs-cutoff ${params.raxmlng_bootstrap_cutoff} \\\n    --seed ${params.raxmlng_seed} \\\n    --threads ${task.cpus}\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "\"\"\"\n    raxml-ng \\\n    --parse \\\n    --model ${params.raxmlng_model} \\\n    --msa ${recruits_aln_fasta_f} \\\n    --seed ${params.raxmlng_seed}\n\n    raxml-ng \\\n    --prefix refpkg \\\n    --model ${params.raxmlng_model} \\\n    --msa ${recruits_aln_fasta_f}.raxml.rba \\\n    --tree pars{${params.raxmlng_parsimony_trees}},rand{${params.raxmlng_random_trees}} \\\n    --bs-cutoff ${params.raxmlng_bootstrap_cutoff} \\\n    --seed ${params.raxmlng_seed} \\\n    --threads ${task.cpus}\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "RAxML-NG"
        ],
        "tools_url": [
            "https://bio.tools/RAxML-NG"
        ],
        "tools_dico": [
            {
                "name": "RAxML-NG",
                "uri": "https://bio.tools/RAxML-NG",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3810",
                            "term": "Agricultural science"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0194",
                            "term": "Phylogenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3293",
                            "term": "Phylogenetics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0547",
                                    "term": "Phylogenetic inference (maximum likelihood and Bayesian methods)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0552",
                                    "term": "Phylogenetic tree bootstrapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0547",
                                    "term": "Phylogenetic tree construction (maximum likelihood and Bayesian methods)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0547",
                                    "term": "Phylogenetic tree generation (maximum likelihood and Bayesian methods)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Phylogenetic tree inference tool which uses maximum-likelihood (ML) optimality criterion.",
                "homepage": "https://raxml-ng.vital-it.ch/"
            }
        ],
        "inputs": [
            "recruits_aln_fasta_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__raxmlng}\"",
            "label = 'mem_veryhigh'",
            "errorStrategy = 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "RaxmlTree": {
        "name_process": "RaxmlTree",
        "string_process": "\nprocess RaxmlTree {\n    container = \"${container__raxml}\"\n    label = 'mem_veryhigh'\n    errorStrategy = 'retry'\n\n    input:\n        file recruits_aln_fasta_f\n    \n    output:\n        file \"RAxML_bestTree.refpkg\"\n        file \"RAxML_info.refpkg\"\n    \n    \"\"\"\n    raxmlHPC-PTHREADS-AVX2 \\\n    -n refpkg \\\n    -m ${params.raxml_model} \\\n    -s ${recruits_aln_fasta_f} \\\n    -p ${params.raxml_parsiomony_seed} \\\n    -T ${task.cpus}\n    \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "\"\"\"\n    raxmlHPC-PTHREADS-AVX2 \\\n    -n refpkg \\\n    -m ${params.raxml_model} \\\n    -s ${recruits_aln_fasta_f} \\\n    -p ${params.raxml_parsiomony_seed} \\\n    -T ${task.cpus}\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "recruits_aln_fasta_f"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__raxml}\"",
            "label = 'mem_veryhigh'",
            "errorStrategy = 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "RaxmlTree_cleanupInfo": {
        "name_process": "RaxmlTree_cleanupInfo",
        "string_process": "\nprocess RaxmlTree_cleanupInfo {\n    container = \"${container__fastatools}\"\n    label = 'io_limited'\n    errorStrategy = 'retry'\n\n    input:\n        file \"RAxML_info.unclean.refpkg\"\n    \n    output:\n        file \"RAxML_info.refpkg\"\n\n\n\"\"\"\n#!/usr/bin/env python\nwith open(\"RAxML_info.refpkg\",'wt') as out_h:\n    with open(\"RAxML_info.unclean.refpkg\", 'rt') as in_h:\n        past_cruft = False\n        for l in in_h:\n            if \"This is RAxML version\" == l[0:21]:\n                past_cruft = True\n            if past_cruft:\n                out_h.write(l)\n\"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "\"\"\"\n#!/usr/bin/env python\nwith open(\"RAxML_info.refpkg\",'wt') as out_h:\n    with open(\"RAxML_info.unclean.refpkg\", 'rt') as in_h:\n        past_cruft = False\n        for l in in_h:\n            if \"This is RAxML version\" == l[0:21]:\n                past_cruft = True\n            if past_cruft:\n                out_h.write(l)\n\"\"\"",
        "nb_lignes_script": 10,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__fastatools}\"",
            "label = 'io_limited'",
            "errorStrategy = 'retry'"
        ],
        "when": "",
        "stub": ""
    },
    "TaxtableForSI": {
        "name_process": "TaxtableForSI",
        "string_process": "\nprocess TaxtableForSI {\n    container = \"${container__taxtastic}\"\n    label = 'io_limited'\n    errorStrategy = 'finish'\n\n    input:\n        file taxonomy_db_f \n        file refpkg_si_corr_f\n    output:\n        file \"refpkg.taxtable.csv\"\n\n    \"\"\"\n    taxit taxtable ${taxonomy_db_f} \\\n    --seq-info ${refpkg_si_corr_f} \\\n    --outfile refpkg.taxtable.csv\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "\"\"\"\n    taxit taxtable ${taxonomy_db_f} \\\n    --seq-info ${refpkg_si_corr_f} \\\n    --outfile refpkg.taxtable.csv\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "TaxIt"
        ],
        "tools_url": [
            "https://bio.tools/TaxIt"
        ],
        "tools_dico": [
            {
                "name": "TaxIt",
                "uri": "https://bio.tools/TaxIt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3520",
                            "term": "Proteomics experiment"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3649",
                                    "term": "Target-Decoy"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An iterative and automated computational pipeline for untargeted strain-level identification using MS/MS spectra from pathogenic samples.",
                "homepage": "https://gitlab.com/rki_bioinformatics/TaxIt"
            }
        ],
        "inputs": [
            "taxonomy_db_f",
            "refpkg_si_corr_f"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__taxtastic}\"",
            "label = 'io_limited'",
            "errorStrategy = 'finish'"
        ],
        "when": "",
        "stub": ""
    },
    "ObtainCM": {
        "name_process": "ObtainCM",
        "string_process": "\nprocess ObtainCM {\n    container = \"${container__infernal}\"\n    label = 'io_net'\n\n    output:\n        file \"SSU_rRNA_bacteria.cm\"\n    \n    \"\"\"\n    wget http://rfam.xfam.org/family/RF00177/cm -O SSU_rRNA_bacteria.cm \n    \"\"\"\n}",
        "nb_lignes_process": 10,
        "string_script": "\"\"\"\n    wget http://rfam.xfam.org/family/RF00177/cm -O SSU_rRNA_bacteria.cm \n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__infernal}\"",
            "label = 'io_net'"
        ],
        "when": "",
        "stub": ""
    },
    "CombineRefpkg_ng": {
        "name_process": "CombineRefpkg_ng",
        "string_process": "\nprocess CombineRefpkg_ng {\n    container = \"${container__taxtastic}\"\n    label = 'io_mem'\n\n    afterScript(\"rm -rf refpkg/*\")\n    publishDir \"${params.output}/refpkg/\", mode: 'copy'\n\n    input:\n        path recruits_aln_fasta_f\n        path recruits_aln_sto_f\n        path refpkg_tree_f \n        path refpkg_tree_stats_clean_f \n        path refpkg_tt_f\n        path refpkg_si_corr_f\n        path refpkg_cm\n        path raxmlng_model\n    \n    output:\n        path \"refpkg.tar.gz\"\n    \n\"\"\"\ntaxit create --locus 16S \\\n--package-name refpkg \\\n--clobber \\\n--aln-fasta ${recruits_aln_fasta_f} \\\n--aln-sto ${recruits_aln_sto_f} \\\n--tree-file ${refpkg_tree_f} \\\n--tree-stats ${refpkg_tree_stats_clean_f} \\\n--taxonomy ${refpkg_tt_f} \\\n--seq-info ${refpkg_si_corr_f} \\\n--profile ${refpkg_cm}\n\ncp ${raxmlng_model} refpkg/raxmlng.model.raw\npython << ENDPYTHON\nimport json\nimport hashlib\n\nmodel_str = open('refpkg/raxmlng.model.raw', 'rt').read()\nmodel = model_str.split(',')[0]\nwith open('refpkg/raxmlng.model', 'wt') as out_h:\n    out_h.write(model)\n\nmodelhash = hashlib.md5(model.encode('utf-8')).hexdigest()\ncontents = json.load(\n    open('refpkg/CONTENTS.json', 'rt')\n)\ncontents['files']['raxml_ng_model'] = 'raxmlng.model'\ncontents['md5']['raxml_ng_model'] = modelhash\n\njson.dump(\n    contents,\n    open('refpkg/CONTENTS.json', 'wt')\n)\nENDPYTHON\ntar cvf refpkg.tar  -C refpkg/ .\ngzip refpkg.tar\n\"\"\"\n}",
        "nb_lignes_process": 57,
        "string_script": "\"\"\"\ntaxit create --locus 16S \\\n--package-name refpkg \\\n--clobber \\\n--aln-fasta ${recruits_aln_fasta_f} \\\n--aln-sto ${recruits_aln_sto_f} \\\n--tree-file ${refpkg_tree_f} \\\n--tree-stats ${refpkg_tree_stats_clean_f} \\\n--taxonomy ${refpkg_tt_f} \\\n--seq-info ${refpkg_si_corr_f} \\\n--profile ${refpkg_cm}\n\ncp ${raxmlng_model} refpkg/raxmlng.model.raw\npython << ENDPYTHON\nimport json\nimport hashlib\n\nmodel_str = open('refpkg/raxmlng.model.raw', 'rt').read()\nmodel = model_str.split(',')[0]\nwith open('refpkg/raxmlng.model', 'wt') as out_h:\n    out_h.write(model)\n\nmodelhash = hashlib.md5(model.encode('utf-8')).hexdigest()\ncontents = json.load(\n    open('refpkg/CONTENTS.json', 'rt')\n)\ncontents['files']['raxml_ng_model'] = 'raxmlng.model'\ncontents['md5']['raxml_ng_model'] = modelhash\n\njson.dump(\n    contents,\n    open('refpkg/CONTENTS.json', 'wt')\n)\nENDPYTHON\ntar cvf refpkg.tar  -C refpkg/ .\ngzip refpkg.tar\n\"\"\"",
        "nb_lignes_script": 36,
        "language_script": "bash",
        "tools": [
            "TaxIt",
            "tximport",
            "MoDEL"
        ],
        "tools_url": [
            "https://bio.tools/TaxIt",
            "https://bio.tools/tximport",
            "https://bio.tools/model"
        ],
        "tools_dico": [
            {
                "name": "TaxIt",
                "uri": "https://bio.tools/TaxIt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3520",
                            "term": "Proteomics experiment"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3649",
                                    "term": "Target-Decoy"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An iterative and automated computational pipeline for untargeted strain-level identification using MS/MS spectra from pathogenic samples.",
                "homepage": "https://gitlab.com/rki_bioinformatics/TaxIt"
            },
            {
                "name": "tximport",
                "uri": "https://bio.tools/tximport",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "Gene transcripts"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Workflows"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "mRNA features"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0769",
                            "term": "Pipelines"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3680",
                                    "term": "RNA-Seq analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2497",
                                    "term": "Pathway or network analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An R/Bioconductor package that imports transcript-level abundance, estimated counts and transcript lengths, and summarizes into matrices for use with downstream gene-level analysis packages.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/tximport.html"
            },
            {
                "name": "MoDEL",
                "uri": "https://bio.tools/model",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0176",
                            "term": "Molecular dynamics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2426",
                                    "term": "Modelling and simulation"
                                }
                            ],
                            []
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0842",
                                "term": "Identifier"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2080",
                                "term": "Database search results"
                            }
                        ]
                    }
                ],
                "description": "Database of protein Molecular Dynamics simulations, with 1800 trajectories representing different structural clusters of the PDB.",
                "homepage": "http://mmb.irbbarcelona.org/MoDEL"
            }
        ],
        "inputs": [
            "recruits_aln_fasta_f",
            "recruits_aln_sto_f",
            "refpkg_tree_f",
            "refpkg_tree_stats_clean_f",
            "refpkg_tt_f",
            "refpkg_si_corr_f",
            "refpkg_cm",
            "raxmlng_model"
        ],
        "nb_inputs": 8,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__taxtastic}\"",
            "label = 'io_mem'",
            "afterScript(\"rm -rf refpkg/*\")",
            "publishDir \"${params.output}/refpkg/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "CombineRefpkg_og": {
        "name_process": "CombineRefpkg_og",
        "string_process": "\nprocess CombineRefpkg_og {\n    container = \"${container__pplacer}\"\n    label = 'io_mem'\n\n    afterScript(\"rm -rf refpkg/*\")\n    publishDir \"${params.output}/refpkg/\", mode: 'copy'\n\n    input:\n        file recruits_aln_fasta_f\n        file recruits_aln_sto_f\n        file refpkg_tree_f \n        file refpkg_tree_stats_clean_f \n        file refpkg_tt_f\n        file refpkg_si_corr_f\n        file refpkg_cm\n    \n    output:\n        file \"refpkg.tar.gz\"\n    \n    \"\"\"\n    taxit create --locus 16S \\\n    --package-name refpkg \\\n    --clobber \\\n    --aln-fasta ${recruits_aln_fasta_f} \\\n    --aln-sto ${recruits_aln_sto_f} \\\n    --tree-file ${refpkg_tree_f} \\\n    --tree-stats ${refpkg_tree_stats_clean_f} \\\n    --taxonomy ${refpkg_tt_f} \\\n    --seq-info ${refpkg_si_corr_f} \\\n    --profile ${refpkg_cm} && \\\n    ls -l refpkg/ && \\\n    tar czvf refpkg.tar.gz  -C refpkg/ .\n    \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "\"\"\"\n    taxit create --locus 16S \\\n    --package-name refpkg \\\n    --clobber \\\n    --aln-fasta ${recruits_aln_fasta_f} \\\n    --aln-sto ${recruits_aln_sto_f} \\\n    --tree-file ${refpkg_tree_f} \\\n    --tree-stats ${refpkg_tree_stats_clean_f} \\\n    --taxonomy ${refpkg_tt_f} \\\n    --seq-info ${refpkg_si_corr_f} \\\n    --profile ${refpkg_cm} && \\\n    ls -l refpkg/ && \\\n    tar czvf refpkg.tar.gz  -C refpkg/ .\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "TaxIt"
        ],
        "tools_url": [
            "https://bio.tools/TaxIt"
        ],
        "tools_dico": [
            {
                "name": "TaxIt",
                "uri": "https://bio.tools/TaxIt",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3520",
                            "term": "Proteomics experiment"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3649",
                                    "term": "Target-Decoy"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "An iterative and automated computational pipeline for untargeted strain-level identification using MS/MS spectra from pathogenic samples.",
                "homepage": "https://gitlab.com/rki_bioinformatics/TaxIt"
            }
        ],
        "inputs": [
            "recruits_aln_fasta_f",
            "recruits_aln_sto_f",
            "refpkg_tree_f",
            "refpkg_tree_stats_clean_f",
            "refpkg_tt_f",
            "refpkg_si_corr_f",
            "refpkg_cm"
        ],
        "nb_inputs": 7,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__pplacer}\"",
            "label = 'io_mem'",
            "afterScript(\"rm -rf refpkg/*\")",
            "publishDir \"${params.output}/refpkg/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "AddRAxMLModel": {
        "name_process": "AddRAxMLModel",
        "string_process": "\nprocess AddRAxMLModel {\n    container = \"${container__taxtastic}\"\n    label = 'io_limited'\n\n    input:\n        path \"refpkg.tgz\"\n        path raxml_ng_model\n    output:\n        path 'refpkg.tar.gz'\n\"\"\"\n#!/usr/bin/env python\n\nimport tarfile\nimport json\nimport os\n\ntar_h = tarfile.open('refpkg.tgz')\ntar_contents_dict = {os.path.basename(f.name): f for f in tar_h.getmembers()}\ncontents = json.loads(\n    tar_h.extractfile(\n        tar_contents_dict['CONTENTS.json']\n    ).read().decode('utf-8')\n)\n\n\"\"\"\n\n}",
        "nb_lignes_process": 26,
        "string_script": "\"\"\"\n#!/usr/bin/env python\n\nimport tarfile\nimport json\nimport os\n\ntar_h = tarfile.open('refpkg.tgz')\ntar_contents_dict = {os.path.basename(f.name): f for f in tar_h.getmembers()}\ncontents = json.loads(\n    tar_h.extractfile(\n        tar_contents_dict['CONTENTS.json']\n    ).read().decode('utf-8')\n)\n\n\"\"\"",
        "nb_lignes_script": 15,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "raxml_ng_model"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "Channing-Zeng__metagenomics-nf",
        "directive": [
            "container = \"${container__taxtastic}\"",
            "label = 'io_limited'"
        ],
        "when": "",
        "stub": ""
    }
}