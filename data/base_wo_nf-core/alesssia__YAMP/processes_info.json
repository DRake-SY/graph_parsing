{
    "get_software_versions": {
        "name_process": "get_software_versions",
        "string_process": "\nprocess get_software_versions {\n\n\t                                                                             \n\t                                                                             \n    if (workflow.containerEngine == 'singularity') {\n        container params.singularity_container_biobakery\n    } else {\n        container params.docker_container_biobakery\n    }\n\n\toutput:\n\tfile \"software_versions_mqc.yaml\" into software_versions_yaml\n\n\tscript:\n\t                                                                            \n\t                                                                                    \n\t                                                                              \n\t                                                                                \n\t                                                                     \n\t\"\"\"\n\techo $workflow.manifest.version > v_pipeline.txt\n\techo $workflow.nextflow.version > v_nextflow.txt\n\n\techo $params.docker_container_fastqc | cut -d: -f 2 > v_fastqc.txt\n\techo $params.docker_container_bbmap | cut -d: -f 2 > v_bbmap.txt\n\t\n\tmetaphlan --version > v_metaphlan.txt\n\thumann --version > v_humann.txt\n\techo $params.docker_container_qiime2 | cut -d: -f 2 > v_qiime.txt\n\t\n\techo $params.docker_container_multiqc | cut -d: -f 2 > v_multiqc.txt\n\t\n\tscrape_software_versions.py > software_versions_mqc.yaml\n\t\"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "\t\"\"\"\n\techo $workflow.manifest.version > v_pipeline.txt\n\techo $workflow.nextflow.version > v_nextflow.txt\n\n\techo $params.docker_container_fastqc | cut -d: -f 2 > v_fastqc.txt\n\techo $params.docker_container_bbmap | cut -d: -f 2 > v_bbmap.txt\n\t\n\tmetaphlan --version > v_metaphlan.txt\n\thumann --version > v_humann.txt\n\techo $params.docker_container_qiime2 | cut -d: -f 2 > v_qiime.txt\n\t\n\techo $params.docker_container_multiqc | cut -d: -f 2 > v_multiqc.txt\n\t\n\tscrape_software_versions.py > software_versions_mqc.yaml\n\t\"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [
            "MetaPhlAn"
        ],
        "tools_url": [
            "https://bio.tools/metaphlan"
        ],
        "tools_dico": [
            {
                "name": "MetaPhlAn",
                "uri": "https://bio.tools/metaphlan",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0194",
                            "term": "Phylogenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0324",
                                    "term": "Phylogenetic analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Nucleic acid sequence analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0324",
                                    "term": "Phylogenetic tree analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Sequence analysis (nucleic acid)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Computational tool for profiling the composition of microbial communities from metagenomic shotgun sequencing data.",
                "homepage": "http://huttenhower.sph.harvard.edu/metaphlan"
            }
        ],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [
            "software_versions_yaml"
        ],
        "nb_outputs": 1,
        "name_workflow": "alesssia__YAMP",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "dedup": {
        "name_process": "dedup",
        "string_process": "\nprocess dedup {\n\t\n    tag \"$name\"\n    \n\t                                \n    if (workflow.containerEngine == 'singularity') {\n        container params.singularity_container_bbmap\n    } else {\n        container params.docker_container_bbmap\n    }\n\t\t\n\tinput:\n\ttuple val(name), file(reads) from read_files_dedup\n\n\toutput:\n\ttuple val(name), path(\"${name}_dedup*.fq.gz\") into to_synthetic_contaminants\n\tfile \"dedup_mqc.yaml\" into dedup_log\n\t\n\twhen:\n\tparams.mode != \"characterisation\" && params.dedup\n\n\tscript:\n\t                                                   \n\tdef input = params.singleEnd ? \"in=\\\"${reads[0]}\\\"\" :  \"in1=\\\"${reads[0]}\\\" in2=\\\"${reads[1]}\\\"\"\n\tdef output = params.singleEnd ? \"out=\\\"${name}_dedup.fq.gz\\\"\" :  \"out1=\\\"${name}_dedup_R1.fq.gz\\\" out2=\\\"${name}_dedup_R2.fq.gz\\\"\"\n\t\n\t\"\"\"\n\t#Sets the maximum memory to the value requested in the config file\n\tmaxmem=\\$(echo \\\"$task.memory\\\" | sed 's/ //g' | sed 's/B//g')\n\techo \\\"$reads\\\"\n    clumpify.sh -Xmx\\\"\\$maxmem\\\" $input $output qin=$params.qin dedupe subs=0 threads=${task.cpus} &> dedup_mqc.txt\n\t\n\t# MultiQC doesn't have a module for clumpify yet. As a consequence, I\n\t# had to create a YAML file with all the info I need via a bash script\n\tbash scrape_dedup_log.sh > dedup_mqc.yaml\n\t\"\"\"\n}",
        "nb_lignes_process": 36,
        "string_script": "\tdef input = params.singleEnd ? \"in=\\\"${reads[0]}\\\"\" :  \"in1=\\\"${reads[0]}\\\" in2=\\\"${reads[1]}\\\"\"\n\tdef output = params.singleEnd ? \"out=\\\"${name}_dedup.fq.gz\\\"\" :  \"out1=\\\"${name}_dedup_R1.fq.gz\\\" out2=\\\"${name}_dedup_R2.fq.gz\\\"\"\n\t\n\t\"\"\"\n\t#Sets the maximum memory to the value requested in the config file\n\tmaxmem=\\$(echo \\\"$task.memory\\\" | sed 's/ //g' | sed 's/B//g')\n\techo \\\"$reads\\\"\n    clumpify.sh -Xmx\\\"\\$maxmem\\\" $input $output qin=$params.qin dedupe subs=0 threads=${task.cpus} &> dedup_mqc.txt\n\t\n\t# MultiQC doesn't have a module for clumpify yet. As a consequence, I\n\t# had to create a YAML file with all the info I need via a bash script\n\tbash scrape_dedup_log.sh > dedup_mqc.yaml\n\t\"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "read_files_dedup"
        ],
        "nb_inputs": 1,
        "outputs": [
            "to_synthetic_contaminants",
            "dedup_log"
        ],
        "nb_outputs": 2,
        "name_workflow": "alesssia__YAMP",
        "directive": [
            "tag \"$name\" if (workflow.containerEngine == 'singularity') { container params.singularity_container_bbmap } else { container params.docker_container_bbmap }"
        ],
        "when": "params.mode != \"characterisation\" && params.dedup",
        "stub": ""
    },
    "remove_synthetic_contaminants": {
        "name_process": "remove_synthetic_contaminants",
        "string_process": "\nprocess remove_synthetic_contaminants {\n\t\n\ttag \"$name\"\n\t\n\t                                \n    if (workflow.containerEngine == 'singularity') {\n        container params.singularity_container_bbmap\n    } else {\n        container params.docker_container_bbmap\n    }\n\n\tinput:\n\ttuple file(artefacts), file(phix174ill), val(name), file(reads) from artefacts.combine(phix174ill).combine(to_synthetic_contaminants)\n   \n\toutput:\n\ttuple val(name), path(\"${name}_no_synthetic_contaminants*.fq.gz\") into to_trim\n\tfile \"synthetic_contaminants_mqc.yaml\" into synthetic_contaminants_log\n\t\n\twhen:\n\tparams.mode != \"characterisation\"\n\n   \tscript:\n\tdef input = params.singleEnd ? \"in=\\\"${reads[0]}\\\"\" :  \"in1=\\\"${reads[0]}\\\" in2=\\\"${reads[1]}\\\"\"\n\tdef output = params.singleEnd ? \"out=\\\"${name}_no_synthetic_contaminants.fq.gz\\\"\" :  \"out=\\\"${name}_no_synthetic_contaminants_R1.fq.gz\\\" out2=\\\"${name}_no_synthetic_contaminants_R2.fq.gz\\\"\"\n\t\"\"\"\n\t#Sets the maximum memory to the value requested in the config file\n\tmaxmem=\\$(echo ${task.memory} | sed 's/ //g' | sed 's/B//g')\n\tbbduk.sh -Xmx\\\"\\$maxmem\\\" $input $output k=31 ref=$phix174ill,$artefacts qin=$params.qin threads=${task.cpus} ow &> synthetic_contaminants_mqc.txt\n\t\n\t# MultiQC doesn't have a module for bbduk yet. As a consequence, I\n\t# had to create a YAML file with all the info I need via a bash script\n\tbash scrape_remove_synthetic_contaminants_log.sh > synthetic_contaminants_mqc.yaml\n\t\"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "\tdef input = params.singleEnd ? \"in=\\\"${reads[0]}\\\"\" :  \"in1=\\\"${reads[0]}\\\" in2=\\\"${reads[1]}\\\"\"\n\tdef output = params.singleEnd ? \"out=\\\"${name}_no_synthetic_contaminants.fq.gz\\\"\" :  \"out=\\\"${name}_no_synthetic_contaminants_R1.fq.gz\\\" out2=\\\"${name}_no_synthetic_contaminants_R2.fq.gz\\\"\"\n\t\"\"\"\n\t#Sets the maximum memory to the value requested in the config file\n\tmaxmem=\\$(echo ${task.memory} | sed 's/ //g' | sed 's/B//g')\n\tbbduk.sh -Xmx\\\"\\$maxmem\\\" $input $output k=31 ref=$phix174ill,$artefacts qin=$params.qin threads=${task.cpus} ow &> synthetic_contaminants_mqc.txt\n\t\n\t# MultiQC doesn't have a module for bbduk yet. As a consequence, I\n\t# had to create a YAML file with all the info I need via a bash script\n\tbash scrape_remove_synthetic_contaminants_log.sh > synthetic_contaminants_mqc.yaml\n\t\"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "artefacts",
            "phix174ill",
            "to_synthetic_contaminants"
        ],
        "nb_inputs": 3,
        "outputs": [
            "to_trim",
            "synthetic_contaminants_log"
        ],
        "nb_outputs": 2,
        "name_workflow": "alesssia__YAMP",
        "directive": [
            "tag \"$name\" if (workflow.containerEngine == 'singularity') { container params.singularity_container_bbmap } else { container params.docker_container_bbmap }"
        ],
        "when": "params.mode != \"characterisation\"",
        "stub": ""
    },
    "trim": {
        "name_process": "trim",
        "string_process": "\nprocess trim {\n\n\ttag \"$name\"\n\t\n\t                                \n    if (workflow.containerEngine == 'singularity') {\n        container params.singularity_container_bbmap\n    } else {\n        container params.docker_container_bbmap\n    }\n\t\n\tinput:\n\ttuple file(adapters), val(name), file(reads) from adapters.combine(to_trim) \n\t\n\toutput:\n\ttuple val(name), path(\"${name}_trimmed*.fq.gz\") into to_decontaminate\n\tfile \"trimming_mqc.yaml\" into trimming_log\n\t\n\twhen:\n\tparams.mode != \"characterisation\"\n\n   \tscript:\n\tdef input = params.singleEnd ? \"in=\\\"${reads[0]}\\\"\" :  \"in1=\\\"${reads[0]}\\\" in2=\\\"${reads[1]}\\\"\"\n\tdef output = params.singleEnd ? \"out=\\\"${name}_trimmed.fq.gz\\\"\" :  \"out=\\\"${name}_trimmed_R1.fq.gz\\\" out2=\\\"${name}_trimmed_R2.fq.gz\\\" outs=\\\"${name}_trimmed_singletons.fq.gz\\\"\"\n\t\"\"\"\n\t#Sets the maximum memory to the value requested in the config file\n\tmaxmem=\\$(echo ${task.memory} | sed 's/ //g' | sed 's/B//g')\n\n\tbbduk.sh -Xmx\\\"\\$maxmem\\\" $input $output ktrim=r k=$params.kcontaminants mink=$params.mink hdist=$params.hdist qtrim=rl trimq=$params.phred  minlength=$params.minlength ref=$adapters qin=$params.qin threads=${task.cpus} tbo tpe ow &> trimming_mqc.txt\n\n\t# MultiQC doesn't have a module for bbduk yet. As a consequence, I\n\t# had to create a YAML file with all the info I need via a bash script\n\tbash scrape_trimming_log.sh > trimming_mqc.yaml\n\t\"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "\tdef input = params.singleEnd ? \"in=\\\"${reads[0]}\\\"\" :  \"in1=\\\"${reads[0]}\\\" in2=\\\"${reads[1]}\\\"\"\n\tdef output = params.singleEnd ? \"out=\\\"${name}_trimmed.fq.gz\\\"\" :  \"out=\\\"${name}_trimmed_R1.fq.gz\\\" out2=\\\"${name}_trimmed_R2.fq.gz\\\" outs=\\\"${name}_trimmed_singletons.fq.gz\\\"\"\n\t\"\"\"\n\t#Sets the maximum memory to the value requested in the config file\n\tmaxmem=\\$(echo ${task.memory} | sed 's/ //g' | sed 's/B//g')\n\n\tbbduk.sh -Xmx\\\"\\$maxmem\\\" $input $output ktrim=r k=$params.kcontaminants mink=$params.mink hdist=$params.hdist qtrim=rl trimq=$params.phred  minlength=$params.minlength ref=$adapters qin=$params.qin threads=${task.cpus} tbo tpe ow &> trimming_mqc.txt\n\n\t# MultiQC doesn't have a module for bbduk yet. As a consequence, I\n\t# had to create a YAML file with all the info I need via a bash script\n\tbash scrape_trimming_log.sh > trimming_mqc.yaml\n\t\"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "adapters",
            "to_trim"
        ],
        "nb_inputs": 2,
        "outputs": [
            "to_decontaminate",
            "trimming_log"
        ],
        "nb_outputs": 2,
        "name_workflow": "alesssia__YAMP",
        "directive": [
            "tag \"$name\" if (workflow.containerEngine == 'singularity') { container params.singularity_container_bbmap } else { container params.docker_container_bbmap }"
        ],
        "when": "params.mode != \"characterisation\"",
        "stub": ""
    },
    "index_foreign_genome": {
        "name_process": "index_foreign_genome",
        "string_process": "\nprocess index_foreign_genome {\n\n\t                                \n    if (workflow.containerEngine == 'singularity') {\n        container params.singularity_container_bbmap\n    } else {\n        container params.docker_container_bbmap\n    }\n\n\tinput:\n\tfile(foreign_genome) from foreign_genome \n\n\toutput:\n\tpath(\"ref/\", type: 'dir') into ref_foreign_genome\n\t\n\twhen:\n\tparams.mode != \"characterisation\" && params.foreign_genome_ref == \"\"\n\n\tscript:\n\t\"\"\"\n\t#Sets the maximum memory to the value requested in the config file\n\tmaxmem=\\$(echo ${task.memory} | sed 's/ //g' | sed 's/B//g')\n\n\t# This step will have a boilerplate log because the information saved by bbmap are not relevant\n\tbbmap.sh -Xmx\\\"\\$maxmem\\\" ref=$foreign_genome &> foreign_genome_index_mqc.txt\t\n\t\"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "\t\"\"\"\n\t#Sets the maximum memory to the value requested in the config file\n\tmaxmem=\\$(echo ${task.memory} | sed 's/ //g' | sed 's/B//g')\n\n\t# This step will have a boilerplate log because the information saved by bbmap are not relevant\n\tbbmap.sh -Xmx\\\"\\$maxmem\\\" ref=$foreign_genome &> foreign_genome_index_mqc.txt\t\n\t\"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "foreign_genome"
        ],
        "nb_inputs": 1,
        "outputs": [
            "ref_foreign_genome"
        ],
        "nb_outputs": 1,
        "name_workflow": "alesssia__YAMP",
        "directive": [],
        "when": "params.mode != \"characterisation\" && params.foreign_genome_ref == \"\"",
        "stub": ""
    },
    "decontaminate": {
        "name_process": "decontaminate",
        "string_process": "\nprocess decontaminate {\n\n    tag \"$name\"\n\n\t                                \n    if (workflow.containerEngine == 'singularity') {\n        container params.singularity_container_bbmap\n    } else {\n        container params.docker_container_bbmap\n    }\n\n\tpublishDir \"${params.outdir}/${params.prefix}\", mode: 'copy', pattern: \"*QCd.fq.gz\"\n\n\tinput:\n\ttuple path(ref_foreign_genome), val(name), file(reads) from ref_foreign_genome.combine(to_decontaminate)\n\n\toutput:\n\ttuple val(name), path(\"*_QCd.fq.gz\") into qcd_reads\n\ttuple val(name), path(\"*_QCd.fq.gz\") into to_profile_taxa_decontaminated\n\ttuple val(name), path(\"*_QCd.fq.gz\") into to_profile_functions_decontaminated\n\tfile \"decontamination_mqc.yaml\" into decontaminate_log\n\n\twhen:\n\tparams.mode != \"characterisation\"\n\n\tscript:\n\t                                                                                        \n\t                                                                                     \n\t                                                                              \n\tdef input = params.singleEnd ? \"in=\\\"${reads[0]}\\\"\" :  \"in1=\\\"${reads[0]}\\\",\\\"${reads[2]}\\\" in2=\\\"${reads[1]}\\\",null\"\n\tdef output = \"outu=\\\"${name}_QCd.fq.gz\\\" outm=\\\"${name}_contamination.fq.gz\\\"\"\n\t\"\"\"\n\t#Sets the maximum memory to the value requested in the config file\n\tmaxmem=\\$(echo ${task.memory} | sed 's/ //g' | sed 's/B//g')\n\n\tbbwrap.sh -Xmx\\\"\\$maxmem\\\"  mapper=bbmap append=t $input $output minid=$params.mind maxindel=$params.maxindel bwr=$params.bwr bw=12 minhits=2 qtrim=rl trimq=$params.phred path=\"./\" qin=$params.qin threads=${task.cpus} untrim quickmatch fast ow &> decontamination_mqc.txt\n\n\t# MultiQC doesn't have a module for bbwrap yet. As a consequence, I\n\t# had to create a YAML file with all the info I need via a bash script\n\tbash scrape_decontamination_log.sh > decontamination_mqc.yaml\n\t\"\"\"\n}",
        "nb_lignes_process": 41,
        "string_script": "\tdef input = params.singleEnd ? \"in=\\\"${reads[0]}\\\"\" :  \"in1=\\\"${reads[0]}\\\",\\\"${reads[2]}\\\" in2=\\\"${reads[1]}\\\",null\"\n\tdef output = \"outu=\\\"${name}_QCd.fq.gz\\\" outm=\\\"${name}_contamination.fq.gz\\\"\"\n\t\"\"\"\n\t#Sets the maximum memory to the value requested in the config file\n\tmaxmem=\\$(echo ${task.memory} | sed 's/ //g' | sed 's/B//g')\n\n\tbbwrap.sh -Xmx\\\"\\$maxmem\\\"  mapper=bbmap append=t $input $output minid=$params.mind maxindel=$params.maxindel bwr=$params.bwr bw=12 minhits=2 qtrim=rl trimq=$params.phred path=\"./\" qin=$params.qin threads=${task.cpus} untrim quickmatch fast ow &> decontamination_mqc.txt\n\n\t# MultiQC doesn't have a module for bbwrap yet. As a consequence, I\n\t# had to create a YAML file with all the info I need via a bash script\n\tbash scrape_decontamination_log.sh > decontamination_mqc.yaml\n\t\"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ref_foreign_genome",
            "to_decontaminate"
        ],
        "nb_inputs": 2,
        "outputs": [
            "qcd_reads",
            "to_profile_taxa_decontaminated",
            "to_profile_functions_decontaminated",
            "decontaminate_log"
        ],
        "nb_outputs": 4,
        "name_workflow": "alesssia__YAMP",
        "directive": [
            "tag \"$name\" if (workflow.containerEngine == 'singularity') { container params.singularity_container_bbmap } else { container params.docker_container_bbmap }",
            "publishDir \"${params.outdir}/${params.prefix}\", mode: 'copy', pattern: \"*QCd.fq.gz\""
        ],
        "when": "params.mode != \"characterisation\"",
        "stub": ""
    },
    "quality_assessment": {
        "name_process": "quality_assessment",
        "string_process": "\nprocess quality_assessment {\n\t\n    tag \"$name\"\n\t\n\t                                \n    if (workflow.containerEngine == 'singularity') {\n        container params.singularity_container_fastqc\n    } else {\n        container params.docker_container_fastqc\n    }\n\t\n\tpublishDir \"${params.outdir}/${params.prefix}/fastqc\", mode: 'copy'    \n\n    input:\n    set val(name), file(reads) from read_files_fastqc.mix(qcd_reads)\n\n    output:\n    path \"*_fastqc.{zip,html}\" into fastqc_log\n\t\n\twhen:\n\tparams.mode != \"characterisation\"\n\n    script:\n    \"\"\"\n    fastqc -q $reads\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "    \"\"\"\n    fastqc -q $reads\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "FastQC"
        ],
        "tools_url": [
            "https://bio.tools/fastqc"
        ],
        "tools_dico": [
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            }
        ],
        "inputs": [
            "read_files_fastqc",
            "qcd_reads"
        ],
        "nb_inputs": 2,
        "outputs": [
            "fastqc_log"
        ],
        "nb_outputs": 1,
        "name_workflow": "alesssia__YAMP",
        "directive": [
            "tag \"$name\" if (workflow.containerEngine == 'singularity') { container params.singularity_container_fastqc } else { container params.docker_container_fastqc }",
            "publishDir \"${params.outdir}/${params.prefix}/fastqc\", mode: 'copy'"
        ],
        "when": "params.mode != \"characterisation\"",
        "stub": ""
    },
    "merge_paired_end_cleaned": {
        "name_process": "merge_paired_end_cleaned",
        "string_process": "\nprocess merge_paired_end_cleaned {\n\n\ttag \"$name\"\n\t\t\n\tinput:\n\ttuple val(name), file(reads) from reads_merge_paired_end_cleaned\n\t\n\toutput:\n\ttuple val(name), path(\"*_QCd.fq.gz\") into to_profile_taxa_merged\n\ttuple val(name), path(\"*_QCd.fq.gz\") into to_profile_functions_merged\n\t\n\twhen:\n\tparams.mode == \"characterisation\" && !params.singleEnd\n\n   \tscript:\n\t\"\"\"\n\t# This step will have no logging because the information are not relevant\n\t# I will simply use a boilerplate YAML to record that this has happened\n\t# If the files were not compressed, they will be at this stage\n\tif (file ${reads[0]} | grep -q compressed ) ; then\n\t    cat ${reads[0]} ${reads[1]} > ${name}_QCd.fq.gz\n\telse\n\t\tcat ${reads[0]} ${reads[1]} | gzip > ${name}_QCd.fq.gz\n\tfi\n\t\"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\t\"\"\"\n\t# This step will have no logging because the information are not relevant\n\t# I will simply use a boilerplate YAML to record that this has happened\n\t# If the files were not compressed, they will be at this stage\n\tif (file ${reads[0]} | grep -q compressed ) ; then\n\t    cat ${reads[0]} ${reads[1]} > ${name}_QCd.fq.gz\n\telse\n\t\tcat ${reads[0]} ${reads[1]} | gzip > ${name}_QCd.fq.gz\n\tfi\n\t\"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "reads_merge_paired_end_cleaned"
        ],
        "nb_inputs": 1,
        "outputs": [
            "to_profile_taxa_merged",
            "to_profile_functions_merged"
        ],
        "nb_outputs": 2,
        "name_workflow": "alesssia__YAMP",
        "directive": [
            "tag \"$name\""
        ],
        "when": "params.mode == \"characterisation\" && !params.singleEnd",
        "stub": ""
    },
    "profile_taxa": {
        "name_process": "profile_taxa",
        "string_process": "\nprocess profile_taxa {\n\n    tag \"$name\"\n\n\t                                \n    if (workflow.containerEngine == 'singularity') {\n        container params.singularity_container_biobakery\n    } else {\n        container params.docker_container_biobakery\n    }\n\n\tpublishDir \"${params.outdir}/${params.prefix}\", mode: 'copy', pattern: \"*.{biom,tsv}\"\n\t\n\tinput:\n\ttuple val(name), file(reads) from to_profile_taxa_decontaminated.mix(to_profile_taxa_merged).mix(reads_profile_taxa)\n\tfile (bowtie2db) from bowtie2_metaphlan_databases\n\t\n\toutput:\n\ttuple val(name), path(\"*.biom\") into to_alpha_diversity\n\ttuple val(name), path(\"*_metaphlan_bugs_list.tsv\") into to_profile_function_bugs\n\tfile \"profile_taxa_mqc.yaml\" into profile_taxa_log\n\t\n\twhen:\n\tparams.mode != \"QC\"\n\t\n\tscript:\n\t\"\"\"\n\t#If a file with the same name is already present, Metaphlan2 used to crash, leaving this here just in case\n\trm -rf ${name}_bt2out.txt\n\t\n\tmetaphlan --input_type fastq --tmp_dir=. --biom ${name}.biom --bowtie2out=${name}_bt2out.txt --bowtie2db $bowtie2db --bt2_ps ${params.bt2options} --add_viruses --sample_id ${name} --nproc ${task.cpus} $reads ${name}_metaphlan_bugs_list.tsv &> profile_taxa_mqc.txt\n\t\n\t# MultiQC doesn't have a module for Metaphlan yet. As a consequence, I\n\t# had to create a YAML file with all the info I need via a bash script\n\tbash scrape_profile_taxa_log.sh ${name}_metaphlan_bugs_list.tsv > profile_taxa_mqc.yaml\n\t\"\"\"\n}",
        "nb_lignes_process": 36,
        "string_script": "\t\"\"\"\n\t#If a file with the same name is already present, Metaphlan2 used to crash, leaving this here just in case\n\trm -rf ${name}_bt2out.txt\n\t\n\tmetaphlan --input_type fastq --tmp_dir=. --biom ${name}.biom --bowtie2out=${name}_bt2out.txt --bowtie2db $bowtie2db --bt2_ps ${params.bt2options} --add_viruses --sample_id ${name} --nproc ${task.cpus} $reads ${name}_metaphlan_bugs_list.tsv &> profile_taxa_mqc.txt\n\t\n\t# MultiQC doesn't have a module for Metaphlan yet. As a consequence, I\n\t# had to create a YAML file with all the info I need via a bash script\n\tbash scrape_profile_taxa_log.sh ${name}_metaphlan_bugs_list.tsv > profile_taxa_mqc.yaml\n\t\"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [
            "MetaPhlAn"
        ],
        "tools_url": [
            "https://bio.tools/metaphlan"
        ],
        "tools_dico": [
            {
                "name": "MetaPhlAn",
                "uri": "https://bio.tools/metaphlan",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0194",
                            "term": "Phylogenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0324",
                                    "term": "Phylogenetic analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Nucleic acid sequence analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0324",
                                    "term": "Phylogenetic tree analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Sequence analysis (nucleic acid)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Computational tool for profiling the composition of microbial communities from metagenomic shotgun sequencing data.",
                "homepage": "http://huttenhower.sph.harvard.edu/metaphlan"
            }
        ],
        "inputs": [
            "to_profile_taxa_decontaminated",
            "to_profile_taxa_merged",
            "reads_profile_taxa",
            "bowtie2_metaphlan_databases"
        ],
        "nb_inputs": 4,
        "outputs": [
            "to_alpha_diversity",
            "to_profile_function_bugs",
            "profile_taxa_log"
        ],
        "nb_outputs": 3,
        "name_workflow": "alesssia__YAMP",
        "directive": [
            "tag \"$name\" if (workflow.containerEngine == 'singularity') { container params.singularity_container_biobakery } else { container params.docker_container_biobakery }",
            "publishDir \"${params.outdir}/${params.prefix}\", mode: 'copy', pattern: \"*.{biom,tsv}\""
        ],
        "when": "params.mode != \"QC\"",
        "stub": ""
    },
    "profile_function": {
        "name_process": "profile_function",
        "string_process": "\nprocess profile_function {\n\t\n    tag \"$name\"\n\n\t                                \n    if (workflow.containerEngine == 'singularity') {\n        container params.singularity_container_biobakery\n    } else {\n        container params.docker_container_biobakery\n    }\n\n\tpublishDir \"${params.outdir}/${params.prefix}\", mode: 'copy', pattern: \"*.{tsv,log}\"\n\t\n\tinput:\n\ttuple val(name), file(reads) from to_profile_functions_decontaminated.mix(to_profile_functions_merged).mix(reads_profile_functions)\n\ttuple val(name), file(metaphlan_bug_list) from to_profile_function_bugs\n\tfile (chocophlan) from chocophlan_databases\n\tfile (uniref) from uniref_databases\n\t\n    output:\n\tfile \"*_HUMAnN.log\"\n\tfile \"*_genefamilies.tsv\"\n\tfile \"*_pathcoverage.tsv\"\n\tfile \"*_pathabundance.tsv\"\n\tfile \"profile_functions_mqc.yaml\" into profile_functions_log\n\n\twhen:\n\tparams.mode != \"QC\"\n\n\tscript:\n\t\"\"\"\n\t#HUMAnN will uses the list of species detected by the profile_taxa process\n\thumann --input $reads --output . --output-basename ${name} --taxonomic-profile $metaphlan_bug_list --nucleotide-database $chocophlan --protein-database $uniref --pathways metacyc --threads ${task.cpus} --memory-use minimum &> ${name}_HUMAnN.log \n\t\n\t# MultiQC doesn't have a module for humann yet. As a consequence, I\n\t# had to create a YAML file with all the info I need via a bash script\n\tbash scrape_profile_functions.sh ${name} ${name}_HUMAnN.log > profile_functions_mqc.yaml\n \t\"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "\t\"\"\"\n\t#HUMAnN will uses the list of species detected by the profile_taxa process\n\thumann --input $reads --output . --output-basename ${name} --taxonomic-profile $metaphlan_bug_list --nucleotide-database $chocophlan --protein-database $uniref --pathways metacyc --threads ${task.cpus} --memory-use minimum &> ${name}_HUMAnN.log \n\t\n\t# MultiQC doesn't have a module for humann yet. As a consequence, I\n\t# had to create a YAML file with all the info I need via a bash script\n\tbash scrape_profile_functions.sh ${name} ${name}_HUMAnN.log > profile_functions_mqc.yaml\n \t\"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "to_profile_functions_decontaminated",
            "to_profile_functions_merged",
            "reads_profile_functions",
            "to_profile_function_bugs",
            "chocophlan_databases",
            "uniref_databases"
        ],
        "nb_inputs": 6,
        "outputs": [
            "profile_functions_log"
        ],
        "nb_outputs": 1,
        "name_workflow": "alesssia__YAMP",
        "directive": [
            "tag \"$name\" if (workflow.containerEngine == 'singularity') { container params.singularity_container_biobakery } else { container params.docker_container_biobakery }",
            "publishDir \"${params.outdir}/${params.prefix}\", mode: 'copy', pattern: \"*.{tsv,log}\""
        ],
        "when": "params.mode != \"QC\"",
        "stub": ""
    },
    "alpha_diversity": {
        "name_process": "alpha_diversity",
        "string_process": "\nprocess alpha_diversity {\n\n    tag \"$name\"\n\n\t                                \n    if (workflow.containerEngine == 'singularity') {\n        container params.singularity_container_qiime2\n    } else {\n        container params.docker_container_qiime2\n    }\n\n\tpublishDir \"${params.outdir}/${params.prefix}\", mode: 'copy', pattern: \"*.{tsv}\"\n\t\n\tinput:\n\ttuple val(name), file(metaphlan_bug_list) from to_alpha_diversity\n\t\t\n    output:\n\tfile \"*_alpha_diversity.tsv\"\n\tfile \"alpha_diversity_mqc.yaml\" into alpha_diversity_log\n\t\n\twhen:\n\tparams.mode != \"QC\"\n\n\tscript:\n\t\"\"\"\n\t#It checks if the profiling was successful, that is if identifies at least three species\n\tn=\\$(grep -o s__ $metaphlan_bug_list | wc -l  | cut -d\\\" \\\" -f 1)\n\tif (( n <= 3 )); then\n\t\t#The file should be created in order to be returned\n\t\ttouch ${name}_alpha_diversity.tsv \n\telse\n\t\techo $name > ${name}_alpha_diversity.tsv\n\t\tqiime tools import --input-path $metaphlan_bug_list --type 'FeatureTable[Frequency]' --input-format BIOMV100Format --output-path ${name}_abundance_table.qza\n\t\tfor alpha in ace berger_parker_d brillouin_d chao1 chao1_ci dominance doubles enspie esty_ci fisher_alpha gini_index goods_coverage heip_e kempton_taylor_q lladser_pe margalef mcintosh_d mcintosh_e menhinick michaelis_menten_fit osd pielou_e robbins shannon simpson simpson_e singles strong\n\t\tdo\n\t\t\tqiime diversity alpha --i-table ${name}_abundance_table.qza --p-metric \\$alpha --output-dir \\$alpha &> /dev/null\n\t\t\tqiime tools export --input-path \\$alpha/alpha_diversity.qza --output-path \\${alpha} &> /dev/null\n\t\t\tvalue=\\$(sed -n '2p' \\${alpha}/alpha-diversity.tsv | cut -f 2)\n\t\t    echo -e  \\$alpha'\\t'\\$value \n\t\tdone >> ${name}_alpha_diversity.tsv  \n\tfi\n\n\t# MultiQC doesn't have a module for qiime yet. As a consequence, I\n\t# had to create a YAML file with all the info I need via a bash script\n\tbash generate_alpha_diversity_log.sh \\${n} > alpha_diversity_mqc.yaml\t\n\t\"\"\"\n}",
        "nb_lignes_process": 46,
        "string_script": "\t\"\"\"\n\t#It checks if the profiling was successful, that is if identifies at least three species\n\tn=\\$(grep -o s__ $metaphlan_bug_list | wc -l  | cut -d\\\" \\\" -f 1)\n\tif (( n <= 3 )); then\n\t\t#The file should be created in order to be returned\n\t\ttouch ${name}_alpha_diversity.tsv \n\telse\n\t\techo $name > ${name}_alpha_diversity.tsv\n\t\tqiime tools import --input-path $metaphlan_bug_list --type 'FeatureTable[Frequency]' --input-format BIOMV100Format --output-path ${name}_abundance_table.qza\n\t\tfor alpha in ace berger_parker_d brillouin_d chao1 chao1_ci dominance doubles enspie esty_ci fisher_alpha gini_index goods_coverage heip_e kempton_taylor_q lladser_pe margalef mcintosh_d mcintosh_e menhinick michaelis_menten_fit osd pielou_e robbins shannon simpson simpson_e singles strong\n\t\tdo\n\t\t\tqiime diversity alpha --i-table ${name}_abundance_table.qza --p-metric \\$alpha --output-dir \\$alpha &> /dev/null\n\t\t\tqiime tools export --input-path \\$alpha/alpha_diversity.qza --output-path \\${alpha} &> /dev/null\n\t\t\tvalue=\\$(sed -n '2p' \\${alpha}/alpha-diversity.tsv | cut -f 2)\n\t\t    echo -e  \\$alpha'\\t'\\$value \n\t\tdone >> ${name}_alpha_diversity.tsv  \n\tfi\n\n\t# MultiQC doesn't have a module for qiime yet. As a consequence, I\n\t# had to create a YAML file with all the info I need via a bash script\n\tbash generate_alpha_diversity_log.sh \\${n} > alpha_diversity_mqc.yaml\t\n\t\"\"\"",
        "nb_lignes_script": 21,
        "language_script": "bash",
        "tools": [
            "QIIME",
            "NullSeq"
        ],
        "tools_url": [
            "https://bio.tools/qiime",
            "https://bio.tools/nullseq"
        ],
        "tools_dico": [
            {
                "name": "QIIME",
                "uri": "https://bio.tools/qiime",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3070",
                            "term": "Biology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3070",
                            "term": "Biological science"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2945",
                                    "term": "Analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Open-source bioinformatics pipeline for performing microbiome analysis from raw DNA sequencing data. The pipeline is designed to take users from raw sequencing data generated on the Illumina or other platforms through publication quality graphics and statistics. This includes demultiplexing and quality filtering, OTU picking, taxonomic assignment, and phylogenetic reconstruction, and diversity analyses and visualizations.",
                "homepage": "http://qiime.org/"
            },
            {
                "name": "NullSeq",
                "uri": "https://bio.tools/nullseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0364",
                                    "term": "Random sequence generation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Creates Random Coding Sequences with specified GC content and Amino Acid usage.",
                "homepage": "https://github.com/amarallab/NullSeq"
            }
        ],
        "inputs": [
            "to_alpha_diversity"
        ],
        "nb_inputs": 1,
        "outputs": [
            "alpha_diversity_log"
        ],
        "nb_outputs": 1,
        "name_workflow": "alesssia__YAMP",
        "directive": [
            "tag \"$name\" if (workflow.containerEngine == 'singularity') { container params.singularity_container_qiime2 } else { container params.docker_container_qiime2 }",
            "publishDir \"${params.outdir}/${params.prefix}\", mode: 'copy', pattern: \"*.{tsv}\""
        ],
        "when": "params.mode != \"QC\"",
        "stub": ""
    },
    "log": {
        "name_process": "log",
        "string_process": "\nprocess log {\n\t\n\tpublishDir \"${params.outdir}/${params.prefix}\", mode: 'copy'\n\n    if (workflow.containerEngine == 'singularity') {\n        container params.singularity_container_multiqc\n    } else {\n        container params.docker_container_multiqc\n    }\n\n\tinput:\n\tfile multiqc_config\n\tfile workflow_summary from create_workflow_summary(summary)\n\tfile \"software_versions_mqc.yaml\" from software_versions_yaml\n\tpath \"fastqc/*\" from fastqc_log.collect().ifEmpty([])\n\tfile \"dedup_mqc.yaml\" from dedup_log.ifEmpty([])\n\tfile \"synthetic_contaminants_mqc.yaml\" from synthetic_contaminants_log.ifEmpty([])\n\tfile \"trimming_mqc.yaml\" from trimming_log.ifEmpty([])\n\tfile \"foreign_genome_indexing_mqc.yaml\" from index_foreign_genome_log.ifEmpty([])\n\tfile \"decontamination_mqc.yaml\" from decontaminate_log.ifEmpty([])\n\tfile \"merge_paired_end_cleaned_mqc.yaml\" from merge_paired_end_cleaned_log.ifEmpty([])\n\tfile \"profile_taxa_mqc.yaml\" from profile_taxa_log.ifEmpty([])\n\tfile \"profile_functions_mqc.yaml\" from profile_functions_log.ifEmpty([])\n\tfile \"alpha_diversity_mqc.yaml\" from alpha_diversity_log.ifEmpty([])\n\t\n\toutput:\n\tpath \"*multiqc_report*.html\" into multiqc_report\n\tpath \"*multiqc_data*\"\n\n\tscript:\n\t\"\"\"\n\tmultiqc --config $multiqc_config . -f\n\tmv multiqc_report.html ${params.prefix}_multiqc_report_${params.mode}.html\n\tmv multiqc_data ${params.prefix}_multiqc_data_${params.mode}\n\t\"\"\"\n}",
        "nb_lignes_process": 35,
        "string_script": "\t\"\"\"\n\tmultiqc --config $multiqc_config . -f\n\tmv multiqc_report.html ${params.prefix}_multiqc_report_${params.mode}.html\n\tmv multiqc_data ${params.prefix}_multiqc_data_${params.mode}\n\t\"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "MultiQC"
        ],
        "tools_url": [
            "https://bio.tools/multiqc"
        ],
        "tools_dico": [
            {
                "name": "MultiQC",
                "uri": "https://bio.tools/multiqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0091",
                            "term": "Bioinformatics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2428",
                                    "term": "Validation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2048",
                                "term": "Report"
                            }
                        ]
                    }
                ],
                "description": "MultiQC aggregates results from multiple bioinformatics analyses across many samples into a single report. It searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.",
                "homepage": "http://multiqc.info/"
            }
        ],
        "inputs": [
            "multiqc_config",
            "summary",
            "software_versions_yaml",
            "fastqc_log",
            "dedup_log",
            "synthetic_contaminants_log",
            "trimming_log",
            "index_foreign_genome_log",
            "decontaminate_log",
            "merge_paired_end_cleaned_log",
            "profile_taxa_log",
            "profile_functions_log",
            "alpha_diversity_log"
        ],
        "nb_inputs": 13,
        "outputs": [
            "multiqc_report"
        ],
        "nb_outputs": 1,
        "name_workflow": "alesssia__YAMP",
        "directive": [
            "publishDir \"${params.outdir}/${params.prefix}\", mode: 'copy' if (workflow.containerEngine == 'singularity') { container params.singularity_container_multiqc } else { container params.docker_container_multiqc }"
        ],
        "when": "",
        "stub": ""
    }
}