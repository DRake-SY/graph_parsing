{
    "COPY_DIR": {
        "name_process": "COPY_DIR",
        "string_process": "\nprocess COPY_DIR {\n    tag \"${in_saves}\"\n    stageInMode = 'copy'\n    conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/python:3.8.3\"\n    } else {\n        container \"quay.io/biocontainers/python:3.8.3\"\n    }\n\n    input:\n    tuple val(sample), path(in_saves)\n\n    output:\n    tuple val(sample), path('*.spades_out.old')\n\n    script:\n    def prefix      = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n\n    \"\"\"\n    mv ${in_saves} ${prefix}.spades_out.old\n    \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "    def prefix      = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n\n    \"\"\"\n    mv ${in_saves} ${prefix}.spades_out.old\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "in_saves"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sample"
        ],
        "nb_outputs": 1,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"${in_saves}\"",
            "stageInMode = 'copy'",
            "conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/python:3.8.3\" } else { container \"quay.io/biocontainers/python:3.8.3\" }"
        ],
        "when": "",
        "stub": ""
    },
    "MPR_NULLIFY": {
        "name_process": "MPR_NULLIFY",
        "string_process": "\nprocess MPR_NULLIFY {\n    tag \"${saves}\"\n    conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/python:3.8.3\"\n    } else {\n        container \"quay.io/biocontainers/python:3.8.3\"\n    }\n\n    input:\n    tuple val(sample), path(saves)\n\n    output:\n    tuple val(sample), path(saves)\n\n    script:\n\n    \"\"\"\n    dirs=(\\$(ls -d -r ${saves}/K*))\n    kDir=\\$(basename \\${dirs[0]})\n\n    # Replace binary alignment files with blank files to get only short reads assembly\n    for path in \\$(ls -d ${saves}/\\${kDir}/saves/distance_estimation/graph_pack_*.mpr); do\n        basename=\\$(basename \\${path})\n        ext=\\${basename##*.}\n        filename=\\${basename%.*}\n        mpr_old=\"${saves}/\\${kDir}/saves/distance_estimation/\\${filename}.old.MPR_NULLIFY.\\${ext}\"\n        mv \\${path} \\${mpr_old}\n        zero_byte=\"\\\\x00\"\n        echo -n -e \\${zero_byte} > \\${path}\n    done\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "    \"\"\"\n    dirs=(\\$(ls -d -r ${saves}/K*))\n    kDir=\\$(basename \\${dirs[0]})\n\n    # Replace binary alignment files with blank files to get only short reads assembly\n    for path in \\$(ls -d ${saves}/\\${kDir}/saves/distance_estimation/graph_pack_*.mpr); do\n        basename=\\$(basename \\${path})\n        ext=\\${basename##*.}\n        filename=\\${basename%.*}\n        mpr_old=\"${saves}/\\${kDir}/saves/distance_estimation/\\${filename}.old.MPR_NULLIFY.\\${ext}\"\n        mv \\${path} \\${mpr_old}\n        zero_byte=\"\\\\x00\"\n        echo -n -e \\${zero_byte} > \\${path}\n    done\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "saves"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sample"
        ],
        "nb_outputs": 1,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"${saves}\"",
            "conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/python:3.8.3\" } else { container \"quay.io/biocontainers/python:3.8.3\" }"
        ],
        "when": "",
        "stub": ""
    },
    "GET_SOFTWARE_VERSIONS": {
        "name_process": "GET_SOFTWARE_VERSIONS",
        "string_process": "\nprocess GET_SOFTWARE_VERSIONS {\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/python:3.8.3\"\n    } else {\n        container \"quay.io/biocontainers/python:3.8.3\"\n    }\n\n    cache false\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.tsv\"     , emit: tsv\n    path 'software_versions_mqc.yaml', emit: yaml\n\n    script:                                                                             \n    \"\"\"\n    echo $workflow.manifest.version > pipeline.version.txt\n    echo $workflow.nextflow.version > nextflow.version.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "    \"\"\"\n    echo $workflow.manifest.version > pipeline.version.txt\n    echo $workflow.nextflow.version > nextflow.version.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "versions"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }",
            "conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/python:3.8.3\" } else { container \"quay.io/biocontainers/python:3.8.3\" }",
            "cache false"
        ],
        "when": "",
        "stub": ""
    },
    "CAT_READS": {
        "name_process": "CAT_READS",
        "string_process": "\nprocess CAT_READS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'merged_reads', meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\"\n    } else {\n        container \"biocontainers/biocontainers:v1.2.0_cv1\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.merged.*\")    , emit: reads\n    path \"versions.yml\"                    , emit: versions\n\n    script:\n\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def readList = reads.collect{ it.toString() }\n    if (meta.single_end) {\n        if (readList.size > 1) {\n            \"\"\"\n            basename=\\$(basename ${readList[0]})\n            ext=\\${basename#*.}\n\n            cat ${readList.sort().join(' ')} > ${prefix}.merged.\\${ext}\n\n            cat <<-END_VERSIONS > versions.yml\n            ${getProcessName(task.process)}:\n                ${getSoftwareName(task.process)}: \\$(echo \\$(cat --version 2>&1) | sed 's/^.*coreutils) //; s/ .*\\$//')\n            END_VERSIONS\n            \"\"\"\n        }\n    } else {\n        if (readList.size > 2) {\n            def read1 = []\n            def read2 = []\n            readList.eachWithIndex{ v, ix -> ( ix & 1 ? read2 : read1 ) << v }\n            \"\"\"\n            basename=\\$(basename ${readList[0]})\n            ext=\\${basename#*.}\n\n            cat ${read1.sort().join(' ')} > ${prefix}_1.merged.\\${ext}\n            cat ${read2.sort().join(' ')} > ${prefix}_2.merged.\\${ext}\n\n            cat <<-END_VERSIONS > versions.yml\n            ${getProcessName(task.process)}:\n                ${getSoftwareName(task.process)}: \\$(echo \\$(cat --version 2>&1) | sed 's/^.*coreutils) //; s/ .*\\$//')\n            END_VERSIONS\n            \"\"\"\n        }\n    }\n}",
        "nb_lignes_process": 58,
        "string_script": "    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def readList = reads.collect{ it.toString() }\n    if (meta.single_end) {\n        if (readList.size > 1) {\n            \"\"\"\n            basename=\\$(basename ${readList[0]})\n            ext=\\${basename#*.}\n\n            cat ${readList.sort().join(' ')} > ${prefix}.merged.\\${ext}\n\n            cat <<-END_VERSIONS > versions.yml\n            ${getProcessName(task.process)}:\n                ${getSoftwareName(task.process)}: \\$(echo \\$(cat --version 2>&1) | sed 's/^.*coreutils) //; s/ .*\\$//')\n            END_VERSIONS\n            \"\"\"\n        }\n    } else {\n        if (readList.size > 2) {\n            def read1 = []\n            def read2 = []\n            readList.eachWithIndex{ v, ix -> ( ix & 1 ? read2 : read1 ) << v }\n            \"\"\"\n            basename=\\$(basename ${readList[0]})\n            ext=\\${basename#*.}\n\n            cat ${read1.sort().join(' ')} > ${prefix}_1.merged.\\${ext}\n            cat ${read2.sort().join(' ')} > ${prefix}_2.merged.\\${ext}\n\n            cat <<-END_VERSIONS > versions.yml\n            ${getProcessName(task.process)}:\n                ${getSoftwareName(task.process)}: \\$(echo \\$(cat --version 2>&1) | sed 's/^.*coreutils) //; s/ .*\\$//')\n            END_VERSIONS\n            \"\"\"\n        }\n    }",
        "nb_lignes_script": 34,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'merged_reads', meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\" } else { container \"biocontainers/biocontainers:v1.2.0_cv1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "AGAINST_PROTEINS_EVALUATION": {
        "name_process": "AGAINST_PROTEINS_EVALUATION",
        "string_process": "\nprocess AGAINST_PROTEINS_EVALUATION {\n    tag \"${sample}\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'metrics_out', meta:[:], publish_by_meta:[]) }\n    conda (params.enable_conda ? \"conda-forge::python=3.8.3 biopython bioconda::diamond bioconda::interproscan bioconda::mmseqs2 bioconda::prodigal bioconda::pyfasta\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/python:3.8.3\"\n    } else {\n        container \"quay.io/biocontainers/python:3.8.3\"\n    }\n\n    input:\n    tuple val(sample), path(short_reads_transcripts), path(all_transcripts), path(clusters_transcripts)\n    path(mgy)\n\n    output:\n    tuple val(sample), path('*.proteins_metrics.txt'), emit: short_report\n\n    script:\n    def prefix      = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n\n    \"\"\"\n    QA_against_proteins.py ${short_reads_transcripts} ${mgy} ${prefix}.proteins_metrics\n    QA_against_proteins.py ${all_transcripts} ${mgy} ${prefix}.proteins_metrics\n    QA_against_proteins.py ${clusters_transcripts} ${mgy} ${prefix}.proteins_metrics\n\n    mv ${prefix}.proteins_metrics/*.proteins_metrics.txt .\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "    def prefix      = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n\n    \"\"\"\n    QA_against_proteins.py ${short_reads_transcripts} ${mgy} ${prefix}.proteins_metrics\n    QA_against_proteins.py ${all_transcripts} ${mgy} ${prefix}.proteins_metrics\n    QA_against_proteins.py ${clusters_transcripts} ${mgy} ${prefix}.proteins_metrics\n\n    mv ${prefix}.proteins_metrics/*.proteins_metrics.txt .\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "short_reads_transcripts",
            "all_transcripts",
            "clusters_transcripts",
            "mgy"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"${sample}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'metrics_out', meta:[:], publish_by_meta:[]) }",
            "conda (params.enable_conda ? \"conda-forge::python=3.8.3 biopython bioconda::diamond bioconda::interproscan bioconda::mmseqs2 bioconda::prodigal bioconda::pyfasta\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/python:3.8.3\" } else { container \"quay.io/biocontainers/python:3.8.3\" }"
        ],
        "when": "",
        "stub": ""
    },
    "FQ2FA": {
        "name_process": "FQ2FA",
        "string_process": "\nprocess FQ2FA {\n    tag \"${sample.id}, ${sample.type}\"\n    conda (params.enable_conda ? \"bioconda::seqtk\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/python:3.8.3\"\n    } else {\n        container \"quay.io/biocontainers/python:3.8.3\"\n    }\n\n    input:\n    tuple val(sample), path(reads)\n\n    output:\n    tuple val(sample), path('*.fasta'), emit: reads_in_fasta\n\n    when:\n    reads.toString().endsWith('.fastq') or reads.toString().endsWith('.fq') or reads.toString().endsWith('.fastq.gz') or reads.toString().endsWith('.fq.gz')\n\n    script:\n    \"\"\"\n    basename=\\$(basename ${reads})\n    ext=\\${basename#*.}\n    filename=\\${basename%%.*}\n    seqtk seq -a ${reads} > \\${filename}.fasta\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "    \"\"\"\n    basename=\\$(basename ${reads})\n    ext=\\${basename#*.}\n    filename=\\${basename%%.*}\n    seqtk seq -a ${reads} > \\${filename}.fasta\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "seqtk"
        ],
        "tools_url": [
            "https://bio.tools/seqtk"
        ],
        "tools_dico": [
            {
                "name": "seqtk",
                "uri": "https://bio.tools/seqtk",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Biological databases"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Data management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Information systems"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Databases and information systems"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Data handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2121",
                                    "term": "Sequence file editing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Utility operation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File processing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Report handling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A tool for processing sequences in the FASTA or FASTQ format. It parses both FASTA and FASTQ files which can also be optionally compressed by gzip.",
                "homepage": "https://github.com/lh3/seqtk"
            }
        ],
        "inputs": [
            "sample",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"${sample.id}, ${sample.type}\"",
            "conda (params.enable_conda ? \"bioconda::seqtk\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/python:3.8.3\" } else { container \"quay.io/biocontainers/python:3.8.3\" }"
        ],
        "when": "reads.toString().endsWith('.fastq') or reads.toString().endsWith('.fq') or reads.toString().endsWith('.fastq.gz') or reads.toString().endsWith('.fq.gz')",
        "stub": ""
    },
    "GZIP_READS": {
        "name_process": "GZIP_READS",
        "string_process": "\nprocess GZIP_READS {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'compressed_reads', meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\"\n    } else {\n        container \"biocontainers/biocontainers:v1.2.0_cv1\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.gz\")          , emit: gzip\n    path \"versions.yml\"                    , emit: versions\n\n    script:\n    if (meta.single_end) {\n        \"\"\"\n        gzip -f ${reads}\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            gzip: \\$(echo \\$(gzip --version 2>&1 | sed '2,\\$d' | sed 's/gzip //'))\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        gzip -f ${reads[0]}\n        gzip -f ${reads[1]}\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            gzip: \\$(echo \\$(gzip --version 2>&1 | sed '2,\\$d' | sed 's/gzip //'))\n        END_VERSIONS\n        \"\"\"\n    }\n}",
        "nb_lignes_process": 40,
        "string_script": "    if (meta.single_end) {\n        \"\"\"\n        gzip -f ${reads}\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            gzip: \\$(echo \\$(gzip --version 2>&1 | sed '2,\\$d' | sed 's/gzip //'))\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        gzip -f ${reads[0]}\n        gzip -f ${reads[1]}\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            gzip: \\$(echo \\$(gzip --version 2>&1 | sed '2,\\$d' | sed 's/gzip //'))\n        END_VERSIONS\n        \"\"\"\n    }",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'compressed_reads', meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\" } else { container \"biocontainers/biocontainers:v1.2.0_cv1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "MULTIQC": {
        "name_process": "MULTIQC",
        "string_process": "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.10.1--py_0\"\n    }\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    multiqc -f $options.args .\n    multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "MultiQC"
        ],
        "tools_url": [
            "https://bio.tools/multiqc"
        ],
        "tools_dico": [
            {
                "name": "MultiQC",
                "uri": "https://bio.tools/multiqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0091",
                            "term": "Bioinformatics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2428",
                                    "term": "Validation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2048",
                                "term": "Report"
                            }
                        ]
                    }
                ],
                "description": "MultiQC aggregates results from multiple bioinformatics analyses across many samples into a single report. It searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.",
                "homepage": "http://multiqc.info/"
            }
        ],
        "inputs": [
            "multiqc_files"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "label 'process_medium'",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }",
            "conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/multiqc:1.10.1--py_0\" } else { container \"quay.io/biocontainers/multiqc:1.10.1--py_0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "CLUSTERING": {
        "name_process": "CLUSTERING",
        "string_process": "\nprocess CLUSTERING {\n    tag \"$sample\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'clustering_out', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ?\n        \"python=3.5.5 biopython=1.72 networkx=2.2 boltons=19.1.0 scipy=1.1 scikit-learn=0.20.0 python-louvain=0.13 umap-learn=0.3.2 seaborn pip=20.1.1\"\n        : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/python:3.8.3\"\n    } else {\n        container \"quay.io/biocontainers/python:3.8.3\"\n    }\n\n    input:\n    tuple val(sample), path(saves), path(gfa), path(grseq), path(readable_mprs)\n\n    output:\n    tuple val(sample), path('*.clustering.tsv'), emit: clustering\n\n    script:                                                                             \n    def prefix     = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n    def alignments = ( readable_mprs.size() == 2 ) ? \" --friendships_reads ${readable_mprs[1]}\" : \" --friendships_reads ${readable_mprs[1]} --friendships_db ${readable_mprs[2]}\"\n\n    \"\"\"\n    export PYTHONPATH=\n    mkdir logs\n    pip install numpy==1.18.5 >> logs/install_numpy.log\n    pip install absl-py==0.6.1 >> logs/install_absl.log\n    pip install gensim==0.13.2 >> logs/install_gensim.log\n    pip install pandas==0.25.3 >> logs/install_pandas.log\n    pip install matplotlib==2.2.2 >> logs/install_matplotlib.log\n\n    basename=\\$(basename ${grseq})\n    ext=\\${basename##*.}\n    filename=\\${basename%.*}\n    show_saves.py \\${filename}.grp > \\${filename}.readable.grp\n\n    dirs=(\\$(ls -d -r ${saves}/K*))\n    kDir=\\$(basename \\${dirs[0]})\n    k_size=\\${kDir:1}\n\n    nxG2clusters.py                                 \\\\\n    --gfa ${gfa}                                    \\\\\n    --grp \\$(realpath \\${filename}.readable.grp)    \\\\\n    ${alignments}                                   \\\\\n    -k \\${k_size}                                   \\\\\n    -o ${prefix}.clustering_out\n\n    if [ -f ${prefix}.clustering_out/clustering.tsv ]; then\n        mv ${prefix}.clustering_out/clustering.tsv ${prefix}.clustering.tsv\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 54,
        "string_script": "    def prefix     = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n    def alignments = ( readable_mprs.size() == 2 ) ? \" --friendships_reads ${readable_mprs[1]}\" : \" --friendships_reads ${readable_mprs[1]} --friendships_db ${readable_mprs[2]}\"\n\n    \"\"\"\n    export PYTHONPATH=\n    mkdir logs\n    pip install numpy==1.18.5 >> logs/install_numpy.log\n    pip install absl-py==0.6.1 >> logs/install_absl.log\n    pip install gensim==0.13.2 >> logs/install_gensim.log\n    pip install pandas==0.25.3 >> logs/install_pandas.log\n    pip install matplotlib==2.2.2 >> logs/install_matplotlib.log\n\n    basename=\\$(basename ${grseq})\n    ext=\\${basename##*.}\n    filename=\\${basename%.*}\n    show_saves.py \\${filename}.grp > \\${filename}.readable.grp\n\n    dirs=(\\$(ls -d -r ${saves}/K*))\n    kDir=\\$(basename \\${dirs[0]})\n    k_size=\\${kDir:1}\n\n    nxG2clusters.py                                 \\\\\n    --gfa ${gfa}                                    \\\\\n    --grp \\$(realpath \\${filename}.readable.grp)    \\\\\n    ${alignments}                                   \\\\\n    -k \\${k_size}                                   \\\\\n    -o ${prefix}.clustering_out\n\n    if [ -f ${prefix}.clustering_out/clustering.tsv ]; then\n        mv ${prefix}.clustering_out/clustering.tsv ${prefix}.clustering.tsv\n    fi\n    \"\"\"",
        "nb_lignes_script": 31,
        "language_script": "bash",
        "tools": [
            "PPIP"
        ],
        "tools_url": [
            "https://bio.tools/ppip"
        ],
        "tools_dico": [
            {
                "name": "PPIP",
                "uri": "https://bio.tools/ppip",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3644",
                                    "term": "de Novo sequencing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3631",
                                    "term": "Peptide identification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3631",
                                    "term": "Peptide-spectrum-matching"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0943",
                                "term": "Mass spectrum"
                            },
                            {
                                "uri": "http://edamontology.org/data_2603",
                                "term": "Expression data"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2974",
                                "term": "Protein sequence (raw)"
                            }
                        ]
                    }
                ],
                "description": "An automated software for identification of bioactive endogenous peptides",
                "homepage": "https://github.com/Shawn-Xu/PPIP"
            }
        ],
        "inputs": [
            "sample",
            "saves",
            "gfa",
            "grseq",
            "readable_mprs"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"$sample\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'clustering_out', meta:[:], publish_by_meta:[]) }",
            "conda (params.enable_conda ? \"python=3.5.5 biopython=1.72 networkx=2.2 boltons=19.1.0 scipy=1.1 scikit-learn=0.20.0 python-louvain=0.13 umap-learn=0.3.2 seaborn pip=20.1.1\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/python:3.8.3\" } else { container \"quay.io/biocontainers/python:3.8.3\" }"
        ],
        "when": "",
        "stub": ""
    },
    "PE_PARAMS_MODIFY": {
        "name_process": "PE_PARAMS_MODIFY",
        "string_process": "\nprocess PE_PARAMS_MODIFY {\n    tag \"${saves}\"\n    conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/python:3.8.3\"\n    } else {\n        container \"quay.io/biocontainers/python:3.8.3\"\n    }\n\n    input:\n    tuple val(sample), path(saves), path(clusters)\n\n    output:\n    tuple val(sample), path(saves)\n\n    script:\n\n    \"\"\"\n    dirs=(\\$(ls -d -r ${saves}/K*))\n    kDir=\\$(basename \\${dirs[0]})\n\n    # Modify pe_params.info\n    pe_params=\"${saves}/\\${kDir}/configs/pe_params.info\"\n    basename=\\$(basename \\${pe_params})\n    ext=\\${basename##*.}\n    filename=\\${basename%.*}\n    pe_params_old=\"${saves}/\\${kDir}/configs/\\${filename}.old.PE_PARAMS_MODIFY.\\${ext}\"\n    cp \\${pe_params} \\${pe_params_old}\n    sed -i \"s|rna_clusters.*|rna_clusters \\$(realpath ${clusters})|\" \\$pe_params\n    \"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "    \"\"\"\n    dirs=(\\$(ls -d -r ${saves}/K*))\n    kDir=\\$(basename \\${dirs[0]})\n\n    # Modify pe_params.info\n    pe_params=\"${saves}/\\${kDir}/configs/pe_params.info\"\n    basename=\\$(basename \\${pe_params})\n    ext=\\${basename##*.}\n    filename=\\${basename%.*}\n    pe_params_old=\"${saves}/\\${kDir}/configs/\\${filename}.old.PE_PARAMS_MODIFY.\\${ext}\"\n    cp \\${pe_params} \\${pe_params_old}\n    sed -i \"s|rna_clusters.*|rna_clusters \\$(realpath ${clusters})|\" \\$pe_params\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "saves",
            "clusters"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sample"
        ],
        "nb_outputs": 1,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"${saves}\"",
            "conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/python:3.8.3\" } else { container \"quay.io/biocontainers/python:3.8.3\" }"
        ],
        "when": "",
        "stub": ""
    },
    "MPR_TO_READABLE": {
        "name_process": "MPR_TO_READABLE",
        "string_process": "\nprocess MPR_TO_READABLE {\n    tag \"${sample}\"\n    conda (params.enable_conda ? \"conda-forge::python=3.8.3 conda-forge::biopython\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/python:3.8.3\"\n    } else {\n        container \"quay.io/biocontainers/python:3.8.3\"\n    }\n\n    input:\n    tuple val(sample), path(mprs)\n\n    output:\n    tuple val(sample), path('*.readable_mpr'), emit: readable_mprs\n\n    script:\n\n    \"\"\"\n    # Get readable alignment files from binary mprs\n    for path in $mprs; do\n        basename=\\$(basename \\${path})\n        ext=\\${basename##*.}\n        filename=\\${basename%.*}\n        readable_path=\"\\${filename}.readable_mpr\"\n        show_saves.py \\${path} > \\${readable_path}\n    done\n    \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "    \"\"\"\n    # Get readable alignment files from binary mprs\n    for path in $mprs; do\n        basename=\\$(basename \\${path})\n        ext=\\${basename##*.}\n        filename=\\${basename%.*}\n        readable_path=\"\\${filename}.readable_mpr\"\n        show_saves.py \\${path} > \\${readable_path}\n    done\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "mprs"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"${sample}\"",
            "conda (params.enable_conda ? \"conda-forge::python=3.8.3 conda-forge::biopython\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/python:3.8.3\" } else { container \"quay.io/biocontainers/python:3.8.3\" }"
        ],
        "when": "",
        "stub": ""
    },
    "RNAQUAST_EVALUATION": {
        "name_process": "RNAQUAST_EVALUATION",
        "string_process": "\nprocess RNAQUAST_EVALUATION {\n    tag \"$sample\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'metrics_out', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? 'bioconda::rnaquast' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/python:3.8.3\"\n    } else {\n        container \"quay.io/biocontainers/python:3.8.3\"\n    }\n\n    input:\n                                                                                               \n    tuple val(sample), path(short_reads_transcripts), path(all_transcripts), path(clusters_transcripts)\n    path fasta\n    path gtf\n\n    output:\n    tuple val(sample), path('*.rnaquast_metrics.txt'), emit: short_report\n\n    script:\n    def software    = getSoftwareName(task.process)\n    def prefix      = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n    def all_transcripts = \"${short_reads_transcripts} ${all_transcripts} ${clusters_transcripts}\"\n\n    \"\"\"\n    # Using rnaQUAST against isoform database\n    rnaQUAST.py                           \\\\\n    --transcripts ${all_transcripts}      \\\\\n    --reference ${fasta}               \\\\\n    --gtf ${gtf}                       \\\\\n    --output_dir ${prefix}.rnaquast_out   \\\\\n    --busco auto-lineage\n\n    if [ -f ${prefix}.rnaquast_out/short_report.txt ]; then\n        mv ${prefix}.rnaquast_out/short_report.txt ${prefix}.rnaquast_metrics.txt\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 41,
        "string_script": "    def software    = getSoftwareName(task.process)\n    def prefix      = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n    def all_transcripts = \"${short_reads_transcripts} ${all_transcripts} ${clusters_transcripts}\"\n\n    \"\"\"\n    # Using rnaQUAST against isoform database\n    rnaQUAST.py                           \\\\\n    --transcripts ${all_transcripts}      \\\\\n    --reference ${fasta}               \\\\\n    --gtf ${gtf}                       \\\\\n    --output_dir ${prefix}.rnaquast_out   \\\\\n    --busco auto-lineage\n\n    if [ -f ${prefix}.rnaquast_out/short_report.txt ]; then\n        mv ${prefix}.rnaquast_out/short_report.txt ${prefix}.rnaquast_metrics.txt\n    fi\n    \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "short_reads_transcripts",
            "all_transcripts",
            "clusters_transcripts",
            "fasta",
            "gtf"
        ],
        "nb_inputs": 6,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"$sample\"",
            "label 'process_high'",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'metrics_out', meta:[:], publish_by_meta:[]) }",
            "conda (params.enable_conda ? 'bioconda::rnaquast' : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/python:3.8.3\" } else { container \"quay.io/biocontainers/python:3.8.3\" }"
        ],
        "when": "",
        "stub": ""
    },
    "SPADES": {
        "name_process": "SPADES",
        "string_process": "\nprocess SPADES {\n    tag \"$meta.id\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::spades=3.15.3 python=3.9' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/spades:3.15.3--h95f258a_0\"\n    } else {\n        container \"quay.io/biocontainers/spades:3.15.3--h95f258a_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  hmm\n\n    output:\n    tuple val(meta), path('*.scaffolds.fa')    , optional:true, emit: scaffolds\n    tuple val(meta), path('*.contigs.fa')      , optional:true, emit: contigs\n    tuple val(meta), path('*.transcripts.fa')  , optional:true, emit: transcripts\n    tuple val(meta), path('*.gene_clusters.fa'), optional:true, emit: gene_clusters\n    tuple val(meta), path('*.assembly.gfa')    , optional:true, emit: gfa\n    tuple val(meta), path('*.log')             , emit: log\n    path  '*.version.txt'                      , emit: version\n\n    script:\n    def software    = getSoftwareName(task.process)\n    def prefix      = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"-s $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    def custom_hmms = params.spades_hmm ? \"--custom-hmms $hmm\" : \"\"\n    \"\"\"\n    spades.py \\\\\n        $options.args \\\\\n        --threads $task.cpus \\\\\n        $custom_hmms \\\\\n        $input_reads \\\\\n        -o ./\n    mv spades.log ${prefix}.spades.log\n\n    if [ -f scaffolds.fasta ]; then\n        mv scaffolds.fasta ${prefix}.scaffolds.fa\n    fi\n    if [ -f contigs.fasta ]; then\n        mv contigs.fasta ${prefix}.contigs.fa\n    fi\n    if [ -f transcripts.fasta ]; then\n        mv transcripts.fasta ${prefix}.transcripts.fa\n    fi\n    if [ -f assembly_graph_with_scaffolds.gfa ]; then\n        mv assembly_graph_with_scaffolds.gfa ${prefix}.assembly.gfa\n    fi\n\n    if [ -f gene_clusters.fasta ]; then\n        mv gene_clusters.fasta ${prefix}.gene_clusters.fa\n    fi\n\n    echo \\$(spades.py --version 2>&1) | sed 's/^.*SPAdes genome assembler v//; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 60,
        "string_script": "    def software    = getSoftwareName(task.process)\n    def prefix      = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def input_reads = meta.single_end ? \"-s $reads\" : \"-1 ${reads[0]} -2 ${reads[1]}\"\n    def custom_hmms = params.spades_hmm ? \"--custom-hmms $hmm\" : \"\"\n    \"\"\"\n    spades.py \\\\\n        $options.args \\\\\n        --threads $task.cpus \\\\\n        $custom_hmms \\\\\n        $input_reads \\\\\n        -o ./\n    mv spades.log ${prefix}.spades.log\n\n    if [ -f scaffolds.fasta ]; then\n        mv scaffolds.fasta ${prefix}.scaffolds.fa\n    fi\n    if [ -f contigs.fasta ]; then\n        mv contigs.fasta ${prefix}.contigs.fa\n    fi\n    if [ -f transcripts.fasta ]; then\n        mv transcripts.fasta ${prefix}.transcripts.fa\n    fi\n    if [ -f assembly_graph_with_scaffolds.gfa ]; then\n        mv assembly_graph_with_scaffolds.gfa ${prefix}.assembly.gfa\n    fi\n\n    if [ -f gene_clusters.fasta ]; then\n        mv gene_clusters.fasta ${prefix}.gene_clusters.fa\n    fi\n\n    echo \\$(spades.py --version 2>&1) | sed 's/^.*SPAdes genome assembler v//; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 31,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads",
            "hmm"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_high'",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? 'bioconda::spades=3.15.3 python=3.9' : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/spades:3.15.3--h95f258a_0\" } else { container \"quay.io/biocontainers/spades:3.15.3--h95f258a_0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "SPADES_RESTART": {
        "name_process": "SPADES_RESTART",
        "string_process": "\nprocess SPADES_RESTART {\n    tag \"$sample\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:['id': sample], publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::spades=3.15.3 python=3.9' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/spades:3.15.3--h95f258a_0\"\n    } else {\n        container \"quay.io/biocontainers/spades:3.15.3--h95f258a_0\"\n    }\n\n    input:\n    tuple val(sample), path(saves)\n\n    output:\n    tuple val(sample), path('*.scaffolds.fa')                    , optional:true, emit: scaffolds\n    tuple val(sample), path('*.contigs.fa')                      , optional:true, emit: contigs\n    tuple val(sample), path('*.transcripts.fa')                  , optional:true, emit: transcripts\n    tuple val(sample), path('*.assembly.gfa')                    , optional:true, emit: gfa\n    tuple val(sample), path('*.log')                             , emit: log\n    path  '*.version.txt'                                        , emit: version\n\n    script:\n    def software    = getSoftwareName(task.process)\n    def prefix      = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n\n    \"\"\"\n    dirs=(\\$(ls -d -r ${saves}/K*))\n    kDir=\\$(basename \\${dirs[0]})\n\n    # Create output directories\n    mkdir ${prefix}.spades_out\n    mkdir ${prefix}.spades_out/tmp\n\n    # Run spades restart\n    config=\"${saves}/\\${kDir}/configs/config.info\"\n    mda_mode=\"${saves}/\\${kDir}/configs/mda_mode.info\"\n    rna_mode=\"${saves}/\\${kDir}/configs/rna_mode.info\"\n    spades-core \\${config} \\${mda_mode} \\${rna_mode} > ${prefix}.spades.log\n\n    # Move resulting files\n    if [ -f ${prefix}.spades_out/\\${kDir}/scaffolds.fasta ]; then\n        mv ${prefix}.spades_out/\\${kDir}/scaffolds.fasta ${prefix}.scaffolds.fa\n    fi\n    if [ -f ${prefix}.spades_out/\\${kDir}/contigs.fasta ]; then\n        mv ${prefix}.spades_out/\\${kDir}/contigs.fasta ${prefix}.contigs.fa\n    fi\n    if [ -f ${prefix}.spades_out/\\${kDir}/transcripts.fasta ]; then\n        mv ${prefix}.spades_out/\\${kDir}/transcripts.fasta ${prefix}.transcripts.fa\n    fi\n    if [ -f ${prefix}.spades_out/\\${kDir}/assembly_graph_with_scaffolds.gfa ]; then\n        mv ${prefix}.spades_out/\\${kDir}/assembly_graph_with_scaffolds.gfa ${prefix}.assembly.gfa\n    fi\n\n    # Get spades version\n    echo \\$(spades.py --version 2>&1) | sed 's/^.*SPAdes genome assembler v//; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 60,
        "string_script": "    def software    = getSoftwareName(task.process)\n    def prefix      = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n\n    \"\"\"\n    dirs=(\\$(ls -d -r ${saves}/K*))\n    kDir=\\$(basename \\${dirs[0]})\n\n    # Create output directories\n    mkdir ${prefix}.spades_out\n    mkdir ${prefix}.spades_out/tmp\n\n    # Run spades restart\n    config=\"${saves}/\\${kDir}/configs/config.info\"\n    mda_mode=\"${saves}/\\${kDir}/configs/mda_mode.info\"\n    rna_mode=\"${saves}/\\${kDir}/configs/rna_mode.info\"\n    spades-core \\${config} \\${mda_mode} \\${rna_mode} > ${prefix}.spades.log\n\n    # Move resulting files\n    if [ -f ${prefix}.spades_out/\\${kDir}/scaffolds.fasta ]; then\n        mv ${prefix}.spades_out/\\${kDir}/scaffolds.fasta ${prefix}.scaffolds.fa\n    fi\n    if [ -f ${prefix}.spades_out/\\${kDir}/contigs.fasta ]; then\n        mv ${prefix}.spades_out/\\${kDir}/contigs.fasta ${prefix}.contigs.fa\n    fi\n    if [ -f ${prefix}.spades_out/\\${kDir}/transcripts.fasta ]; then\n        mv ${prefix}.spades_out/\\${kDir}/transcripts.fasta ${prefix}.transcripts.fa\n    fi\n    if [ -f ${prefix}.spades_out/\\${kDir}/assembly_graph_with_scaffolds.gfa ]; then\n        mv ${prefix}.spades_out/\\${kDir}/assembly_graph_with_scaffolds.gfa ${prefix}.assembly.gfa\n    fi\n\n    # Get spades version\n    echo \\$(spades.py --version 2>&1) | sed 's/^.*SPAdes genome assembler v//; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 33,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "saves"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"$sample\"",
            "label 'process_high'",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:['id': sample], publish_by_meta:['id']) }",
            "conda (params.enable_conda ? 'bioconda::spades=3.15.3 python=3.9' : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/spades:3.15.3--h95f258a_0\" } else { container \"quay.io/biocontainers/spades:3.15.3--h95f258a_0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "FASTQC": {
        "name_process": "FASTQC",
        "string_process": "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}",
        "nb_lignes_process": 40,
        "string_script": "    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "FastQC"
        ],
        "tools_url": [
            "https://bio.tools/fastqc"
        ],
        "tools_dico": [
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            }
        ],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\" } else { container \"quay.io/biocontainers/fastqc:0.11.9--0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "SPADES_SAVES": {
        "name_process": "SPADES_SAVES",
        "string_process": "\nprocess SPADES_SAVES {\n    tag \"$sample\"\n    label 'process_high'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:['id': sample], publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::spades=3.15.3 python=3.9' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/spades:3.15.3--h95f258a_0\"\n    } else {\n        container \"quay.io/biocontainers/spades:3.15.3--h95f258a_0\"\n    }\n\n    input:\n    tuple val(sample), path(short_reads), path(long_reads), path(db_seq)\n\n    output:\n    tuple val(sample), path('*.spades_out')           , emit: saves\n    tuple val(sample), path('*.scaffolds.fa')         , optional:true, emit: scaffolds\n    tuple val(sample), path('*.contigs.fa')           , optional:true, emit: contigs\n    tuple val(sample), path('*.transcripts.fa')       , emit: transcripts\n    tuple val(sample), path('*.assembly.gfa')         , emit: gfa\n    tuple val(sample), path('*.grseq')                , emit: grseq\n    tuple val(sample), path('*.mpr')                  , emit: mprs\n    tuple val(sample), path('*.spades.log')           , emit: log\n    path  '*.version.txt'                             , emit: version\n\n    script:\n    def software    = getSoftwareName(task.process)\n    def prefix      = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n    def input_reads = ( short_reads.size() == 1 ) ? \"-s ${short_reads[0]}\" : \"-1 ${short_reads[0]} -2 ${short_reads[1]}\"\n    input_reads     += long_reads.isFile() ? \" --pacbio ${long_reads}\" : \"\"\n    input_reads     += db_seq.isFile() ? \" --nanopore ${db_seq}\" : \"\"\n\n    \"\"\"\n    spades.py \\\\\n        $options.args \\\\\n        --threads $task.cpus \\\\\n        ${input_reads} \\\\\n        -o ${prefix}.spades_out\n    mv ${prefix}.spades_out/spades.log ${prefix}.spades.log\n    if [ -f ${prefix}.spades_out/scaffolds.fasta ]; then\n        mv ${prefix}.spades_out/scaffolds.fasta ${prefix}.scaffolds.fa\n    fi\n    if [ -f ${prefix}.spades_out/contigs.fasta ]; then\n        mv ${prefix}.spades_out/contigs.fasta ${prefix}.contigs.fa\n    fi\n    if [ -f ${prefix}.spades_out/transcripts.fasta ]; then\n        mv ${prefix}.spades_out/transcripts.fasta ${prefix}.transcripts.fa\n    fi\n    if [ -f ${prefix}.spades_out/assembly_graph_with_scaffolds.gfa ]; then\n        mv ${prefix}.spades_out/assembly_graph_with_scaffolds.gfa ${prefix}.assembly.gfa\n    fi\n\n    dirs=(\\$(ls -d -r ${prefix}.spades_out/K*))\n    kDir=\\$(basename \\${dirs[0]})\n    if [ -f ${prefix}.spades_out/\\${kDir}/saves/distance_estimation/graph_pack.grseq ]; then\n        cp ${prefix}.spades_out/\\${kDir}/saves/distance_estimation/graph_pack.grseq ${prefix}.grseq\n    fi\n\n    cp ${prefix}.spades_out/\\${kDir}/saves/distance_estimation/graph_pack_*.mpr .\n\n    echo \\$(spades.py --version 2>&1) | sed 's/^.*SPAdes genome assembler v//; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 65,
        "string_script": "    def software    = getSoftwareName(task.process)\n    def prefix      = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n    def input_reads = ( short_reads.size() == 1 ) ? \"-s ${short_reads[0]}\" : \"-1 ${short_reads[0]} -2 ${short_reads[1]}\"\n    input_reads     += long_reads.isFile() ? \" --pacbio ${long_reads}\" : \"\"\n    input_reads     += db_seq.isFile() ? \" --nanopore ${db_seq}\" : \"\"\n\n    \"\"\"\n    spades.py \\\\\n        $options.args \\\\\n        --threads $task.cpus \\\\\n        ${input_reads} \\\\\n        -o ${prefix}.spades_out\n    mv ${prefix}.spades_out/spades.log ${prefix}.spades.log\n    if [ -f ${prefix}.spades_out/scaffolds.fasta ]; then\n        mv ${prefix}.spades_out/scaffolds.fasta ${prefix}.scaffolds.fa\n    fi\n    if [ -f ${prefix}.spades_out/contigs.fasta ]; then\n        mv ${prefix}.spades_out/contigs.fasta ${prefix}.contigs.fa\n    fi\n    if [ -f ${prefix}.spades_out/transcripts.fasta ]; then\n        mv ${prefix}.spades_out/transcripts.fasta ${prefix}.transcripts.fa\n    fi\n    if [ -f ${prefix}.spades_out/assembly_graph_with_scaffolds.gfa ]; then\n        mv ${prefix}.spades_out/assembly_graph_with_scaffolds.gfa ${prefix}.assembly.gfa\n    fi\n\n    dirs=(\\$(ls -d -r ${prefix}.spades_out/K*))\n    kDir=\\$(basename \\${dirs[0]})\n    if [ -f ${prefix}.spades_out/\\${kDir}/saves/distance_estimation/graph_pack.grseq ]; then\n        cp ${prefix}.spades_out/\\${kDir}/saves/distance_estimation/graph_pack.grseq ${prefix}.grseq\n    fi\n\n    cp ${prefix}.spades_out/\\${kDir}/saves/distance_estimation/graph_pack_*.mpr .\n\n    echo \\$(spades.py --version 2>&1) | sed 's/^.*SPAdes genome assembler v//; s/ .*\\$//' > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 35,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "short_reads",
            "long_reads",
            "db_seq"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"$sample\"",
            "label 'process_high'",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:['id': sample], publish_by_meta:['id']) }",
            "conda (params.enable_conda ? 'bioconda::spades=3.15.3 python=3.9' : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/spades:3.15.3--h95f258a_0\" } else { container \"quay.io/biocontainers/spades:3.15.3--h95f258a_0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "CLUSTERING_EVALUATION": {
        "name_process": "CLUSTERING_EVALUATION",
        "string_process": "\nprocess CLUSTERING_EVALUATION {\n    tag \"${sample}\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'metrics_out', meta:[:], publish_by_meta:[]) }\n    conda (params.enable_conda ? \"python biopython matplotlib networkx scipy pandas\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/python:3.8.3\"\n    } else {\n        container \"quay.io/biocontainers/python:3.8.3\"\n    }\n\n    input:\n    tuple val(sample), path(reconstructed_clusters), path(readable_mprs)\n\n    output:\n    tuple val(sample), path('*.clustering_metrics.txt'), emit: short_report\n\n    when:\n    readable_mprs.size() > 2\n\n    script:\n    def prefix      = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n\n    \"\"\"\n    evaluate_clustering.py          \\\\\n    ${reconstructed_clusters}       \\\\\n    ${readable_mprs[2]}             \\\\\n    ${prefix}.clustering_metrics\n\n    if [ -f ${prefix}.clustering_metrics/short_report.txt ]; then\n        mv ${prefix}.clustering_metrics/short_report.txt ${prefix}.clustering_metrics.txt\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def prefix      = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n\n    \"\"\"\n    evaluate_clustering.py          \\\\\n    ${reconstructed_clusters}       \\\\\n    ${readable_mprs[2]}             \\\\\n    ${prefix}.clustering_metrics\n\n    if [ -f ${prefix}.clustering_metrics/short_report.txt ]; then\n        mv ${prefix}.clustering_metrics/short_report.txt ${prefix}.clustering_metrics.txt\n    fi\n    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample",
            "reconstructed_clusters",
            "readable_mprs"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"${sample}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'metrics_out', meta:[:], publish_by_meta:[]) }",
            "conda (params.enable_conda ? \"python biopython matplotlib networkx scipy pandas\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/python:3.8.3\" } else { container \"quay.io/biocontainers/python:3.8.3\" }"
        ],
        "when": "readable_mprs.size() > 2",
        "stub": ""
    },
    "CONFIG_MODIFY": {
        "name_process": "CONFIG_MODIFY",
        "string_process": "\nprocess CONFIG_MODIFY {\n    tag \"${saves}\"\n    conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/python:3.8.3\"\n    } else {\n        container \"quay.io/biocontainers/python:3.8.3\"\n    }\n\n    input:\n    tuple val(sample), path(saves)\n\n    output:\n    tuple val(sample), path(saves)\n\n    script:\n    def prefix      = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n\n    \"\"\"\n    dirs=(\\$(ls -d -r ${saves}/K*))\n    kDir=\\$(basename \\${dirs[0]})\n    kDir_previous=\\$(basename \\${dirs[1]})\n\n\n    # Modify config.info\n    config=\"${saves}/\\${kDir}/configs/config.info\"\n    basename=\\$(basename \\${config})\n    ext=\\${basename##*.}\n    filename=\\${basename%.*}\n    config_old=\"${saves}/\\${kDir}/configs/\\${filename}.old.CONFIG_MODIFY.\\${ext}\"\n    cp \\${config} \\${config_old}\n    sed -i \"s|output_base.*|output_base ${prefix}.spades_out|\" \\$config\n    sed -i \"s|tmp_dir.*|tmp_dir ${prefix}.spades_out/tmp|\" \\$config\n    sed -i \"s|dataset.*|dataset ${saves}/dataset.info|\" \\$config\n    sed -i \"s|^additional_contigs.*|additional_contigs ${saves}/\\${kDir_previous}/simplified_contigs|\" \\$config\n    sed -i \"s|load_from.*|load_from ${saves}/\\${kDir}/saves|\" \\$config\n    sed -i \"s|entry_point read_conversion.*|;entry_point read_conversion|\" \\$config\n    sed -i \"s|;entry_point repeat_resolving.*|entry_point repeat_resolving|\" \\$config\n    \"\"\"\n}",
        "nb_lignes_process": 39,
        "string_script": "    def prefix      = options.suffix ? \"${sample}${options.suffix}\" : \"${sample}\"\n\n    \"\"\"\n    dirs=(\\$(ls -d -r ${saves}/K*))\n    kDir=\\$(basename \\${dirs[0]})\n    kDir_previous=\\$(basename \\${dirs[1]})\n\n\n    # Modify config.info\n    config=\"${saves}/\\${kDir}/configs/config.info\"\n    basename=\\$(basename \\${config})\n    ext=\\${basename##*.}\n    filename=\\${basename%.*}\n    config_old=\"${saves}/\\${kDir}/configs/\\${filename}.old.CONFIG_MODIFY.\\${ext}\"\n    cp \\${config} \\${config_old}\n    sed -i \"s|output_base.*|output_base ${prefix}.spades_out|\" \\$config\n    sed -i \"s|tmp_dir.*|tmp_dir ${prefix}.spades_out/tmp|\" \\$config\n    sed -i \"s|dataset.*|dataset ${saves}/dataset.info|\" \\$config\n    sed -i \"s|^additional_contigs.*|additional_contigs ${saves}/\\${kDir_previous}/simplified_contigs|\" \\$config\n    sed -i \"s|load_from.*|load_from ${saves}/\\${kDir}/saves|\" \\$config\n    sed -i \"s|entry_point read_conversion.*|;entry_point read_conversion|\" \\$config\n    sed -i \"s|;entry_point repeat_resolving.*|entry_point repeat_resolving|\" \\$config\n    \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [
            "MultiDataSet"
        ],
        "tools_url": [
            "https://bio.tools/multidataset"
        ],
        "tools_dico": [
            {
                "name": "MultiDataSet",
                "uri": "https://bio.tools/multidataset",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3295",
                            "term": "Epigenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3391",
                            "term": "Omics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3204",
                                    "term": "Methylation analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3204",
                                    "term": "Methylation profile analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Implementation of the BRGE's (Bioinformatic Research Group in Epidemiology from Center for Research in Environmental Epidemiology) dataset and MethylationSet. It is designed for integrating multi omics data sets and MethylationSet to contain normalized methylation data. These package contains base classes for MEAL and rexposome packages.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/MultiDataSet.html"
            }
        ],
        "inputs": [
            "sample",
            "saves"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sample"
        ],
        "nb_outputs": 1,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"${saves}\"",
            "conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/python:3.8.3\" } else { container \"quay.io/biocontainers/python:3.8.3\" }"
        ],
        "when": "",
        "stub": ""
    },
    "SAMPLESHEET_CHECK": {
        "name_process": "SAMPLESHEET_CHECK",
        "string_process": "\nprocess SAMPLESHEET_CHECK {\n    tag \"$samplesheet\"\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/python:3.8.3\"\n    } else {\n        container \"quay.io/biocontainers/python:3.8.3\"\n    }\n\n    input:\n    path samplesheet\n\n    output:\n    path '*.csv'\n\n    script:                                                                             \n    \"\"\"\n    check_samplesheet.py \\\\\n        $samplesheet \\\\\n        samplesheet.valid.csv\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "    \"\"\"\n    check_samplesheet.py \\\\\n        $samplesheet \\\\\n        samplesheet.valid.csv\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "samplesheet"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"$samplesheet\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }",
            "conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/python:3.8.3\" } else { container \"quay.io/biocontainers/python:3.8.3\" }"
        ],
        "when": "",
        "stub": ""
    },
    "GUNZIP": {
        "name_process": "GUNZIP",
        "string_process": "process GUNZIP {\n    tag \"$archive\"\n    label 'process_low'\n\n    conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img' :\n        'biocontainers/biocontainers:v1.2.0_cv1' }\"\n\n    input:\n    tuple val(meta), path(archive)\n\n    output:\n    tuple val(meta), path(\"$gunzip\"), emit: gunzip\n    path \"versions.yml\"             , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    gunzip = archive.toString() - '.gz'\n    \"\"\"\n    gunzip \\\\\n        -f \\\\\n        $args \\\\\n        $archive\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gunzip: \\$(echo \\$(gunzip --version 2>&1) | sed 's/^.*(gzip) //; s/ Copyright.*\\$//')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "    def args = task.ext.args ?: ''\n    gunzip = archive.toString() - '.gz'\n    \"\"\"\n    gunzip \\\\\n        -f \\\\\n        $args \\\\\n        $archive\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        gunzip: \\$(echo \\$(gunzip --version 2>&1) | sed 's/^.*(gzip) //; s/ Copyright.*\\$//')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "archive"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "letovesnoi__clusterassembly",
        "directive": [
            "tag \"$archive\"",
            "label 'process_low'",
            "conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img' : 'biocontainers/biocontainers:v1.2.0_cv1' }\""
        ],
        "when": "task.ext.when == null || task.ext.when",
        "stub": ""
    }
}