{
    "combine_ariba_summaries": {
        "name_process": "combine_ariba_summaries",
        "string_process": "process combine_ariba_summaries{\n    tag {'combine_ariba_summaries'}\n    publishDir \"${params.output_dir}/ariba\", mode: 'copy'\n\n    input:\n    file(summary_files)\n\n    output:\n    path 'combined_summary.csv'\n\n    script:\n    \"\"\"\n    # combine headers\n    echo \"\\$(head -1 ${summary_files[0]}),known_variants,\\$(head -1 ${summary_files[1]} | cut -d',' -f2-)\" > combined_summary.csv\n    # remove headers and sort\n    mkdir sorted_summaries\n    tail -n +2 ${summary_files[0]} | sort > sorted_summaries/${summary_files[0]}\n    # add blank column to 2nd file to add in known_variants column seperator\n    tail -n +2 ${summary_files[1]} | sort | awk -F, '\\$1=FS\\$1' OFS=, > sorted_summaries/${summary_files[1]}\n    # join sorted body text\n    join -t , -2 2 sorted_summaries/${summary_files[0]} sorted_summaries/${summary_files[1]} >> combined_summary.csv\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "    \"\"\"\n    # combine headers\n    echo \"\\$(head -1 ${summary_files[0]}),known_variants,\\$(head -1 ${summary_files[1]} | cut -d',' -f2-)\" > combined_summary.csv\n    # remove headers and sort\n    mkdir sorted_summaries\n    tail -n +2 ${summary_files[0]} | sort > sorted_summaries/${summary_files[0]}\n    # add blank column to 2nd file to add in known_variants column seperator\n    tail -n +2 ${summary_files[1]} | sort | awk -F, '\\$1=FS\\$1' OFS=, > sorted_summaries/${summary_files[1]}\n    # join sorted body text\n    join -t , -2 2 sorted_summaries/${summary_files[0]} sorted_summaries/${summary_files[1]} >> combined_summary.csv\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [
            "joineRML"
        ],
        "tools_url": [
            "https://bio.tools/joinerml"
        ],
        "tools_dico": [
            {
                "name": "joineRML",
                "uri": "https://bio.tools/joinerml",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3474",
                            "term": "Machine learning"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3569",
                            "term": "Applied mathematics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Joint Modelling of Multivariate Longitudinal Data and Time-to-Event Outcomes.",
                "homepage": "https://cran.r-project.org/web/packages/joineRML/"
            }
        ],
        "inputs": [
            "summary_files"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "antunderwood__amr_prediction",
        "directive": [
            "tag {'combine_ariba_summaries'}",
            "publishDir \"${params.output_dir}/ariba\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "run_ariba": {
        "name_process": "run_ariba",
        "string_process": " process run_ariba {\n  tag {sample_id}\n  publishDir \"${params.output_dir}/ariba\",\n  mode: 'copy'\n\n   input:\n   tuple sample_id, file(reads)\n   file(database_dir)\n\n   output:\n   file(\"${sample_id}_${database_dir}.report.csv\")\n\n   \"\"\"\n   ariba run ${database_dir} ${reads[0]} ${reads[1]} ${sample_id}.ariba\n   mv ${sample_id}.ariba/report.tsv ${sample_id}_${database_dir}.report.csv\n   \"\"\"\n }",
        "nb_lignes_process": 15,
        "string_script": "\"\"\"\n   ariba run ${database_dir} ${reads[0]} ${reads[1]} ${sample_id}.ariba\n   mv ${sample_id}.ariba/report.tsv ${sample_id}_${database_dir}.report.csv\n   \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "reads",
            "sample_id",
            "database_dir"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "antunderwood__amr_prediction",
        "directive": [
            "tag {sample_id}",
            "publishDir \"${params.output_dir}/ariba\" , mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "ariba_summary": {
        "name_process": "ariba_summary",
        "string_process": " process ariba_summary {\n  tag {'ariba summary'}\n  publishDir \"${params.output_dir}/ariba\", mode: 'copy'\n\n  input:\n  file(report_tsvs)\n  file(database_dir)\n  val summary_arguments\n\n  output:\n  path \"ariba_${database_dir}_summary.csv\", emit: summary_csv\n  path \"ariba_${database_dir}_summary.phandango.*\", emit: phandango_files\n\n  script:\n  \"\"\"\n  mkdir renamed_reports\n  for report_tsv in ${report_tsvs}; do\n    mv \\$report_tsv renamed_reports/\\${report_tsv%_${database_dir}.report.csv}\n  done\n  renamed_report_files=\\$(ls renamed_reports)\n  mv renamed_reports/* .\n  ariba summary ${summary_arguments} ariba_${database_dir}_summary \\$renamed_report_files\n  \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "  \"\"\"\n  mkdir renamed_reports\n  for report_tsv in ${report_tsvs}; do\n    mv \\$report_tsv renamed_reports/\\${report_tsv%_${database_dir}.report.csv}\n  done\n  renamed_report_files=\\$(ls renamed_reports)\n  mv renamed_reports/* .\n  ariba summary ${summary_arguments} ariba_${database_dir}_summary \\$renamed_report_files\n  \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "report_tsvs",
            "database_dir",
            "summary_arguments"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "antunderwood__amr_prediction",
        "directive": [
            "tag {'ariba summary'}",
            "publishDir \"${params.output_dir}/ariba\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "determine_min_read_length": {
        "name_process": "determine_min_read_length",
        "string_process": "\nprocess determine_min_read_length {\n  tag { pair_id }\n\n  input:\n  tuple pair_id, file(file_pair)\n\n  output:\n  tuple pair_id, stdout\n\n  script:\n  \"\"\"\n  gzip -cd ${file_pair[0]} | head -n 400000 | printf \"%.0f\" \\$(awk 'NR%4==2{sum+=length(\\$0)}END{print sum/(NR/4)}')\n  \"\"\"\n}",
        "nb_lignes_process": 13,
        "string_script": "  \"\"\"\n  gzip -cd ${file_pair[0]} | head -n 400000 | printf \"%.0f\" \\$(awk 'NR%4==2{sum+=length(\\$0)}END{print sum/(NR/4)}')\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "file_pair",
            "pair_id"
        ],
        "nb_inputs": 2,
        "outputs": [
            "pair_id"
        ],
        "nb_outputs": 1,
        "name_workflow": "antunderwood__amr_prediction",
        "directive": [
            "tag { pair_id }"
        ],
        "when": "",
        "stub": ""
    },
    "trim_reads": {
        "name_process": "trim_reads",
        "string_process": "\nprocess trim_reads {\n  memory '4 GB'\n  \n  tag { pair_id }\n  \n  input:\n  tuple pair_id, file(file_pair), min_read_length\n  file('adapter_file.fas')\n\n  output:\n  tuple pair_id, file('trimmed_fastqs/*.f*q.gz')\n\n  script:\n  shortest_read_length_to_keep = (min_read_length.toInteger()/3).toInteger()\n  \"\"\"\n  mkdir trimmed_fastqs\n  trimmomatic PE -threads 1 -phred33 ${file_pair[0]} ${file_pair[1]} trimmed_fastqs/${file_pair[0]} /dev/null trimmed_fastqs/${file_pair[1]} /dev/null ILLUMINACLIP:adapter_file.fas:2:30:10 SLIDINGWINDOW:4:20 LEADING:25 TRAILING:25 MINLEN:${shortest_read_length_to_keep}  \n  \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "  shortest_read_length_to_keep = (min_read_length.toInteger()/3).toInteger()\n  \"\"\"\n  mkdir trimmed_fastqs\n  trimmomatic PE -threads 1 -phred33 ${file_pair[0]} ${file_pair[1]} trimmed_fastqs/${file_pair[0]} /dev/null trimmed_fastqs/${file_pair[1]} /dev/null ILLUMINACLIP:adapter_file.fas:2:30:10 SLIDINGWINDOW:4:20 LEADING:25 TRAILING:25 MINLEN:${shortest_read_length_to_keep}  \n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "Trimmomatic"
        ],
        "tools_url": [
            "https://bio.tools/trimmomatic"
        ],
        "tools_dico": [
            {
                "name": "Trimmomatic",
                "uri": "https://bio.tools/trimmomatic",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Sequence trimming"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Trimming"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0006",
                                "term": "Data"
                            },
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0006",
                                "term": "Data"
                            }
                        ]
                    }
                ],
                "description": "A flexible read trimming tool for Illumina NGS data",
                "homepage": "http://www.usadellab.org/cms/index.php?page=trimmomatic"
            }
        ],
        "inputs": [
            "file_pair",
            "pair_id"
        ],
        "nb_inputs": 2,
        "outputs": [
            "pair_id"
        ],
        "nb_outputs": 1,
        "name_workflow": "antunderwood__amr_prediction",
        "directive": [
            "memory '4 GB'",
            "tag { pair_id }"
        ],
        "when": "",
        "stub": ""
    },
    "estimate_genome_size": {
        "name_process": "estimate_genome_size",
        "string_process": "\nprocess estimate_genome_size {\n  tag { pair_id }\n  \n  input:\n  tuple pair_id, file(file_pair)\n\n  output:\n  tuple pair_id, file('mash_stats.out')\n\n  \"\"\"\n  kat hist --mer_len 21  --thread 1 --output_prefix ${pair_id} ${file_pair[0]} > /dev/null 2>&1 \\\n  && minima=`cat  ${pair_id}.dist_analysis.json | jq '.global_minima .freq' | tr -d '\\\\n'`\n  mash sketch -o sketch_${pair_id}  -k 32 -m \\$minima -r ${file_pair[0]}  2> mash_stats.out\n  \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "\"\"\"\n  kat hist --mer_len 21  --thread 1 --output_prefix ${pair_id} ${file_pair[0]} > /dev/null 2>&1 \\\n  && minima=`cat  ${pair_id}.dist_analysis.json | jq '.global_minima .freq' | tr -d '\\\\n'`\n  mash sketch -o sketch_${pair_id}  -k 32 -m \\$minima -r ${file_pair[0]}  2> mash_stats.out\n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "KAT",
            "NullSeq",
            "Mash"
        ],
        "tools_url": [
            "https://bio.tools/kat",
            "https://bio.tools/nullseq",
            "https://bio.tools/mash"
        ],
        "tools_dico": [
            {
                "name": "KAT",
                "uri": "https://bio.tools/kat",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0253",
                                    "term": "Sequence feature detection"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0253",
                                    "term": "Sequence feature recognition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0253",
                                    "term": "Sequence feature prediction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Suite of tools that generate, analyse and compare k-mer spectra produced from sequence files",
                "homepage": "http://www.earlham.ac.uk/kat-tools"
            },
            {
                "name": "NullSeq",
                "uri": "https://bio.tools/nullseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0364",
                                    "term": "Random sequence generation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Creates Random Coding Sequences with specified GC content and Amino Acid usage.",
                "homepage": "https://github.com/amarallab/NullSeq"
            },
            {
                "name": "Mash",
                "uri": "https://bio.tools/mash",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2533",
                            "term": "DNA mutation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix generation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Phylogenetic distance matrix generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0289",
                                    "term": "Sequence distance matrix construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Fast genome and metagenome distance estimation using MinHash.",
                "homepage": "https://github.com/marbl/mash"
            }
        ],
        "inputs": [
            "file_pair",
            "pair_id"
        ],
        "nb_inputs": 2,
        "outputs": [
            "pair_id"
        ],
        "nb_outputs": 1,
        "name_workflow": "antunderwood__amr_prediction",
        "directive": [
            "tag { pair_id }"
        ],
        "when": "",
        "stub": ""
    },
    "correct_reads": {
        "name_process": "correct_reads",
        "string_process": "\nprocess correct_reads {\n  tag { pair_id }\n  \n  input:\n  tuple pair_id, file(file_pair), genome_size\n\n  output:\n  tuple pair_id, file('corrected_fastqs/*.fastq.gz')\n  tuple pair_id, file('lighter.out') \n\n  script:\n  \"\"\"\n  lighter -od corrected_fastqs -r  ${file_pair[0]} -r  ${file_pair[1]} -K 32 ${genome_size}  -maxcor 1 2> lighter.out\n  for file in corrected_fastqs/*.cor.fq.gz\n  do\n      new_file=\\${file%.cor.fq.gz}.fastq.gz\n      mv \\${file} \\${new_file}\n  done\n  \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "  \"\"\"\n  lighter -od corrected_fastqs -r  ${file_pair[0]} -r  ${file_pair[1]} -K 32 ${genome_size}  -maxcor 1 2> lighter.out\n  for file in corrected_fastqs/*.cor.fq.gz\n  do\n      new_file=\\${file%.cor.fq.gz}.fastq.gz\n      mv \\${file} \\${new_file}\n  done\n  \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [
            "Lighter"
        ],
        "tools_url": [
            "https://bio.tools/lighter"
        ],
        "tools_dico": [
            {
                "name": "Lighter",
                "uri": "https://bio.tools/lighter",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Whole genome sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Genome sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "WGS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3195",
                                    "term": "Sequencing error detection"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3195",
                                    "term": "Short-read error correction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3195",
                                    "term": "Short read error correction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Kmer-based error correction method for whole genome sequencing data. Lighter uses sampling (rather than counting) to obtain a set of kmers that are likely from the genome. Using this information, Lighter can correct the reads containing sequence errors.",
                "homepage": "https://github.com/mourisl/Lighter"
            }
        ],
        "inputs": [
            "file_pair",
            "pair_id"
        ],
        "nb_inputs": 2,
        "outputs": [
            "pair_id",
            "pair_id"
        ],
        "nb_outputs": 2,
        "name_workflow": "antunderwood__amr_prediction",
        "directive": [
            "tag { pair_id }"
        ],
        "when": "",
        "stub": ""
    },
    "downsample_reads": {
        "name_process": "downsample_reads",
        "string_process": "\nprocess downsample_reads {\n  tag { pair_id }\n\n  input:\n  tuple pair_id, file(file_pair), read_depth\n  val depth_cutoff\n\n  output:\n  tuple pair_id, file(\"output_fastqs/*.fastq.gz\")\n\n  script:\n  if ( depth_cutoff && read_depth > depth_cutoff.toInteger()){\n    downsampling_factor = depth_cutoff.toInteger()/read_depth\n    \"\"\"\n    mkdir output_fastqs\n    seqtk sample -s 12345 ${file_pair[0]} ${downsampling_factor} | gzip > output_fastqs/${pair_id}_1.fastq.gz\n    seqtk sample -s 12345 ${file_pair[1]} ${downsampling_factor} | gzip > output_fastqs/${pair_id}_2.fastq.gz\n    \"\"\"\n\n  } else {\n                 \n    \"\"\"\n    mkdir output_fastqs\n    ln -s \\$(readlink -f ${file_pair[0]}) output_fastqs/${pair_id}_1.fastq.gz\n    ln -s \\$(readlink -f ${file_pair[1]}) output_fastqs/${pair_id}_2.fastq.gz\n    \"\"\"\n  }\n}",
        "nb_lignes_process": 27,
        "string_script": "  if ( depth_cutoff && read_depth > depth_cutoff.toInteger()){\n    downsampling_factor = depth_cutoff.toInteger()/read_depth\n    \"\"\"\n    mkdir output_fastqs\n    seqtk sample -s 12345 ${file_pair[0]} ${downsampling_factor} | gzip > output_fastqs/${pair_id}_1.fastq.gz\n    seqtk sample -s 12345 ${file_pair[1]} ${downsampling_factor} | gzip > output_fastqs/${pair_id}_2.fastq.gz\n    \"\"\"\n\n  } else {\n                 \n    \"\"\"\n    mkdir output_fastqs\n    ln -s \\$(readlink -f ${file_pair[0]}) output_fastqs/${pair_id}_1.fastq.gz\n    ln -s \\$(readlink -f ${file_pair[1]}) output_fastqs/${pair_id}_2.fastq.gz\n    \"\"\"\n  }",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "seqtk"
        ],
        "tools_url": [
            "https://bio.tools/seqtk"
        ],
        "tools_dico": [
            {
                "name": "seqtk",
                "uri": "https://bio.tools/seqtk",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Biological databases"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Data management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Information systems"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Databases and information systems"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Data handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2121",
                                    "term": "Sequence file editing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Utility operation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File processing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Report handling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A tool for processing sequences in the FASTA or FASTQ format. It parses both FASTA and FASTQ files which can also be optionally compressed by gzip.",
                "homepage": "https://github.com/lh3/seqtk"
            }
        ],
        "inputs": [
            "file_pair",
            "pair_id",
            "depth_cutoff"
        ],
        "nb_inputs": 3,
        "outputs": [
            "pair_id"
        ],
        "nb_outputs": 1,
        "name_workflow": "antunderwood__amr_prediction",
        "directive": [
            "tag { pair_id }"
        ],
        "when": "",
        "stub": ""
    }
}