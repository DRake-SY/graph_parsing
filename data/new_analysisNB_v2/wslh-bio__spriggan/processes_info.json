{
    "preProcess": {
        "name_process": "preProcess",
        "string_process": "\nprocess preProcess {\n  input:\n  set val(name), file(reads) from raw_reads\n\n  output:\n  tuple name, file(outfiles) into read_files_fastqc, read_files_trimming\n\n  script:\n  if(params.name_split_on!=\"\"){\n    name = name.split(params.name_split_on)[0]\n    outfiles = [\"${name}_R1.fastq.gz\",\"${name}_R2.fastq.gz\"]\n    \"\"\"\n    mv ${reads[0]} ${name}_R1.fastq.gz\n    mv ${reads[1]} ${name}_R2.fastq.gz\n    \"\"\"\n  }else{\n    outfiles = reads\n    \"\"\"\n    \"\"\"\n  }\n}",
        "nb_lignes_process": 20,
        "string_script": "  if(params.name_split_on!=\"\"){\n    name = name.split(params.name_split_on)[0]\n    outfiles = [\"${name}_R1.fastq.gz\",\"${name}_R2.fastq.gz\"]\n    \"\"\"\n    mv ${reads[0]} ${name}_R1.fastq.gz\n    mv ${reads[1]} ${name}_R2.fastq.gz\n    \"\"\"\n  }else{\n    outfiles = reads\n    \"\"\"\n    \"\"\"\n  }",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "goname"
        ],
        "tools_url": [
            "https://bio.tools/goname"
        ],
        "tools_dico": [
            {
                "name": "goname",
                "uri": "https://bio.tools/goname",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0089",
                            "term": "Ontology and terminology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data retrieval"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data extraction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Retrieval"
                                }
                            ]
                        ],
                        "input": [],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2223",
                                "term": "Ontology metadata"
                            }
                        ]
                    }
                ],
                "description": "Find GO ontology terms by name.",
                "homepage": "http://emboss.open-bio.org/rel/rel6/apps/goname.html"
            }
        ],
        "inputs": [
            "raw_reads"
        ],
        "nb_inputs": 1,
        "outputs": [
            "read_files_fastqc",
            "read_files_trimming"
        ],
        "nb_outputs": 2,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "clean_reads": {
        "name_process": "clean_reads",
        "string_process": "\nprocess clean_reads {\n  tag \"$name\"\n  publishDir \"${params.outdir}/trimming\", mode: 'copy',pattern:\"*.trim.txt\"\n\n  input:\n  set val(name), file(reads) from read_files_trimming\n\n  output:\n  tuple name, file(\"${name}_clean{_1,_2}.fastq.gz\") into cleaned_reads_shovill, cleaned_reads_fastqc, read_files_kraken\n  file(\"${name}.phix.stats.txt\") into phix_cleanning_stats\n  file(\"${name}.adapters.stats.txt\") into adapter_cleanning_stats\n  file(\"${name}.trim.txt\") into bbduk_files\n  tuple file(\"${name}.phix.stats.txt\"),file(\"${name}.adapters.stats.txt\"),file(\"${name}.trim.txt\") into multiqc_clean_reads\n\n  script:\n  \"\"\"\n  bbduk.sh in1=${reads[0]} in2=${reads[1]} out1=${name}.trimmed_1.fastq.gz out2=${name}.trimmed_2.fastq.gz qtrim=${params.trimdirection} qtrim=${params.qualitytrimscore} minlength=${params.minlength} tbo tbe &> ${name}.out\n  repair.sh in1=${name}.trimmed_1.fastq.gz in2=${name}.trimmed_2.fastq.gz out1=${name}.paired_1.fastq.gz out2=${name}.paired_2.fastq.gz\n  bbduk.sh in1=${name}.paired_1.fastq.gz in2=${name}.paired_2.fastq.gz out1=${name}.rmadpt_1.fastq.gz out2=${name}.rmadpt_2.fastq.gz ref=/bbmap/resources/adapters.fa stats=${name}.adapters.stats.txt ktrim=r k=23 mink=11 hdist=1 tpe tbo\n  bbduk.sh in1=${name}.rmadpt_1.fastq.gz in2=${name}.rmadpt_2.fastq.gz out1=${name}_clean_1.fastq.gz out2=${name}_clean_2.fastq.gz outm=${name}.matched_phix.fq ref=/bbmap/resources/phix174_ill.ref.fa.gz k=31 hdist=1 stats=${name}.phix.stats.txt\n  grep -E 'Input:|QTrimmed:|Trimmed by overlap:|Total Removed:|Result:' ${name}.out > ${name}.trim.txt\n  \"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "  \"\"\"\n  bbduk.sh in1=${reads[0]} in2=${reads[1]} out1=${name}.trimmed_1.fastq.gz out2=${name}.trimmed_2.fastq.gz qtrim=${params.trimdirection} qtrim=${params.qualitytrimscore} minlength=${params.minlength} tbo tbe &> ${name}.out\n  repair.sh in1=${name}.trimmed_1.fastq.gz in2=${name}.trimmed_2.fastq.gz out1=${name}.paired_1.fastq.gz out2=${name}.paired_2.fastq.gz\n  bbduk.sh in1=${name}.paired_1.fastq.gz in2=${name}.paired_2.fastq.gz out1=${name}.rmadpt_1.fastq.gz out2=${name}.rmadpt_2.fastq.gz ref=/bbmap/resources/adapters.fa stats=${name}.adapters.stats.txt ktrim=r k=23 mink=11 hdist=1 tpe tbo\n  bbduk.sh in1=${name}.rmadpt_1.fastq.gz in2=${name}.rmadpt_2.fastq.gz out1=${name}_clean_1.fastq.gz out2=${name}_clean_2.fastq.gz outm=${name}.matched_phix.fq ref=/bbmap/resources/phix174_ill.ref.fa.gz k=31 hdist=1 stats=${name}.phix.stats.txt\n  grep -E 'Input:|QTrimmed:|Trimmed by overlap:|Total Removed:|Result:' ${name}.out > ${name}.trim.txt\n  \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "totalVI"
        ],
        "tools_url": [
            "https://bio.tools/totalVI"
        ],
        "tools_dico": [
            {
                "name": "totalVI",
                "uri": "https://bio.tools/totalVI",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "Gene transcripts"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2229",
                            "term": "Cell biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0108",
                            "term": "Protein expression"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0166",
                            "term": "Protein structural motifs and surfaces"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "mRNA features"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0166",
                            "term": "Protein 3D motifs"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3435",
                                    "term": "Standardisation and normalisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3630",
                                    "term": "Protein quantification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2495",
                                    "term": "Expression data analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3630",
                                    "term": "Protein quantitation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A Joint Model of RNA Expression and Surface Protein Abundance in Single Cells.",
                "homepage": "http://github.com/adamgayoso/totalVI_reproducibility"
            }
        ],
        "inputs": [
            "read_files_trimming"
        ],
        "nb_inputs": 1,
        "outputs": [
            "cleaned_reads_shovill",
            "cleaned_reads_fastqc",
            "read_files_kraken",
            "phix_cleanning_stats",
            "adapter_cleanning_stats",
            "bbduk_files",
            "multiqc_clean_reads"
        ],
        "nb_outputs": 7,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "tag \"$name\"",
            "publishDir \"${params.outdir}/trimming\", mode: 'copy',pattern:\"*.trim.txt\""
        ],
        "when": "",
        "stub": ""
    },
    "fastqc": {
        "name_process": "fastqc",
        "string_process": "\nprocess fastqc {\n  tag \"$name\"\n  publishDir \"${params.outdir}/fastqc\", mode: 'copy',saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}\n\n  input:\n  set val(name), file(reads) from combined_reads\n\n  output:\n  file(\"*_fastqc.{zip,html}\") into fastqc_results, fastqc_multiqc\n\n  script:\n  \"\"\"\n  fastqc -q  ${reads}\n  \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "  \"\"\"\n  fastqc -q  ${reads}\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "FastQC"
        ],
        "tools_url": [
            "https://bio.tools/fastqc"
        ],
        "tools_dico": [
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            }
        ],
        "inputs": [
            "combined_reads"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fastqc_results",
            "fastqc_multiqc"
        ],
        "nb_outputs": 2,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "tag \"$name\"",
            "publishDir \"${params.outdir}/fastqc\", mode: 'copy',saveAs: {filename -> filename.indexOf(\".zip\") > 0 ? \"zips/$filename\" : \"$filename\"}"
        ],
        "when": "",
        "stub": ""
    },
    "fastqc_summary": {
        "name_process": "fastqc_summary",
        "string_process": "\nprocess fastqc_summary {\n  publishDir \"${params.outdir}/fastqc\", mode: 'copy'\n\n  input:\n  file(fastqc) from fastqc_results.collect()\n\n  output:\n  file(\"fastqc_summary.tsv\") into fastqc_summary\n\n  shell:\n  \"\"\"\n  zips=`ls *.zip`\n\n  for i in \\$zips; do\n      unzip -o \\$i &>/dev/null;\n  done\n\n  fq_folders=\\${zips}\n\n  for folder in \\$fq_folders; do\n    folder=\\${folder%.*}\n    cat \\$folder/summary.txt >> fastqc_summary.tsv\n    ls .\n  done;\n\n  sed -i 's/.fastq.gz//g' fastqc_summary.tsv\n  \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "  \"\"\"\n  zips=`ls *.zip`\n\n  for i in \\$zips; do\n      unzip -o \\$i &>/dev/null;\n  done\n\n  fq_folders=\\${zips}\n\n  for folder in \\$fq_folders; do\n    folder=\\${folder%.*}\n    cat \\$folder/summary.txt >> fastqc_summary.tsv\n    ls .\n  done;\n\n  sed -i 's/.fastq.gz//g' fastqc_summary.tsv\n  \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [
            "NullSeq"
        ],
        "tools_url": [
            "https://bio.tools/nullseq"
        ],
        "tools_dico": [
            {
                "name": "NullSeq",
                "uri": "https://bio.tools/nullseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0364",
                                    "term": "Random sequence generation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Creates Random Coding Sequences with specified GC content and Amino Acid usage.",
                "homepage": "https://github.com/amarallab/NullSeq"
            }
        ],
        "inputs": [
            "fastqc_results"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fastqc_summary"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "publishDir \"${params.outdir}/fastqc\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "shovill": {
        "name_process": "shovill",
        "string_process": "\nprocess shovill {\n  errorStrategy 'ignore'\n  tag \"$name\"\n\n  publishDir \"${params.outdir}/assembled\", mode: 'copy',pattern:\"*.fa\"\n  publishDir \"${params.outdir}/alignments\", mode: 'copy',pattern:\"*.sam\"\n\n  input:\n  set val(name), file(reads) from cleaned_reads_shovill\n\n  output:\n  tuple name, file(\"${name}.contigs.fa\") into assembled_genomes_quality, assembled_genomes_ar, assembled_genomes_mlst\n  tuple name, file(\"${name}.sam\") into sam_files\n\n  script:\n  \"\"\"\n  shovill --cpus ${task.cpus} --ram ${task.memory} --outdir ./output --R1 ${reads[0]} --R2 ${reads[1]} --force\n  mv ./output/contigs.fa ${name}.contigs.fa\n  bwa index ${name}.contigs.fa\n  bwa mem ${name}.contigs.fa ${reads[0]} ${reads[1]} > ${name}.sam\n  \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "  \"\"\"\n  shovill --cpus ${task.cpus} --ram ${task.memory} --outdir ./output --R1 ${reads[0]} --R2 ${reads[1]} --force\n  mv ./output/contigs.fa ${name}.contigs.fa\n  bwa index ${name}.contigs.fa\n  bwa mem ${name}.contigs.fa ${reads[0]} ${reads[1]} > ${name}.sam\n  \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "shovill",
            "BWA"
        ],
        "tools_url": [
            "https://bio.tools/shovill",
            "https://bio.tools/bwa"
        ],
        "tools_dico": [
            {
                "name": "shovill",
                "uri": "https://bio.tools/shovill",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            }
                        ]
                    }
                ],
                "description": "Shovill is a pipeline for assembly of bacterial isolate genomes from Illumina paired-end reads.  Shovill uses SPAdes at its core, but alters the steps before and after the primary assembly step to get similar results in less time. Shovill also supports other assemblers like SKESA, Velvet and Megahit, so you can take advantage of the pre- and post-processing the Shovill provides with those too.",
                "homepage": "https://github.com/tseemann/shovill"
            },
            {
                "name": "BWA",
                "uri": "https://bio.tools/bwa",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3211",
                                    "term": "Genome indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3429",
                                    "term": "Construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_2012",
                                "term": "Sequence coordinates"
                            },
                            {
                                "uri": "http://edamontology.org/data_1916",
                                "term": "Alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_3210",
                                "term": "Genome index"
                            }
                        ]
                    }
                ],
                "description": "Fast, accurate, memory-efficient aligner for short and long sequencing reads",
                "homepage": "http://bio-bwa.sourceforge.net"
            }
        ],
        "inputs": [
            "cleaned_reads_shovill"
        ],
        "nb_inputs": 1,
        "outputs": [
            "assembled_genomes_quality",
            "assembled_genomes_ar",
            "assembled_genomes_mlst",
            "sam_files"
        ],
        "nb_outputs": 4,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "errorStrategy 'ignore'",
            "tag \"$name\"",
            "publishDir \"${params.outdir}/assembled\", mode: 'copy',pattern:\"*.fa\"",
            "publishDir \"${params.outdir}/alignments\", mode: 'copy',pattern:\"*.sam\""
        ],
        "when": "",
        "stub": ""
    },
    "samtools": {
        "name_process": "samtools",
        "string_process": "\nprocess samtools {\n  tag \"$name\"\n\n  publishDir \"${params.outdir}/alignments\", mode: 'copy',pattern:\"*.sorted.*\"\n  publishDir \"${params.outdir}/alignments\", mode: 'copy', pattern:\"*.stats.txt*\"\n  publishDir \"${params.outdir}/coverage\", mode: 'copy', pattern:\"*.depth.tsv*\"\n\n  input:\n  set val(name), file(sam) from sam_files\n\n  output:\n  file(\"${name}.depth.tsv\") into cov_files\n  file(\"${name}.stats.txt\") into stats_multiqc\n  file(\"*.sorted.*\")\n\n  shell:\n  \"\"\"\n  samtools view -S -b ${name}.sam > ${name}.bam\n  samtools sort ${name}.bam > ${name}.sorted.bam\n  samtools index ${name}.sorted.bam\n  samtools depth -a ${name}.sorted.bam > ${name}.depth.tsv\n  samtools stats ${name}.sorted.bam > ${name}.stats.txt\n  \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "  \"\"\"\n  samtools view -S -b ${name}.sam > ${name}.bam\n  samtools sort ${name}.bam > ${name}.sorted.bam\n  samtools index ${name}.sorted.bam\n  samtools depth -a ${name}.sorted.bam > ${name}.depth.tsv\n  samtools stats ${name}.sorted.bam > ${name}.stats.txt\n  \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "sam_files"
        ],
        "nb_inputs": 1,
        "outputs": [
            "cov_files",
            "stats_multiqc"
        ],
        "nb_outputs": 2,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "tag \"$name\"",
            "publishDir \"${params.outdir}/alignments\", mode: 'copy',pattern:\"*.sorted.*\"",
            "publishDir \"${params.outdir}/alignments\", mode: 'copy', pattern:\"*.stats.txt*\"",
            "publishDir \"${params.outdir}/coverage\", mode: 'copy', pattern:\"*.depth.tsv*\""
        ],
        "when": "",
        "stub": ""
    },
    "coverage_stats": {
        "name_process": "coverage_stats",
        "string_process": "\nprocess coverage_stats {\n  publishDir \"${params.outdir}/coverage\", mode: 'copy'\n\n  input:\n  file(cov) from cov_files.collect()\n\n  output:\n  file('coverage_stats.tsv') into coverage_tsv\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import glob\n  import os\n  from numpy import median\n  from numpy import average\n\n  # function for summarizing samtools depth files\n  def summarize_depth(file):\n      # get sample id from file name and set up data list\n      sid = os.path.basename(file).split('.')[0]\n      data = []\n      # open samtools depth file and get depth\n      with open(file,'r') as inFile:\n          for line in inFile:\n              data.append(int(line.strip().split()[2]))\n      # get median and average depth\n      med = int(median(data))\n      avg = int(average(data))\n      # return sample id, median and average depth\n      result = f\"{sid}\\\\t{med}\\\\t{avg}\\\\n\"\n      return result\n\n  # get all samtools depth files\n  files = glob.glob(\"*.depth.tsv\")\n\n  # summarize samtools depth files\n  results = map(summarize_depth,files)\n\n  # write results to file\n  with open('coverage_stats.tsv', 'w') as outFile:\n      outFile.write(\"Sample\\\\tMedian Coverage\\\\tAverage Coverage\\\\n\")\n      for result in results:\n          outFile.write(result)\n  \"\"\"\n}",
        "nb_lignes_process": 45,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n  import glob\n  import os\n  from numpy import median\n  from numpy import average\n\n  # function for summarizing samtools depth files\n  def summarize_depth(file):\n      # get sample id from file name and set up data list\n      sid = os.path.basename(file).split('.')[0]\n      data = []\n      # open samtools depth file and get depth\n      with open(file,'r') as inFile:\n          for line in inFile:\n              data.append(int(line.strip().split()[2]))\n      # get median and average depth\n      med = int(median(data))\n      avg = int(average(data))\n      # return sample id, median and average depth\n      result = f\"{sid}\\\\t{med}\\\\t{avg}\\\\n\"\n      return result\n\n  # get all samtools depth files\n  files = glob.glob(\"*.depth.tsv\")\n\n  # summarize samtools depth files\n  results = map(summarize_depth,files)\n\n  # write results to file\n  with open('coverage_stats.tsv', 'w') as outFile:\n      outFile.write(\"Sample\\\\tMedian Coverage\\\\tAverage Coverage\\\\n\")\n      for result in results:\n          outFile.write(result)\n  \"\"\"",
        "nb_lignes_script": 34,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "cov_files"
        ],
        "nb_inputs": 1,
        "outputs": [
            "coverage_tsv"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "publishDir \"${params.outdir}/coverage\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "quast": {
        "name_process": "quast",
        "string_process": "\nprocess quast {\n  tag \"$name\"\n\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/quast\",mode:'copy',pattern: \"${name}.quast.tsv\"\n\n  input:\n  set val(name), file(assembly) from assembled_genomes_quality\n\n  output:\n  file(\"${name}.quast.tsv\") into quast_files\n  file(\"${name}.report.quast.tsv\") into quast_multiqc\n\n  script:\n  \"\"\"\n  quast.py ${assembly} -o .\n  mv report.tsv ${name}.report.quast.tsv\n  mv transposed_report.tsv ${name}.quast.tsv\n  \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "  \"\"\"\n  quast.py ${assembly} -o .\n  mv report.tsv ${name}.report.quast.tsv\n  mv transposed_report.tsv ${name}.quast.tsv\n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "assembled_genomes_quality"
        ],
        "nb_inputs": 1,
        "outputs": [
            "quast_files",
            "quast_multiqc"
        ],
        "nb_outputs": 2,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "tag \"$name\"",
            "errorStrategy 'ignore'",
            "publishDir \"${params.outdir}/quast\",mode:'copy',pattern: \"${name}.quast.tsv\""
        ],
        "when": "",
        "stub": ""
    },
    "quast_summary": {
        "name_process": "quast_summary",
        "string_process": "\nprocess quast_summary {\n  publishDir \"${params.outdir}/quast\",mode:'copy'\n\n  input:\n  file(files) from quast_files.collect()\n\n  output:\n  file(\"quast_results.tsv\") into quast_tsv\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n\n  # function for summarizing quast output\n  def summarize_quast(file):\n      # get sample id from file name and set up data list\n      sample_id = os.path.basename(file).split(\".\")[0]\n      # read in data frame from file\n      df = pd.read_csv(file, sep='\\\\t')\n      # get contigs, total length and assembly length columns\n      df = df.iloc[:,[1,7,17]]\n      # assign sample id as column\n      df = df.assign(Sample=sample_id)\n      # rename columns\n      df = df.rename(columns={'# contigs (>= 0 bp)':'Contigs','Total length (>= 0 bp)':'Assembly Length (bp)'})\n      # re-order data frame\n      df = df[['Sample', 'Contigs','Assembly Length (bp)', 'N50']]\n      return df\n\n  # get quast output files\n  files = glob.glob(\"*.quast.tsv\")\n\n  # summarize quast output files\n  dfs = map(summarize_quast,files)\n\n  # concatenate dfs and write data frame to file\n  dfs_concat = pd.concat(dfs)\n  dfs_concat.to_csv(f'quast_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"\n}",
        "nb_lignes_process": 43,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n\n  # function for summarizing quast output\n  def summarize_quast(file):\n      # get sample id from file name and set up data list\n      sample_id = os.path.basename(file).split(\".\")[0]\n      # read in data frame from file\n      df = pd.read_csv(file, sep='\\\\t')\n      # get contigs, total length and assembly length columns\n      df = df.iloc[:,[1,7,17]]\n      # assign sample id as column\n      df = df.assign(Sample=sample_id)\n      # rename columns\n      df = df.rename(columns={'# contigs (>= 0 bp)':'Contigs','Total length (>= 0 bp)':'Assembly Length (bp)'})\n      # re-order data frame\n      df = df[['Sample', 'Contigs','Assembly Length (bp)', 'N50']]\n      return df\n\n  # get quast output files\n  files = glob.glob(\"*.quast.tsv\")\n\n  # summarize quast output files\n  dfs = map(summarize_quast,files)\n\n  # concatenate dfs and write data frame to file\n  dfs_concat = pd.concat(dfs)\n  dfs_concat.to_csv(f'quast_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"",
        "nb_lignes_script": 32,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "quast_files"
        ],
        "nb_inputs": 1,
        "outputs": [
            "quast_tsv"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "publishDir \"${params.outdir}/quast\",mode:'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "mlst": {
        "name_process": "mlst",
        "string_process": "\nprocess mlst {\n  tag \"$name\"\n\n  publishDir \"${params.outdir}/mlst\", mode: 'copy', pattern: \"*.mlst.tsv*\"\n\n  input:\n  set val(name), file(input) from assembled_genomes_mlst\n\n  output:\n  file(\"*.tsv\") into mlst_results\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n\n  import os\n  import subprocess as sub\n  import shlex\n  import pandas as pd\n  from functools import reduce\n  import shutil\n  import glob\n\n  # Function for running mlst on samples with multiple schemes\n  def run_schemes(first_scheme,all_schemes,sample):\n      # Subtract selected scheme from list of schemes to get remaining schemes\n      remaining_schemes = list(set(all_schemes) - set([first_scheme]))\n      # Run mlst on remaining schemes\n      for i in remaining_schemes:\n          outFile = open(f'{sample}.{i}.tsv','w')\n          cmd = shlex.split(f'mlst --nopath --exclude sthermophilus --scheme {i} {sample}.contigs.fa')\n          sub.Popen(cmd, stdout=outFile).wait()\n\n  # Read in fasta file\n  inFile = '${input}'\n  sid = inFile.split('.')[0]\n\n  # Open outfile and run mlst\n  outFile = open(f'{sid}.tsv','w')\n  cmd = shlex.split(f'mlst --nopath --exclude sthermophilus {sid}.contigs.fa')\n  sub.Popen(cmd, stdout=outFile).wait()\n\n  # Lists organisms with multiple schemes\n  abaumannii_schemes = ['abaumannii','abaumannii_2']\n  ecoli_schemes = ['ecoli','ecoli_2']\n  leptospira_schemes = ['leptospira','leptospira_2','leptospira_3']\n  vcholerae_schemes = ['vcholerae','vcholerae2']\n\n  # Dictionary of scheme names\n  ids = {'mlstID':['abaumannii','abaumannii_2','ecoli','ecoli_2','leptospira','leptospira_2','leptospira_3','vcholerae2','vcholerae'],\n  'PubMLSTID':['Oxford','Pasteur','Achtman','Pasteur ','Scheme 1','Scheme 2', 'Scheme 3','O1 and O139','']}\n  ids = dict(zip(ids['mlstID'], ids['PubMLSTID']))\n\n  # read in mlst output and get scheme\n  df = pd.read_csv(f'{sid}.tsv', header=None, delimiter='\\\\t')\n  scheme = df.iloc[0][1]\n\n  # Add scheme to mlst file name\n  if scheme == '-':\n      shutil.move(f'{sid}.tsv',f'{sid}.NA.tsv')\n  else:\n      shutil.move(f'{sid}.tsv', f'{sid}.{scheme}.tsv')\n\n  dfs = []\n\n  # Check and run multiple schemes\n  if any(x in scheme for x in abaumannii_schemes):\n      run_schemes(scheme,abaumannii_schemes,sid)\n  if any(x in scheme for x in ecoli_schemes):\n      run_schemes(scheme,ecoli_schemes,sid)\n  if any(x in scheme for x in leptospira_schemes):\n      run_schemes(scheme,leptospira_schemes,sid)\n  if any(x in scheme for x in vcholerae_schemes):\n      run_schemes(scheme,vcholerae_schemes,sid)\n\n  # Get list of mlst files and set up empty list\n  mlst_files = glob.glob('*.tsv')\n\n  # Reformat MLST results and append to empty list\n  for file in mlst_files:\n      df = pd.read_csv(file, header=None, delimiter='\\\\t')\n      df[0] = df[0].str.replace('.contigs.fa', '')\n      df[2] = 'ST' + df[2].astype(str)\n      df[2] = df[2].str.replace('ST-', 'NA')\n\n      if len(mlst_files) > 1:\n          # Replace mlst scheme names with PubMLST scheme names\n          for old, new in ids.items():\n              df[1] = df[1].replace(to_replace=old, value=new)\n      else:\n          # Remove scheme name\n          df.iloc[0,1] = ''\n          df[2] = df[2].str.replace('NA', 'No scheme available')\n      # Join ST to PubMLST scheme names\n      df['MLST Scheme'] = df[[1,2]].agg(' '.join, axis=1)\n      df = df[[0,'MLST Scheme']]\n      df.columns =['Sample','MLST Scheme']\n      df['MLST Scheme'] = df['MLST Scheme'].replace('\\\\s+', ' ', regex=True)\n      df['MLST Scheme'] = df['MLST Scheme'].str.replace('NA -', 'NA', regex=True)\n      dfs.append(df)\n\n  # Merge multiple dataframes (separated by ;) and write to file\n  if len(dfs) > 1:\n      merged = reduce(lambda  left,right: pd.merge(left,right,on=['Sample'], how='left'), dfs)\n      merged = merged.reindex(sorted(merged.columns,reverse=True), axis=1)\n      merged['MLST Scheme'] = merged.iloc[: , 1:].agg(';'.join, axis=1)\n      merged = merged[['Sample','MLST Scheme']]\n      merged.to_csv(f'{sid}.mlst.tsv', index=False, sep='\\\\t', encoding='utf-8')\n\n  else:\n      df = dfs[0]\n      df['MLST Scheme'] = df['MLST Scheme'].str.replace(' S', 'S')\n      df['MLST Scheme'] = df['MLST Scheme'].str.replace(' N', 'N')\n      df.to_csv(f'{sid}.mlst.tsv', index=False, sep='\\\\t', encoding='utf-8')\n  \"\"\"\n}",
        "nb_lignes_process": 115,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n\n  import os\n  import subprocess as sub\n  import shlex\n  import pandas as pd\n  from functools import reduce\n  import shutil\n  import glob\n\n  # Function for running mlst on samples with multiple schemes\n  def run_schemes(first_scheme,all_schemes,sample):\n      # Subtract selected scheme from list of schemes to get remaining schemes\n      remaining_schemes = list(set(all_schemes) - set([first_scheme]))\n      # Run mlst on remaining schemes\n      for i in remaining_schemes:\n          outFile = open(f'{sample}.{i}.tsv','w')\n          cmd = shlex.split(f'mlst --nopath --exclude sthermophilus --scheme {i} {sample}.contigs.fa')\n          sub.Popen(cmd, stdout=outFile).wait()\n\n  # Read in fasta file\n  inFile = '${input}'\n  sid = inFile.split('.')[0]\n\n  # Open outfile and run mlst\n  outFile = open(f'{sid}.tsv','w')\n  cmd = shlex.split(f'mlst --nopath --exclude sthermophilus {sid}.contigs.fa')\n  sub.Popen(cmd, stdout=outFile).wait()\n\n  # Lists organisms with multiple schemes\n  abaumannii_schemes = ['abaumannii','abaumannii_2']\n  ecoli_schemes = ['ecoli','ecoli_2']\n  leptospira_schemes = ['leptospira','leptospira_2','leptospira_3']\n  vcholerae_schemes = ['vcholerae','vcholerae2']\n\n  # Dictionary of scheme names\n  ids = {'mlstID':['abaumannii','abaumannii_2','ecoli','ecoli_2','leptospira','leptospira_2','leptospira_3','vcholerae2','vcholerae'],\n  'PubMLSTID':['Oxford','Pasteur','Achtman','Pasteur ','Scheme 1','Scheme 2', 'Scheme 3','O1 and O139','']}\n  ids = dict(zip(ids['mlstID'], ids['PubMLSTID']))\n\n  # read in mlst output and get scheme\n  df = pd.read_csv(f'{sid}.tsv', header=None, delimiter='\\\\t')\n  scheme = df.iloc[0][1]\n\n  # Add scheme to mlst file name\n  if scheme == '-':\n      shutil.move(f'{sid}.tsv',f'{sid}.NA.tsv')\n  else:\n      shutil.move(f'{sid}.tsv', f'{sid}.{scheme}.tsv')\n\n  dfs = []\n\n  # Check and run multiple schemes\n  if any(x in scheme for x in abaumannii_schemes):\n      run_schemes(scheme,abaumannii_schemes,sid)\n  if any(x in scheme for x in ecoli_schemes):\n      run_schemes(scheme,ecoli_schemes,sid)\n  if any(x in scheme for x in leptospira_schemes):\n      run_schemes(scheme,leptospira_schemes,sid)\n  if any(x in scheme for x in vcholerae_schemes):\n      run_schemes(scheme,vcholerae_schemes,sid)\n\n  # Get list of mlst files and set up empty list\n  mlst_files = glob.glob('*.tsv')\n\n  # Reformat MLST results and append to empty list\n  for file in mlst_files:\n      df = pd.read_csv(file, header=None, delimiter='\\\\t')\n      df[0] = df[0].str.replace('.contigs.fa', '')\n      df[2] = 'ST' + df[2].astype(str)\n      df[2] = df[2].str.replace('ST-', 'NA')\n\n      if len(mlst_files) > 1:\n          # Replace mlst scheme names with PubMLST scheme names\n          for old, new in ids.items():\n              df[1] = df[1].replace(to_replace=old, value=new)\n      else:\n          # Remove scheme name\n          df.iloc[0,1] = ''\n          df[2] = df[2].str.replace('NA', 'No scheme available')\n      # Join ST to PubMLST scheme names\n      df['MLST Scheme'] = df[[1,2]].agg(' '.join, axis=1)\n      df = df[[0,'MLST Scheme']]\n      df.columns =['Sample','MLST Scheme']\n      df['MLST Scheme'] = df['MLST Scheme'].replace('\\\\s+', ' ', regex=True)\n      df['MLST Scheme'] = df['MLST Scheme'].str.replace('NA -', 'NA', regex=True)\n      dfs.append(df)\n\n  # Merge multiple dataframes (separated by ;) and write to file\n  if len(dfs) > 1:\n      merged = reduce(lambda  left,right: pd.merge(left,right,on=['Sample'], how='left'), dfs)\n      merged = merged.reindex(sorted(merged.columns,reverse=True), axis=1)\n      merged['MLST Scheme'] = merged.iloc[: , 1:].agg(';'.join, axis=1)\n      merged = merged[['Sample','MLST Scheme']]\n      merged.to_csv(f'{sid}.mlst.tsv', index=False, sep='\\\\t', encoding='utf-8')\n\n  else:\n      df = dfs[0]\n      df['MLST Scheme'] = df['MLST Scheme'].str.replace(' S', 'S')\n      df['MLST Scheme'] = df['MLST Scheme'].str.replace(' N', 'N')\n      df.to_csv(f'{sid}.mlst.tsv', index=False, sep='\\\\t', encoding='utf-8')\n  \"\"\"",
        "nb_lignes_script": 102,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "assembled_genomes_mlst"
        ],
        "nb_inputs": 1,
        "outputs": [
            "mlst_results"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "tag \"$name\"",
            "publishDir \"${params.outdir}/mlst\", mode: 'copy', pattern: \"*.mlst.tsv*\""
        ],
        "when": "",
        "stub": ""
    },
    "mlst_formatting": {
        "name_process": "mlst_formatting",
        "string_process": "\nprocess mlst_formatting {\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/mlst\",mode:'copy'\n\n  input:\n  file(mlst) from mlst_results.collect()\n\n  output:\n  file(\"mlst_results.tsv\") into mlst_tsv\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n\n  files = glob.glob('*.mlst.tsv')\n  dfs = []\n  for file in files:\n      df = pd.read_csv(file, sep='\\\\t')\n      dfs.append(df)\n  dfs_concat = pd.concat(dfs)\n  dfs_concat['MLST Scheme'] = dfs_concat['MLST Scheme'].str.replace('-:NA', 'No Scheme Available')\n  dfs_concat.to_csv(f'mlst_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n\n  files = glob.glob('*.mlst.tsv')\n  dfs = []\n  for file in files:\n      df = pd.read_csv(file, sep='\\\\t')\n      dfs.append(df)\n  dfs_concat = pd.concat(dfs)\n  dfs_concat['MLST Scheme'] = dfs_concat['MLST Scheme'].str.replace('-:NA', 'No Scheme Available')\n  dfs_concat.to_csv(f'mlst_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "mlst_results"
        ],
        "nb_inputs": 1,
        "outputs": [
            "mlst_tsv"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "errorStrategy 'ignore'",
            "publishDir \"${params.outdir}/mlst\",mode:'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "kraken": {
        "name_process": "kraken",
        "string_process": "\nprocess kraken {\n  tag \"$name\"\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/kraken\", mode: 'copy', pattern: \"*.kraken2.txt*\"\n\n  input:\n  set val(name), file(reads) from read_files_kraken\n\n  output:\n  tuple name, file(\"${name}.kraken2.txt\") into kraken_files, kraken_multiqc\n  file(\"Kraken2_DB.txt\") into kraken_version\n\n  script:\n  \"\"\"\n  kraken2 --db /kraken2-db/minikraken2_v1_8GB --threads ${task.cpus} --report ${name}.kraken2.txt --paired ${reads[0]} ${reads[1]}\n\n  ls /kraken2-db/ > Kraken2_DB.txt\n  \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "  \"\"\"\n  kraken2 --db /kraken2-db/minikraken2_v1_8GB --threads ${task.cpus} --report ${name}.kraken2.txt --paired ${reads[0]} ${reads[1]}\n\n  ls /kraken2-db/ > Kraken2_DB.txt\n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "kraken2"
        ],
        "tools_url": [
            "https://bio.tools/kraken2"
        ],
        "tools_dico": [
            {
                "name": "kraken2",
                "uri": "https://bio.tools/kraken2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3028",
                                "term": "Taxonomy"
                            }
                        ]
                    }
                ],
                "description": "Kraken 2 is the newest version of Kraken, a taxonomic classification system using exact k-mer matches to achieve high accuracy and fast classification speeds. This classifier matches each k-mer within a query sequence to the lowest common ancestor (LCA) of all genomes containing the given k-mer. The k-mer assignments inform the classification algorithm.",
                "homepage": "https://ccb.jhu.edu/software/kraken2/"
            }
        ],
        "inputs": [
            "read_files_kraken"
        ],
        "nb_inputs": 1,
        "outputs": [
            "kraken_files",
            "kraken_multiqc",
            "kraken_version"
        ],
        "nb_outputs": 3,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "tag \"$name\"",
            "errorStrategy 'ignore'",
            "publishDir \"${params.outdir}/kraken\", mode: 'copy', pattern: \"*.kraken2.txt*\""
        ],
        "when": "",
        "stub": ""
    },
    "kraken_summary": {
        "name_process": "kraken_summary",
        "string_process": "\nprocess kraken_summary {\n  tag \"$name\"\n  errorStrategy 'ignore'\n  publishDir \"${params.outdir}/kraken\",mode:'copy'\n\n  input:\n  file(files) from kraken_files.collect()\n\n  output:\n  file(\"kraken_results.tsv\") into kraken_tsv\n  file(\"kraken_results.tsv\") into kraken_amr\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n  \n  # function for summarizing kraken2 report files\n  def summarize_kraken(file):\n      # get sample id from file name\n      sample_id = os.path.basename(file).split('.')[0].replace('.kraken2.txt','')\n      data = []\n      # read kraken2 report file\n      with open(file,'r') as inFile:\n          for line in inFile:\n              line = line.strip()\n              sline = line.split('\\\\t')\n              # get unclassified reads result (denoted by 'unclassified') and append to data\n              if sline[5] == 'unclassified':\n                  data.append(sline)\n              # get species results (denoted by 'S') and append to data\n              if sline[3] == 'S':\n                  data.append(sline)\n      # convert data list to data frame\n      data_df = DataFrame(data, columns=['Percentage','Num_Covered','Num_Assigned','Rank','TaxID','Name'])\n      # remove left leading spaces from the Name column\n      data_df['Name'] = data_df['Name'].str.lstrip()\n      # sort data frame by percentages (largest to smallest)\n      data_df = data_df.sort_values(by=['Percentage'], ascending=False)\n      # make new data frame for unclassified reads only\n      unclass = data_df[data_df['Name']=='unclassified']\n      # exception for if no unclassified reads found\n      if unclass.empty:\n          # import pandas as pd\n          lst = [['0','NA','NA','NA','NA','NA']]\n          unclass = pd.DataFrame(lst, columns =['Percentage','Num_Covered','Num_Assigned','Rank','TaxID','Name'])\n      # subset data frame by species\n      species_df = data_df[data_df['Name']!='unclassified']\n      # get first two species matches (first two largest percentages) in data frame\n      species_df = species_df.head(2)\n      # check if species data frame has two rows\n      if len(species_df) == 0:\n          # add two empty rows to species data frame\n          species_df = species_df.append(pd.Series(), ignore_index=True)\n          species_df = species_df.append(pd.Series(), ignore_index=True)\n      if len(species_df) == 1:\n          # add one empty row to species data frame\n          species_df = species_df.append(pd.Series(), ignore_index=True)\n      # concatenate unclassified data frame and species data frame\n      df_concat = pd.concat([unclass,species_df])\n      # add sample name column to concatenated data frame\n      df_concat = df_concat.assign(Sample=sample_id)\n      # keep only Sample Percentage and Name columns in concatenated data frame\n      df_concat = df_concat[['Sample','Percentage','Name']]\n      # reset index of concatenated data frame using drop parameter to avoid old index added as column\n      df_concat = df_concat.reset_index(drop=True)\n      # add percentage sign to unclassified column\n      unclassified = df_concat.iloc[0]['Percentage'] + '%'\n      # convert to lists\n      # if primary species is nan, replace with NA\n      if str(df_concat.iloc[1]['Name']) == 'nan':\n          primary_species = 'NA'\n      # otherwise convert to (#%)\n      else:\n          primary_species = df_concat.iloc[1]['Name'] + ' (' + df_concat.iloc[1]['Percentage'] + '%)'\n      # repeat for secondary species\n      if str(df_concat.iloc[2]['Name']) == 'nan':\n          secondary_species = 'NA'\n      else:\n          secondary_species = df_concat.iloc[2]['Name'] + ' (' + df_concat.iloc[2]['Percentage'] + '%)'\n      # list of lists\n      combined = [[sample_id, unclassified, primary_species, secondary_species]]\n      # convert list of lists to data frame\n      combined_df = DataFrame(combined, columns=['Sample','Unclassified Reads (%)','Primary Species (%)','Secondary Species (%)'])\n      return combined_df\n  # get all kraken2 report files\n  files = glob.glob(\"*.kraken2.txt*\")\n  # summarize kraken2 report files\n  results = map(summarize_kraken, files)\n  # concatenate summary results and write to tsv\n  data_concat = pd.concat(results)\n  data_concat.to_csv(f'kraken_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"\n}",
        "nb_lignes_process": 96,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n  \n  # function for summarizing kraken2 report files\n  def summarize_kraken(file):\n      # get sample id from file name\n      sample_id = os.path.basename(file).split('.')[0].replace('.kraken2.txt','')\n      data = []\n      # read kraken2 report file\n      with open(file,'r') as inFile:\n          for line in inFile:\n              line = line.strip()\n              sline = line.split('\\\\t')\n              # get unclassified reads result (denoted by 'unclassified') and append to data\n              if sline[5] == 'unclassified':\n                  data.append(sline)\n              # get species results (denoted by 'S') and append to data\n              if sline[3] == 'S':\n                  data.append(sline)\n      # convert data list to data frame\n      data_df = DataFrame(data, columns=['Percentage','Num_Covered','Num_Assigned','Rank','TaxID','Name'])\n      # remove left leading spaces from the Name column\n      data_df['Name'] = data_df['Name'].str.lstrip()\n      # sort data frame by percentages (largest to smallest)\n      data_df = data_df.sort_values(by=['Percentage'], ascending=False)\n      # make new data frame for unclassified reads only\n      unclass = data_df[data_df['Name']=='unclassified']\n      # exception for if no unclassified reads found\n      if unclass.empty:\n          # import pandas as pd\n          lst = [['0','NA','NA','NA','NA','NA']]\n          unclass = pd.DataFrame(lst, columns =['Percentage','Num_Covered','Num_Assigned','Rank','TaxID','Name'])\n      # subset data frame by species\n      species_df = data_df[data_df['Name']!='unclassified']\n      # get first two species matches (first two largest percentages) in data frame\n      species_df = species_df.head(2)\n      # check if species data frame has two rows\n      if len(species_df) == 0:\n          # add two empty rows to species data frame\n          species_df = species_df.append(pd.Series(), ignore_index=True)\n          species_df = species_df.append(pd.Series(), ignore_index=True)\n      if len(species_df) == 1:\n          # add one empty row to species data frame\n          species_df = species_df.append(pd.Series(), ignore_index=True)\n      # concatenate unclassified data frame and species data frame\n      df_concat = pd.concat([unclass,species_df])\n      # add sample name column to concatenated data frame\n      df_concat = df_concat.assign(Sample=sample_id)\n      # keep only Sample Percentage and Name columns in concatenated data frame\n      df_concat = df_concat[['Sample','Percentage','Name']]\n      # reset index of concatenated data frame using drop parameter to avoid old index added as column\n      df_concat = df_concat.reset_index(drop=True)\n      # add percentage sign to unclassified column\n      unclassified = df_concat.iloc[0]['Percentage'] + '%'\n      # convert to lists\n      # if primary species is nan, replace with NA\n      if str(df_concat.iloc[1]['Name']) == 'nan':\n          primary_species = 'NA'\n      # otherwise convert to (#%)\n      else:\n          primary_species = df_concat.iloc[1]['Name'] + ' (' + df_concat.iloc[1]['Percentage'] + '%)'\n      # repeat for secondary species\n      if str(df_concat.iloc[2]['Name']) == 'nan':\n          secondary_species = 'NA'\n      else:\n          secondary_species = df_concat.iloc[2]['Name'] + ' (' + df_concat.iloc[2]['Percentage'] + '%)'\n      # list of lists\n      combined = [[sample_id, unclassified, primary_species, secondary_species]]\n      # convert list of lists to data frame\n      combined_df = DataFrame(combined, columns=['Sample','Unclassified Reads (%)','Primary Species (%)','Secondary Species (%)'])\n      return combined_df\n  # get all kraken2 report files\n  files = glob.glob(\"*.kraken2.txt*\")\n  # summarize kraken2 report files\n  results = map(summarize_kraken, files)\n  # concatenate summary results and write to tsv\n  data_concat = pd.concat(results)\n  data_concat.to_csv(f'kraken_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"",
        "nb_lignes_script": 82,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "kraken_files"
        ],
        "nb_inputs": 1,
        "outputs": [
            "kraken_tsv",
            "kraken_amr"
        ],
        "nb_outputs": 2,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "tag \"$name\"",
            "errorStrategy 'ignore'",
            "publishDir \"${params.outdir}/kraken\",mode:'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "amrfinder_setup": {
        "name_process": "amrfinder_setup",
        "string_process": "\nprocess amrfinder_setup {\n  tag \"$name\"\n\n  input:\n  file(kraken) from kraken_amr\n  set val(name), file(input) from assembled_genomes_ar\n\n  output:\n  tuple name, file(\"${name}.*.fa\") into ar_input\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import pandas as pd\n  import glob\n  import shutil\n  \n  # species and genus lists\n  species = ['Acinetobacter_baumannii','Enterococcus_faecalis','Enterococcus_faecium','Staphylococcus_aureus','Staphylococcus_pseudintermedius','Streptococcus_agalactiae','Streptococcus_pneumoniae','Streptococcus_pyogenes']\n  genus = ['Campylobacter','Escherichia','Klebsiella','Salmonella']\n  \n  # get sample name from fasta file\n  genomeFile = '${input}'\n  sid = genomeFile.split('.')[0]\n  \n  # read in kraken results as data frame\n  df = pd.read_csv('kraken_results.tsv', header=0, delimiter='\\\\t')\n  \n  # subset data frame by sample id\n  df = df[df['Sample'] == sid]\n  \n  # get primary species and genus identified\n  if df.empty:\n      taxa_species = 'NA'\n      taxa_genus = 'NA'\n  else:\n      taxa = df.iloc[0]['Primary Species (%)']\n      taxa = taxa.split(' ')\n      taxa_species = taxa[0] + '_' + taxa[1]\n      taxa_genus = taxa[0]\n  \n  # add taxa or genus name to file name if present in lists\n  if any(x in taxa_species for x in species):\n      shutil.copyfile(genomeFile, f'{sid}.{taxa_species}.fa')\n  elif any(x in taxa_genus for x in genus):\n      shutil.copyfile(genomeFile, f'{sid}.{taxa_genus}.fa')\n  elif taxa_genus == 'Shigella':\n      shutil.copyfile(genomeFile, f'{sid}.Escherichia.fa')\n  else:\n      shutil.copyfile(genomeFile, f'{sid}.NA.fa')\n\n  \"\"\"\n}",
        "nb_lignes_process": 52,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n  import pandas as pd\n  import glob\n  import shutil\n  \n  # species and genus lists\n  species = ['Acinetobacter_baumannii','Enterococcus_faecalis','Enterococcus_faecium','Staphylococcus_aureus','Staphylococcus_pseudintermedius','Streptococcus_agalactiae','Streptococcus_pneumoniae','Streptococcus_pyogenes']\n  genus = ['Campylobacter','Escherichia','Klebsiella','Salmonella']\n  \n  # get sample name from fasta file\n  genomeFile = '${input}'\n  sid = genomeFile.split('.')[0]\n  \n  # read in kraken results as data frame\n  df = pd.read_csv('kraken_results.tsv', header=0, delimiter='\\\\t')\n  \n  # subset data frame by sample id\n  df = df[df['Sample'] == sid]\n  \n  # get primary species and genus identified\n  if df.empty:\n      taxa_species = 'NA'\n      taxa_genus = 'NA'\n  else:\n      taxa = df.iloc[0]['Primary Species (%)']\n      taxa = taxa.split(' ')\n      taxa_species = taxa[0] + '_' + taxa[1]\n      taxa_genus = taxa[0]\n  \n  # add taxa or genus name to file name if present in lists\n  if any(x in taxa_species for x in species):\n      shutil.copyfile(genomeFile, f'{sid}.{taxa_species}.fa')\n  elif any(x in taxa_genus for x in genus):\n      shutil.copyfile(genomeFile, f'{sid}.{taxa_genus}.fa')\n  elif taxa_genus == 'Shigella':\n      shutil.copyfile(genomeFile, f'{sid}.Escherichia.fa')\n  else:\n      shutil.copyfile(genomeFile, f'{sid}.NA.fa')\n\n  \"\"\"",
        "nb_lignes_script": 40,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "kraken_amr",
            "assembled_genomes_ar"
        ],
        "nb_inputs": 2,
        "outputs": [
            "ar_input"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "tag \"$name\""
        ],
        "when": "",
        "stub": ""
    },
    "amrfinder": {
        "name_process": "amrfinder",
        "string_process": "\nprocess amrfinder {\n  tag \"$name\"\n  publishDir \"${params.outdir}/amrfinder\",mode: 'copy',pattern:\"*.amr.tsv\"\n\n  input:\n  set val(name), file(input) from ar_input\n\n  output:\n  tuple name, file(\"${name}.amr.tsv\") into ar_predictions\n  file(\"AMRFinderPlus_DB.txt\") into amrfinder_version\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n\n  import subprocess as sub\n  import shlex\n  import glob\n  import shutil\n\n  # organism list\n  organisms = ['Acinetobacter_baumannii','Enterococcus_faecalis','Enterococcus_faecium','Staphylococcus_aureus','Staphylococcus_pseudintermedius','Streptococcus_agalactiae','Streptococcus_pneumoniae','Streptococcus_pyogenes','Campylobacter','Escherichia','Klebsiella','Salmonella','Escherichia']\n\n  # get sample id and organism name from fasta file\n  fastaFile = '${input}'\n  sid = fastaFile.split('.')[0]\n  organism = fastaFile.split('.')[1]\n\n  # run amrfinder using --organism if present in organism list\n  if any(x in organism for x in organisms):\n      outFile = open(f'{sid}.amr.tsv','w')\n      cmd = shlex.split(f'amrfinder -n {sid}.{organism}.fa --organism {organism}')\n      sub.Popen(cmd, stdout=outFile).wait()\n  # otherwise run amrfinder without --organism\n  else:\n      outFile = open(f'{sid}.amr.tsv','w')\n      cmd = shlex.split(f'amrfinder -n {sid}.{organism}.fa')\n      sub.Popen(cmd, stdout=outFile).wait()\n\n  # get version information from version file\n  versionFile = \"/amrfinder/data/latest/version.txt\"\n  shutil.copy(versionFile,\"AMRFinderPlus_DB.txt\")\n  \"\"\"\n}",
        "nb_lignes_process": 43,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n\n  import subprocess as sub\n  import shlex\n  import glob\n  import shutil\n\n  # organism list\n  organisms = ['Acinetobacter_baumannii','Enterococcus_faecalis','Enterococcus_faecium','Staphylococcus_aureus','Staphylococcus_pseudintermedius','Streptococcus_agalactiae','Streptococcus_pneumoniae','Streptococcus_pyogenes','Campylobacter','Escherichia','Klebsiella','Salmonella','Escherichia']\n\n  # get sample id and organism name from fasta file\n  fastaFile = '${input}'\n  sid = fastaFile.split('.')[0]\n  organism = fastaFile.split('.')[1]\n\n  # run amrfinder using --organism if present in organism list\n  if any(x in organism for x in organisms):\n      outFile = open(f'{sid}.amr.tsv','w')\n      cmd = shlex.split(f'amrfinder -n {sid}.{organism}.fa --organism {organism}')\n      sub.Popen(cmd, stdout=outFile).wait()\n  # otherwise run amrfinder without --organism\n  else:\n      outFile = open(f'{sid}.amr.tsv','w')\n      cmd = shlex.split(f'amrfinder -n {sid}.{organism}.fa')\n      sub.Popen(cmd, stdout=outFile).wait()\n\n  # get version information from version file\n  versionFile = \"/amrfinder/data/latest/version.txt\"\n  shutil.copy(versionFile,\"AMRFinderPlus_DB.txt\")\n  \"\"\"",
        "nb_lignes_script": 30,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ar_input"
        ],
        "nb_inputs": 1,
        "outputs": [
            "ar_predictions",
            "amrfinder_version"
        ],
        "nb_outputs": 2,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "tag \"$name\"",
            "publishDir \"${params.outdir}/amrfinder\",mode: 'copy',pattern:\"*.amr.tsv\""
        ],
        "when": "",
        "stub": ""
    },
    "amrfinder_summary": {
        "name_process": "amrfinder_summary",
        "string_process": "\nprocess amrfinder_summary {\n  publishDir \"${params.outdir}/amrfinder\",mode:'copy'\n\n  input:\n  file(predictions) from ar_predictions.collect()\n\n  output:\n  file(\"ar_predictions.tsv\")\n  file(\"ar_summary.tsv\") into ar_tsv\n  file(\"selected_ar_genes.tsv\") into selected_ar_tsv\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n\n  # get amrfinder output files and set up lists\n  files = glob.glob('*.amr.tsv')\n  dfs = []\n  all_ar_dfs = []\n  selected_ar_dfs = []\n\n  # function for cleanining up amrfinder output\n  def pretty_df(data,sample):\n      data.columns = data.columns.str.replace(' ', '_')\n      data = data.assign(Sample=sample)\n      data = data[['Sample','Gene_symbol','%_Coverage_of_reference_sequence','%_Identity_to_reference_sequence']]\n      pretty_data = data.set_axis(['Sample', 'Gene', 'Coverage', 'Identity'], axis=1, inplace=False)\n      return pretty_data\n\n  # function for joining amrfinder results by a delimiter\n  def join_df(data,sample,delim):\n      gene = data['Gene'].tolist()\n      gene = delim.join(gene)\n      coverage = data['Coverage'].tolist()\n      coverage = delim.join(map(str, coverage))\n      identity = data['Identity'].tolist()\n      identity = delim.join(map(str, identity))\n      joined_data = [[sample,gene,coverage,identity]]\n      joined_data = pd.DataFrame(joined_data, columns = ['Sample', 'Gene', 'Coverage', 'Identity'])\n      return joined_data\n\n  for file in files:\n      # get sample id from file name\n      sample_id = os.path.basename(file).split('.')[0]\n      # read in amrfinder results as data frame\n      df = pd.read_csv(file, header=0, delimiter='\\\\t')\n\n      # clean up data frame\n      df = pretty_df(df,sample_id)\n      dfs.append(df)\n\n      # summarize all results\n      all_ar_df = join_df(df,sample_id,';')\n      all_ar_dfs.append(all_ar_df)\n\n      # subset data frame by selected genes\n      mask = df['Gene'].str.contains(${params.selected_genes}, case=False, na=False)\n      masked_df = df[mask]\n      # check if any select genes were found\n      if masked_df.empty:\n          masked_df = masked_df.append({'Sample' : sample_id, 'Gene' : 'None', 'Coverage' : 'None','Identity' : 'None'}, ignore_index = True)\n      selected_ar_df = join_df(masked_df,sample_id,';')\n      selected_ar_df = selected_ar_df.set_axis(['Sample', 'Selected AMR Genes', 'Selected AMR Genes Coverage', 'Selected AMR Genes Identity'], axis=1, inplace=False)\n      selected_ar_dfs.append(selected_ar_df)\n\n  # concatenate results and write to tsv\n  concat_dfs = pd.concat(dfs)\n  concat_dfs.to_csv('ar_predictions.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n\n  # concatenate joined restults and write to tsv\n  concat_all_ar_dfs = pd.concat(all_ar_dfs)\n  concat_selected_ar_dfs = pd.concat(selected_ar_dfs)\n\n  # concatenate selected genes and write to tsv\n  concat_all_ar_dfs.to_csv('ar_summary.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  concat_selected_ar_dfs.to_csv('selected_ar_genes.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"\n}",
        "nb_lignes_process": 80,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n\n  # get amrfinder output files and set up lists\n  files = glob.glob('*.amr.tsv')\n  dfs = []\n  all_ar_dfs = []\n  selected_ar_dfs = []\n\n  # function for cleanining up amrfinder output\n  def pretty_df(data,sample):\n      data.columns = data.columns.str.replace(' ', '_')\n      data = data.assign(Sample=sample)\n      data = data[['Sample','Gene_symbol','%_Coverage_of_reference_sequence','%_Identity_to_reference_sequence']]\n      pretty_data = data.set_axis(['Sample', 'Gene', 'Coverage', 'Identity'], axis=1, inplace=False)\n      return pretty_data\n\n  # function for joining amrfinder results by a delimiter\n  def join_df(data,sample,delim):\n      gene = data['Gene'].tolist()\n      gene = delim.join(gene)\n      coverage = data['Coverage'].tolist()\n      coverage = delim.join(map(str, coverage))\n      identity = data['Identity'].tolist()\n      identity = delim.join(map(str, identity))\n      joined_data = [[sample,gene,coverage,identity]]\n      joined_data = pd.DataFrame(joined_data, columns = ['Sample', 'Gene', 'Coverage', 'Identity'])\n      return joined_data\n\n  for file in files:\n      # get sample id from file name\n      sample_id = os.path.basename(file).split('.')[0]\n      # read in amrfinder results as data frame\n      df = pd.read_csv(file, header=0, delimiter='\\\\t')\n\n      # clean up data frame\n      df = pretty_df(df,sample_id)\n      dfs.append(df)\n\n      # summarize all results\n      all_ar_df = join_df(df,sample_id,';')\n      all_ar_dfs.append(all_ar_df)\n\n      # subset data frame by selected genes\n      mask = df['Gene'].str.contains(${params.selected_genes}, case=False, na=False)\n      masked_df = df[mask]\n      # check if any select genes were found\n      if masked_df.empty:\n          masked_df = masked_df.append({'Sample' : sample_id, 'Gene' : 'None', 'Coverage' : 'None','Identity' : 'None'}, ignore_index = True)\n      selected_ar_df = join_df(masked_df,sample_id,';')\n      selected_ar_df = selected_ar_df.set_axis(['Sample', 'Selected AMR Genes', 'Selected AMR Genes Coverage', 'Selected AMR Genes Identity'], axis=1, inplace=False)\n      selected_ar_dfs.append(selected_ar_df)\n\n  # concatenate results and write to tsv\n  concat_dfs = pd.concat(dfs)\n  concat_dfs.to_csv('ar_predictions.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n\n  # concatenate joined restults and write to tsv\n  concat_all_ar_dfs = pd.concat(all_ar_dfs)\n  concat_selected_ar_dfs = pd.concat(selected_ar_dfs)\n\n  # concatenate selected genes and write to tsv\n  concat_all_ar_dfs.to_csv('ar_summary.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  concat_selected_ar_dfs.to_csv('selected_ar_genes.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"",
        "nb_lignes_script": 67,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ar_predictions"
        ],
        "nb_inputs": 1,
        "outputs": [
            "ar_tsv",
            "selected_ar_tsv"
        ],
        "nb_outputs": 2,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "publishDir \"${params.outdir}/amrfinder\",mode:'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "bbduk_summary": {
        "name_process": "bbduk_summary",
        "string_process": "\nprocess bbduk_summary {\n  publishDir \"${params.outdir}/trimming\",mode:'copy'\n\n  input:\n  file(files) from bbduk_files.collect()\n\n  output:\n  file(\"bbduk_results.tsv\") into bbduk_tsv\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n\n  # function for summarizing bbduk output\n  def summarize_bbduk(file):\n      # get sample id from file name and set up data list\n      sample_id = os.path.basename(file).split(\".\")[0]\n      data = []\n      data.append(sample_id)\n      with open(file,\"r\") as inFile:\n          for i, line in enumerate(inFile):\n              # get total number of reads\n              if i == 0:\n                  num_reads = line.strip().split(\"\\\\t\")[1].replace(\" reads \",\"\")\n                  data.append(num_reads)\n              # get total number of reads removed\n              if i == 3:\n                  rm_reads = line.strip().split(\"\\\\t\")[1].replace(\"reads \",\"\")\n                  rm_reads = rm_reads.rstrip()\n                  data.append(rm_reads)\n      return data\n\n  # get all bbduk output files\n  files = glob.glob(\"*.trim.txt\")\n\n  # summarize bbduk output files\n  results = map(summarize_bbduk,files)\n\n  # convert results to data frame and write to tsv\n  df = DataFrame(results,columns=['Sample','Total Reads','Reads Removed'])\n  df.to_csv(f'bbduk_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"\n}",
        "nb_lignes_process": 46,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n  import os\n  import glob\n  import pandas as pd\n  from pandas import DataFrame\n\n  # function for summarizing bbduk output\n  def summarize_bbduk(file):\n      # get sample id from file name and set up data list\n      sample_id = os.path.basename(file).split(\".\")[0]\n      data = []\n      data.append(sample_id)\n      with open(file,\"r\") as inFile:\n          for i, line in enumerate(inFile):\n              # get total number of reads\n              if i == 0:\n                  num_reads = line.strip().split(\"\\\\t\")[1].replace(\" reads \",\"\")\n                  data.append(num_reads)\n              # get total number of reads removed\n              if i == 3:\n                  rm_reads = line.strip().split(\"\\\\t\")[1].replace(\"reads \",\"\")\n                  rm_reads = rm_reads.rstrip()\n                  data.append(rm_reads)\n      return data\n\n  # get all bbduk output files\n  files = glob.glob(\"*.trim.txt\")\n\n  # summarize bbduk output files\n  results = map(summarize_bbduk,files)\n\n  # convert results to data frame and write to tsv\n  df = DataFrame(results,columns=['Sample','Total Reads','Reads Removed'])\n  df.to_csv(f'bbduk_results.tsv',sep='\\\\t', index=False, header=True, na_rep='NaN')\n  \"\"\"",
        "nb_lignes_script": 35,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "bbduk_files"
        ],
        "nb_inputs": 1,
        "outputs": [
            "bbduk_tsv"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "publishDir \"${params.outdir}/trimming\",mode:'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "merge_results": {
        "name_process": "merge_results",
        "string_process": "\nprocess merge_results {\n  publishDir \"${params.outdir}/\", mode: 'copy'\n\n  input:\n  file(bbduk) from bbduk_tsv\n  file(quast) from quast_tsv\n  file(coverage) from coverage_tsv\n  file(mlst) from mlst_tsv\n  file(kraken) from kraken_tsv\n  file(amr) from ar_tsv\n  file(selected_ar) from selected_ar_tsv\n  file(vkraken) from kraken_version.first()\n  file(vamrfinder) from amrfinder_version.first()\n\n  output:\n  file('spriggan_report.csv')\n\n  script:\n  \"\"\"\n  #!/usr/bin/env python3\n\n  import os\n  import glob\n  import pandas as pd\n  from functools import reduce\n\n  with open('AMRFinderPlus_DB.txt', 'r') as amrFile:\n      amrfinderDB_version = amrFile.readline().strip()\n\n  with open('Kraken2_DB.txt', 'r') as krakenFile:\n      krakenDB_version = krakenFile.readline().strip()\n\n  files = glob.glob('*.tsv')\n\n  dfs = []\n\n  for file in files:\n      df = pd.read_csv(file, header=0, delimiter='\\\\t')\n      dfs.append(df)\n\n  merged = reduce(lambda  left,right: pd.merge(left,right,on=['Sample'],how='left'), dfs)\n  merged = merged.assign(krakenDB=krakenDB_version)\n  merged = merged.assign(amrDB=amrfinderDB_version)\n  merged = merged[['Sample','Total Reads','Reads Removed','Median Coverage','Average Coverage','Contigs','Assembly Length (bp)','N50','Primary Species (%)','Secondary Species (%)','Unclassified Reads (%)','krakenDB','MLST Scheme','Gene','Coverage','Identity','Selected AMR Genes','Selected AMR Genes Coverage','Selected AMR Genes Identity','amrDB']]\n  merged = merged.rename(columns={'Contigs':'Contigs (#)','Average Coverage':'Mean Coverage','Gene':'AMR','Coverage':'AMR Coverage','Identity':'AMR Identity','krakenDB':'Kraken Database Verion','amrDB':'AMRFinderPlus Database Version'})\n\n  merged.to_csv('spriggan_report.csv', index=False, sep=',', encoding='utf-8')\n  \"\"\"\n}",
        "nb_lignes_process": 48,
        "string_script": "  \"\"\"\n  #!/usr/bin/env python3\n\n  import os\n  import glob\n  import pandas as pd\n  from functools import reduce\n\n  with open('AMRFinderPlus_DB.txt', 'r') as amrFile:\n      amrfinderDB_version = amrFile.readline().strip()\n\n  with open('Kraken2_DB.txt', 'r') as krakenFile:\n      krakenDB_version = krakenFile.readline().strip()\n\n  files = glob.glob('*.tsv')\n\n  dfs = []\n\n  for file in files:\n      df = pd.read_csv(file, header=0, delimiter='\\\\t')\n      dfs.append(df)\n\n  merged = reduce(lambda  left,right: pd.merge(left,right,on=['Sample'],how='left'), dfs)\n  merged = merged.assign(krakenDB=krakenDB_version)\n  merged = merged.assign(amrDB=amrfinderDB_version)\n  merged = merged[['Sample','Total Reads','Reads Removed','Median Coverage','Average Coverage','Contigs','Assembly Length (bp)','N50','Primary Species (%)','Secondary Species (%)','Unclassified Reads (%)','krakenDB','MLST Scheme','Gene','Coverage','Identity','Selected AMR Genes','Selected AMR Genes Coverage','Selected AMR Genes Identity','amrDB']]\n  merged = merged.rename(columns={'Contigs':'Contigs (#)','Average Coverage':'Mean Coverage','Gene':'AMR','Coverage':'AMR Coverage','Identity':'AMR Identity','krakenDB':'Kraken Database Verion','amrDB':'AMRFinderPlus Database Version'})\n\n  merged.to_csv('spriggan_report.csv', index=False, sep=',', encoding='utf-8')\n  \"\"\"",
        "nb_lignes_script": 29,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "bbduk_tsv",
            "quast_tsv",
            "coverage_tsv",
            "mlst_tsv",
            "kraken_tsv",
            "ar_tsv",
            "selected_ar_tsv",
            "kraken_version",
            "amrfinder_version"
        ],
        "nb_inputs": 9,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "publishDir \"${params.outdir}/\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "multiqc": {
        "name_process": "multiqc",
        "string_process": "\nprocess multiqc {\n  publishDir \"${params.outdir}\",mode:'copy'\n\n  input:\n  file(a) from multiqc_clean_reads.collect()\n  file(b) from fastqc_multiqc.collect()\n  file(c) from stats_multiqc.collect()\n  file(d) from kraken_multiqc.collect()\n  file(e) from quast_multiqc.collect()\n  file(config) from multiqc_config\n\n  output:\n  file(\"*.html\") into multiqc_output\n\n  script:\n  \"\"\"\n  multiqc -c ${config} .\n  \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "  \"\"\"\n  multiqc -c ${config} .\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "MultiQC"
        ],
        "tools_url": [
            "https://bio.tools/multiqc"
        ],
        "tools_dico": [
            {
                "name": "MultiQC",
                "uri": "https://bio.tools/multiqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0091",
                            "term": "Bioinformatics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2428",
                                    "term": "Validation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2048",
                                "term": "Report"
                            }
                        ]
                    }
                ],
                "description": "MultiQC aggregates results from multiple bioinformatics analyses across many samples into a single report. It searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.",
                "homepage": "http://multiqc.info/"
            }
        ],
        "inputs": [
            "multiqc_clean_reads",
            "fastqc_multiqc",
            "stats_multiqc",
            "kraken_multiqc",
            "quast_multiqc",
            "multiqc_config"
        ],
        "nb_inputs": 6,
        "outputs": [
            "multiqc_output"
        ],
        "nb_outputs": 1,
        "name_workflow": "wslh-bio__spriggan",
        "directive": [
            "publishDir \"${params.outdir}\",mode:'copy'"
        ],
        "when": "",
        "stub": ""
    }
}