{
    "GetNucleotideSequences": {
        "name_process": "GetNucleotideSequences",
        "string_process": "\nprocess GetNucleotideSequences {\n\n  tag \"$sample\"\n\n  input:\n  set val(sample), file(gtf) from gtfs\n  file genome_fasta\n\n  output:\n  set val(\"${sample}\"), file(\"${sample}.fasta\") into nucleotide_fastas\n\n  script:\n  \"\"\"\n  gffread -F -w ${sample}.fasta -g $genome_fasta ${gtf}\n  \"\"\"\n\n}",
        "nb_lignes_process": 16,
        "string_script": "  \"\"\"\n  gffread -F -w ${sample}.fasta -g $genome_fasta ${gtf}\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "gffread"
        ],
        "tools_url": [
            "https://bio.tools/gffread"
        ],
        "tools_dico": [
            {
                "name": "gffread",
                "uri": "https://bio.tools/gffread",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acids"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0361",
                                    "term": "Sequence annotation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "program for filtering, converting and manipulating GFF files",
                "homepage": "https://ccb.jhu.edu/software/stringtie/gff.shtml"
            }
        ],
        "inputs": [
            "gtfs",
            "genome_fasta"
        ],
        "nb_inputs": 2,
        "outputs": [
            "nucleotide_fastas"
        ],
        "nb_outputs": 1,
        "name_workflow": "higsch__togetherforever",
        "directive": [
            "tag \"$sample\""
        ],
        "when": "",
        "stub": ""
    },
    "ThreeFrameTranslation": {
        "name_process": "ThreeFrameTranslation",
        "string_process": "\nprocess ThreeFrameTranslation {\n  \n  tag \"$sample\"\n\n  input:\n  set val(sample), file(nucleotide_fasta) from nucleotide_fastas\n\n  output:\n  set val(\"${sample}\"), file(\"${sample}.prot.fasta\") into aa_fastas\n\n  script:\n  \"\"\"\n  threeFrameTranslator.py -i $nucleotide_fasta -o ${sample}.prot.fasta\n  \"\"\"\n\n}",
        "nb_lignes_process": 15,
        "string_script": "  \"\"\"\n  threeFrameTranslator.py -i $nucleotide_fasta -o ${sample}.prot.fasta\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "nucleotide_fastas"
        ],
        "nb_inputs": 1,
        "outputs": [
            "aa_fastas"
        ],
        "nb_outputs": 1,
        "name_workflow": "higsch__togetherforever",
        "directive": [
            "tag \"$sample\""
        ],
        "when": "",
        "stub": ""
    },
    "MergeSampleFastas": {
        "name_process": "MergeSampleFastas",
        "string_process": "\nprocess MergeSampleFastas {\n\n  input:\n  file fastas from aa_fastas_combined\n\n  output:\n  file 'combined_unique.fasta' into fasta_combined_unique\n\n  script:\n  \"\"\"\n  for fasta in $fastas; do\n    cat \\${fasta} >> combined.fasta\n  done\n  awk 'NR%2 && !a[\\$0]++ { print; getline l ; print l }' combined.fasta > combined_unique.fasta\n  \"\"\"\n\n}",
        "nb_lignes_process": 16,
        "string_script": "  \"\"\"\n  for fasta in $fastas; do\n    cat \\${fasta} >> combined.fasta\n  done\n  awk 'NR%2 && !a[\\$0]++ { print; getline l ; print l }' combined.fasta > combined_unique.fasta\n  \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "aa_fastas_combined"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fasta_combined_unique"
        ],
        "nb_outputs": 1,
        "name_workflow": "higsch__togetherforever",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "SplitStopCodons": {
        "name_process": "SplitStopCodons",
        "string_process": "\nprocess SplitStopCodons {\n\n  input:\n  file stop_fasta from fasta_combined_unique\n\n  output:\n  file 'no_stop.fasta' into fasta_nostop\n\n  script:\n  \"\"\"\n  codonsplitter.py -i $stop_fasta -o no_stop.fasta -c \\\"*\\\"\n  \"\"\"\n\n}",
        "nb_lignes_process": 13,
        "string_script": "  \"\"\"\n  codonsplitter.py -i $stop_fasta -o no_stop.fasta -c \\\"*\\\"\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fasta_combined_unique"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fasta_nostop"
        ],
        "nb_outputs": 1,
        "name_workflow": "higsch__togetherforever",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "DigestTranscriptome": {
        "name_process": "DigestTranscriptome",
        "string_process": "\nprocess DigestTranscriptome {\n\n  input:\n  file proteins_fasta from fasta_nostop\n\n  output:\n  file 'peptides.fasta' into peptides\n\n  script:\n  \"\"\"\n  Digestor -in $proteins_fasta \\\n           -out peptides.fasta \\\n           -out_type fasta \\\n           -missed_cleavages $params.missed_cleavages \\\n           -enzyme $params.enzyme\n  \"\"\"\n\n}",
        "nb_lignes_process": 17,
        "string_script": "  \"\"\"\n  Digestor -in $proteins_fasta \\\n           -out peptides.fasta \\\n           -out_type fasta \\\n           -missed_cleavages $params.missed_cleavages \\\n           -enzyme $params.enzyme\n  \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fasta_nostop"
        ],
        "nb_inputs": 1,
        "outputs": [
            "peptides"
        ],
        "nb_outputs": 1,
        "name_workflow": "higsch__togetherforever",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "PIPredictionOnTranscriptome": {
        "name_process": "PIPredictionOnTranscriptome",
        "string_process": "\nprocess PIPredictionOnTranscriptome {\n\n  input:\n  file peptides from peptides\n\n  output:\n  file 'peptides_pI.fasta' into peptides_pI\n\n  script:\n  \"\"\"\n  java -jar /piDeepNet/piDeep/h2o.3.14.0.3/h2o/java/h2o.jar &\n  Rscript /piDeepNet/getpiScores.R $peptides peptides_pI.fasta\n  \"\"\"\n\n}",
        "nb_lignes_process": 14,
        "string_script": "  \"\"\"\n  java -jar /piDeepNet/piDeep/h2o.3.14.0.3/h2o/java/h2o.jar &\n  Rscript /piDeepNet/getpiScores.R $peptides peptides_pI.fasta\n  \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "peptides"
        ],
        "nb_inputs": 1,
        "outputs": [
            "peptides_pI"
        ],
        "nb_outputs": 1,
        "name_workflow": "higsch__togetherforever",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "SplitTranscriptomePeptidesToPIDBs": {
        "name_process": "SplitTranscriptomePeptidesToPIDBs",
        "string_process": "\nprocess SplitTranscriptomePeptidesToPIDBs {\n\n  input:\n  file peptides_pI from peptides_pI\n  val fractions from fractions\n  file normPsms\n\n  output:\n  file 'db_*' into pI_fastas\n\n  script:\n  \"\"\"\n  dbsplitter.py --pi-peptides $peptides_pI \\\n                --normpsms $normPsms \\\n                --intercept $params.intercept \\\n                --width $params.width \\\n                --tolerance $params.tolerance \\\n                --amount $params.amount \\\n                --fractions ${fractions.join(',')} \\\n                --out db_*.fasta\n  \"\"\"\n\n}",
        "nb_lignes_process": 22,
        "string_script": "  \"\"\"\n  dbsplitter.py --pi-peptides $peptides_pI \\\n                --normpsms $normPsms \\\n                --intercept $params.intercept \\\n                --width $params.width \\\n                --tolerance $params.tolerance \\\n                --amount $params.amount \\\n                --fractions ${fractions.join(',')} \\\n                --out db_*.fasta\n  \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "peptides_pI",
            "fractions",
            "normPsms"
        ],
        "nb_inputs": 3,
        "outputs": [
            "pI_fastas"
        ],
        "nb_outputs": 1,
        "name_workflow": "higsch__togetherforever",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "DigestKnownProteome": {
        "name_process": "DigestKnownProteome",
        "string_process": "\nprocess DigestKnownProteome {\n\n  input:\n  file canonical_proteins_fasta\n\n  output:\n  file 'canonical_peptides.fasta' into canonical_peptides\n\n  script:\n  \"\"\"\n  Digestor -in $canonical_proteins_fasta \\\n           -out canonical_peptides.fasta \\\n           -out_type fasta \\\n           -missed_cleavages $params.missed_cleavages \\\n           -enzyme $params.enzyme\n  \"\"\"\n\n}",
        "nb_lignes_process": 17,
        "string_script": "  \"\"\"\n  Digestor -in $canonical_proteins_fasta \\\n           -out canonical_peptides.fasta \\\n           -out_type fasta \\\n           -missed_cleavages $params.missed_cleavages \\\n           -enzyme $params.enzyme\n  \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "canonical_proteins_fasta"
        ],
        "nb_inputs": 1,
        "outputs": [
            "canonical_peptides"
        ],
        "nb_outputs": 1,
        "name_workflow": "higsch__togetherforever",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "MergeTranscriptomeCanonicalsAndAddDecoys": {
        "name_process": "MergeTranscriptomeCanonicalsAndAddDecoys",
        "string_process": "\nprocess MergeTranscriptomeCanonicalsAndAddDecoys {\n\n  tag \"$fraction\"\n\n  input:\n  set val(fraction), file(db) from pI_tdbs\n  file(canonical_peptides) from canonical_peptides\n\n  output:\n  set val(\"${fraction}\"), file(\"tddb_${fraction}.fasta\") into combined_tdbs\n\n  script:\n  \"\"\"\n  if [ -s \"$db\" ]; then\n    DecoyDatabase -in $db $canonical_peptides \\\n                  -out tddb_${fraction}.fasta\n  else\n    DecoyDatabase -in $canonical_peptides \\\n                  -out tddb_${fraction}.fasta\n  fi\n  \"\"\"\n\n}",
        "nb_lignes_process": 22,
        "string_script": "  \"\"\"\n  if [ -s \"$db\" ]; then\n    DecoyDatabase -in $db $canonical_peptides \\\n                  -out tddb_${fraction}.fasta\n  else\n    DecoyDatabase -in $canonical_peptides \\\n                  -out tddb_${fraction}.fasta\n  fi\n  \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "pI_tdbs",
            "canonical_peptides"
        ],
        "nb_inputs": 2,
        "outputs": [
            "combined_tdbs"
        ],
        "nb_outputs": 1,
        "name_workflow": "higsch__togetherforever",
        "directive": [
            "tag \"$fraction\""
        ],
        "when": "",
        "stub": ""
    },
    "MSGFPlus": {
        "name_process": "MSGFPlus",
        "string_process": "\nprocess MSGFPlus {\n\n  tag \"$set $fraction\"\n\n  input:\n  set val(fraction), val(set), val(sample), file(mzml), file(db) from mzmls_fastas\n  file mods\n\n  output:\n  set val(set), val(fraction), val(sample), file(\"${sample}.mzid\") into mzids\n  set val(set), val(fraction), val(sample), file(\"${sample}.mzid\"), file(\"${sample}.tsv\") into mzidtsvs\n\n  script:\n  \"\"\"\n  msgf_plus -Xmx16G -d $db -s $mzml -o \"${sample}.mzid\" -thread 4 -mod $mods -tda 0 -t 10.0ppm -ti -1,2 -m 0 -inst 3 -e 9 -protocol 4 -ntt 2 -minLength 7 -maxLength 50 -minCharge 2 -maxCharge 6 -n 1 -addFeatures 1\n  msgf_plus -Xmx3500M edu.ucsd.msjava.ui.MzIDToTsv -i \"${sample}.mzid\" -o \"${sample}.tsv\"\n  \"\"\"\n\n}",
        "nb_lignes_process": 18,
        "string_script": "  \"\"\"\n  msgf_plus -Xmx16G -d $db -s $mzml -o \"${sample}.mzid\" -thread 4 -mod $mods -tda 0 -t 10.0ppm -ti -1,2 -m 0 -inst 3 -e 9 -protocol 4 -ntt 2 -minLength 7 -maxLength 50 -minCharge 2 -maxCharge 6 -n 1 -addFeatures 1\n  msgf_plus -Xmx3500M edu.ucsd.msjava.ui.MzIDToTsv -i \"${sample}.mzid\" -o \"${sample}.tsv\"\n  \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "mzmls_fastas",
            "mods"
        ],
        "nb_inputs": 2,
        "outputs": [
            "mzids",
            "mzidtsvs"
        ],
        "nb_outputs": 2,
        "name_workflow": "higsch__togetherforever",
        "directive": [
            "tag \"$set $fraction\""
        ],
        "when": "",
        "stub": ""
    },
    "Percolator": {
        "name_process": "Percolator",
        "string_process": "\nprocess Percolator {\n\n  tag \"$set $fractions\"\n\n  publishDir 'results', mode: \"copy\" \n\n  input:\n  set val(set), val(fractions), val(samples), file(\"mzid?\") from mzids2pin\n\n  output:\n  set val(set), file(\"Set${set}.perco.xml\") into percolated\n\n  \"\"\"\n  echo $samples\n  mkdir mzids\n  count=1;for sam in ${samples.join(' ')}; do ln -s `pwd`/mzid\\$count mzids/\\${sam}.mzid; echo mzids/\\${sam}.mzid >> metafile; ((count++));done\n  msgf2pin -o percoin.xml -e trypsin -P \"DECOY_\" metafile\n  percolator -j percoin.xml -X \"Set${set}.perco.xml\" -N 500000 --decoy-xml-output -y\n  \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "\"\"\"\n  echo $samples\n  mkdir mzids\n  count=1;for sam in ${samples.join(' ')}; do ln -s `pwd`/mzid\\$count mzids/\\${sam}.mzid; echo mzids/\\${sam}.mzid >> metafile; ((count++));done\n  msgf2pin -o percoin.xml -e trypsin -P \"DECOY_\" metafile\n  percolator -j percoin.xml -X \"Set${set}.perco.xml\" -N 500000 --decoy-xml-output -y\n  \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [
            "Percolator"
        ],
        "tools_url": [
            "https://bio.tools/percolator"
        ],
        "tools_dico": [
            {
                "name": "Percolator",
                "uri": "https://bio.tools/percolator",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3520",
                            "term": "Proteomics experiment"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0121",
                            "term": "Proteomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3474",
                            "term": "Machine learning"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3631",
                                    "term": "Peptide identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3648",
                                    "term": "Validation of peptide-spectrum matches"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein identification"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2929",
                                    "term": "Protein fragment weight comparison"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3647",
                                    "term": "Blind peptide database search"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3649",
                                    "term": "Target-Decoy"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3631",
                                    "term": "Peptide-spectrum-matching"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3767",
                                    "term": "Protein inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2929",
                                    "term": "PMF"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2929",
                                    "term": "Peptide mass fingerprinting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2929",
                                    "term": "Protein fingerprinting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3647",
                                    "term": "Modification-tolerant peptide database search"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3647",
                                    "term": "Unrestricted peptide database search"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0945",
                                "term": "Peptide identification"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0945",
                                "term": "Peptide identification"
                            }
                        ]
                    }
                ],
                "description": "Semi-supervised learning for peptide identification from MS/MS data.",
                "homepage": "http://noble.gs.washington.edu/proj/percolator/"
            }
        ],
        "inputs": [
            "mzids2pin"
        ],
        "nb_inputs": 1,
        "outputs": [
            "percolated"
        ],
        "nb_outputs": 1,
        "name_workflow": "higsch__togetherforever",
        "directive": [
            "tag \"$set $fractions\"",
            "publishDir 'results', mode: \"copy\""
        ],
        "when": "",
        "stub": ""
    }
}