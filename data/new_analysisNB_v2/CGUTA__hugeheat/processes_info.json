{
    "compile": {
        "name_process": "compile",
        "string_process": "\nprocess compile {\n\tstoreDir \"$params.outdir/bin/\" \n\n\t                                                                                                      \n    \n    input:\n    val repo from \"https://github.com/CGUTA/csvmipmap.git\"\n\n    output:\n    file 'csvmipmap/target/release/csvmipmap' into binary\n\n\n    \"\"\"\n    git clone $repo\n    cd csvmipmap/\n    cargo build --release\n    #cp target/release/csvmipmap ../csvmipmap_b\n    \"\"\"\n\n}",
        "nb_lignes_process": 19,
        "string_script": "\"\"\"\n    git clone $repo\n    cd csvmipmap/\n    cargo build --release\n    #cp target/release/csvmipmap ../csvmipmap_b\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "CARGO"
        ],
        "tools_url": [
            "https://bio.tools/cargo"
        ],
        "tools_dico": [
            {
                "name": "CARGO",
                "uri": "https://bio.tools/cargo",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0218",
                            "term": "Natural language processing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "Pathology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2815",
                            "term": "Human biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0602",
                            "term": "Molecular interactions, pathways and networks"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2640",
                            "term": "Oncology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0218",
                            "term": "NLP"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "Disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "https://en.wikipedia.org/wiki/Pathology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2815",
                            "term": "Humans"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2640",
                            "term": "Cancer biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2640",
                            "term": "https://en.wikipedia.org/wiki/Oncology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Query and retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3778",
                                    "term": "Text annotation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0306",
                                    "term": "Text mining"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data retrieval"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Database retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0306",
                                    "term": "Text data mining"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0306",
                                    "term": "Literature mining"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0306",
                                    "term": "Text analytics"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data extraction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Retrieval"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Portal that uses widgets to allow users to mine literature using iHOP, retrieve disease information from OMIM, visualize 3D SNPs, query protein interactions, and view summarized gene annotation information for cancer related genes in human.",
                "homepage": "http://cargo2.bioinfo.cnio.es/docs/aboutCargo.html"
            }
        ],
        "inputs": [
            "\"https://github"
        ],
        "nb_inputs": 1,
        "outputs": [
            "binary"
        ],
        "nb_outputs": 1,
        "name_workflow": "CGUTA__hugeheat",
        "directive": [
            "storeDir \"$params.outdir/bin/\""
        ],
        "when": "",
        "stub": ""
    },
    "box_sampling_into_long": {
        "name_process": "box_sampling_into_long",
        "string_process": "\nprocess box_sampling_into_long {\n\n\t                                                                                                      \n    \n    input:\n    set datasetID, file(dataset_to_convert) from raw_file\n    file csvmipmap from binary\n\n    output:\n    set datasetID, file(\"data.csv\") into to_normalize\n\n\n    \"\"\"\n    ./$csvmipmap $params.size $dataset_to_convert > data.csv\n    \"\"\"\n\n}",
        "nb_lignes_process": 16,
        "string_script": "\"\"\"\n    ./$csvmipmap $params.size $dataset_to_convert > data.csv\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "raw_file",
            "binary"
        ],
        "nb_inputs": 2,
        "outputs": [
            "to_normalize"
        ],
        "nb_outputs": 1,
        "name_workflow": "CGUTA__hugeheat",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "normalization_colorization": {
        "name_process": "normalization_colorization",
        "string_process": "\nprocess normalization_colorization {\n\t           \n\n\t                                                                                              \n\n    label 'rscript'\n    \n    input:\n    set datasetID, file(long_file) from to_normalize\n    file tmatrix  from file(\"${baseDir}/data/T\")\n    file coldcurves from file(\"${baseDir}/data/blue_no_darkening.csv\")\n    file hotcurves from file(\"${baseDir}/data/fire_no_darkening.csv\")\n    file column_gaps from file(params.column_gaps)\n    file row_gaps from file(params.row_gaps)\n    file grid from file(params.grid) \n\n    output:\n    set datasetID, file(\"data_output.csv\") into to_heat\n\n\n    \"\"\"\n    #!/usr/bin/env Rscript\n    library(data.table)\n    library(magrittr)\n    library(glue)\n\n\n\n    melted <- fread(\"$long_file\")\n    blue <- fread(\"$coldcurves\")\n\tfire <- fread(\"$hotcurves\")\n    Tmat <- read.table(\"$tmatrix\", header = F) %>% as.matrix\n\n    gamma_correction <- function(n){\n\t  if (n < 0.0031308){\n\t    n * 12.92\n\t  } else if (n < 1){\n\t    1.055 * n^(1/2.4) - 0.055\n\t  } else {\n\t    1\n\t  }\n\t}\n\n\tscale_to_u8 <- function(n){\n\t  round(n * 255)\n\t}\n\n\tinto_rgb <- function(curve){\n\t  Tmat %*% curve %>%\n\t    apply(.,1,gamma_correction) %>%\n\t    scale_to_u8\n\t}\n\n\tinto_hex <- function(rgb){\n\t    sprintf(\"%x\",rgb) %>% # hex encoding\n\t    paste0(., collapse=\"\") %>% # formatting\n\t    sprintf(\"#%s\",.)\n\t}\n\n\tinto_string <- function(rgb){\n\t    paste0(rgb, collapse=\"_\") # formatting\n\t}\n\n\tcolormix <- function(curve1, curve2, curve1_proportion){\n\t  \n\t  curve_of_mix <- curve1^curve1_proportion * curve2^(1-curve1_proportion)\n\t  \n\t  curve_of_mix %>% \n\t    into_rgb \n\t}\n\n\tchoose_color <- function(x, output) {\n\t color <- x[1]\n\t color_negative <- x[2]\n\t proportion <- x[3]\n\t intensity <- x[4]\n\n\tif(\"$params.intensity\" == \"false\"){\n\t\tintensity = 1.0\n\t}\n\t \n\t sRGB_to_linear <- function(rgb){\n\t    out <- numeric(length = length(rgb))\n\t    for(i in seq_along(rgb)) {\n\t        if (rgb[i] < 0.04045){\n\t          out[i] <- rgb[i]/12.92\n\t        } else {\n\t          out[i] <- ((rgb[i] + 0.055)/1.055)^2.4\n\t        }\n\t    }\n\t    out\n\t }\n\t \n\t linear_to_sRGB <- function(rgb){\n\t   out <- numeric(length = length(rgb))\n\t    for(i in seq_along(rgb)) {\n\t        if (rgb[i] < 0.0031308){\n\t          out[i] <- rgb[i] * 12.92\n\t        } else {\n\t          out[i] <- 1.055 * rgb[i]^(1/2.4) - 0.055\n\t        } \n\t    }\n\t    round(out)\n\t}\n\t \n\t mask <- function(rgb, intensity){\n\t   round(rgb * intensity)\n\t }\n\n\t gamma_correct_mask <- function(rgb, intensity){\n\t   linear_to_sRGB(sRGB_to_linear(rgb) * intensity)\n\t }\n\n\tgamma_correct_rgb_mix <- function(rgb1, rgb2, rgb1_proportion){\n  \n\t\tlinear_to_sRGB(sRGB_to_linear(rgb1) * rgb1_proportion + sRGB_to_linear(rgb2) * (1 - rgb1_proportion))\n\t\t  \n\t}\n\n\trgb_mix <- function(rgb1, rgb2, rgb1_proportion){\n\t\t  \n\t\tround(rgb1 * rgb1_proportion + rgb2 * (1 - rgb1_proportion))\n\t\t  \n\t}\n\n\thex_to_rgb <- function(hex){\n    \tcol2rgb(hex)[,1]\n\t}\n\n\tbackground_color <- hex_to_rgb(\"$params.background_color\")\n\t \n\t if(!is.na(proportion)){\n\t   \n\t   if(\"$params.bicolor\" == \"NOT_PROVIDED\"){ \n\t\t\tcolor <- colormix(fire[[color]], blue[[color_negative]], proportion) %>% rgb_mix(background_color, intensity) %>% into_string\n\t   } \n\t   else {\n\t   \t\tcolor_pair <- \"$params.bicolor\" %>% strsplit(\",\") %>% unlist\n\t   \t\tcolor_positive <- color_pair[1] %>% hex_to_rgb\n\t   \t\tcolor_negative <- ifelse(length(color_pair) == 2, color_pair[2] %>% hex_to_rgb, color_positive)\n\n\t   \t\tcolor <- rgb_mix(color_positive, color_negative, proportion) %>% rgb_mix(background_color, intensity) %>% into_string\n\t   }\n\t   \n\t } else {\n\t   \n\t   color <- background_color %>% into_string\n\t }\n\t \n\t}\n\n\n\thead(melted)\n\n\tdefault_truncation <- ifelse(\"$params.truncate_extremes_at\" == \"NOT_PROVIDED\", 1.0, $params.truncate_extremes_at) # hardcoded default\n\ttruncate_positive_at <- ifelse(\"$params.truncate_positive_at\" == \"NOT_PROVIDED\", default_truncation, $params.truncate_positive_at)\n\ttruncate_negative_at <- ifelse(\"$params.truncate_negative_at\" == \"NOT_PROVIDED\", default_truncation, $params.truncate_negative_at)\n\n\tthreshold <- ifelse(\"$params.automatic_threshold\" == \"NOT_PROVIDED\", 1, $params.automatic_threshold) # hardcoded default\n\n\tmax <- ifelse(\"$params.automatic_threshold\" == \"NOT_PROVIDED\", truncate_positive_at, quantile(melted[,value], probs = c(threshold)))\n\tmin <- ifelse(\"$params.automatic_threshold\" == \"NOT_PROVIDED\", -truncate_negative_at, quantile(melted[,value_negative], probs = c(1 - threshold)))\n\n\tmelted[value > max, value := max]\n\tmelted[value_negative < min, value_negative := min]\n\n\tif (max > 0){\n\t\tmelted[, value := value/max]\n\t}\n\tif (min < 0){\n\t\tmelted[, value_negative := value_negative/min]\n\t}\n\n\n\tmelted[, color := 9]\n\tmelted[, color_negative := 9]\n\n\tmelted[, intensity := (value + value_negative)/2]\n\tmelted[value == 0, intensity := value_negative]\n\tmelted[value_negative == 0, intensity := value]\n\tmelted[, intensity := round(intensity, 2)]\n\n\tfor (i in 8:1){\n\t  melted[ value < i/9, color := i]\n\t  melted[ value_negative < i/9, color_negative := i]\n\t}\n\n\tmelted[, proprotion_of_positive := round(value/(value + abs(value_negative)), 2)]\n\n\tconversion_table <- melted[,.N,by=.(color, color_negative, proprotion_of_positive, intensity)]\n\thex_colors <- apply(conversion_table, 1, choose_color)\n\n\tconversion_table[[5]] <- hex_colors\n\tsetkey(conversion_table, color, color_negative, proprotion_of_positive, intensity)\n\tsetkey(melted, color, color_negative, proprotion_of_positive, intensity)\n\n\tmerged <- melted[conversion_table, ]\n\tsetnames(merged, \"N\", \"hex_color\")\n\n\t### Adding gaps in heatmap #####\n\n\tcalcualte_gap_mapping <- function(gap_dir, compression_size){ \n\t\t# Depending on kernel size gaps must be adjusted\n\n\t\tgaps <- fread(gap_dir)\n\t\tgaps[,.(mapped_position = floor(position/compression_size))][, mapped_position]\n\t}\n\n\tcarve_gap <- function(what, gap_size, gap_locations){\n\n\n\t\ti_operation <- \"{what} >= i\" %>% glue %>% parse(text = .)\n\t\tj_operation <- \"{what} := {what} + {gap_size}\" %>% glue %>% parse(text = .)\n\n\t\tfor (i in sort(gap_locations, decreasing = T)){\n\t\t\tmerged[ eval(i_operation), eval(j_operation)] #  pixel gap\n\t\t}\n\t\t\n\t}\n\n\n\t# default_gap_size will be used if no gap specified\n\tgap_size <- ifelse(\"$params.default_gap_size\" == \"NOT_PROVIDED\", 5, $params.default_gap_size) # hardcoded default gap size\n\tvertical_gap_size <- ifelse(\"$params.column_gap_size\" == \"NOT_PROVIDED\", gap_size, $params.column_gap_size)\n\thorizontal_gap_size <- ifelse(\"$params.row_gap_size\" == \"NOT_PROVIDED\", gap_size, $params.row_gap_size)\n\n\thorizontal_compression <- strsplit(\"${params.size}\", split=\",\")[[1]][1] %>% as.numeric()\n\tvertical_compression <- strsplit(\"${params.size}\", split=\",\")[[1]][2] %>% as.numeric()\n\n\n\tif(\"$params.column_gaps\" != \"NO_FILE_COLUMNS\") {\n\t\tcarve_gap(\"col\", vertical_gap_size, calcualte_gap_mapping(\"$column_gaps\", horizontal_compression))\n\t}\n\tif(\"$params.row_gaps\" != \"NO_FILE_ROWS\") {\n\t\tcarve_gap(\"id\", horizontal_gap_size, calcualte_gap_mapping(\"$row_gaps\", vertical_compression))\n\t}\n\tif(\"$params.grid\" != \"NO_FILE_GRID\") {\n\t\tcarve_gap(\"id\", horizontal_gap_size, calcualte_gap_mapping(\"$grid\", vertical_compression))\n\t\tcarve_gap(\"col\", vertical_gap_size, calcualte_gap_mapping(\"$grid\", horizontal_compression))\n\t}\n\n\n\n\t###\n\n\t### pixel to parallelogram in render\n\n\n\texageration <-c($params.pixel)\n\tmax_row <- merged[,max(id)]\n\t#row_exageration <- data.table(id = rep(0:max_row, each=exageration[1]), exagerated_row = 0:(max_row*exageration[1]))\n\trow_exageration <- data.table(id = rep(0:max_row, each=exageration[1]), exagerated_row = 0:((max_row+1)*exageration[1] -1))\n\n\tmax_col <- merged[,max(col)]\n\tcol_exageration <- data.table(col = rep(0:max_col, each=exageration[2]), exagerated_col = 0:((max_col+1)*exageration[2] -1))\n\n\texagerated_merged <- merged[row_exageration, on=\"id\", allow.cartesian=T][col_exageration, on=\"col\", allow.cartesian=T][order(-exagerated_row,-exagerated_col),.(exagerated_row,exagerated_col,hex_color)][!is.na(hex_color)]\n\n\t##\n\n\n\tfwrite(exagerated_merged, \"data_output.csv\", col.names = FALSE)\n\t#fwrite(merged[order(-id,-col),.(id, col, hex_color)], \"data_output.csv\", col.names = FALSE)\n    \"\"\"\n\n}",
        "nb_lignes_process": 265,
        "string_script": "\"\"\"\n    #!/usr/bin/env Rscript\n    library(data.table)\n    library(magrittr)\n    library(glue)\n\n\n\n    melted <- fread(\"$long_file\")\n    blue <- fread(\"$coldcurves\")\n\tfire <- fread(\"$hotcurves\")\n    Tmat <- read.table(\"$tmatrix\", header = F) %>% as.matrix\n\n    gamma_correction <- function(n){\n\t  if (n < 0.0031308){\n\t    n * 12.92\n\t  } else if (n < 1){\n\t    1.055 * n^(1/2.4) - 0.055\n\t  } else {\n\t    1\n\t  }\n\t}\n\n\tscale_to_u8 <- function(n){\n\t  round(n * 255)\n\t}\n\n\tinto_rgb <- function(curve){\n\t  Tmat %*% curve %>%\n\t    apply(.,1,gamma_correction) %>%\n\t    scale_to_u8\n\t}\n\n\tinto_hex <- function(rgb){\n\t    sprintf(\"%x\",rgb) %>% # hex encoding\n\t    paste0(., collapse=\"\") %>% # formatting\n\t    sprintf(\"#%s\",.)\n\t}\n\n\tinto_string <- function(rgb){\n\t    paste0(rgb, collapse=\"_\") # formatting\n\t}\n\n\tcolormix <- function(curve1, curve2, curve1_proportion){\n\t  \n\t  curve_of_mix <- curve1^curve1_proportion * curve2^(1-curve1_proportion)\n\t  \n\t  curve_of_mix %>% \n\t    into_rgb \n\t}\n\n\tchoose_color <- function(x, output) {\n\t color <- x[1]\n\t color_negative <- x[2]\n\t proportion <- x[3]\n\t intensity <- x[4]\n\n\tif(\"$params.intensity\" == \"false\"){\n\t\tintensity = 1.0\n\t}\n\t \n\t sRGB_to_linear <- function(rgb){\n\t    out <- numeric(length = length(rgb))\n\t    for(i in seq_along(rgb)) {\n\t        if (rgb[i] < 0.04045){\n\t          out[i] <- rgb[i]/12.92\n\t        } else {\n\t          out[i] <- ((rgb[i] + 0.055)/1.055)^2.4\n\t        }\n\t    }\n\t    out\n\t }\n\t \n\t linear_to_sRGB <- function(rgb){\n\t   out <- numeric(length = length(rgb))\n\t    for(i in seq_along(rgb)) {\n\t        if (rgb[i] < 0.0031308){\n\t          out[i] <- rgb[i] * 12.92\n\t        } else {\n\t          out[i] <- 1.055 * rgb[i]^(1/2.4) - 0.055\n\t        } \n\t    }\n\t    round(out)\n\t}\n\t \n\t mask <- function(rgb, intensity){\n\t   round(rgb * intensity)\n\t }\n\n\t gamma_correct_mask <- function(rgb, intensity){\n\t   linear_to_sRGB(sRGB_to_linear(rgb) * intensity)\n\t }\n\n\tgamma_correct_rgb_mix <- function(rgb1, rgb2, rgb1_proportion){\n  \n\t\tlinear_to_sRGB(sRGB_to_linear(rgb1) * rgb1_proportion + sRGB_to_linear(rgb2) * (1 - rgb1_proportion))\n\t\t  \n\t}\n\n\trgb_mix <- function(rgb1, rgb2, rgb1_proportion){\n\t\t  \n\t\tround(rgb1 * rgb1_proportion + rgb2 * (1 - rgb1_proportion))\n\t\t  \n\t}\n\n\thex_to_rgb <- function(hex){\n    \tcol2rgb(hex)[,1]\n\t}\n\n\tbackground_color <- hex_to_rgb(\"$params.background_color\")\n\t \n\t if(!is.na(proportion)){\n\t   \n\t   if(\"$params.bicolor\" == \"NOT_PROVIDED\"){ \n\t\t\tcolor <- colormix(fire[[color]], blue[[color_negative]], proportion) %>% rgb_mix(background_color, intensity) %>% into_string\n\t   } \n\t   else {\n\t   \t\tcolor_pair <- \"$params.bicolor\" %>% strsplit(\",\") %>% unlist\n\t   \t\tcolor_positive <- color_pair[1] %>% hex_to_rgb\n\t   \t\tcolor_negative <- ifelse(length(color_pair) == 2, color_pair[2] %>% hex_to_rgb, color_positive)\n\n\t   \t\tcolor <- rgb_mix(color_positive, color_negative, proportion) %>% rgb_mix(background_color, intensity) %>% into_string\n\t   }\n\t   \n\t } else {\n\t   \n\t   color <- background_color %>% into_string\n\t }\n\t \n\t}\n\n\n\thead(melted)\n\n\tdefault_truncation <- ifelse(\"$params.truncate_extremes_at\" == \"NOT_PROVIDED\", 1.0, $params.truncate_extremes_at) # hardcoded default\n\ttruncate_positive_at <- ifelse(\"$params.truncate_positive_at\" == \"NOT_PROVIDED\", default_truncation, $params.truncate_positive_at)\n\ttruncate_negative_at <- ifelse(\"$params.truncate_negative_at\" == \"NOT_PROVIDED\", default_truncation, $params.truncate_negative_at)\n\n\tthreshold <- ifelse(\"$params.automatic_threshold\" == \"NOT_PROVIDED\", 1, $params.automatic_threshold) # hardcoded default\n\n\tmax <- ifelse(\"$params.automatic_threshold\" == \"NOT_PROVIDED\", truncate_positive_at, quantile(melted[,value], probs = c(threshold)))\n\tmin <- ifelse(\"$params.automatic_threshold\" == \"NOT_PROVIDED\", -truncate_negative_at, quantile(melted[,value_negative], probs = c(1 - threshold)))\n\n\tmelted[value > max, value := max]\n\tmelted[value_negative < min, value_negative := min]\n\n\tif (max > 0){\n\t\tmelted[, value := value/max]\n\t}\n\tif (min < 0){\n\t\tmelted[, value_negative := value_negative/min]\n\t}\n\n\n\tmelted[, color := 9]\n\tmelted[, color_negative := 9]\n\n\tmelted[, intensity := (value + value_negative)/2]\n\tmelted[value == 0, intensity := value_negative]\n\tmelted[value_negative == 0, intensity := value]\n\tmelted[, intensity := round(intensity, 2)]\n\n\tfor (i in 8:1){\n\t  melted[ value < i/9, color := i]\n\t  melted[ value_negative < i/9, color_negative := i]\n\t}\n\n\tmelted[, proprotion_of_positive := round(value/(value + abs(value_negative)), 2)]\n\n\tconversion_table <- melted[,.N,by=.(color, color_negative, proprotion_of_positive, intensity)]\n\thex_colors <- apply(conversion_table, 1, choose_color)\n\n\tconversion_table[[5]] <- hex_colors\n\tsetkey(conversion_table, color, color_negative, proprotion_of_positive, intensity)\n\tsetkey(melted, color, color_negative, proprotion_of_positive, intensity)\n\n\tmerged <- melted[conversion_table, ]\n\tsetnames(merged, \"N\", \"hex_color\")\n\n\t### Adding gaps in heatmap #####\n\n\tcalcualte_gap_mapping <- function(gap_dir, compression_size){ \n\t\t# Depending on kernel size gaps must be adjusted\n\n\t\tgaps <- fread(gap_dir)\n\t\tgaps[,.(mapped_position = floor(position/compression_size))][, mapped_position]\n\t}\n\n\tcarve_gap <- function(what, gap_size, gap_locations){\n\n\n\t\ti_operation <- \"{what} >= i\" %>% glue %>% parse(text = .)\n\t\tj_operation <- \"{what} := {what} + {gap_size}\" %>% glue %>% parse(text = .)\n\n\t\tfor (i in sort(gap_locations, decreasing = T)){\n\t\t\tmerged[ eval(i_operation), eval(j_operation)] #  pixel gap\n\t\t}\n\t\t\n\t}\n\n\n\t# default_gap_size will be used if no gap specified\n\tgap_size <- ifelse(\"$params.default_gap_size\" == \"NOT_PROVIDED\", 5, $params.default_gap_size) # hardcoded default gap size\n\tvertical_gap_size <- ifelse(\"$params.column_gap_size\" == \"NOT_PROVIDED\", gap_size, $params.column_gap_size)\n\thorizontal_gap_size <- ifelse(\"$params.row_gap_size\" == \"NOT_PROVIDED\", gap_size, $params.row_gap_size)\n\n\thorizontal_compression <- strsplit(\"${params.size}\", split=\",\")[[1]][1] %>% as.numeric()\n\tvertical_compression <- strsplit(\"${params.size}\", split=\",\")[[1]][2] %>% as.numeric()\n\n\n\tif(\"$params.column_gaps\" != \"NO_FILE_COLUMNS\") {\n\t\tcarve_gap(\"col\", vertical_gap_size, calcualte_gap_mapping(\"$column_gaps\", horizontal_compression))\n\t}\n\tif(\"$params.row_gaps\" != \"NO_FILE_ROWS\") {\n\t\tcarve_gap(\"id\", horizontal_gap_size, calcualte_gap_mapping(\"$row_gaps\", vertical_compression))\n\t}\n\tif(\"$params.grid\" != \"NO_FILE_GRID\") {\n\t\tcarve_gap(\"id\", horizontal_gap_size, calcualte_gap_mapping(\"$grid\", vertical_compression))\n\t\tcarve_gap(\"col\", vertical_gap_size, calcualte_gap_mapping(\"$grid\", horizontal_compression))\n\t}\n\n\n\n\t###\n\n\t### pixel to parallelogram in render\n\n\n\texageration <-c($params.pixel)\n\tmax_row <- merged[,max(id)]\n\t#row_exageration <- data.table(id = rep(0:max_row, each=exageration[1]), exagerated_row = 0:(max_row*exageration[1]))\n\trow_exageration <- data.table(id = rep(0:max_row, each=exageration[1]), exagerated_row = 0:((max_row+1)*exageration[1] -1))\n\n\tmax_col <- merged[,max(col)]\n\tcol_exageration <- data.table(col = rep(0:max_col, each=exageration[2]), exagerated_col = 0:((max_col+1)*exageration[2] -1))\n\n\texagerated_merged <- merged[row_exageration, on=\"id\", allow.cartesian=T][col_exageration, on=\"col\", allow.cartesian=T][order(-exagerated_row,-exagerated_col),.(exagerated_row,exagerated_col,hex_color)][!is.na(hex_color)]\n\n\t##\n\n\n\tfwrite(exagerated_merged, \"data_output.csv\", col.names = FALSE)\n\t#fwrite(merged[order(-id,-col),.(id, col, hex_color)], \"data_output.csv\", col.names = FALSE)\n    \"\"\"",
        "nb_lignes_script": 243,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "to_normalize"
        ],
        "nb_inputs": 1,
        "outputs": [
            "to_heat"
        ],
        "nb_outputs": 1,
        "name_workflow": "CGUTA__hugeheat",
        "directive": [
            "label 'rscript'"
        ],
        "when": "",
        "stub": ""
    },
    "compile_display": {
        "name_process": "compile_display",
        "string_process": "\nprocess compile_display {\n\tstoreDir \"$params.outdir/bin/\" \n\n\t                                                                                                      \n    \n    input:\n    val repo from \"https://github.com/CGUTA/display_heatmap.git\"\n\n    output:\n    file 'display_heatmap/target/release/display_heatmap' into binary_display\n\n\n    \"\"\"\n    git clone $repo\n    cd display_heatmap/\n    cargo build --release\n    \"\"\"\n\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\n    git clone $repo\n    cd display_heatmap/\n    cargo build --release\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "CARGO"
        ],
        "tools_url": [
            "https://bio.tools/cargo"
        ],
        "tools_dico": [
            {
                "name": "CARGO",
                "uri": "https://bio.tools/cargo",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0218",
                            "term": "Natural language processing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "Pathology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2815",
                            "term": "Human biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0602",
                            "term": "Molecular interactions, pathways and networks"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2640",
                            "term": "Oncology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0218",
                            "term": "NLP"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "Disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "https://en.wikipedia.org/wiki/Pathology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2815",
                            "term": "Humans"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2640",
                            "term": "Cancer biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2640",
                            "term": "https://en.wikipedia.org/wiki/Oncology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Query and retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3778",
                                    "term": "Text annotation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0306",
                                    "term": "Text mining"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data retrieval"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Database retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0306",
                                    "term": "Text data mining"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0306",
                                    "term": "Literature mining"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0306",
                                    "term": "Text analytics"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data extraction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Retrieval"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Portal that uses widgets to allow users to mine literature using iHOP, retrieve disease information from OMIM, visualize 3D SNPs, query protein interactions, and view summarized gene annotation information for cancer related genes in human.",
                "homepage": "http://cargo2.bioinfo.cnio.es/docs/aboutCargo.html"
            }
        ],
        "inputs": [
            "\"https://github"
        ],
        "nb_inputs": 1,
        "outputs": [
            "binary_display"
        ],
        "nb_outputs": 1,
        "name_workflow": "CGUTA__hugeheat",
        "directive": [
            "storeDir \"$params.outdir/bin/\""
        ],
        "when": "",
        "stub": ""
    },
    "render_to_png": {
        "name_process": "render_to_png",
        "string_process": "\nprocess render_to_png {\n\n\tpublishDir \"$params.outdir/\", mode: 'copy', saveAs: { filename -> \"${datasetID}_t${params.threshold}_b${params.size}_p${params.pixel}_$filename\" }\n\n    \n    input:\n    set datasetID, file(long_file) from to_heat\n    file display from binary_display\n\n    output:\n    file \"heatmap.png\" into out\n\n\n\t\"\"\"\n\t./$display \"$long_file\"\n\t\"\"\"\n\n}",
        "nb_lignes_process": 17,
        "string_script": "\"\"\"\n\t./$display \"$long_file\"\n\t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "to_heat",
            "binary_display"
        ],
        "nb_inputs": 2,
        "outputs": [
            "out"
        ],
        "nb_outputs": 1,
        "name_workflow": "CGUTA__hugeheat",
        "directive": [
            "publishDir \"$params.outdir/\", mode: 'copy', saveAs: { filename -> \"${datasetID}_t${params.threshold}_b${params.size}_p${params.pixel}_$filename\" }"
        ],
        "when": "",
        "stub": ""
    }
}