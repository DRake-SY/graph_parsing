{
    "download_db": {
        "name_process": "download_db",
        "string_process": " process download_db {\n\n        output:\n            file('silva_nr_v132_train_set.fa') into silva_db\n            file('silva_species_assignment_v132.fa') into silva_species_db\n        script:\n            \"\"\"\n            wget https://zenodo.org/record/1172783/files/silva_nr_v132_train_set.fa.gz?download=1 -O silva_nr_v132_train_set.fa.gz\n            gunzip silva_nr_v132_train_set.fa.gz\n            wget https://zenodo.org/record/1172783/files/silva_species_assignment_v132.fa.gz?download=1 -O silva_species_assignment_v132.fa.gz\n            gunzip silva_species_assignment_v132.fa.gz\n            \"\"\"\n    }",
        "nb_lignes_process": 11,
        "string_script": "            \"\"\"\n            wget https://zenodo.org/record/1172783/files/silva_nr_v132_train_set.fa.gz?download=1 -O silva_nr_v132_train_set.fa.gz\n            gunzip silva_nr_v132_train_set.fa.gz\n            wget https://zenodo.org/record/1172783/files/silva_species_assignment_v132.fa.gz?download=1 -O silva_species_assignment_v132.fa.gz\n            gunzip silva_species_assignment_v132.fa.gz\n            \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [
            "silva_db",
            "silva_species_db"
        ],
        "nb_outputs": 2,
        "name_workflow": "maxibor__dada2-nf",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "AdapterRemoval": {
        "name_process": "AdapterRemoval",
        "string_process": "\nprocess AdapterRemoval {\n    tag \"$name\"\n\n    input:\n        set val(name), file(reads) from reads_to_trim\n\n    output:\n        set val(name), file('*.trimmed.fastq') into trimmed_reads\n        file(\"*.settings\") into adapter_removal_results\n\n    script:\n        settings = name+\".settings\"\n        if (params.pairedEnd && !params.collapse){\n            out1 = name+\".pair1.trimmed.fastq\"\n            out2 = name+\".pair2.trimmed.fastq\"\n            \"\"\"\n            AdapterRemoval --basename $name --file1 ${reads[0]} --file2 ${reads[1]} --trimns --trimqualities --minquality 20 --minlength 30 --output1 $out1 --output2 $out2 --threads ${task.cpus} --qualitybase ${params.phred} --settings $settings\n            \"\"\"\n        } else if (params.pairedEnd && params.collapse) {\n            se_out = name+\".trimmed.fastq\"\n            \"\"\"\n            AdapterRemoval --basename $name --file1 ${reads[0]} --file2 ${reads[1]} --trimns --trimqualities --minquality 20 --minlength 30 --collapse --outputcollapsed $se_out --threads ${task.cpus} --qualitybase ${params.phred} --settings $settings\n            \"\"\"\n        } \n        else {\n            se_out = name+\".trimmed.fastq\"\n            \"\"\"\n            AdapterRemoval --basename $name --file1 ${reads[0]} --trimns --trimqualities --minquality 20 --minlength 30 --output1 $se_out --threads ${task.cpus} --qualitybase ${params.phred} --settings $settings\n            \"\"\"\n        }           \n}",
        "nb_lignes_process": 30,
        "string_script": "        settings = name+\".settings\"\n        if (params.pairedEnd && !params.collapse){\n            out1 = name+\".pair1.trimmed.fastq\"\n            out2 = name+\".pair2.trimmed.fastq\"\n            \"\"\"\n            AdapterRemoval --basename $name --file1 ${reads[0]} --file2 ${reads[1]} --trimns --trimqualities --minquality 20 --minlength 30 --output1 $out1 --output2 $out2 --threads ${task.cpus} --qualitybase ${params.phred} --settings $settings\n            \"\"\"\n        } else if (params.pairedEnd && params.collapse) {\n            se_out = name+\".trimmed.fastq\"\n            \"\"\"\n            AdapterRemoval --basename $name --file1 ${reads[0]} --file2 ${reads[1]} --trimns --trimqualities --minquality 20 --minlength 30 --collapse --outputcollapsed $se_out --threads ${task.cpus} --qualitybase ${params.phred} --settings $settings\n            \"\"\"\n        } \n        else {\n            se_out = name+\".trimmed.fastq\"\n            \"\"\"\n            AdapterRemoval --basename $name --file1 ${reads[0]} --trimns --trimqualities --minquality 20 --minlength 30 --output1 $se_out --threads ${task.cpus} --qualitybase ${params.phred} --settings $settings\n            \"\"\"\n        }",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "reads_to_trim"
        ],
        "nb_inputs": 1,
        "outputs": [
            "trimmed_reads",
            "adapter_removal_results"
        ],
        "nb_outputs": 2,
        "name_workflow": "maxibor__dada2-nf",
        "directive": [
            "tag \"$name\""
        ],
        "when": "",
        "stub": ""
    },
    "dada2": {
        "name_process": "dada2",
        "string_process": "\nprocess dada2 {\n    tag \"$name\"\n\n    label 'dada'\n\n    publishDir \"${params.results}/dada\", mode: 'copy'\n\n    input:\n        set val(name), file(fq) from trimmed_reads\n        file(silva) from silva_db.first()\n        file(silva_species) from silva_species_db.first()\n    output:\n        set val(name), file(\"*.species_dada2.csv\") into dada_out\n        set val(name), file(\"*.dada2.csv\") into dada_classify\n        set val(name), file(\"*.read_count.csv\") into dada_read_count_table\n    script:\n        outname = name+\".dada2.csv\"\n        species_out = name+\".species_dada2.csv\"\n        read_count_name = name+\".read_count.csv\"\n         if (!params.pairedEnd || params.collapse)  {\n            \"\"\"\n            #!/usr/bin/env Rscript\n            \n            library(dada2)\n            library(plyr)\n\n            setwd(\".\")\n\n            sample.name = \"$name\"\n\n            fwd = \"${fq[0]}\"\n\n            silva_db = \"${params.silva_db}\"\n            silva_species_db = \"${params.silva_species_db}\"\n\n            # Dereplication            \n            derepFs <- derepFastq(fwd, verbose=TRUE)\n\n            # Learning error rate\n            err_forward_reads <- learnErrors(derepFs, multithread = ${task.cpus})\n\n            # Sample Inference\n            dadaFs <- dada(derepFs, err=err_forward_reads, multithread=${task.cpus})\n\n            # Make sequence table\n            seqtab <- makeSequenceTable(dadaFs)\n\n            # Remove chimeras\n            seqtab.nochim <- removeBimeraDenovo(seqtab, method=\"consensus\", multithread=${task.cpus}, verbose=TRUE)\n\n             # Track read numbers\n            getN <- function(x) sum(getUniques(x))\n            track <- cbind(getN(dadaFs), rowSums(seqtab.nochim))\n            colnames(track) <- c(\"denoisedF\", \"nonchim\")\n            rownames(track) <- sample.name\n            write.csv(track, \"${read_count_name}\")\n\n            # Assign taxonomy \n            taxa <- assignTaxonomy(seqtab.nochim, silva_db, tryRC = TRUE, taxLevels = c(\"Genus\",\"Species\"), multithread=${task.cpus})\n            write.csv(taxa, \"$outname\")\n\n            # Assign species \n            taxa2 = addSpecies(taxtab = taxa, refFasta = silva_species_db)\n\n            rownames(mergers) = mergers[,'sequence']\n            \n            # Write results to disk \n            print(taxa2)\n\n            spec = taxa2[,c(ncol(taxa2)-1,ncol(taxa2))]\n            colnames(spec) = c('Genus','Species')\n            spec_abund = merge(mergers, spec, by = 'row.names')[,c('Genus','Species','abundance')]\n\n\n            write.csv(spec_abund, \"$species_out\")\n            \"\"\"\n        } else {\n            \"\"\"\n            #!/usr/bin/env Rscript\n            \n            library(dada2)\n            library(plyr)\n\n            setwd(\".\")\n\n            sample.name = \"$name\"\n\n            fwd = \"${fq[0]}\"\n            rev = \"${fq[1]}\"\n\n            silva_db = \"${silva}\"\n            silva_species_db = \"${silva_species}\"\n\n            # Dereplication            \n            derepFs <- derepFastq(fwd, verbose=TRUE)\n            derepRs <- derepFastq(rev, verbose=TRUE)\n\n            # Learning error rate\n            err_forward_reads <- learnErrors(derepFs, multithread = ${task.cpus})\n            err_reverse_reads <- learnErrors(derepRs, multithread=${task.cpus})\n\n            # Sample Inference\n            dadaFs <- dada(derepFs, err=err_forward_reads, multithread=${task.cpus})\n            dadaRs <- dada(derepRs, err=err_reverse_reads, multithread=${task.cpus})\n\n            # Merge paired reads \n            mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)\n\n            # Make sequence table\n            seqtab <- makeSequenceTable(mergers)\n\n            # Remove chimeras\n            seqtab.nochim <- removeBimeraDenovo(seqtab, method=\"consensus\", multithread=${task.cpus}, verbose=TRUE)\n\n            # Track read numbers\n            getN <- function(x) sum(getUniques(x))\n            track <- cbind(getN(dadaFs), getN(dadaRs), getN(mergers), rowSums(seqtab.nochim))\n            colnames(track) <- c(\"denoisedF\", \"denoisedR\", \"merged\", \"nonchim\")\n            rownames(track) <- sample.name\n            write.csv(track, \"${read_count_name}\")\n\n            # Assign taxonomy \n            taxa <- assignTaxonomy(seqtab.nochim, silva_db, tryRC = TRUE, taxLevels = c(\"Genus\",\"Species\"), multithread=${task.cpus})\n            write.csv(taxa, \"$outname\")\n\n            # Assign species \n            taxa2 = addSpecies(taxtab = taxa, refFasta = silva_species_db)\n\n            rownames(mergers) = mergers[,'sequence']\n            \n            # Write results to disk \n            print(taxa2)\n\n            spec = taxa2[,c(ncol(taxa2)-1,ncol(taxa2))]\n            colnames(spec) = c('Genus','Species')\n            spec_abund = merge(mergers, spec, by = 'row.names')[,c('Genus','Species','abundance')]\n\n\n            write.csv(spec_abund, \"$species_out\")\n            \"\"\"\n        }\n        \n}",
        "nb_lignes_process": 142,
        "string_script": "        outname = name+\".dada2.csv\"\n        species_out = name+\".species_dada2.csv\"\n        read_count_name = name+\".read_count.csv\"\n         if (!params.pairedEnd || params.collapse)  {\n            \"\"\"\n            #!/usr/bin/env Rscript\n            \n            library(dada2)\n            library(plyr)\n\n            setwd(\".\")\n\n            sample.name = \"$name\"\n\n            fwd = \"${fq[0]}\"\n\n            silva_db = \"${params.silva_db}\"\n            silva_species_db = \"${params.silva_species_db}\"\n\n            # Dereplication            \n            derepFs <- derepFastq(fwd, verbose=TRUE)\n\n            # Learning error rate\n            err_forward_reads <- learnErrors(derepFs, multithread = ${task.cpus})\n\n            # Sample Inference\n            dadaFs <- dada(derepFs, err=err_forward_reads, multithread=${task.cpus})\n\n            # Make sequence table\n            seqtab <- makeSequenceTable(dadaFs)\n\n            # Remove chimeras\n            seqtab.nochim <- removeBimeraDenovo(seqtab, method=\"consensus\", multithread=${task.cpus}, verbose=TRUE)\n\n             # Track read numbers\n            getN <- function(x) sum(getUniques(x))\n            track <- cbind(getN(dadaFs), rowSums(seqtab.nochim))\n            colnames(track) <- c(\"denoisedF\", \"nonchim\")\n            rownames(track) <- sample.name\n            write.csv(track, \"${read_count_name}\")\n\n            # Assign taxonomy \n            taxa <- assignTaxonomy(seqtab.nochim, silva_db, tryRC = TRUE, taxLevels = c(\"Genus\",\"Species\"), multithread=${task.cpus})\n            write.csv(taxa, \"$outname\")\n\n            # Assign species \n            taxa2 = addSpecies(taxtab = taxa, refFasta = silva_species_db)\n\n            rownames(mergers) = mergers[,'sequence']\n            \n            # Write results to disk \n            print(taxa2)\n\n            spec = taxa2[,c(ncol(taxa2)-1,ncol(taxa2))]\n            colnames(spec) = c('Genus','Species')\n            spec_abund = merge(mergers, spec, by = 'row.names')[,c('Genus','Species','abundance')]\n\n\n            write.csv(spec_abund, \"$species_out\")\n            \"\"\"\n        } else {\n            \"\"\"\n            #!/usr/bin/env Rscript\n            \n            library(dada2)\n            library(plyr)\n\n            setwd(\".\")\n\n            sample.name = \"$name\"\n\n            fwd = \"${fq[0]}\"\n            rev = \"${fq[1]}\"\n\n            silva_db = \"${silva}\"\n            silva_species_db = \"${silva_species}\"\n\n            # Dereplication            \n            derepFs <- derepFastq(fwd, verbose=TRUE)\n            derepRs <- derepFastq(rev, verbose=TRUE)\n\n            # Learning error rate\n            err_forward_reads <- learnErrors(derepFs, multithread = ${task.cpus})\n            err_reverse_reads <- learnErrors(derepRs, multithread=${task.cpus})\n\n            # Sample Inference\n            dadaFs <- dada(derepFs, err=err_forward_reads, multithread=${task.cpus})\n            dadaRs <- dada(derepRs, err=err_reverse_reads, multithread=${task.cpus})\n\n            # Merge paired reads \n            mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)\n\n            # Make sequence table\n            seqtab <- makeSequenceTable(mergers)\n\n            # Remove chimeras\n            seqtab.nochim <- removeBimeraDenovo(seqtab, method=\"consensus\", multithread=${task.cpus}, verbose=TRUE)\n\n            # Track read numbers\n            getN <- function(x) sum(getUniques(x))\n            track <- cbind(getN(dadaFs), getN(dadaRs), getN(mergers), rowSums(seqtab.nochim))\n            colnames(track) <- c(\"denoisedF\", \"denoisedR\", \"merged\", \"nonchim\")\n            rownames(track) <- sample.name\n            write.csv(track, \"${read_count_name}\")\n\n            # Assign taxonomy \n            taxa <- assignTaxonomy(seqtab.nochim, silva_db, tryRC = TRUE, taxLevels = c(\"Genus\",\"Species\"), multithread=${task.cpus})\n            write.csv(taxa, \"$outname\")\n\n            # Assign species \n            taxa2 = addSpecies(taxtab = taxa, refFasta = silva_species_db)\n\n            rownames(mergers) = mergers[,'sequence']\n            \n            # Write results to disk \n            print(taxa2)\n\n            spec = taxa2[,c(ncol(taxa2)-1,ncol(taxa2))]\n            colnames(spec) = c('Genus','Species')\n            spec_abund = merge(mergers, spec, by = 'row.names')[,c('Genus','Species','abundance')]\n\n\n            write.csv(spec_abund, \"$species_out\")\n            \"\"\"\n        }",
        "nb_lignes_script": 124,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "trimmed_reads",
            "silva_db",
            "silva_species_db"
        ],
        "nb_inputs": 3,
        "outputs": [
            "dada_out",
            "dada_classify",
            "dada_read_count_table"
        ],
        "nb_outputs": 3,
        "name_workflow": "maxibor__dada2-nf",
        "directive": [
            "tag \"$name\"",
            "label 'dada'",
            "publishDir \"${params.results}/dada\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "dada2_to_taxo": {
        "name_process": "dada2_to_taxo",
        "string_process": "\nprocess dada2_to_taxo {\n    tag \"$name\"\n\n    label 'ristretto'\n\n    publishDir \"${params.results}/taxo\", mode: 'copy'\n\n    input:\n        set val(name), file(dd) from dada_out\n    output:\n        set val(name), file(\"*.dadataxo.csv\") into dada_taxo\n    script:\n        \"\"\"\n        dada2taxo.py -s $name -r ${params.rank} $dd\n        \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "        \"\"\"\n        dada2taxo.py -s $name -r ${params.rank} $dd\n        \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "dada_out"
        ],
        "nb_inputs": 1,
        "outputs": [
            "dada_taxo"
        ],
        "nb_outputs": 1,
        "name_workflow": "maxibor__dada2-nf",
        "directive": [
            "tag \"$name\"",
            "label 'ristretto'",
            "publishDir \"${params.results}/taxo\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    },
    "dada_merge": {
        "name_process": "dada_merge",
        "string_process": "\nprocess dada_merge {\n\n    label 'ristretto'\n\n    publishDir \"${params.results}/merged\", mode: 'copy'\n\n    input:\n        file(csv_count) from dada_taxo.collect()\n\n    output:\n        file('dada2_otu_table.csv') into dada_merged\n\n    script:\n        out = \"dada2_otu_table.csv\"\n        \"\"\"\n        dada_merge.py -o $out\n        \"\"\"    \n}",
        "nb_lignes_process": 17,
        "string_script": "        out = \"dada2_otu_table.csv\"\n        \"\"\"\n        dada_merge.py -o $out\n        \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "Ragout"
        ],
        "tools_url": [
            "https://bio.tools/ragout"
        ],
        "tools_dico": [
            {
                "name": "Ragout",
                "uri": "https://bio.tools/ragout",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Tool for assisted assembly using multiple references. It takes a short read assembly (a set of contigs), a set of related references and a corresponding phylogenetic tree and then assembles the contigs into scaffolds.",
                "homepage": "http://fenderglass.github.io/Ragout/"
            }
        ],
        "inputs": [
            "dada_taxo"
        ],
        "nb_inputs": 1,
        "outputs": [
            "dada_merged"
        ],
        "nb_outputs": 1,
        "name_workflow": "maxibor__dada2-nf",
        "directive": [
            "label 'ristretto'",
            "publishDir \"${params.results}/merged\", mode: 'copy'"
        ],
        "when": "",
        "stub": ""
    }
}