{
    "asm_megahit": {
        "name_process": "asm_megahit",
        "string_process": "\nprocess asm_megahit{\n  tag { \"${sample_id}\" }\n  publishDir \"${params.publish_base_dir}/${sample_id}/megahit\", mode: 'copy', pattern: \"1_assembly\"\n\n  input:\n  set sample_id, reads from asm_megahit_in\n\n  output:\n  set sample_id,val('megahit'), \"contigs.fa\" optional true into asm_megahit_out\n  file \"1_assembly\" optional true into asm_megahit_dir_out\n\n  script:\n  \"\"\"\n  megahit -t ${task.cpus} --presets meta-sensitive -1 ${reads[0]} -2 ${reads[1]} -r ${reads[2]} --cpu-only  -o 1_assembly\n  if [ -s 1_assembly/final.contigs.fa ]; then cat 1_assembly/final.contigs.fa | sed -e 's/\\\\s\\\\+/,/g' > contigs.fa; fi\n  \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "  \"\"\"\n  megahit -t ${task.cpus} --presets meta-sensitive -1 ${reads[0]} -2 ${reads[1]} -r ${reads[2]} --cpu-only  -o 1_assembly\n  if [ -s 1_assembly/final.contigs.fa ]; then cat 1_assembly/final.contigs.fa | sed -e 's/\\\\s\\\\+/,/g' > contigs.fa; fi\n  \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "MEGAHIT"
        ],
        "tools_url": [
            "https://bio.tools/megahit"
        ],
        "tools_dico": [
            {
                "name": "MEGAHIT",
                "uri": "https://bio.tools/megahit",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0610",
                            "term": "Ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Single node assembler for large and complex metagenomics NGS reads, such as soil. It makes use of succinct de Bruijn graph to achieve low memory usage, whereas its goal is not to make memory usage as low as possible.",
                "homepage": "https://github.com/voutcn/megahit"
            }
        ],
        "inputs": [
            "asm_megahit_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "asm_megahit_out",
            "asm_megahit_dir_out"
        ],
        "nb_outputs": 2,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag { \"${sample_id}\" }",
            "publishDir \"${params.publish_base_dir}/${sample_id}/megahit\", mode: 'copy', pattern: \"1_assembly\""
        ],
        "when": "",
        "stub": ""
    },
    "asm_metaspades": {
        "name_process": "asm_metaspades",
        "string_process": "\nprocess asm_metaspades{\n  tag { \"${sample_id}\" }\n  publishDir \"${params.publish_base_dir}/${sample_id}/metaspades\", mode: 'copy', pattern: \"1_assembly\"\n\n  input:\n                                               \n  set sample_id, reads from Channel.empty()\n\n  output:\n  set sample_id,val('metaspades'),\"contigs.fa\" optional true into asm_metaspades_out\n  file \"1_assembly\" optional true into asm_metaspades_dir_out\n\n  script:\n  \"\"\"\n  spades.py --meta -t ${task.cpus} -1 ${reads[0]} -2 ${reads[1]} -o 1_assembly -m ${params.max_spades_mem}\n  if [ -s 1_assembly/contigs.fasta ]; then ln 1_assembly/contigs.fasta contigs.fa; fi\n  \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "  \"\"\"\n  spades.py --meta -t ${task.cpus} -1 ${reads[0]} -2 ${reads[1]} -o 1_assembly -m ${params.max_spades_mem}\n  if [ -s 1_assembly/contigs.fasta ]; then ln 1_assembly/contigs.fasta contigs.fa; fi\n  \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [
            "asm_metaspades_out",
            "asm_metaspades_dir_out"
        ],
        "nb_outputs": 2,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag { \"${sample_id}\" }",
            "publishDir \"${params.publish_base_dir}/${sample_id}/metaspades\", mode: 'copy', pattern: \"1_assembly\""
        ],
        "when": "",
        "stub": ""
    },
    "asm_filter_contigs": {
        "name_process": "asm_filter_contigs",
        "string_process": "\nprocess asm_filter_contigs{\n  tag { \"${sample_id}/${assembler}\" }\n  publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}/2_filt_contigs\", mode:'link'\n\n  input:\n  set sample_id,assembler,\"contigs.fa\" from all_assemblies\n\n  output:\n  set sample_id,assembler,\"contigs_filt.fa\" into asm_filter_contigs_out\n\n  script:\n  \"\"\"\n  seqtk seq -L ${params.min_ctg_size} contigs.fa > contigs_filt.fa\n  \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "  \"\"\"\n  seqtk seq -L ${params.min_ctg_size} contigs.fa > contigs_filt.fa\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "seqtk"
        ],
        "tools_url": [
            "https://bio.tools/seqtk"
        ],
        "tools_dico": [
            {
                "name": "seqtk",
                "uri": "https://bio.tools/seqtk",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Biological databases"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Data management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Information systems"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Databases and information systems"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Data handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2121",
                                    "term": "Sequence file editing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Utility operation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File processing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Report handling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A tool for processing sequences in the FASTA or FASTQ format. It parses both FASTA and FASTQ files which can also be optionally compressed by gzip.",
                "homepage": "https://github.com/lh3/seqtk"
            }
        ],
        "inputs": [
            "all_assemblies"
        ],
        "nb_inputs": 1,
        "outputs": [
            "asm_filter_contigs_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag { \"${sample_id}/${assembler}\" }",
            "publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}/2_filt_contigs\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "asm_map_reads_to_contigs": {
        "name_process": "asm_map_reads_to_contigs",
        "string_process": "\nprocess asm_map_reads_to_contigs{\n  tag { \"${sample_id}/${assembler}\" }\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}/2_filt_contigs\", mode:'link'\n\n  input:\n  set sample_id, 'reads_*.fq.gz',assembler,\"contigs_filt.fa\" from asm_map_reads_to_contigs_in\n\n  output:\n  set sample_id, assembler,\"reads_to_contigs.sam.gz\" into asm_map_reads_to_contigs_out\n\n  script:\n  \"\"\"\n  bbwrap.sh ref=contigs_filt.fa in=reads_1.fq.gz,reads_3.fq.gz in2=reads_2.fq.gz,null out=reads_to_contigs.sam.gz usejni=t kfilter=22 subfilter=15 maxindel=80\n  \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "  \"\"\"\n  bbwrap.sh ref=contigs_filt.fa in=reads_1.fq.gz,reads_3.fq.gz in2=reads_2.fq.gz,null out=reads_to_contigs.sam.gz usejni=t kfilter=22 subfilter=15 maxindel=80\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "asm_map_reads_to_contigs_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "asm_map_reads_to_contigs_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag { \"${sample_id}/${assembler}\" }",
            "publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}/2_filt_contigs\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "asm_mapping_stats": {
        "name_process": "asm_mapping_stats",
        "string_process": "\nprocess asm_mapping_stats{\n  tag {\"${sample_id}/${assembler}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}/2_filt_contigs\", mode:'link'\n\n  input:\n  set sample_id, assembler,'reads_to_contigs.sam.gz' from asm_mapping_stats_in\n\n  output:\n  set sample_id, assembler, \"${sample_id}_${assembler}_flagstat.txt\" into asm_mapping_stats_out\n\n  script:\n  \"\"\"\n  samtools flagstat reads_to_contigs.sam.gz > ${sample_id}_${assembler}_flagstat.txt\n  \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "  \"\"\"\n  samtools flagstat reads_to_contigs.sam.gz > ${sample_id}_${assembler}_flagstat.txt\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "asm_mapping_stats_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "asm_mapping_stats_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"${sample_id}/${assembler}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}/2_filt_contigs\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "asm_per_ctg_coverage": {
        "name_process": "asm_per_ctg_coverage",
        "string_process": "\nprocess asm_per_ctg_coverage{\n  tag {\"${sample_id}/${assembler}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}/2_filt_contigs\", mode:'link'\n\n  input:\n  set sample_id, assembler,\"reads_to_contigs.sam.gz\" from asm_per_ctg_coverage_in\n\n  output:\n  set sample_id, assembler,\"reads_to_contigs.cov.txt\" into asm_per_ctg_coverage_out\n\n  script:\n  \"\"\"\n  pileup.sh in=reads_to_contigs.sam.gz out=reads_to_contigs.cov.txt\n  \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "  \"\"\"\n  pileup.sh in=reads_to_contigs.sam.gz out=reads_to_contigs.cov.txt\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "asm_per_ctg_coverage_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "asm_per_ctg_coverage_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"${sample_id}/${assembler}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}/2_filt_contigs\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "tax_reads_metaphlan2": {
        "name_process": "tax_reads_metaphlan2",
        "string_process": "\nprocess tax_reads_metaphlan2{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/reads\", mode:'link'\n\n  input:\n  set sample_id, 'reads_*fq.gz' from tax_reads_metaphlan2_in\n\n  output:\n  set sample_id, \"${sample_id}_metaphlan2.tsv\" into tax_reads_metaphlan2_out\n\n  script:\n  \"\"\"\n  zcat *fq.gz > seq.fq\n  metaphlan2.py --nproc ${task.cpus} --input_type multifastq --sample_id_key '#clade' \\\n  \t\t--sample_id '${sample_id}' seq.fq ${sample_id}_metaphlan2.tsv\n  rm seq.fq\n  \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "  \"\"\"\n  zcat *fq.gz > seq.fq\n  metaphlan2.py --nproc ${task.cpus} --input_type multifastq --sample_id_key '#clade' \\\n  \t\t--sample_id '${sample_id}' seq.fq ${sample_id}_metaphlan2.tsv\n  rm seq.fq\n  \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "tax_reads_metaphlan2_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "tax_reads_metaphlan2_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/reads\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "tax_reads_kraken2": {
        "name_process": "tax_reads_kraken2",
        "string_process": "\nprocess tax_reads_kraken2{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/reads\", mode:'link'\n\n  input:\n  set sample_id, 'reads_*.fq.gz' from tax_reads_kraken2_in\n\n  output:\n  set sample_id, \"${sample_id}_kraken2.txt\",\"${sample_id}_kraken2_report.txt\" into tax_reads_kraken2_out\n  set sample_id, \"${sample_id}_kraken2_unmapped.fq\" into tax_reads_unmapped_kraken2\n  \n  script:\n  \"\"\"\n  kraken2 --db ${params.kraken2_db} --threads ${task.cpus} --output ${sample_id}_kraken2.txt \\\n    --unclassified-out ${sample_id}_kraken2_unmapped.fq \\\n    --report ${sample_id}_kraken2_report.txt --gzip-compressed reads_*.fq.gz\n  \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "  \"\"\"\n  kraken2 --db ${params.kraken2_db} --threads ${task.cpus} --output ${sample_id}_kraken2.txt \\\n    --unclassified-out ${sample_id}_kraken2_unmapped.fq \\\n    --report ${sample_id}_kraken2_report.txt --gzip-compressed reads_*.fq.gz\n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "kraken2"
        ],
        "tools_url": [
            "https://bio.tools/kraken2"
        ],
        "tools_dico": [
            {
                "name": "kraken2",
                "uri": "https://bio.tools/kraken2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3028",
                                "term": "Taxonomy"
                            }
                        ]
                    }
                ],
                "description": "Kraken 2 is the newest version of Kraken, a taxonomic classification system using exact k-mer matches to achieve high accuracy and fast classification speeds. This classifier matches each k-mer within a query sequence to the lowest common ancestor (LCA) of all genomes containing the given k-mer. The k-mer assignments inform the classification algorithm.",
                "homepage": "https://ccb.jhu.edu/software/kraken2/"
            }
        ],
        "inputs": [
            "tax_reads_kraken2_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "tax_reads_kraken2_out",
            "tax_reads_unmapped_kraken2"
        ],
        "nb_outputs": 2,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/reads\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "tax_reads_FastViromeExplorer": {
        "name_process": "tax_reads_FastViromeExplorer",
        "string_process": "\nprocess tax_reads_FastViromeExplorer{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/reads\", mode:'link'\n\n  input:\n  set sample_id, 'reads_*.fq.gz' from tax_reads_FastViromeExplorer_in\n\n  output:\n  file \"${sample_id}_fastviromeexplorer.sam\" into fve_out_1\n  file \"${sample_id}_fastviromeexplorer_abundance.tsv\" into fve_out_2\n\n  script:\n  \"\"\"\n  java -cp \\${FVE_PATH}/bin FastViromeExplorer -i ${params.FVE_index} -l ${params.FVE_viruslist} -1 reads_1.fq.gz -2 reads_2.fq.gz  -o ./\n  mv FastViromeExplorer-reads-mapped-sorted.sam ${sample_id}_fastviromeexplorer.sam\n  mv FastViromeExplorer-final-sorted-abundance.tsv ${sample_id}_fastviromeexplorer_abundance.tsv\n  \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "  \"\"\"\n  java -cp \\${FVE_PATH}/bin FastViromeExplorer -i ${params.FVE_index} -l ${params.FVE_viruslist} -1 reads_1.fq.gz -2 reads_2.fq.gz  -o ./\n  mv FastViromeExplorer-reads-mapped-sorted.sam ${sample_id}_fastviromeexplorer.sam\n  mv FastViromeExplorer-final-sorted-abundance.tsv ${sample_id}_fastviromeexplorer_abundance.tsv\n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "tax_reads_FastViromeExplorer_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fve_out_1",
            "fve_out_2"
        ],
        "nb_outputs": 2,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/reads\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "tax_contigs_kraken2": {
        "name_process": "tax_contigs_kraken2",
        "string_process": "\nprocess tax_contigs_kraken2{\n  tag {\"${sample_id}_${assembler}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}\", mode:'link'\n\n  input:\n  set sample_id, assembler, 'contigs.fa' from tax_contigs_kraken2_in\n\n  output:\n  set sample_id, assembler,\"${sample_id}_${assembler}_kraken2.txt\",\"${sample_id}_${assembler}_kraken2_report.txt\" into tax_contigs_kraken2_out\n  set sample_id, assembler,\"${sample_id}_${assembler}_kraken2_unmapped.fa\" into tax_contigs_unmapped_kraken2                          \n  file \"${sample_id}_${assembler}_kraken2_unmapped.fa\" into tax_contigs_kraken2_unmapped\n\n  script:\n  \"\"\"\n  kraken2 --db ${params.kraken2_db} --threads ${task.cpus} --use-names \\\n    --unclassified-out ${sample_id}_${assembler}_kraken2_unmapped.fa \\\n    --output ${sample_id}_${assembler}_kraken2.txt \\\n    --report ${sample_id}_${assembler}_kraken2_report.txt contigs.fa\n  \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "  \"\"\"\n  kraken2 --db ${params.kraken2_db} --threads ${task.cpus} --use-names \\\n    --unclassified-out ${sample_id}_${assembler}_kraken2_unmapped.fa \\\n    --output ${sample_id}_${assembler}_kraken2.txt \\\n    --report ${sample_id}_${assembler}_kraken2_report.txt contigs.fa\n  \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "kraken2"
        ],
        "tools_url": [
            "https://bio.tools/kraken2"
        ],
        "tools_dico": [
            {
                "name": "kraken2",
                "uri": "https://bio.tools/kraken2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3028",
                                "term": "Taxonomy"
                            }
                        ]
                    }
                ],
                "description": "Kraken 2 is the newest version of Kraken, a taxonomic classification system using exact k-mer matches to achieve high accuracy and fast classification speeds. This classifier matches each k-mer within a query sequence to the lowest common ancestor (LCA) of all genomes containing the given k-mer. The k-mer assignments inform the classification algorithm.",
                "homepage": "https://ccb.jhu.edu/software/kraken2/"
            }
        ],
        "inputs": [
            "tax_contigs_kraken2_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "tax_contigs_kraken2_out",
            "tax_contigs_unmapped_kraken2",
            "tax_contigs_kraken2_unmapped"
        ],
        "nb_outputs": 3,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"${sample_id}_${assembler}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "tax_contigs_diamond": {
        "name_process": "tax_contigs_diamond",
        "string_process": "\nprocess tax_contigs_diamond{\n  tag {\"${sample_id}_${assembler}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}\", mode:'link'\n\n  input:\n  set sample_id, assembler, 'contigs.fa' from tax_contigs_diamond_in\n\n  output:\n  set sample_id, assembler,\"${sample_id}_${assembler}_diamond.tsv\" into tax_contigs_diamond_out\n  set sample_id, assembler,\"${sample_id}_${assembler}_diamond_unmapped.fa\" into tax_contigs_unmapped_diamond                          \n  file \"${sample_id}_${assembler}_diamond_unmapped.fa\" into diamond_unmapped_out \t\t\t\t\t\t\n  file 'diamond_blast_cols.txt' into tax_diamond_blast_cols\n  \n  \n\n  script:\n  blast_cols='qseqid sseqid qstart qend sstart send evalue score length pident nident mismatch positive ppos gapopen gaps staxids'\n  \"\"\"\n  diamond blastx --sensitive --masking 1 -p ${task.cpus} --db ${params.diamond_db} \\\n    --taxonmap ${params.diamond_taxonmap} --taxonnodes ${params.diamond_taxonnodes} \\\n    --query contigs.fa --out ${sample_id}_${assembler}_diamond.tsv \\\n    --outfmt 6 ${blast_cols} \\\n    --un ${sample_id}_${assembler}_diamond_unmapped.fa\n  echo '${blast_cols}' > diamond_blast_cols.txt \n  \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "  blast_cols='qseqid sseqid qstart qend sstart send evalue score length pident nident mismatch positive ppos gapopen gaps staxids'\n  \"\"\"\n  diamond blastx --sensitive --masking 1 -p ${task.cpus} --db ${params.diamond_db} \\\n    --taxonmap ${params.diamond_taxonmap} --taxonnodes ${params.diamond_taxonnodes} \\\n    --query contigs.fa --out ${sample_id}_${assembler}_diamond.tsv \\\n    --outfmt 6 ${blast_cols} \\\n    --un ${sample_id}_${assembler}_diamond_unmapped.fa\n  echo '${blast_cols}' > diamond_blast_cols.txt \n  \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "Diamond"
        ],
        "tools_url": [
            "https://bio.tools/diamond"
        ],
        "tools_dico": [
            {
                "name": "Diamond",
                "uri": "https://bio.tools/diamond",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0258",
                                    "term": "Sequence alignment analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Sequence aligner for protein and translated DNA searches and functions as a drop-in replacement for the NCBI BLAST software tools. It is suitable for protein-protein search as well as DNA-protein search on short reads and longer sequences including contigs and assemblies, providing a speedup of BLAST ranging up to x20,000.",
                "homepage": "https://github.com/bbuchfink/diamond"
            }
        ],
        "inputs": [
            "tax_contigs_diamond_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "tax_contigs_diamond_out",
            "tax_contigs_unmapped_diamond",
            "diamond_unmapped_out",
            "tax_diamond_blast_cols"
        ],
        "nb_outputs": 4,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"${sample_id}_${assembler}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "tax_contigs_diamond_view": {
        "name_process": "tax_contigs_diamond_view",
        "string_process": "\nprocess tax_contigs_diamond_view{\n  tag {\"${sample_id}_${assembler}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}\", mode:'link'\n\n  input:\n  set sample_id, assembler, 'diamond.daa' from Channel.empty()\n\n  output:\n  set sample_id, assembler,\"${sample_id}_${assembler}_diamond.daa\", \"${sample_id}_${assembler}_diamond.tsv\" into tax_contigs_diamond_view_out\n\n  script:\n  \"\"\"\n  diamond view -p ${task.cpus} --db ${params.diamond_db} \\\n    --outfmt 6 qseqid sseqid qstart qend sstart send evalue score length pident nident mismatch positive ppos gapopen gaps staxids \\\n    --daa diamond.daa \\\n    --out ${sample_id}_${assembler}_diamond.tsv\n  \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "  \"\"\"\n  diamond view -p ${task.cpus} --db ${params.diamond_db} \\\n    --outfmt 6 qseqid sseqid qstart qend sstart send evalue score length pident nident mismatch positive ppos gapopen gaps staxids \\\n    --daa diamond.daa \\\n    --out ${sample_id}_${assembler}_diamond.tsv\n  \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "Diamond"
        ],
        "tools_url": [
            "https://bio.tools/diamond"
        ],
        "tools_dico": [
            {
                "name": "Diamond",
                "uri": "https://bio.tools/diamond",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0258",
                                    "term": "Sequence alignment analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Sequence aligner for protein and translated DNA searches and functions as a drop-in replacement for the NCBI BLAST software tools. It is suitable for protein-protein search as well as DNA-protein search on short reads and longer sequences including contigs and assemblies, providing a speedup of BLAST ranging up to x20,000.",
                "homepage": "https://github.com/bbuchfink/diamond"
            }
        ],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [
            "tax_contigs_diamond_view_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"${sample_id}_${assembler}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "tax_contigs_virfinder": {
        "name_process": "tax_contigs_virfinder",
        "string_process": "\nprocess tax_contigs_virfinder{\n  tag {\"${sample_id}_${assembler}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}\", mode:'link'\n\n  input:\n  set sample_id, assembler, 'contigs.fa' from tax_contigs_virfinder_in\n\n  output:\n  set sample_id, assembler,\"${sample_id}_${assembler}_virfinder.csv\" into tax_contigs_virfinder_out\n\n  script:\n  \"\"\"\n  #!/usr/bin/env Rscript\n  library(VirFinder)\n  predResult <- VF.pred(\"contigs.fa\")\n  write.csv(predResult,file=\"${sample_id}_${assembler}_virfinder.csv\")\n  \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "  \"\"\"\n  #!/usr/bin/env Rscript\n  library(VirFinder)\n  predResult <- VF.pred(\"contigs.fa\")\n  write.csv(predResult,file=\"${sample_id}_${assembler}_virfinder.csv\")\n  \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "Rscript",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "tax_contigs_virfinder_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "tax_contigs_virfinder_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"${sample_id}_${assembler}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "tax_contigs_FragGeneScan": {
        "name_process": "tax_contigs_FragGeneScan",
        "string_process": "\nprocess tax_contigs_FragGeneScan{\n  tag {\"${sample_id}_${assembler}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}/orfs\", mode:'link'\n\n  input:\n  set sample_id, assembler, 'contigs.fa' from tax_contigs_fgs_in\n\n  output:\n  set sample_id, assembler,\"${sample_id}_${assembler}_fgs_orfs.*\" into tax_contigs_fgs_out\n\n  script:\n  \"\"\"\n  run_FragGeneScan.pl -thread ${task.cpus} -complete 1 -train=illumina_10 \\\n    -genome=contigs.fa -out=${sample_id}_${assembler}_fgs_orfs\n  \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "  \"\"\"\n  run_FragGeneScan.pl -thread ${task.cpus} -complete 1 -train=illumina_10 \\\n    -genome=contigs.fa -out=${sample_id}_${assembler}_fgs_orfs\n  \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "tax_contigs_fgs_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "tax_contigs_fgs_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"${sample_id}_${assembler}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}/orfs\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "tax_contigs_unmapped_merged": {
        "name_process": "tax_contigs_unmapped_merged",
        "string_process": "\nprocess tax_contigs_unmapped_merged{\n  tag {\"${sample_id}_${assembler}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}\", mode:'link'\n\n  input:\n  set sample_id, assembler, un_kraken2, un_diamond from unmapped_contigs\n \n  output:\n  set sample_id, assembler, \"${sample_id}_${assembler}_unmapped_contigs.fa\" into tax_contigs_merged_unmapped\n  file \"${sample_id}_${assembler}_unmapped_contigs.fa\" into tax_contigs_merged_unmapped_out\n\n  script:\n   \"\"\"\n  #!/bin/bash\n  cat ${un_kraken2} ${un_diamond} > tmp.fa \n  #This previous awk script was combining fasta reads from kraken2 and diamond and filtering duplicated reads\n  #awk '/^>/{f=!d[\\$1];d[\\$1]=1}f' tmp.fa > ${sample_id}_${assembler}_unmapped_contigs.fa\n  \n  # New version of the script makes multiline fasta code into one line fasta code and keeps the fasta reads that exist in both kraken2 and diamond\n  awk '/^>/ {printf(\"\\\\n%s\\\\n\",\\$0);next; } { printf(\"%s\",\\$0);}  END {printf(\"\\\\n\");}' < tmp.fa > tmp1.fa\n  rm tmp.fa\n  awk 'seen[\\$0]++ &&seen[\\$0] > N' tmp1.fa > ${sample_id}_${assembler}_unmapped_contigs.fa\n  rm tmp1.fa\n  \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "   \"\"\"\n  #!/bin/bash\n  cat ${un_kraken2} ${un_diamond} > tmp.fa \n  #This previous awk script was combining fasta reads from kraken2 and diamond and filtering duplicated reads\n  #awk '/^>/{f=!d[\\$1];d[\\$1]=1}f' tmp.fa > ${sample_id}_${assembler}_unmapped_contigs.fa\n  \n  # New version of the script makes multiline fasta code into one line fasta code and keeps the fasta reads that exist in both kraken2 and diamond\n  awk '/^>/ {printf(\"\\\\n%s\\\\n\",\\$0);next; } { printf(\"%s\",\\$0);}  END {printf(\"\\\\n\");}' < tmp.fa > tmp1.fa\n  rm tmp.fa\n  awk 'seen[\\$0]++ &&seen[\\$0] > N' tmp1.fa > ${sample_id}_${assembler}_unmapped_contigs.fa\n  rm tmp1.fa\n  \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "NextSV"
        ],
        "tools_url": [
            "https://bio.tools/nextsv"
        ],
        "tools_dico": [
            {
                "name": "NextSV",
                "uri": "https://bio.tools/nextsv",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "Structural variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "Genomic structural variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "DNA structural variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3228",
                                    "term": "Structural variation detection"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3228",
                                    "term": "Structural variation discovery"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A meta SV caller and a computational pipeline to perform SV calling from low coverage long-read sequencing data. It integrates three aligners and three SV callers and generates two integrated call sets (sensitive/stringent) for different analysis purpose.",
                "homepage": "http://github.com/Nextomics/NextSV"
            }
        ],
        "inputs": [
            "unmapped_contigs"
        ],
        "nb_inputs": 1,
        "outputs": [
            "tax_contigs_merged_unmapped",
            "tax_contigs_merged_unmapped_out"
        ],
        "nb_outputs": 2,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"${sample_id}_${assembler}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "tax_contigs_classifier": {
        "name_process": "tax_contigs_classifier",
        "string_process": "\nprocess tax_contigs_classifier{\n tag {\"${sample_id}\"}\n\n publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}/taxonomy\", mode:'link'\n\n input:\n set sample_id, assembler, diamond from tax_contigs_diamond_out\n\n output:\n set sample_id, assembler, \"${sample_id}_contigs_classified_all_sequences.tsv\", \"${sample_id}_contigs_classified_viruses.tsv\", \"${sample_id}_contigs_classified_bacteria.tsv\",\"${sample_id}_contigs_classified_eukaryota.tsv\", \"${sample_id}_contigs_classified_other_sequences.tsv\" into tax_contigs_taxonomy_out\n\n script:\n \"\"\"\n #!/bin/bash\n touch \"${sample_id}_contigs_classified.tsv\"\n echo -e \"sample_id\" '\\\\t' \"contig_id\" '\\\\t' \"accession_nr\" '\\\\t' \"scientific_name\" '\\\\t' \"title\" '\\\\t' \"percentage\" '\\\\t' \"evalue\" '\\\\t' \"mismatch\" '\\\\t' \"aln_len\" '\\\\t' \"contig_len\" '\\\\t' \"full_lineage\" >> \"${sample_id}_contigs_classified_all_sequences.tsv\"\n echo -e \"sample_id\" '\\\\t' \"contig_id\" '\\\\t' \"accession_nr\" '\\\\t' \"scientific_name\" '\\\\t' \"title\" '\\\\t' \"percentage\" '\\\\t' \"evalue\" '\\\\t' \"mismatch\" '\\\\t' \"aln_len\" '\\\\t' \"contig_len\" '\\\\t' \"full_lineage\" >> \"${sample_id}_contigs_classified_viruses.tsv\"\n echo -e \"sample_id\" '\\\\t' \"contig_id\" '\\\\t' \"accession_nr\" '\\\\t' \"scientific_name\" '\\\\t' \"title\" '\\\\t' \"percentage\" '\\\\t' \"evalue\" '\\\\t' \"mismatch\" '\\\\t' \"aln_len\" '\\\\t' \"contig_len\" '\\\\t' \"full_lineage\" >> \"${sample_id}_contigs_classified_bacteria.tsv\"\n echo -e \"sample_id\" '\\\\t' \"contig_id\" '\\\\t' \"accession_nr\" '\\\\t' \"scientific_name\" '\\\\t' \"title\" '\\\\t' \"percentage\" '\\\\t' \"evalue\" '\\\\t' \"mismatch\" '\\\\t' \"aln_len\" '\\\\t' \"contig_len\" '\\\\t' \"full_lineage\" >> \"${sample_id}_contigs_classified_eukaryota.tsv\"\n echo -e \"sample_id\" '\\\\t' \"contig_id\" '\\\\t' \"accession_nr\" '\\\\t' \"scientific_name\" '\\\\t' \"title\" '\\\\t' \"percentage\" '\\\\t' \"evalue\" '\\\\t' \"mismatch\" '\\\\t' \"aln_len\" '\\\\t' \"contig_len\" '\\\\t' \"full_lineage\" >> \"${sample_id}_contigs_classified_other_sequences.tsv\"\n while read p; do\n     full_lineage=\"-\"\n     taxid=\\$(echo \"\\$p\" | awk '{print \\$17}')\n     SUB=\";\"\n     if [[ \"\\$taxid\" == *\"\\$SUB\"* ]]; then\n     \tIFS=';'\n     \tread -a taxid_arr <<< \"\\$taxid\"\n     \ttaxid=\\${lineage_arr[0]}\n     elif [[ -z \\$taxid ]]; then\n     \ttaxid=\"empty\"\n     fi\n     accid=\\$(echo \"\\$p\" | awk '{print \\$2}')\n     contig_line=\\$(echo \"\\$p\" | awk '{print \\$1}')\n     IFS=','\n     read -a contig_arr <<< \"\\$contig_line\"     \n     contig_id=\"\\$(echo \\${contig_arr[0]} | xargs)\"\n     contig_len=\"\\$(echo \\${contig_arr[3]} | xargs)\"\n     wordToRemove=\"len=\"\n     contig_len=\\${contig_len//\\$wordToRemove/}\n     full_lineage=\\$(grep -w \\$taxid ${params.ncbi_full_lineage})\n     if [ -n \"\\$full_lineage\" ]; then\n     IFS='|'\n     read -a lineage_arr <<< \"\\$full_lineage\"\n     \tscientific_name=\"\\$(echo \\${lineage_arr[1]} | xargs | sed -e 's/ /_/g')\"\n     \tfull_lineage2=\"\\$(echo \\${lineage_arr[2]} | xargs | sed -e 's/ /_/g')\"\n     else\n     \tscientific_name=\"-\"\n     \tfull_lineage2=\"-\"\n     fi\n     title=\"\\$(blastdbcmd -db nr -entry \"\\$accid\" -range 1 | head -n 1 | sed 's/>//g' | sed 's/:1-1//g' | sed \"s/\\$accid//g\" | xargs | sed -e 's/ /_/g')\"\n     perc=\\$(echo \"\\$p\" | awk '{print \\$10}')\n     aln_len=\\$(echo \"\\$p\" | awk '{print \\$9}')\n     evalue=\\$(echo \"\\$p\" | awk '{print \\$7}')\n     mismatch=\\$(echo \"\\$p\" | awk '{print \\$12}')\n     if [[ \"\\$full_lineage2\" == *\"Virus\"* ]]; then\n        echo -e ${sample_id} '\\\\t' \\$contig_id '\\\\t' \\$accid '\\\\t' \\$scientific_name '\\\\t' \\$title '\\\\t' \\$perc '\\\\t' \\$evalue '\\\\t' \\$mismatch '\\\\t' \\$aln_len '\\\\t' \\$contig_len '\\\\t' \\$full_lineage2 >> \"${sample_id}_contigs_classified_viruses.tsv\"\n     elif [[ \"\\$full_lineage2\" == *\"Bacteria\"* ]]; then\n        echo -e ${sample_id} '\\\\t' \\$contig_id '\\\\t' \\$accid '\\\\t' \\$scientific_name '\\\\t' \\$title '\\\\t' \\$perc '\\\\t' \\$evalue '\\\\t' \\$mismatch '\\\\t' \\$aln_len '\\\\t' \\$contig_len '\\\\t' \\$full_lineage2 >> \"${sample_id}_contigs_classified_bacteria.tsv\"\n     elif [[ \"\\$full_lineage2\" == *\"Eukaryota\"* ]]; then\n        echo -e ${sample_id} '\\\\t' \\$contig_id '\\\\t' \\$accid '\\\\t' \\$scientific_name '\\\\t' \\$title '\\\\t' \\$perc '\\\\t' \\$evalue '\\\\t' \\$mismatch '\\\\t' \\$aln_len '\\\\t' \\$contig_len '\\\\t' \\$full_lineage2 >> \"${sample_id}_contigs_classified_eukaryota.tsv\"\n     else\n        echo -e ${sample_id} '\\\\t' \\$contig_id '\\\\t' \\$accid '\\\\t' \\$scientific_name '\\\\t' \\$title '\\\\t' \\$perc '\\\\t' \\$evalue '\\\\t' \\$mismatch '\\\\t' \\$aln_len '\\\\t' \\$contig_len '\\\\t' \\$full_lineage2 >> \"${sample_id}_contigs_classified_other_sequences.tsv\"\n     fi\n     echo -e ${sample_id} '\\\\t' \\$contig_id '\\\\t' \\$accid '\\\\t' \\$scientific_name '\\\\t' \\$title '\\\\t' \\$perc '\\\\t' \\$evalue '\\\\t' \\$mismatch '\\\\t' \\$aln_len '\\\\t' \\$contig_len '\\\\t' \\$full_lineage2 >> \"${sample_id}_contigs_classified_all_sequences.tsv\"\n done < ${diamond}\n\n \"\"\"\n}",
        "nb_lignes_process": 67,
        "string_script": " \"\"\"\n #!/bin/bash\n touch \"${sample_id}_contigs_classified.tsv\"\n echo -e \"sample_id\" '\\\\t' \"contig_id\" '\\\\t' \"accession_nr\" '\\\\t' \"scientific_name\" '\\\\t' \"title\" '\\\\t' \"percentage\" '\\\\t' \"evalue\" '\\\\t' \"mismatch\" '\\\\t' \"aln_len\" '\\\\t' \"contig_len\" '\\\\t' \"full_lineage\" >> \"${sample_id}_contigs_classified_all_sequences.tsv\"\n echo -e \"sample_id\" '\\\\t' \"contig_id\" '\\\\t' \"accession_nr\" '\\\\t' \"scientific_name\" '\\\\t' \"title\" '\\\\t' \"percentage\" '\\\\t' \"evalue\" '\\\\t' \"mismatch\" '\\\\t' \"aln_len\" '\\\\t' \"contig_len\" '\\\\t' \"full_lineage\" >> \"${sample_id}_contigs_classified_viruses.tsv\"\n echo -e \"sample_id\" '\\\\t' \"contig_id\" '\\\\t' \"accession_nr\" '\\\\t' \"scientific_name\" '\\\\t' \"title\" '\\\\t' \"percentage\" '\\\\t' \"evalue\" '\\\\t' \"mismatch\" '\\\\t' \"aln_len\" '\\\\t' \"contig_len\" '\\\\t' \"full_lineage\" >> \"${sample_id}_contigs_classified_bacteria.tsv\"\n echo -e \"sample_id\" '\\\\t' \"contig_id\" '\\\\t' \"accession_nr\" '\\\\t' \"scientific_name\" '\\\\t' \"title\" '\\\\t' \"percentage\" '\\\\t' \"evalue\" '\\\\t' \"mismatch\" '\\\\t' \"aln_len\" '\\\\t' \"contig_len\" '\\\\t' \"full_lineage\" >> \"${sample_id}_contigs_classified_eukaryota.tsv\"\n echo -e \"sample_id\" '\\\\t' \"contig_id\" '\\\\t' \"accession_nr\" '\\\\t' \"scientific_name\" '\\\\t' \"title\" '\\\\t' \"percentage\" '\\\\t' \"evalue\" '\\\\t' \"mismatch\" '\\\\t' \"aln_len\" '\\\\t' \"contig_len\" '\\\\t' \"full_lineage\" >> \"${sample_id}_contigs_classified_other_sequences.tsv\"\n while read p; do\n     full_lineage=\"-\"\n     taxid=\\$(echo \"\\$p\" | awk '{print \\$17}')\n     SUB=\";\"\n     if [[ \"\\$taxid\" == *\"\\$SUB\"* ]]; then\n     \tIFS=';'\n     \tread -a taxid_arr <<< \"\\$taxid\"\n     \ttaxid=\\${lineage_arr[0]}\n     elif [[ -z \\$taxid ]]; then\n     \ttaxid=\"empty\"\n     fi\n     accid=\\$(echo \"\\$p\" | awk '{print \\$2}')\n     contig_line=\\$(echo \"\\$p\" | awk '{print \\$1}')\n     IFS=','\n     read -a contig_arr <<< \"\\$contig_line\"     \n     contig_id=\"\\$(echo \\${contig_arr[0]} | xargs)\"\n     contig_len=\"\\$(echo \\${contig_arr[3]} | xargs)\"\n     wordToRemove=\"len=\"\n     contig_len=\\${contig_len//\\$wordToRemove/}\n     full_lineage=\\$(grep -w \\$taxid ${params.ncbi_full_lineage})\n     if [ -n \"\\$full_lineage\" ]; then\n     IFS='|'\n     read -a lineage_arr <<< \"\\$full_lineage\"\n     \tscientific_name=\"\\$(echo \\${lineage_arr[1]} | xargs | sed -e 's/ /_/g')\"\n     \tfull_lineage2=\"\\$(echo \\${lineage_arr[2]} | xargs | sed -e 's/ /_/g')\"\n     else\n     \tscientific_name=\"-\"\n     \tfull_lineage2=\"-\"\n     fi\n     title=\"\\$(blastdbcmd -db nr -entry \"\\$accid\" -range 1 | head -n 1 | sed 's/>//g' | sed 's/:1-1//g' | sed \"s/\\$accid//g\" | xargs | sed -e 's/ /_/g')\"\n     perc=\\$(echo \"\\$p\" | awk '{print \\$10}')\n     aln_len=\\$(echo \"\\$p\" | awk '{print \\$9}')\n     evalue=\\$(echo \"\\$p\" | awk '{print \\$7}')\n     mismatch=\\$(echo \"\\$p\" | awk '{print \\$12}')\n     if [[ \"\\$full_lineage2\" == *\"Virus\"* ]]; then\n        echo -e ${sample_id} '\\\\t' \\$contig_id '\\\\t' \\$accid '\\\\t' \\$scientific_name '\\\\t' \\$title '\\\\t' \\$perc '\\\\t' \\$evalue '\\\\t' \\$mismatch '\\\\t' \\$aln_len '\\\\t' \\$contig_len '\\\\t' \\$full_lineage2 >> \"${sample_id}_contigs_classified_viruses.tsv\"\n     elif [[ \"\\$full_lineage2\" == *\"Bacteria\"* ]]; then\n        echo -e ${sample_id} '\\\\t' \\$contig_id '\\\\t' \\$accid '\\\\t' \\$scientific_name '\\\\t' \\$title '\\\\t' \\$perc '\\\\t' \\$evalue '\\\\t' \\$mismatch '\\\\t' \\$aln_len '\\\\t' \\$contig_len '\\\\t' \\$full_lineage2 >> \"${sample_id}_contigs_classified_bacteria.tsv\"\n     elif [[ \"\\$full_lineage2\" == *\"Eukaryota\"* ]]; then\n        echo -e ${sample_id} '\\\\t' \\$contig_id '\\\\t' \\$accid '\\\\t' \\$scientific_name '\\\\t' \\$title '\\\\t' \\$perc '\\\\t' \\$evalue '\\\\t' \\$mismatch '\\\\t' \\$aln_len '\\\\t' \\$contig_len '\\\\t' \\$full_lineage2 >> \"${sample_id}_contigs_classified_eukaryota.tsv\"\n     else\n        echo -e ${sample_id} '\\\\t' \\$contig_id '\\\\t' \\$accid '\\\\t' \\$scientific_name '\\\\t' \\$title '\\\\t' \\$perc '\\\\t' \\$evalue '\\\\t' \\$mismatch '\\\\t' \\$aln_len '\\\\t' \\$contig_len '\\\\t' \\$full_lineage2 >> \"${sample_id}_contigs_classified_other_sequences.tsv\"\n     fi\n     echo -e ${sample_id} '\\\\t' \\$contig_id '\\\\t' \\$accid '\\\\t' \\$scientific_name '\\\\t' \\$title '\\\\t' \\$perc '\\\\t' \\$evalue '\\\\t' \\$mismatch '\\\\t' \\$aln_len '\\\\t' \\$contig_len '\\\\t' \\$full_lineage2 >> \"${sample_id}_contigs_classified_all_sequences.tsv\"\n done < ${diamond}\n\n \"\"\"",
        "nb_lignes_script": 54,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "tax_contigs_diamond_out"
        ],
        "nb_inputs": 1,
        "outputs": [
            "tax_contigs_taxonomy_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}/taxonomy\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "TSV_to_HTML_each_sample": {
        "name_process": "TSV_to_HTML_each_sample",
        "string_process": "\nprocess TSV_to_HTML_each_sample{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}/taxonomy\", mode:'link'\n\n  input:\n  set sample_id, assembler, all, viral, bacteria, eukaryota, other, hvalue, html from html_data_each\n \n  output:\n  set sample_id, \"${sample_id}_all_asm_results.html\", \"${sample_id}_viral_asm_results.html\", \"${sample_id}_bacteria_asm_results.html\", \"${sample_id}_eukaryota_asm_results.html\", \"${sample_id}_other_asm_results.html\" into html_each_out\n  script:\n\"\"\"\n#!/usr/bin/python3\nimport string\nimport csv\nimport json\nimport collections\n\nOrderedDict = collections.OrderedDict\n\ndef HTML_table(src, header, htmlEndStr, name):\n    \n    f = open(src)\n    lines = f.readlines()\n    f.close()\n    f1 = open(header)\n    header_lines = f1.readlines()\n    f2 = open(htmlEndStr)\n    end_lines = f2.readlines()\n    f1.close()\n    f2.close()\n    \n    with open(name, 'w') as f:\n        for h_line in header_lines:\n            f.write(h_line)\n        for line in lines:\n            f.write(line)\n        for e_line in end_lines:\n            f.write(e_line)\n    f.close()\n    return None\n\ndef TSV_file_into_JSON(src, dst, header):\n    data = []\n    with open(src, 'r') as csvfile:\n        reader = csv.reader(csvfile, delimiter='\\\\t', quotechar='\"')\n        for row in reader:\n            if row[0] == 'sample_id':\n                print(\"\\\\n\")\n            else:\n                if row[0].strip()[0] == '#':  #\n                    continue\n                row = filter(None, row)\n                data.append(OrderedDict(zip(header, row)))\n\n    with open(dst, 'w') as jsonfile:\n        json.dump(data, jsonfile, indent=2)\n    return None\n\nheader = ['sample_id', 'contig_id', 'accession_nr', 'scientific_name', 'title', 'percentage', 'evalue', 'mismatch', 'aln_len', 'contig_len', 'full_lineage']\nsrc = \"${all}\"\ndst = \"all.json\"  \nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${sample_id}_all_asm_results.html\")\n\nsrc = \"${viral}\"\ndst = \"viral.json\"  \nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${sample_id}_viral_asm_results.html\")\n\nsrc = \"${bacteria}\"\ndst = \"bact.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${sample_id}_bacteria_asm_results.html\")\n\nsrc = \"${eukaryota}\"\ndst = \"eukar.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${sample_id}_eukaryota_asm_results.html\")\n\nsrc = \"${other}\"\ndst = \"other.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${sample_id}_other_asm_results.html\")\n\"\"\"\n}",
        "nb_lignes_process": 85,
        "string_script": "\"\"\"\n#!/usr/bin/python3\nimport string\nimport csv\nimport json\nimport collections\n\nOrderedDict = collections.OrderedDict\n\ndef HTML_table(src, header, htmlEndStr, name):\n    \n    f = open(src)\n    lines = f.readlines()\n    f.close()\n    f1 = open(header)\n    header_lines = f1.readlines()\n    f2 = open(htmlEndStr)\n    end_lines = f2.readlines()\n    f1.close()\n    f2.close()\n    \n    with open(name, 'w') as f:\n        for h_line in header_lines:\n            f.write(h_line)\n        for line in lines:\n            f.write(line)\n        for e_line in end_lines:\n            f.write(e_line)\n    f.close()\n    return None\n\ndef TSV_file_into_JSON(src, dst, header):\n    data = []\n    with open(src, 'r') as csvfile:\n        reader = csv.reader(csvfile, delimiter='\\\\t', quotechar='\"')\n        for row in reader:\n            if row[0] == 'sample_id':\n                print(\"\\\\n\")\n            else:\n                if row[0].strip()[0] == '#':  #\n                    continue\n                row = filter(None, row)\n                data.append(OrderedDict(zip(header, row)))\n\n    with open(dst, 'w') as jsonfile:\n        json.dump(data, jsonfile, indent=2)\n    return None\n\nheader = ['sample_id', 'contig_id', 'accession_nr', 'scientific_name', 'title', 'percentage', 'evalue', 'mismatch', 'aln_len', 'contig_len', 'full_lineage']\nsrc = \"${all}\"\ndst = \"all.json\"  \nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${sample_id}_all_asm_results.html\")\n\nsrc = \"${viral}\"\ndst = \"viral.json\"  \nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${sample_id}_viral_asm_results.html\")\n\nsrc = \"${bacteria}\"\ndst = \"bact.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${sample_id}_bacteria_asm_results.html\")\n\nsrc = \"${eukaryota}\"\ndst = \"eukar.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${sample_id}_eukaryota_asm_results.html\")\n\nsrc = \"${other}\"\ndst = \"other.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${sample_id}_other_asm_results.html\")\n\"\"\"",
        "nb_lignes_script": 73,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "html_data_each"
        ],
        "nb_inputs": 1,
        "outputs": [
            "html_each_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/${assembler}/taxonomy\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "collect_all_tsv_tables": {
        "name_process": "collect_all_tsv_tables",
        "string_process": "\nprocess collect_all_tsv_tables{\n  tag {\"All\"}\n\n  publishDir \"${params.publish_base_dir}/All/tsv_tables\", mode:'link'\n\n  input:\n  file \"tsv_table\" from collect_all_tsv.map{it[2]}.collect()\n \n  output:\n  file \"${params.project_id}_all_asm_results.tsv\" into all_tsv_collected\n  script:\n\"\"\"\n#!/bin/bash\ntouch tmp.tsv\necho -e \"sample_id\\\\tcontig_id\\\\taccession_nr\\\\tscientific_name\\\\ttitle\\\\tpercentage\\\\tevalue\\\\tmismatch\\\\taln_len\\\\tcontig_len\\\\tfull_lineage\" > header.tsv\n\nfor sample_file in ${tsv_table}\ndo\n\tcat \\$sample_file | sed '1d' >> tmp.tsv\ndone\n\ncat tmp.tsv | sort -k1,1 -k2,2 > tmp2.tsv\ncat header.tsv tmp2.tsv > \"${params.project_id}_all_asm_results.tsv\"\nrm tmp.tsv tmp2.tsv header.tsv\n\"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n#!/bin/bash\ntouch tmp.tsv\necho -e \"sample_id\\\\tcontig_id\\\\taccession_nr\\\\tscientific_name\\\\ttitle\\\\tpercentage\\\\tevalue\\\\tmismatch\\\\taln_len\\\\tcontig_len\\\\tfull_lineage\" > header.tsv\n\nfor sample_file in ${tsv_table}\ndo\n\tcat \\$sample_file | sed '1d' >> tmp.tsv\ndone\n\ncat tmp.tsv | sort -k1,1 -k2,2 > tmp2.tsv\ncat header.tsv tmp2.tsv > \"${params.project_id}_all_asm_results.tsv\"\nrm tmp.tsv tmp2.tsv header.tsv\n\"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "collect_all_tsv"
        ],
        "nb_inputs": 1,
        "outputs": [
            "all_tsv_collected"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"All\"}",
            "publishDir \"${params.publish_base_dir}/All/tsv_tables\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "collect_virus_tsv_tables": {
        "name_process": "collect_virus_tsv_tables",
        "string_process": "\nprocess collect_virus_tsv_tables{\n  tag {\"All\"}\n\n  publishDir \"${params.publish_base_dir}/All/tsv_tables\", mode:'link'\n\n  input:\n  file \"tsv_table\" from collect_virus_tsv.map{it[3]}.collect()\n\n  output:\n  file \"${params.project_id}_virus_asm_results.tsv\" into virus_tsv_collected\n  script:\n\"\"\"\n#!/bin/bash\ntouch tmp.tsv\necho -e \"sample_id\\\\tcontig_id\\\\taccession_nr\\\\tscientific_name\\\\ttitle\\\\tpercentage\\\\tevalue\\\\tmismatch\\\\taln_len\\\\tcontig_len\\\\tfull_lineage\" > header.tsv\n\nfor sample_file in ${tsv_table}\ndo\n        cat \\$sample_file | sed '1d' >> tmp.tsv\ndone\n\ncat tmp.tsv | sort -k1,1 -k2,2 > tmp2.tsv\ncat header.tsv tmp2.tsv > \"${params.project_id}_virus_asm_results.tsv\"\nrm tmp.tsv tmp2.tsv header.tsv\n\"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n#!/bin/bash\ntouch tmp.tsv\necho -e \"sample_id\\\\tcontig_id\\\\taccession_nr\\\\tscientific_name\\\\ttitle\\\\tpercentage\\\\tevalue\\\\tmismatch\\\\taln_len\\\\tcontig_len\\\\tfull_lineage\" > header.tsv\n\nfor sample_file in ${tsv_table}\ndo\n        cat \\$sample_file | sed '1d' >> tmp.tsv\ndone\n\ncat tmp.tsv | sort -k1,1 -k2,2 > tmp2.tsv\ncat header.tsv tmp2.tsv > \"${params.project_id}_virus_asm_results.tsv\"\nrm tmp.tsv tmp2.tsv header.tsv\n\"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "collect_virus_tsv"
        ],
        "nb_inputs": 1,
        "outputs": [
            "virus_tsv_collected"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"All\"}",
            "publishDir \"${params.publish_base_dir}/All/tsv_tables\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "collect_bacteria_tsv_tables": {
        "name_process": "collect_bacteria_tsv_tables",
        "string_process": "\nprocess collect_bacteria_tsv_tables{\n  tag {\"All\"}\n\n  publishDir \"${params.publish_base_dir}/All/tsv_tables\", mode:'link'\n\n  input:\n  file \"tsv_table\" from collect_bact_tsv.map{it[4]}.collect()\n\n  output:\n  file \"${params.project_id}_bacteria_asm_results.tsv\" into bact_tsv_collected\n  script:\n\"\"\"\n#!/bin/bash\ntouch tmp.tsv\necho -e \"sample_id\\\\tcontig_id\\\\taccession_nr\\\\tscientific_name\\\\ttitle\\\\tpercentage\\\\tevalue\\\\tmismatch\\\\taln_len\\\\tcontig_len\\\\tfull_lineage\" > header.tsv\n\nfor sample_file in ${tsv_table}\ndo\n        cat \\$sample_file | sed '1d' >> tmp.tsv\ndone\n\ncat tmp.tsv | sort -k1,1 -k2,2 > tmp2.tsv\ncat header.tsv tmp2.tsv > \"${params.project_id}_bacteria_asm_results.tsv\"\nrm tmp.tsv tmp2.tsv header.tsv\n\"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n#!/bin/bash\ntouch tmp.tsv\necho -e \"sample_id\\\\tcontig_id\\\\taccession_nr\\\\tscientific_name\\\\ttitle\\\\tpercentage\\\\tevalue\\\\tmismatch\\\\taln_len\\\\tcontig_len\\\\tfull_lineage\" > header.tsv\n\nfor sample_file in ${tsv_table}\ndo\n        cat \\$sample_file | sed '1d' >> tmp.tsv\ndone\n\ncat tmp.tsv | sort -k1,1 -k2,2 > tmp2.tsv\ncat header.tsv tmp2.tsv > \"${params.project_id}_bacteria_asm_results.tsv\"\nrm tmp.tsv tmp2.tsv header.tsv\n\"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "collect_bact_tsv"
        ],
        "nb_inputs": 1,
        "outputs": [
            "bact_tsv_collected"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"All\"}",
            "publishDir \"${params.publish_base_dir}/All/tsv_tables\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "collect_eukaryotic_tsv_tables": {
        "name_process": "collect_eukaryotic_tsv_tables",
        "string_process": "\nprocess collect_eukaryotic_tsv_tables{\n  tag {\"All\"}\n\n  publishDir \"${params.publish_base_dir}/All/tsv_tables\", mode:'link'\n\n  input:\n  file \"tsv_table\" from collect_eukar_tsv.map{it[5]}.collect()\n\n  output:\n  file \"${params.project_id}_eukaryotic_asm_results.tsv\" into eukar_tsv_collected\n  script:\n\"\"\"\n#!/bin/bash\ntouch tmp.tsv\necho -e \"sample_id\\\\tcontig_id\\\\taccession_nr\\\\tscientific_name\\\\ttitle\\\\tpercentage\\\\tevalue\\\\tmismatch\\\\taln_len\\\\tcontig_len\\\\tfull_lineage\" > header.tsv\n\nfor sample_file in ${tsv_table}\ndo\n        cat \\$sample_file | sed '1d' >> tmp.tsv\ndone\n\ncat tmp.tsv | sort -k1,1 -k2,2 > tmp2.tsv\ncat header.tsv tmp2.tsv > \"${params.project_id}_eukaryotic_asm_results.tsv\"\nrm tmp.tsv tmp2.tsv header.tsv\n\"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n#!/bin/bash\ntouch tmp.tsv\necho -e \"sample_id\\\\tcontig_id\\\\taccession_nr\\\\tscientific_name\\\\ttitle\\\\tpercentage\\\\tevalue\\\\tmismatch\\\\taln_len\\\\tcontig_len\\\\tfull_lineage\" > header.tsv\n\nfor sample_file in ${tsv_table}\ndo\n        cat \\$sample_file | sed '1d' >> tmp.tsv\ndone\n\ncat tmp.tsv | sort -k1,1 -k2,2 > tmp2.tsv\ncat header.tsv tmp2.tsv > \"${params.project_id}_eukaryotic_asm_results.tsv\"\nrm tmp.tsv tmp2.tsv header.tsv\n\"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "collect_eukar_tsv"
        ],
        "nb_inputs": 1,
        "outputs": [
            "eukar_tsv_collected"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"All\"}",
            "publishDir \"${params.publish_base_dir}/All/tsv_tables\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "collect_otherSeq_tsv_tables": {
        "name_process": "collect_otherSeq_tsv_tables",
        "string_process": "\nprocess collect_otherSeq_tsv_tables{\n  tag {\"All\"}\n\n  publishDir \"${params.publish_base_dir}/All/tsv_tables\", mode:'link'\n\n  input:\n  file \"tsv_table\" from collect_other_tsv.map{it[6]}.collect()\n\n  output:\n  file \"${params.project_id}_other_sequences_asm_results.tsv\" into other_tsv_collected\n  script:\n\"\"\"\n#!/bin/bash\ntouch tmp.tsv\necho -e \"sample_id\\\\tcontig_id\\\\taccession_nr\\\\tscientific_name\\\\ttitle\\\\tpercentage\\\\tevalue\\\\tmismatch\\\\taln_len\\\\tcontig_len\\\\tfull_lineage\" > header.tsv\n\nfor sample_file in ${tsv_table}\ndo\n        cat \\$sample_file | sed '1d' >> tmp.tsv\ndone\n\ncat tmp.tsv | sort -k1,1 -k2,2 > tmp2.tsv\ncat header.tsv tmp2.tsv > \"${params.project_id}_other_sequences_asm_results.tsv\"\nrm tmp.tsv tmp2.tsv header.tsv\n\"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n#!/bin/bash\ntouch tmp.tsv\necho -e \"sample_id\\\\tcontig_id\\\\taccession_nr\\\\tscientific_name\\\\ttitle\\\\tpercentage\\\\tevalue\\\\tmismatch\\\\taln_len\\\\tcontig_len\\\\tfull_lineage\" > header.tsv\n\nfor sample_file in ${tsv_table}\ndo\n        cat \\$sample_file | sed '1d' >> tmp.tsv\ndone\n\ncat tmp.tsv | sort -k1,1 -k2,2 > tmp2.tsv\ncat header.tsv tmp2.tsv > \"${params.project_id}_other_sequences_asm_results.tsv\"\nrm tmp.tsv tmp2.tsv header.tsv\n\"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "collect_other_tsv"
        ],
        "nb_inputs": 1,
        "outputs": [
            "other_tsv_collected"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"All\"}",
            "publishDir \"${params.publish_base_dir}/All/tsv_tables\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "tsv_to_simplified_tsv": {
        "name_process": "tsv_to_simplified_tsv",
        "string_process": "\nprocess tsv_to_simplified_tsv{\n  tag {\"All\"}\n\n  publishDir \"${params.publish_base_dir}/All/simplified_tsv_tables\", mode:'link'\n\n  input:\n  set all, virus, bacteria, eukaryotic, other from collected_tsv_to_simplified\n\n  output:\n  set \"${params.project_id}_all_simplified_asm_results.tsv\", \"${params.project_id}_virus_simplified_asm_results.tsv\", \"${params.project_id}_bacteria_simplified_asm_results.tsv\", \"${params.project_id}_eukaryota_simplified_asm_results.tsv\", \"${params.project_id}_other_simplified_asm_results.tsv\" into tsv_simplifed_out\n  script:\n\"\"\"\n#!/bin/bash\necho -e \"sample_id\\\\tcontig_id\\\\taccession_nr\\\\tscientific_name\\\\ttitle\\\\tpercentage\\\\tevalue\\\\tmismatch\\\\taln_len\\\\tcontig_len\\\\tfull_lineage\" > header.tsv\ncat ${all} | sed 1d | sed 's/ //g' | datamash -s -g1,2 min 7 -f | awk '{print \\$1 \"\\\\t\" \\$2 \"\\\\t\" \\$3 \"\\\\t\" \\$4 \"\\\\t\" \\$5 \"\\\\t\" \\$6 \"\\\\t\" \\$7 \"\\\\t\" \\$8 \"\\\\t\" \\$9 \"\\\\t\" \\$10 \"\\\\t\" \\$11}' | sed 's/\\\\,/./g' | sort -k1,1 -k3,3 > all_tmp.tsv\n\ncat ${virus} | sed 1d | sed 's/ //g' | datamash -s -g1,2 min 7 -f | awk '{print \\$1 \"\\\\t\" \\$2 \"\\\\t\" \\$3 \"\\\\t\" \\$4 \"\\\\t\" \\$5 \"\\\\t\" \\$6 \"\\\\t\" \\$7 \"\\\\t\" \\$8 \"\\\\t\" \\$9 \"\\\\t\" \\$10 \"\\\\t\" \\$11}' | sed 's/\\\\,/./g' | sort -k1,1 -k3,3 > virus_tmp.tsv\n\ncat ${bacteria} | sed 1d | sed 's/ //g' | datamash -s -g1,2 min 7 -f | awk '{print \\$1 \"\\\\t\" \\$2 \"\\\\t\" \\$3 \"\\\\t\" \\$4 \"\\\\t\" \\$5 \"\\\\t\" \\$6 \"\\\\t\" \\$7 \"\\\\t\" \\$8 \"\\\\t\" \\$9 \"\\\\t\" \\$10 \"\\\\t\" \\$11}' | sed 's/\\\\,/./g' | sort -k1,1 -k3,3 > bact_tmp.tsv\n\ncat ${eukaryotic} | sed 1d | sed 's/ //g' | datamash -s -g1,2 min 7 -f | awk '{print \\$1 \"\\\\t\" \\$2 \"\\\\t\" \\$3 \"\\\\t\" \\$4 \"\\\\t\" \\$5 \"\\\\t\" \\$6 \"\\\\t\" \\$7 \"\\\\t\" \\$8 \"\\\\t\" \\$9 \"\\\\t\" \\$10 \"\\\\t\" \\$11}' | sed 's/\\\\,/./g' | sort -k1,1 -k3,3 > eukar_tmp.tsv\n\ncat ${other} | sed 1d | sed 's/ //g' | datamash -s -g1,2 min 7 -f | awk '{print \\$1 \"\\\\t\" \\$2 \"\\\\t\" \\$3 \"\\\\t\" \\$4 \"\\\\t\" \\$5 \"\\\\t\" \\$6 \"\\\\t\" \\$7 \"\\\\t\" \\$8 \"\\\\t\" \\$9 \"\\\\t\" \\$10 \"\\\\t\" \\$11}' | sed 's/\\\\,/./g' | sort -k1,1 -k3,3 > other_tmp.tsv\n\ncat header.tsv all_tmp.tsv > \"${params.project_id}_all_simplified_asm_results.tsv\"\ncat header.tsv virus_tmp.tsv > \"${params.project_id}_virus_simplified_asm_results.tsv\"\ncat header.tsv bact_tmp.tsv > \"${params.project_id}_bacteria_simplified_asm_results.tsv\"\ncat header.tsv eukar_tmp.tsv > \"${params.project_id}_eukaryota_simplified_asm_results.tsv\"\ncat header.tsv other_tmp.tsv > \"${params.project_id}_other_simplified_asm_results.tsv\"\nrm header.tsv all_tmp.tsv virus_tmp.tsv bact_tmp.tsv eukar_tmp.tsv other_tmp.tsv\n\"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "\"\"\"\n#!/bin/bash\necho -e \"sample_id\\\\tcontig_id\\\\taccession_nr\\\\tscientific_name\\\\ttitle\\\\tpercentage\\\\tevalue\\\\tmismatch\\\\taln_len\\\\tcontig_len\\\\tfull_lineage\" > header.tsv\ncat ${all} | sed 1d | sed 's/ //g' | datamash -s -g1,2 min 7 -f | awk '{print \\$1 \"\\\\t\" \\$2 \"\\\\t\" \\$3 \"\\\\t\" \\$4 \"\\\\t\" \\$5 \"\\\\t\" \\$6 \"\\\\t\" \\$7 \"\\\\t\" \\$8 \"\\\\t\" \\$9 \"\\\\t\" \\$10 \"\\\\t\" \\$11}' | sed 's/\\\\,/./g' | sort -k1,1 -k3,3 > all_tmp.tsv\n\ncat ${virus} | sed 1d | sed 's/ //g' | datamash -s -g1,2 min 7 -f | awk '{print \\$1 \"\\\\t\" \\$2 \"\\\\t\" \\$3 \"\\\\t\" \\$4 \"\\\\t\" \\$5 \"\\\\t\" \\$6 \"\\\\t\" \\$7 \"\\\\t\" \\$8 \"\\\\t\" \\$9 \"\\\\t\" \\$10 \"\\\\t\" \\$11}' | sed 's/\\\\,/./g' | sort -k1,1 -k3,3 > virus_tmp.tsv\n\ncat ${bacteria} | sed 1d | sed 's/ //g' | datamash -s -g1,2 min 7 -f | awk '{print \\$1 \"\\\\t\" \\$2 \"\\\\t\" \\$3 \"\\\\t\" \\$4 \"\\\\t\" \\$5 \"\\\\t\" \\$6 \"\\\\t\" \\$7 \"\\\\t\" \\$8 \"\\\\t\" \\$9 \"\\\\t\" \\$10 \"\\\\t\" \\$11}' | sed 's/\\\\,/./g' | sort -k1,1 -k3,3 > bact_tmp.tsv\n\ncat ${eukaryotic} | sed 1d | sed 's/ //g' | datamash -s -g1,2 min 7 -f | awk '{print \\$1 \"\\\\t\" \\$2 \"\\\\t\" \\$3 \"\\\\t\" \\$4 \"\\\\t\" \\$5 \"\\\\t\" \\$6 \"\\\\t\" \\$7 \"\\\\t\" \\$8 \"\\\\t\" \\$9 \"\\\\t\" \\$10 \"\\\\t\" \\$11}' | sed 's/\\\\,/./g' | sort -k1,1 -k3,3 > eukar_tmp.tsv\n\ncat ${other} | sed 1d | sed 's/ //g' | datamash -s -g1,2 min 7 -f | awk '{print \\$1 \"\\\\t\" \\$2 \"\\\\t\" \\$3 \"\\\\t\" \\$4 \"\\\\t\" \\$5 \"\\\\t\" \\$6 \"\\\\t\" \\$7 \"\\\\t\" \\$8 \"\\\\t\" \\$9 \"\\\\t\" \\$10 \"\\\\t\" \\$11}' | sed 's/\\\\,/./g' | sort -k1,1 -k3,3 > other_tmp.tsv\n\ncat header.tsv all_tmp.tsv > \"${params.project_id}_all_simplified_asm_results.tsv\"\ncat header.tsv virus_tmp.tsv > \"${params.project_id}_virus_simplified_asm_results.tsv\"\ncat header.tsv bact_tmp.tsv > \"${params.project_id}_bacteria_simplified_asm_results.tsv\"\ncat header.tsv eukar_tmp.tsv > \"${params.project_id}_eukaryota_simplified_asm_results.tsv\"\ncat header.tsv other_tmp.tsv > \"${params.project_id}_other_simplified_asm_results.tsv\"\nrm header.tsv all_tmp.tsv virus_tmp.tsv bact_tmp.tsv eukar_tmp.tsv other_tmp.tsv\n\"\"\"",
        "nb_lignes_script": 19,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "collected_tsv_to_simplified"
        ],
        "nb_inputs": 1,
        "outputs": [
            "tsv_simplifed_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"All\"}",
            "publishDir \"${params.publish_base_dir}/All/simplified_tsv_tables\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "simplified_tsv_to_html": {
        "name_process": "simplified_tsv_to_html",
        "string_process": "\nprocess simplified_tsv_to_html{\n  tag {\"All\"}\n\n  publishDir \"${params.publish_base_dir}/All/simplified_html_tables\", mode:'link'\n\n  input:\n  set all, virus, bacteria, eukaryotic, other, hvalue, html from html_simplifed_in\n\n  output:\n  set \"${params.project_id}_all_simplified_asm_results.html\", \"${params.project_id}_virus_simplified_asm_results.html\", \"${params.project_id}_bacteria_simplified_asm_results.html\", \"${params.project_id}_eukaryota_simplified_asm_results.html\", \"${params.project_id}_other_simplified_asm_results.html\" into html_simplifed_out\n  script:\n\"\"\"\n#!/usr/bin/python3\nimport string\nimport csv\nimport json\nimport collections\n\nOrderedDict = collections.OrderedDict\n\ndef HTML_table(src, header, htmlEndStr, name):\n\n    f = open(src)\n    lines = f.readlines()\n    f.close()\n    f1 = open(header)\n    header_lines = f1.readlines()\n    f2 = open(htmlEndStr)\n    end_lines = f2.readlines()\n    f1.close()\n    f2.close()\n\n    with open(name, 'w') as f:\n        for h_line in header_lines:\n            f.write(h_line)\n        for line in lines:\n            f.write(line)\n        for e_line in end_lines:\n            f.write(e_line)\n    f.close()\n    return None\n\ndef TSV_file_into_JSON(src, dst, header):\n    data = []\n    with open(src, 'r') as csvfile:\n        reader = csv.reader(csvfile, delimiter='\\\\t', quotechar='\"')\n        for row in reader:\n            if row[0] == 'sample_id':\n                print(\"\\\\n\")\n            else:\n                if row[0].strip()[0] == '#':  #\n                    continue\n                row = filter(None, row)\n                data.append(OrderedDict(zip(header, row)))\n\n    with open(dst, 'w') as jsonfile:\n        json.dump(data, jsonfile, indent=2)\n    return None\n\nheader = ['sample_id', 'contig_id', 'accession_nr', 'scientific_name', 'title', 'percentage', 'evalue', 'mismatch', 'aln_len', 'contig_len', 'full_lineage']\nsrc = \"${all}\"\ndst = \"all.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_all_simplified_asm_results.html\")\n\nsrc = \"${virus}\"\ndst = \"viral.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_virus_simplified_asm_results.html\")\n\nsrc = \"${bacteria}\"\ndst = \"bact.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_bacteria_simplified_asm_results.html\")\n\nsrc = \"${eukaryotic}\"\ndst = \"eukar.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_eukaryota_simplified_asm_results.html\")\n\nsrc = \"${other}\"\ndst = \"other.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_other_simplified_asm_results.html\")\n\n\"\"\"\n}",
        "nb_lignes_process": 86,
        "string_script": "\"\"\"\n#!/usr/bin/python3\nimport string\nimport csv\nimport json\nimport collections\n\nOrderedDict = collections.OrderedDict\n\ndef HTML_table(src, header, htmlEndStr, name):\n\n    f = open(src)\n    lines = f.readlines()\n    f.close()\n    f1 = open(header)\n    header_lines = f1.readlines()\n    f2 = open(htmlEndStr)\n    end_lines = f2.readlines()\n    f1.close()\n    f2.close()\n\n    with open(name, 'w') as f:\n        for h_line in header_lines:\n            f.write(h_line)\n        for line in lines:\n            f.write(line)\n        for e_line in end_lines:\n            f.write(e_line)\n    f.close()\n    return None\n\ndef TSV_file_into_JSON(src, dst, header):\n    data = []\n    with open(src, 'r') as csvfile:\n        reader = csv.reader(csvfile, delimiter='\\\\t', quotechar='\"')\n        for row in reader:\n            if row[0] == 'sample_id':\n                print(\"\\\\n\")\n            else:\n                if row[0].strip()[0] == '#':  #\n                    continue\n                row = filter(None, row)\n                data.append(OrderedDict(zip(header, row)))\n\n    with open(dst, 'w') as jsonfile:\n        json.dump(data, jsonfile, indent=2)\n    return None\n\nheader = ['sample_id', 'contig_id', 'accession_nr', 'scientific_name', 'title', 'percentage', 'evalue', 'mismatch', 'aln_len', 'contig_len', 'full_lineage']\nsrc = \"${all}\"\ndst = \"all.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_all_simplified_asm_results.html\")\n\nsrc = \"${virus}\"\ndst = \"viral.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_virus_simplified_asm_results.html\")\n\nsrc = \"${bacteria}\"\ndst = \"bact.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_bacteria_simplified_asm_results.html\")\n\nsrc = \"${eukaryotic}\"\ndst = \"eukar.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_eukaryota_simplified_asm_results.html\")\n\nsrc = \"${other}\"\ndst = \"other.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_other_simplified_asm_results.html\")\n\n\"\"\"",
        "nb_lignes_script": 74,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "html_simplifed_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "html_simplifed_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"All\"}",
            "publishDir \"${params.publish_base_dir}/All/simplified_html_tables\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "collected_tsv_to_html": {
        "name_process": "collected_tsv_to_html",
        "string_process": "\nprocess collected_tsv_to_html{\n  tag {\"All\"}\n\n  publishDir \"${params.publish_base_dir}/All/html_tables\", mode:'link'\n\n  input:\n  set all, virus, bacteria, eukaryotic, other, hvalue, html from html_collection_tsv_in\n\n  output:\n  set \"${params.project_id}_all_asm_results.html\", \"${params.project_id}_virus_asm_results.html\", \"${params.project_id}_bacteria_asm_results.html\", \"${params.project_id}_eukaryota_asm_results.html\", \"${params.project_id}_other_asm_results.html\" into html_collection_tsv_out\n  script:\n\"\"\"\n#!/usr/bin/python3\nimport string\nimport csv\nimport json\nimport collections\n\nOrderedDict = collections.OrderedDict\n\ndef HTML_table(src, header, htmlEndStr, name):\n\n    f = open(src)\n    lines = f.readlines()\n    f.close()\n    f1 = open(header)\n    header_lines = f1.readlines()\n    f2 = open(htmlEndStr)\n    end_lines = f2.readlines()\n    f1.close()\n    f2.close()\n\n    with open(name, 'w') as f:\n        for h_line in header_lines:\n            f.write(h_line)\n        for line in lines:\n            f.write(line)\n        for e_line in end_lines:\n            f.write(e_line)\n    f.close()\n    return None\n\ndef TSV_file_into_JSON(src, dst, header):\n    data = []\n    with open(src, 'r') as csvfile:\n        reader = csv.reader(csvfile, delimiter='\\\\t', quotechar='\"')\n        for row in reader:\n            if row[0] == 'sample_id':\n                print(\"\\\\n\")\n            else:\n                if row[0].strip()[0] == '#':  #\n                    continue\n                row = filter(None, row)\n                data.append(OrderedDict(zip(header, row)))\n\n    with open(dst, 'w') as jsonfile:\n        json.dump(data, jsonfile, indent=2)\n    return None\n\nheader = ['sample_id', 'contig_id', 'accession_nr', 'scientific_name', 'title', 'percentage', 'evalue', 'mismatch', 'aln_len', 'contig_len', 'full_lineage']\nsrc = \"${all}\"\ndst = \"all.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_all_asm_results.html\")\n\nsrc = \"${virus}\"\ndst = \"viral.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_virus_asm_results.html\")\n\nsrc = \"${bacteria}\"\ndst = \"bact.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_bacteria_asm_results.html\")\n\nsrc = \"${eukaryotic}\"\ndst = \"eukar.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_eukaryota_asm_results.html\")\n\nsrc = \"${other}\"\ndst = \"other.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_other_asm_results.html\")\n\"\"\"\n}",
        "nb_lignes_process": 85,
        "string_script": "\"\"\"\n#!/usr/bin/python3\nimport string\nimport csv\nimport json\nimport collections\n\nOrderedDict = collections.OrderedDict\n\ndef HTML_table(src, header, htmlEndStr, name):\n\n    f = open(src)\n    lines = f.readlines()\n    f.close()\n    f1 = open(header)\n    header_lines = f1.readlines()\n    f2 = open(htmlEndStr)\n    end_lines = f2.readlines()\n    f1.close()\n    f2.close()\n\n    with open(name, 'w') as f:\n        for h_line in header_lines:\n            f.write(h_line)\n        for line in lines:\n            f.write(line)\n        for e_line in end_lines:\n            f.write(e_line)\n    f.close()\n    return None\n\ndef TSV_file_into_JSON(src, dst, header):\n    data = []\n    with open(src, 'r') as csvfile:\n        reader = csv.reader(csvfile, delimiter='\\\\t', quotechar='\"')\n        for row in reader:\n            if row[0] == 'sample_id':\n                print(\"\\\\n\")\n            else:\n                if row[0].strip()[0] == '#':  #\n                    continue\n                row = filter(None, row)\n                data.append(OrderedDict(zip(header, row)))\n\n    with open(dst, 'w') as jsonfile:\n        json.dump(data, jsonfile, indent=2)\n    return None\n\nheader = ['sample_id', 'contig_id', 'accession_nr', 'scientific_name', 'title', 'percentage', 'evalue', 'mismatch', 'aln_len', 'contig_len', 'full_lineage']\nsrc = \"${all}\"\ndst = \"all.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_all_asm_results.html\")\n\nsrc = \"${virus}\"\ndst = \"viral.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_virus_asm_results.html\")\n\nsrc = \"${bacteria}\"\ndst = \"bact.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_bacteria_asm_results.html\")\n\nsrc = \"${eukaryotic}\"\ndst = \"eukar.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_eukaryota_asm_results.html\")\n\nsrc = \"${other}\"\ndst = \"other.json\"\nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_other_asm_results.html\")\n\"\"\"",
        "nb_lignes_script": 73,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "html_collection_tsv_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "html_collection_tsv_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__Discovery2",
        "directive": [
            "tag {\"All\"}",
            "publishDir \"${params.publish_base_dir}/All/html_tables\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    }
}