{
    "DOWNLOAD_GENOME": {
        "name_process": "DOWNLOAD_GENOME",
        "string_process": "\nprocess DOWNLOAD_GENOME{\n    tag \"${assembly}\"\n    storeDir getOutputDir('genome')\n\n    input:\n    val assembly from GENOME_ASSEMBLY\n\n    output:\n    file \"${assembly}.fa\" into GENOME_FASTA\n    set file(\"${assembly}.chromsizes.txt\"), assembly into GENOME_CHROMSIZES\n                                               \n    set \"${assembly}.fa\", file(\"${assembly}.fa.*\") into GENOME_INDEX\n\n    script:\n    if (auto_download_genome) {\n        \"\"\"\n        wget http://hgdownload.cse.ucsc.edu/goldenPath/${assembly}/bigZips/${assembly}.fa.gz -O ${assembly}.fa.gz\n        bgzip -d -@ ${task.cpus} ${assembly}.fa.gz\n        faidx ${assembly}.fa -i chromsizes > ${assembly}.chromsizes.txt\n        hisat2-build -p ${task.cpus} ${assembly}.fa ${assembly}.fa\n        \"\"\"\n    }\n    else {\n                                      \n        def fasta_file = params.genome.get(\"fasta\", \"\")\n        def suffix = isGZ(fasta_file) ? \".gz\" : \"\"\n\n                                     \n        def getGenomeCmd = \"\"\n        if (isURL(fasta_file)){\n            getGenomeCmd = \"\"\"\n                wget ${fasta_file} -O ${assembly}.fa${suffix}\n            \"\"\"\n        }\n        else {\n            getGenomeCmd = \"\"\"\n                cp ${fasta_file} ${assembly}.fa${suffix}\n            \"\"\"\n        }\n\n                               \n        def unpackGenomeCmd = \"\"\n        if (isGZ(fasta_file)){\n            unpackGenomeCmd = \"\"\"\n                bgzip -d -@ ${task.cpus} ${assembly}.fa.gz\n            \"\"\"\n        }\n\n                                               \n        def chromsizes_file = params.genome.get(\"chromsizes\", \"\")\n        def getChromsizesCmd = \"\"\n        if (isURL(chromsizes_file)){\n            getChromsizesCmd = \"\"\"\n                wget ${chromsizes_file} -O ${assembly}.chromsizes.txt\n            \"\"\"\n        }\n        else if (chromsizes_file.size()>0) {\n            getChromsizesCmd = \"\"\"\n                cp ${chromsizes_file} ${assembly}.chromsizes.txt\n            \"\"\"\n        }\n        else {\n            getChromsizesCmd = \"\"\"\n                faidx ${assembly}.fa -i chromsizes > ${assembly}.chromsizes.txt\n            \"\"\"\n        }\n\n                                                                               \n        suffix_list = [\".1.ht2\", \".2.ht2\", \".3.ht2\", \".4.ht2\", \".5.ht2\", \".6.ht2\", \".7.ht2\", \".8.ht2\"]\n        def getIndexCmd = \"\"\n        if ((params.genome.get('index_prefix', '').size()==0) | (isURL(fasta_file))) {\n            getIndexCmd = \"hisat2-build -p ${task.cpus} ${assembly}.fa ${assembly}.fa\"\n        } else {\n            for (suf in suffix_list) {\n                getIndexCmd = getIndexCmd + \"cp ${params.genome.index_prefix}${suf} ${assembly}.fa${suf}; \"\n            }\n        }\n\n        \"\"\"\n        ${getGenomeCmd}\n        ${unpackGenomeCmd}\n        ${getChromsizesCmd}\n        ${getIndexCmd}\n        \"\"\"\n    }\n}",
        "nb_lignes_process": 85,
        "string_script": "    if (auto_download_genome) {\n        \"\"\"\n        wget http://hgdownload.cse.ucsc.edu/goldenPath/${assembly}/bigZips/${assembly}.fa.gz -O ${assembly}.fa.gz\n        bgzip -d -@ ${task.cpus} ${assembly}.fa.gz\n        faidx ${assembly}.fa -i chromsizes > ${assembly}.chromsizes.txt\n        hisat2-build -p ${task.cpus} ${assembly}.fa ${assembly}.fa\n        \"\"\"\n    }\n    else {\n                                      \n        def fasta_file = params.genome.get(\"fasta\", \"\")\n        def suffix = isGZ(fasta_file) ? \".gz\" : \"\"\n\n                                     \n        def getGenomeCmd = \"\"\n        if (isURL(fasta_file)){\n            getGenomeCmd = \"\"\"\n                wget ${fasta_file} -O ${assembly}.fa${suffix}\n            \"\"\"\n        }\n        else {\n            getGenomeCmd = \"\"\"\n                cp ${fasta_file} ${assembly}.fa${suffix}\n            \"\"\"\n        }\n\n                               \n        def unpackGenomeCmd = \"\"\n        if (isGZ(fasta_file)){\n            unpackGenomeCmd = \"\"\"\n                bgzip -d -@ ${task.cpus} ${assembly}.fa.gz\n            \"\"\"\n        }\n\n                                               \n        def chromsizes_file = params.genome.get(\"chromsizes\", \"\")\n        def getChromsizesCmd = \"\"\n        if (isURL(chromsizes_file)){\n            getChromsizesCmd = \"\"\"\n                wget ${chromsizes_file} -O ${assembly}.chromsizes.txt\n            \"\"\"\n        }\n        else if (chromsizes_file.size()>0) {\n            getChromsizesCmd = \"\"\"\n                cp ${chromsizes_file} ${assembly}.chromsizes.txt\n            \"\"\"\n        }\n        else {\n            getChromsizesCmd = \"\"\"\n                faidx ${assembly}.fa -i chromsizes > ${assembly}.chromsizes.txt\n            \"\"\"\n        }\n\n                                                                               \n        suffix_list = [\".1.ht2\", \".2.ht2\", \".3.ht2\", \".4.ht2\", \".5.ht2\", \".6.ht2\", \".7.ht2\", \".8.ht2\"]\n        def getIndexCmd = \"\"\n        if ((params.genome.get('index_prefix', '').size()==0) | (isURL(fasta_file))) {\n            getIndexCmd = \"hisat2-build -p ${task.cpus} ${assembly}.fa ${assembly}.fa\"\n        } else {\n            for (suf in suffix_list) {\n                getIndexCmd = getIndexCmd + \"cp ${params.genome.index_prefix}${suf} ${assembly}.fa${suf}; \"\n            }\n        }\n\n        \"\"\"\n        ${getGenomeCmd}\n        ${unpackGenomeCmd}\n        ${getChromsizesCmd}\n        ${getIndexCmd}\n        \"\"\"\n    }",
        "nb_lignes_script": 70,
        "language_script": "bash",
        "tools": [
            "pyfaidx"
        ],
        "tools_url": [
            "https://bio.tools/pyfaidx"
        ],
        "tools_dico": [
            {
                "name": "pyfaidx",
                "uri": "https://bio.tools/pyfaidx",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Data handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Utility operation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File processing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Report handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This python module implements pure Python classes for indexing, retrieval, and in-place modification of FASTA files using a samtools compatible index.",
                "homepage": "https://pythonhosted.org/pyfaidx/"
            }
        ],
        "inputs": [
            "GENOME_ASSEMBLY"
        ],
        "nb_inputs": 1,
        "outputs": [
            "GENOME_FASTA",
            "GENOME_CHROMSIZES",
            "GENOME_INDEX"
        ],
        "nb_outputs": 3,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"${assembly}\"",
            "storeDir getOutputDir('genome')"
        ],
        "when": "",
        "stub": ""
    },
    "RESTRICT_GENOME": {
        "name_process": "RESTRICT_GENOME",
        "string_process": " process RESTRICT_GENOME{\n        tag \"${assembly} ${renz}\"\n        storeDir getOutputDir('genome')\n\n        input:\n        val(assembly) from GENOME_ASSEMBLY\n        file(genome_fasta) from GENOME_FASTA                       \n        set renz_key, renz from LIST_RENZ\n\n        output:\n        set renz_key, \"${assembly}.${renz}.bed\" into GENOME_RENZ\n\n        script:\n        \"\"\"\n        detect_restriction_sites.py ${genome_fasta} ${renz} ${assembly}.${renz}.nonsorted.bed\n        sort -k1,1 -k2,2n --parallel=${task.cpus} ${assembly}.${renz}.nonsorted.bed > ${assembly}.${renz}.bed\n        \"\"\"\n    }",
        "nb_lignes_process": 16,
        "string_script": "        \"\"\"\n        detect_restriction_sites.py ${genome_fasta} ${renz} ${assembly}.${renz}.nonsorted.bed\n        sort -k1,1 -k2,2n --parallel=${task.cpus} ${assembly}.${renz}.nonsorted.bed > ${assembly}.${renz}.bed\n        \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "GENOME_ASSEMBLY",
            "GENOME_FASTA",
            "LIST_RENZ"
        ],
        "nb_inputs": 3,
        "outputs": [
            "GENOME_RENZ"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"${assembly} ${renz}\"",
            "storeDir getOutputDir('genome')"
        ],
        "when": "",
        "stub": ""
    },
    "PREPARE_RNA_ANNOTATION": {
        "name_process": "PREPARE_RNA_ANNOTATION",
        "string_process": "\nprocess PREPARE_RNA_ANNOTATION{\n    tag \"${rna_annot_name}\"\n    storeDir getOutputDir('genome')\n\n    input:\n    set val(rna_annot_name), val(preprocessing_output) from GENOME_RNA_ANNOT_NAME\n\n    output:\n    file \"${rna_annot_name}.spliced_genes.txt\" into GENOME_SPLICESITES\n    file \"${rna_annot_name}.gtf\" into RNA_ANNOT_FILE\n\n    script:\n    def suffix = isGZ(params.rna_annotation.genes_gtf) ? \".gz\" : \"\"\n    def getRNAAnnot = \"\"\n    if (isURL(params.rna_annotation.genes_gtf)) {\n        getRNAAnnot = \"\"\"\n        wget ${params.rna_annotation.genes_gtf} -O ${rna_annot_name}.gtf${suffix}\n        \"\"\"\n    }\n    else {\n        getRNAAnnot = \"\"\"\n        cp ${params.rna_annotation.genes_gtf} ${rna_annot_name}.gtf${suffix}\n        \"\"\"\n    }\n    def unpackRNAAnnot = \"\"\n    if (isGZ(params.rna_annotation.genes_gtf)){\n        unpackRNAAnnot = \"\"\"\n            bgzip -d -@ ${task.cpus} ${rna_annot_name}.gtf.gz\n        \"\"\"\n    }\n\n    \"\"\"\n    ${getRNAAnnot}\n    ${unpackRNAAnnot}\n    hisat2_extract_splice_sites.py ${rna_annot_name}.gtf > ${rna_annot_name}.spliced_genes.txt\n    \"\"\"\n}",
        "nb_lignes_process": 36,
        "string_script": "    def suffix = isGZ(params.rna_annotation.genes_gtf) ? \".gz\" : \"\"\n    def getRNAAnnot = \"\"\n    if (isURL(params.rna_annotation.genes_gtf)) {\n        getRNAAnnot = \"\"\"\n        wget ${params.rna_annotation.genes_gtf} -O ${rna_annot_name}.gtf${suffix}\n        \"\"\"\n    }\n    else {\n        getRNAAnnot = \"\"\"\n        cp ${params.rna_annotation.genes_gtf} ${rna_annot_name}.gtf${suffix}\n        \"\"\"\n    }\n    def unpackRNAAnnot = \"\"\n    if (isGZ(params.rna_annotation.genes_gtf)){\n        unpackRNAAnnot = \"\"\"\n            bgzip -d -@ ${task.cpus} ${rna_annot_name}.gtf.gz\n        \"\"\"\n    }\n\n    \"\"\"\n    ${getRNAAnnot}\n    ${unpackRNAAnnot}\n    hisat2_extract_splice_sites.py ${rna_annot_name}.gtf > ${rna_annot_name}.spliced_genes.txt\n    \"\"\"",
        "nb_lignes_script": 23,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "GENOME_RNA_ANNOT_NAME"
        ],
        "nb_inputs": 1,
        "outputs": [
            "GENOME_SPLICESITES",
            "RNA_ANNOT_FILE"
        ],
        "nb_outputs": 2,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"${rna_annot_name}\"",
            "storeDir getOutputDir('genome')"
        ],
        "when": "",
        "stub": ""
    },
    "DOWNLOAD_FASTQ": {
        "name_process": "DOWNLOAD_FASTQ",
        "string_process": "\nprocess DOWNLOAD_FASTQ {\n    tag \"library:${library}\"\n\n    storeDir getOutputDir('fastq')\n\n    input:\n    tuple val(library), val(name) from FASTQ_PATHS.for_download\n\n    output:\n    tuple val(library), \"${library}_1.fastq.gz\", \"${library}_2.fastq.gz\" into DOWNLOADED\n\n    script:\n    def sra = ( name=~ /SRR\\d+/ )[0]\n    \"\"\"\n    fastq-dump ${sra} -Z --split-spot \\\n                   | pyfilesplit --lines 4 \\\n                     >(bgzip -c -@${task.cpus} > ${library}_1.fastq.gz) \\\n                     >(bgzip -c -@${task.cpus} > ${library}_2.fastq.gz) \\\n                     | cat\n    \"\"\"\n\n}",
        "nb_lignes_process": 21,
        "string_script": "    def sra = ( name=~ /SRR\\d+/ )[0]\n    \"\"\"\n    fastq-dump ${sra} -Z --split-spot \\\n                   | pyfilesplit --lines 4 \\\n                     >(bgzip -c -@${task.cpus} > ${library}_1.fastq.gz) \\\n                     >(bgzip -c -@${task.cpus} > ${library}_2.fastq.gz) \\\n                     | cat\n    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "FASTQ_PATHS"
        ],
        "nb_inputs": 1,
        "outputs": [
            "DOWNLOADED"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library}\"",
            "storeDir getOutputDir('fastq')"
        ],
        "when": "",
        "stub": ""
    },
    "SPLIT_FASTQ_INTO_CHUNKS": {
        "name_process": "SPLIT_FASTQ_INTO_CHUNKS",
        "string_process": "\nprocess SPLIT_FASTQ_INTO_CHUNKS{\n    tag \"library:${library}\"\n\n    storeDir getOutputDir('fastq')\n\n    input:\n    set val(library), path(input_fq1), path(input_fq2) from LIB_FASTQ\n\n    output:\n    set val(library), \"${library}.*.1.fq\", \"${library}.*.2.fq\" into LIB_SPLIT_FASTQ_RAW\n\n    script:\n    def readCmd = (isGZ(input_fq1.toString())) ?  \"bgzip -dc -@ ${task.cpus}\" : \"cat\"\n\n    \"\"\"\n    echo\n    ${readCmd} ${input_fq1} | split -l ${chunksize} --numeric-suffixes=1 \\\n        --additional-suffix=\".1.fq\" - ${library}.\n    ${readCmd} ${input_fq2} | split -l ${chunksize} --numeric-suffixes=1 \\\n        --additional-suffix=\".2.fq\" - ${library}.\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "    def readCmd = (isGZ(input_fq1.toString())) ?  \"bgzip -dc -@ ${task.cpus}\" : \"cat\"\n\n    \"\"\"\n    echo\n    ${readCmd} ${input_fq1} | split -l ${chunksize} --numeric-suffixes=1 \\\n        --additional-suffix=\".1.fq\" - ${library}.\n    ${readCmd} ${input_fq2} | split -l ${chunksize} --numeric-suffixes=1 \\\n        --additional-suffix=\".2.fq\" - ${library}.\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_FASTQ"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_SPLIT_FASTQ_RAW"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library}\"",
            "storeDir getOutputDir('fastq')"
        ],
        "when": "",
        "stub": ""
    },
    "CREATE_READS_TABLE_CHUNKS": {
        "name_process": "CREATE_READS_TABLE_CHUNKS",
        "string_process": "\nprocess CREATE_READS_TABLE_CHUNKS{\n    tag \"library:${library} chunk:${chunk}\"\n\n    storeDir getOutputDir('table')\n\n    input:\n    set val(library), val(chunk), val(input1), file(input_fq1), val(input2), file(input_fq2) from LIB_SPLIT_FASTQ_TO_TABLE\n\n    output:\n    set library, chunk, \"${library}.${chunk}.fastq.txt\" into LIB_TABLE_FASTQ\n\n    script:\n    \"\"\"\n    paste <(awk '{print \\$1}' ${input_fq1} | sed 'N;N;N;s/\\\\n/ /g' | \\\n            awk 'BEGIN{OFS=\"\\\\t\"}{print \\$1, \"${library}.${chunk}\", \\$2, \\$4}' ) \\\n          <(awk '{print \\$1}' ${input_fq2} | sed 'N;N;N;s/\\\\n/ /g' | \\\n            awk 'BEGIN{OFS=\"\\\\t\"}{print \\$2, \\$4}' ) > ${library}.${chunk}.fastq.txt\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "    \"\"\"\n    paste <(awk '{print \\$1}' ${input_fq1} | sed 'N;N;N;s/\\\\n/ /g' | \\\n            awk 'BEGIN{OFS=\"\\\\t\"}{print \\$1, \"${library}.${chunk}\", \\$2, \\$4}' ) \\\n          <(awk '{print \\$1}' ${input_fq2} | sed 'N;N;N;s/\\\\n/ /g' | \\\n            awk 'BEGIN{OFS=\"\\\\t\"}{print \\$2, \\$4}' ) > ${library}.${chunk}.fastq.txt\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_SPLIT_FASTQ_TO_TABLE"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_TABLE_FASTQ"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk}\"",
            "storeDir getOutputDir('table')"
        ],
        "when": "",
        "stub": ""
    },
    "DEDUP": {
        "name_process": "DEDUP",
        "string_process": "\nprocess DEDUP{\n    tag \"library:${library}\"\n\n    storeDir getOutputDir('table')\n\n    input:\n    set val(library), file(input_fq1), file(input_fq2) from LIB_FASTQ_TO_FASTUNIQ\n\n    output:\n    set library, \"${library}.ids.unique.txt\" into IDS_FASTUNIQ\n\n    script:\n    \"\"\"\n    # Trim N basepairs\n    trimmomatic PE -threads ${task.cpus} ${input_fq1} ${input_fq2} \\\n                                         ${library}_1P ${library}_1U \\\n                                         ${library}_2P ${library}_2U CROP:${cropped_length}\n\n    # Create input file for fastuniq\n    echo ${library}_1P >  filelist.txt\n    echo ${library}_2P >> filelist.txt\n\n    # Run fastuniq\n    fastuniq -i filelist.txt -tq -c 0 -o ${library}.1.unique.fq -p ${library}.2.unique.fq\n\n    # Parse fastuniq output\n    awk 'NR%4==1' ${library}.1.unique.fq | gawk '{match(\\$0, \"@([^ ,/]+)\", a)} {print a[1]}' \\\n        > ${library}.ids.unique.txt\n\n    \"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "    \"\"\"\n    # Trim N basepairs\n    trimmomatic PE -threads ${task.cpus} ${input_fq1} ${input_fq2} \\\n                                         ${library}_1P ${library}_1U \\\n                                         ${library}_2P ${library}_2U CROP:${cropped_length}\n\n    # Create input file for fastuniq\n    echo ${library}_1P >  filelist.txt\n    echo ${library}_2P >> filelist.txt\n\n    # Run fastuniq\n    fastuniq -i filelist.txt -tq -c 0 -o ${library}.1.unique.fq -p ${library}.2.unique.fq\n\n    # Parse fastuniq output\n    awk 'NR%4==1' ${library}.1.unique.fq | gawk '{match(\\$0, \"@([^ ,/]+)\", a)} {print a[1]}' \\\n        > ${library}.ids.unique.txt\n\n    \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [
            "Trimmomatic"
        ],
        "tools_url": [
            "https://bio.tools/trimmomatic"
        ],
        "tools_dico": [
            {
                "name": "Trimmomatic",
                "uri": "https://bio.tools/trimmomatic",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Sequence trimming"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Trimming"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0006",
                                "term": "Data"
                            },
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0006",
                                "term": "Data"
                            }
                        ]
                    }
                ],
                "description": "A flexible read trimming tool for Illumina NGS data",
                "homepage": "http://www.usadellab.org/cms/index.php?page=trimmomatic"
            }
        ],
        "inputs": [
            "LIB_FASTQ_TO_FASTUNIQ"
        ],
        "nb_inputs": 1,
        "outputs": [
            "IDS_FASTUNIQ"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library}\"",
            "storeDir getOutputDir('table')"
        ],
        "when": "",
        "stub": ""
    },
    "TRIM_CHUNKS": {
        "name_process": "TRIM_CHUNKS",
        "string_process": "\nprocess TRIM_CHUNKS{\n    tag \"library:${library} chunk:${chunk}\"\n\n    storeDir getOutputDir('table')\n\n    input:\n    set val(library), val(chunk),\n        val(input1), file(input_fq1),\n        val(input2), file(input_fq2) from LIB_SPLIT_FASTQ_TO_TRIM\n\n    output:\n    set library, chunk, \"${library}.${chunk}.1.trimmed.fq\", \"${library}.${chunk}.2.trimmed.fq\" into LIB_TRIMMED\n\n    script:\n    \"\"\"\n    # Trim with specified parameters\n    trimmomatic PE -phred33 -threads ${task.cpus} ${input_fq1} ${input_fq2} \\\n                            ${library}.${chunk}.1.trimmed.fq ${library}_1U \\\n                            ${library}.${chunk}.2.trimmed.fq ${library}_2U ${params_trimmomatic}\n    \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "    \"\"\"\n    # Trim with specified parameters\n    trimmomatic PE -phred33 -threads ${task.cpus} ${input_fq1} ${input_fq2} \\\n                            ${library}.${chunk}.1.trimmed.fq ${library}_1U \\\n                            ${library}.${chunk}.2.trimmed.fq ${library}_2U ${params_trimmomatic}\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "Trimmomatic"
        ],
        "tools_url": [
            "https://bio.tools/trimmomatic"
        ],
        "tools_dico": [
            {
                "name": "Trimmomatic",
                "uri": "https://bio.tools/trimmomatic",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Sequence trimming"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Trimming"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0006",
                                "term": "Data"
                            },
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0006",
                                "term": "Data"
                            }
                        ]
                    }
                ],
                "description": "A flexible read trimming tool for Illumina NGS data",
                "homepage": "http://www.usadellab.org/cms/index.php?page=trimmomatic"
            }
        ],
        "inputs": [
            "LIB_SPLIT_FASTQ_TO_TRIM"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_TRIMMED"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk}\"",
            "storeDir getOutputDir('table')"
        ],
        "when": "",
        "stub": ""
    },
    "CREATE_TRIM_TABLE_CHUNKS": {
        "name_process": "CREATE_TRIM_TABLE_CHUNKS",
        "string_process": "\nprocess CREATE_TRIM_TABLE_CHUNKS{\n    tag \"library:${library} chunk:${chunk}\"\n\n    storeDir getOutputDir('table')\n\n    input:\n    set val(library), val(chunk), file(input_table), file(input_fq1), file(input_fq2) from LIB_FOR_GET_TRIM_OUTPUT\n\n    output:\n    set library, chunk, \"${library}.${chunk}.trimtable.txt\" into LIB_TRIMTABLE\n\n    script:\n    \"\"\"\n    paste <(sed -n '1~4p' ${input_fq1} | awk 'BEGIN{OFS=\"\\\\t\"}{print \"${library}.${chunk}\", \"trimmomatic\", \\$1}') \\\n          <(sed -n '2~4p' ${input_fq1} | awk 'BEGIN{OFS=\"\\\\t\"}{print 0, length(\\$0);}') \\\n          <(sed -n '2~4p' ${input_fq2} | awk 'BEGIN{OFS=\"\\\\t\"}{print 0, length(\\$0);}') \\\n          > ${library}.${chunk}.trim.info\n\n    awk 'NR==FNR {vals[\\$3] = \\$1 \"\\\\t\" \\$2 \"\\\\t\" \\$3 \"\\\\t\" \\$4 \"\\\\t\" \\$5 \"\\\\t\" \\$6 \"\\\\t\" \\$7 ; next} \\\n        !(\\$1 in vals) {vals[\\$1] = \"${library}.${chunk}\" \"\\\\t\" \"trimmomatic\" \"\\\\t\" \\$1 \"\\\\t\" \"0\\\\t0\\\\t0\\\\t0\"} \\\n        {\\$(NF+1) = vals[\\$1]; print vals[\\$1]}' ${library}.${chunk}.trim.info ${input_table} \\\n        > ${library}.${chunk}.trimtable.txt\n    \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "    \"\"\"\n    paste <(sed -n '1~4p' ${input_fq1} | awk 'BEGIN{OFS=\"\\\\t\"}{print \"${library}.${chunk}\", \"trimmomatic\", \\$1}') \\\n          <(sed -n '2~4p' ${input_fq1} | awk 'BEGIN{OFS=\"\\\\t\"}{print 0, length(\\$0);}') \\\n          <(sed -n '2~4p' ${input_fq2} | awk 'BEGIN{OFS=\"\\\\t\"}{print 0, length(\\$0);}') \\\n          > ${library}.${chunk}.trim.info\n\n    awk 'NR==FNR {vals[\\$3] = \\$1 \"\\\\t\" \\$2 \"\\\\t\" \\$3 \"\\\\t\" \\$4 \"\\\\t\" \\$5 \"\\\\t\" \\$6 \"\\\\t\" \\$7 ; next} \\\n        !(\\$1 in vals) {vals[\\$1] = \"${library}.${chunk}\" \"\\\\t\" \"trimmomatic\" \"\\\\t\" \\$1 \"\\\\t\" \"0\\\\t0\\\\t0\\\\t0\"} \\\n        {\\$(NF+1) = vals[\\$1]; print vals[\\$1]}' ${library}.${chunk}.trim.info ${input_table} \\\n        > ${library}.${chunk}.trimtable.txt\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_FOR_GET_TRIM_OUTPUT"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_TRIMTABLE"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk}\"",
            "storeDir getOutputDir('table')"
        ],
        "when": "",
        "stub": ""
    },
    "INDEX_OLIGOS": {
        "name_process": "INDEX_OLIGOS",
        "string_process": "\nprocess INDEX_OLIGOS{\n    tag \"oligo:${oligo}\"\n\n    storeDir getOutputDir('cindex')\n\n    input:\n    set val(oligo), input_oligo, file(input_oligo_fa), val(preprocessing_output) from LIB_OLIGOS_RAW\n\n    output:\n    set oligo, \"${oligo}.bin\" into LIB_OLIGOS_CINDEX\n\n    script:\n    \"\"\"\n    fasta2bin ${input_oligo_fa} ${oligo}.bin\n    \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "    \"\"\"\n    fasta2bin ${input_oligo_fa} ${oligo}.bin\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_OLIGOS_RAW"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_OLIGOS_CINDEX"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"oligo:${oligo}\"",
            "storeDir getOutputDir('cindex')"
        ],
        "when": "",
        "stub": ""
    },
    "INDEX_CHUNKS": {
        "name_process": "INDEX_CHUNKS",
        "string_process": "\nprocess INDEX_CHUNKS{\n    tag \"library:${library} chunk:${chunk}\"\n\n    storeDir getOutputDir('cindex')\n\n    input:\n    set val(library), val(chunk),\n        val(input1), file(input_fq1),\n        val(input2), file(input_fq2) from LIB_SPLIT_FASTQ_TO_CINDEX\n\n    output:\n    set library, chunk, \"${library}.${chunk}.1.bin\", \"${library}.${chunk}.2.bin\" into LIB_FASTQ_CINDEX\n\n    script:\n    \"\"\"\n    fastq2bin ${input_fq1} ${library}.${chunk}.1.bin\n    fastq2bin ${input_fq2} ${library}.${chunk}.2.bin\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "    \"\"\"\n    fastq2bin ${input_fq1} ${library}.${chunk}.1.bin\n    fastq2bin ${input_fq2} ${library}.${chunk}.2.bin\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_SPLIT_FASTQ_TO_CINDEX"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_FASTQ_CINDEX"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk}\"",
            "storeDir getOutputDir('cindex')"
        ],
        "when": "",
        "stub": ""
    },
    "SEARCH_OLIGOS_CHUNKS": {
        "name_process": "SEARCH_OLIGOS_CHUNKS",
        "string_process": "\nprocess SEARCH_OLIGOS_CHUNKS{\n    tag \"library:${library} chunk:${chunk} side:${apply_to} oligo:${oligo}\"\n\n    storeDir getOutputDir('cout')\n\n    input:\n    set val(oligo), file(oligo_cindex), val(library), val(chunk),\n        file(fq_cindex), val(apply_to), val(map_params), val(read_length) from LIB_FOR_OLIGOS_MAPPING\n\n    output:\n    set library, chunk, oligo, apply_to, \"${library}.${chunk}.${apply_to}.${oligo}.txt\", read_length into LIB_MAPPED_OLIGOS\n\n    script:\n    def seqlen_converted = lengths[ map_params.read_length ]\n    \"\"\"\n    align_universal ${oligo_cindex} ${fq_cindex} 1 ${map_params.read_length} ${seqlen_converted} \\\n      ${map_params.n_primers} ${map_params.left_shift} ${map_params.right_shift} \\\n      ${map_params.mismatch_general} 0 ${map_params.report_len} > ${library}.${chunk}.${apply_to}.${oligo}.txt\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "    def seqlen_converted = lengths[ map_params.read_length ]\n    \"\"\"\n    align_universal ${oligo_cindex} ${fq_cindex} 1 ${map_params.read_length} ${seqlen_converted} \\\n      ${map_params.n_primers} ${map_params.left_shift} ${map_params.right_shift} \\\n      ${map_params.mismatch_general} 0 ${map_params.report_len} > ${library}.${chunk}.${apply_to}.${oligo}.txt\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_FOR_OLIGOS_MAPPING"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_MAPPED_OLIGOS"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk} side:${apply_to} oligo:${oligo}\"",
            "storeDir getOutputDir('cout')"
        ],
        "when": "",
        "stub": ""
    },
    "CHECK_GA": {
        "name_process": "CHECK_GA",
        "string_process": "\nprocess CHECK_GA{\n    tag \"library:${library} chunk:${chunk}\"\n\n    storeDir getOutputDir('table')\n\n    input:\n    set val(library), val(chunk), file(table_fq), file(cout_br_for) from LIB_FOR_GA\n\n    output:\n    set library, chunk, \"${library}.${chunk}.GA.txt\" into LIB_MAPPED_GA\n\n    script:\n    def checkGACmd=\"\"\n    if (params.run.get('check_GA', true)) {\n        checkGACmd=\"check_oligo_presence.py ${table_fq} ${cout_br_for} ${library}.${chunk}.GA.txt\"\n    } else {\n        checkGACmd=\"awk '{print NR-1\\\"\\\\t0\\\"}' ${table_fq} > ${library}.${chunk}.GA.txt\"\n    }\n    \"\"\"\n    ${checkGACmd}\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "    def checkGACmd=\"\"\n    if (params.run.get('check_GA', true)) {\n        checkGACmd=\"check_oligo_presence.py ${table_fq} ${cout_br_for} ${library}.${chunk}.GA.txt\"\n    } else {\n        checkGACmd=\"awk '{print NR-1\\\"\\\\t0\\\"}' ${table_fq} > ${library}.${chunk}.GA.txt\"\n    }\n    \"\"\"\n    ${checkGACmd}\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_FOR_GA"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_MAPPED_GA"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk}\"",
            "storeDir getOutputDir('table')"
        ],
        "when": "",
        "stub": ""
    },
    "CHECK_COMPLEMENTARY_RNA_CHUNKS": {
        "name_process": "CHECK_COMPLEMENTARY_RNA_CHUNKS",
        "string_process": "\nprocess CHECK_COMPLEMENTARY_RNA_CHUNKS{\n    tag \"library:${library} chunk:${chunk}\"\n\n    storeDir getOutputDir('cout')\n\n    input:\n    set val(library), val(chunk), file(table_fq),\n        file(cout_br_for), file(cout_ggg_rev), read_length,\n        file(cindex_fq1), file(cindex_fq2) from LIB_FOR_RNACOMP\n\n    output:\n    set library, chunk, \"${library}.${chunk}.1.rnacomp.txt\", \"${library}.${chunk}.2.rnacomp.txt\" into LIB_COUT_RNACOMP\n\n    script:\n\n    def rna_complementary_length = params.run.rna_complementary_length\n    def rna_comp_length_converted = ((rna_complementary_length+3).intdiv(8))+1\n    def extraN = \"N\"*(500+3*rna_complementary_length)\n    def seqlen_converted = lengths[ read_length ]\n    \"\"\"\n    # Get the complemetary regions\n    paste <(awk '{print \\$1, \\$3, \\$4}' ${table_fq}) \\\n          <(head -n -1 ${cout_br_for} | tail -n +2 | awk '{print \\$4}') \\\n          | awk 'BEGIN{OFS=\"\\\\n\";} {bgn=1; if (\\$4<500) bgn=\\$4+${br_length}+1; \\\n          print \\$1, substr(\\$2\"${extraN}\", bgn, ${rna_complementary_length}), \\\n          \"+\", substr(\\$3\"${extraN}\", bgn, ${rna_complementary_length})}' > ${library}.${chunk}.rna-end.1.fq\n\n    paste <(awk '{print \\$1, \\$5, \\$6}' ${table_fq}) \\\n          <(head -n -1 ${cout_ggg_rev} | tail -n +2 | awk '{print \\$5+1}') \\\n          | awk 'BEGIN{OFS=\"\\\\n\";} {bgn=1; if (\\$4<500) bgn=\\$4;\n          print \\$1, substr(\\$2\"${extraN}\", bgn, ${rna_complementary_length}), \\\n          \"+\", substr(\\$3\"${extraN}\", bgn, ${rna_complementary_length})}' > ${library}.${chunk}.rna-end.2.fq\n\n    # Convert to reverse complement\n    paste <(sed -n '1~4p' ${library}.${chunk}.rna-end.1.fq) \\\n          <(sed -n '2~4p' ${library}.${chunk}.rna-end.1.fq | rev | tr \"ATGC\" \"TACG\") \\\n          <(sed -n '3~4p' ${library}.${chunk}.rna-end.1.fq) \\\n          <(sed -n '4~4p' ${library}.${chunk}.rna-end.1.fq | rev) | tr \"\\\\t\" \"\\\\n\" \\\n          > ${library}.${chunk}.rna-end.1.revcomp.fq\n\n    paste <(sed -n '1~4p' ${library}.${chunk}.rna-end.2.fq) \\\n          <(sed -n '2~4p' ${library}.${chunk}.rna-end.2.fq | rev | tr \"ATGC\" \"TACG\") \\\n          <(sed -n '3~4p' ${library}.${chunk}.rna-end.2.fq) \\\n          <(sed -n '4~4p' ${library}.${chunk}.rna-end.2.fq | rev) | tr \"\\\\t\" \"\\\\n\" \\\n          > ${library}.${chunk}.rna-end.2.revcomp.fq\n\n    # Convert to binary file\n    fastq2bin ${library}.${chunk}.rna-end.1.revcomp.fq ${library}.${chunk}.rna-end.1.revcomp.bin\n    fastq2bin ${library}.${chunk}.rna-end.2.revcomp.fq ${library}.${chunk}.rna-end.2.revcomp.bin\n\n    # Align complementary regions to each library\n    align_pairwise ${library}.${chunk}.rna-end.2.revcomp.bin ${cindex_fq1} 1 ${read_length} \\\n                   ${seqlen_converted} ${rna_comp_length_converted} 0 \\\n                   ${read_length-rna_complementary_length} \\\n                   1 0 ${rna_complementary_length} > ${library}.${chunk}.1.rnacomp.txt\n\n    align_pairwise ${library}.${chunk}.rna-end.1.revcomp.bin ${cindex_fq2} 1 ${read_length} \\\n               ${seqlen_converted} ${rna_comp_length_converted} 0 \\\n               ${read_length-rna_complementary_length} \\\n               1 0 ${rna_complementary_length} > ${library}.${chunk}.2.rnacomp.txt\n\n    \"\"\"\n}",
        "nb_lignes_process": 62,
        "string_script": "    def rna_complementary_length = params.run.rna_complementary_length\n    def rna_comp_length_converted = ((rna_complementary_length+3).intdiv(8))+1\n    def extraN = \"N\"*(500+3*rna_complementary_length)\n    def seqlen_converted = lengths[ read_length ]\n    \"\"\"\n    # Get the complemetary regions\n    paste <(awk '{print \\$1, \\$3, \\$4}' ${table_fq}) \\\n          <(head -n -1 ${cout_br_for} | tail -n +2 | awk '{print \\$4}') \\\n          | awk 'BEGIN{OFS=\"\\\\n\";} {bgn=1; if (\\$4<500) bgn=\\$4+${br_length}+1; \\\n          print \\$1, substr(\\$2\"${extraN}\", bgn, ${rna_complementary_length}), \\\n          \"+\", substr(\\$3\"${extraN}\", bgn, ${rna_complementary_length})}' > ${library}.${chunk}.rna-end.1.fq\n\n    paste <(awk '{print \\$1, \\$5, \\$6}' ${table_fq}) \\\n          <(head -n -1 ${cout_ggg_rev} | tail -n +2 | awk '{print \\$5+1}') \\\n          | awk 'BEGIN{OFS=\"\\\\n\";} {bgn=1; if (\\$4<500) bgn=\\$4;\n          print \\$1, substr(\\$2\"${extraN}\", bgn, ${rna_complementary_length}), \\\n          \"+\", substr(\\$3\"${extraN}\", bgn, ${rna_complementary_length})}' > ${library}.${chunk}.rna-end.2.fq\n\n    # Convert to reverse complement\n    paste <(sed -n '1~4p' ${library}.${chunk}.rna-end.1.fq) \\\n          <(sed -n '2~4p' ${library}.${chunk}.rna-end.1.fq | rev | tr \"ATGC\" \"TACG\") \\\n          <(sed -n '3~4p' ${library}.${chunk}.rna-end.1.fq) \\\n          <(sed -n '4~4p' ${library}.${chunk}.rna-end.1.fq | rev) | tr \"\\\\t\" \"\\\\n\" \\\n          > ${library}.${chunk}.rna-end.1.revcomp.fq\n\n    paste <(sed -n '1~4p' ${library}.${chunk}.rna-end.2.fq) \\\n          <(sed -n '2~4p' ${library}.${chunk}.rna-end.2.fq | rev | tr \"ATGC\" \"TACG\") \\\n          <(sed -n '3~4p' ${library}.${chunk}.rna-end.2.fq) \\\n          <(sed -n '4~4p' ${library}.${chunk}.rna-end.2.fq | rev) | tr \"\\\\t\" \"\\\\n\" \\\n          > ${library}.${chunk}.rna-end.2.revcomp.fq\n\n    # Convert to binary file\n    fastq2bin ${library}.${chunk}.rna-end.1.revcomp.fq ${library}.${chunk}.rna-end.1.revcomp.bin\n    fastq2bin ${library}.${chunk}.rna-end.2.revcomp.fq ${library}.${chunk}.rna-end.2.revcomp.bin\n\n    # Align complementary regions to each library\n    align_pairwise ${library}.${chunk}.rna-end.2.revcomp.bin ${cindex_fq1} 1 ${read_length} \\\n                   ${seqlen_converted} ${rna_comp_length_converted} 0 \\\n                   ${read_length-rna_complementary_length} \\\n                   1 0 ${rna_complementary_length} > ${library}.${chunk}.1.rnacomp.txt\n\n    align_pairwise ${library}.${chunk}.rna-end.1.revcomp.bin ${cindex_fq2} 1 ${read_length} \\\n               ${seqlen_converted} ${rna_comp_length_converted} 0 \\\n               ${read_length-rna_complementary_length} \\\n               1 0 ${rna_complementary_length} > ${library}.${chunk}.2.rnacomp.txt\n\n    \"\"\"",
        "nb_lignes_script": 46,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_FOR_RNACOMP"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_COUT_RNACOMP"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk}\"",
            "storeDir getOutputDir('cout')"
        ],
        "when": "",
        "stub": ""
    },
    "GET_DNA_FRAGMENTS_CHUNKS": {
        "name_process": "GET_DNA_FRAGMENTS_CHUNKS",
        "string_process": "\nprocess GET_DNA_FRAGMENTS_CHUNKS{\n    tag \"library:${library} chunk:${chunk}\"\n\n    storeDir getOutputDir('filtered_fastq')\n\n    input:\n    set val(library), val(chunk), file(fastq_table),\n        file(cout_r1_br_for), file(cout_r1_for),\n        file(trim_table), val(read_length) from LIB_FOR_SUBSTR_DNA\n\n    output:\n    set library, chunk, \"${library}.${chunk}.dna_nonextended.fq\" into LIB_SUBSTR_DNA\n    set library, chunk, \"${library}.${chunk}.dna.fq\" optional true into LIB_SUBSTR_DNA_EXT\n    set library, chunk, \"${library}.${chunk}.dna.info.txt\" into LIB_SUBSTR_DNA_INFO\n\n    script:\n    def limit = params.run.min_substring_size\n\n    def extended_Cmd = \"\"\n    def qual_extension = \"\"\n    if (dna_extension.size()>0) {\n        qual_extension = \"~\"*dna_extension.size()\n        extended_Cmd = \"\"\"\n        paste <(awk '{print \\$1, \\$3, \\$4}' ${fastq_table}) \\\n              <(head -n -1 ${cout_r1_for} | tail -n +2 | awk '{print \\$5+1}') \\\n              <(awk '{print \\$5}' ${trim_table}) \\\n              <(head -n -1 ${cout_r1_br_for} | tail -n +2 | awk '{print \\$4}') \\\n              | awk 'BEGIN{OFS=\"\\\\n\";} {bgn=1; if (\\$4<500) bgn=\\$4; end=\\$5; if (\\$6<end) end=\\$6; \\\n              if (end-bgn+1>=${limit}) print \\$1, substr(\\$2, bgn, end-bgn+1)\"${dna_extension}\", \\\n              \"+\", substr(\\$3, bgn, end-bgn+1)\"${qual_extension}\"}' > ${library}.${chunk}.dna.fq\n        \"\"\"\n    }\n    \"\"\"\n    ${extended_Cmd}\n    paste <(awk '{print \\$1, \\$3, \\$4}' ${fastq_table}) \\\n          <(head -n -1 ${cout_r1_for} | tail -n +2 | awk '{print \\$5+1}') \\\n          <(awk '{print \\$5}' ${trim_table}) \\\n          <(head -n -1 ${cout_r1_br_for} | tail -n +2 | awk '{print \\$4}') \\\n          | awk 'BEGIN{OFS=\"\\\\n\";} {bgn=1; if (\\$4<500) bgn=\\$4; end=\\$5; if (\\$6<end) end=\\$6; \\\n          if (end-bgn+1>=${limit}) print \\$1, substr(\\$2, bgn, end-bgn+1), \\\n          \"+\", substr(\\$3, bgn, end-bgn+1)}' > ${library}.${chunk}.dna_nonextended.fq\n\n    # Write info about selected segments:\n    paste <(awk '{print \\$1, \\$3, \\$4}' ${fastq_table}) \\\n          <(head -n -1 ${cout_r1_for} | tail -n +2 | awk '{print \\$5+1}') \\\n          <(awk '{print \\$5}' ${trim_table}) \\\n          <(head -n -1 ${cout_r1_br_for} | tail -n +2 | awk '{print \\$4}') \\\n          | awk 'BEGIN{OFS=\"\\\\t\";} {bgn=1; if (\\$4<500) bgn=\\$4; \\\n          end=${read_length}; if (\\$6<end) end=\\$6; \\\n          print \\$1, bgn, end, end-bgn+1, \\$5, \\$5-bgn+1}' > ${library}.${chunk}.dna.info.txt\n    \"\"\"\n}",
        "nb_lignes_process": 51,
        "string_script": "    def limit = params.run.min_substring_size\n\n    def extended_Cmd = \"\"\n    def qual_extension = \"\"\n    if (dna_extension.size()>0) {\n        qual_extension = \"~\"*dna_extension.size()\n        extended_Cmd = \"\"\"\n        paste <(awk '{print \\$1, \\$3, \\$4}' ${fastq_table}) \\\n              <(head -n -1 ${cout_r1_for} | tail -n +2 | awk '{print \\$5+1}') \\\n              <(awk '{print \\$5}' ${trim_table}) \\\n              <(head -n -1 ${cout_r1_br_for} | tail -n +2 | awk '{print \\$4}') \\\n              | awk 'BEGIN{OFS=\"\\\\n\";} {bgn=1; if (\\$4<500) bgn=\\$4; end=\\$5; if (\\$6<end) end=\\$6; \\\n              if (end-bgn+1>=${limit}) print \\$1, substr(\\$2, bgn, end-bgn+1)\"${dna_extension}\", \\\n              \"+\", substr(\\$3, bgn, end-bgn+1)\"${qual_extension}\"}' > ${library}.${chunk}.dna.fq\n        \"\"\"\n    }\n    \"\"\"\n    ${extended_Cmd}\n    paste <(awk '{print \\$1, \\$3, \\$4}' ${fastq_table}) \\\n          <(head -n -1 ${cout_r1_for} | tail -n +2 | awk '{print \\$5+1}') \\\n          <(awk '{print \\$5}' ${trim_table}) \\\n          <(head -n -1 ${cout_r1_br_for} | tail -n +2 | awk '{print \\$4}') \\\n          | awk 'BEGIN{OFS=\"\\\\n\";} {bgn=1; if (\\$4<500) bgn=\\$4; end=\\$5; if (\\$6<end) end=\\$6; \\\n          if (end-bgn+1>=${limit}) print \\$1, substr(\\$2, bgn, end-bgn+1), \\\n          \"+\", substr(\\$3, bgn, end-bgn+1)}' > ${library}.${chunk}.dna_nonextended.fq\n\n    # Write info about selected segments:\n    paste <(awk '{print \\$1, \\$3, \\$4}' ${fastq_table}) \\\n          <(head -n -1 ${cout_r1_for} | tail -n +2 | awk '{print \\$5+1}') \\\n          <(awk '{print \\$5}' ${trim_table}) \\\n          <(head -n -1 ${cout_r1_br_for} | tail -n +2 | awk '{print \\$4}') \\\n          | awk 'BEGIN{OFS=\"\\\\t\";} {bgn=1; if (\\$4<500) bgn=\\$4; \\\n          end=${read_length}; if (\\$6<end) end=\\$6; \\\n          print \\$1, bgn, end, end-bgn+1, \\$5, \\$5-bgn+1}' > ${library}.${chunk}.dna.info.txt\n    \"\"\"",
        "nb_lignes_script": 34,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_FOR_SUBSTR_DNA"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_SUBSTR_DNA",
            "LIB_SUBSTR_DNA_EXT",
            "LIB_SUBSTR_DNA_INFO"
        ],
        "nb_outputs": 3,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk}\"",
            "storeDir getOutputDir('filtered_fastq')"
        ],
        "when": "",
        "stub": ""
    },
    "GET_RNA1_FRAGMENTS_CHUNKS": {
        "name_process": "GET_RNA1_FRAGMENTS_CHUNKS",
        "string_process": "\nprocess GET_RNA1_FRAGMENTS_CHUNKS{\n    tag \"library:${library} chunk:${chunk}\"\n\n    storeDir getOutputDir('filtered_fastq')\n\n    input:\n    set val(library), val(chunk), file(fastq_table),\n        file(cout_r1_br_for), file(cout_r1_rev),\n        file(cout_compl_1), file(trim_table), val(read_length) from LIB_FOR_SUBSTR_RNA1\n\n    output:\n    set library, chunk, \"${library}.${chunk}.rna1.fq\" into LIB_SUBSTR_RNA1\n    set library, chunk, \"${library}.${chunk}.rna1.info.txt\" into LIB_SUBSTR_RNA1_INFO\n\n    script:\n    def limit = params.run.min_substring_size\n\n    \"\"\"\n    paste <(awk '{print \\$1, \\$3, \\$4}' ${fastq_table}) <(awk '{print \\$5}' ${trim_table}) \\\n        <(head -n -1 ${cout_r1_br_for} | tail -n +2 | awk '{print \\$4}') \\\n        <(head -n -1 ${cout_r1_rev} | tail -n +2 | awk '{print \\$4}') \\\n        <(head -n -1 ${cout_compl_1} | tail -n +2 | awk '{print \\$5}') \\\n        | awk 'BEGIN{OFS=\"\\\\n\";} {bgn=\\$4; if (\\$5<500) bgn=\\$5+${br_length}+1; \\\n        end=\\$4; if (\\$6<end) end=\\$6; if (\\$7<end) end=\\$7; \\\n        if (end-bgn+1>=${limit}) print \\$1, substr(\\$2, bgn, end-bgn+1), \\\n        \"+\", substr(\\$3, bgn, end-bgn+1)}' > ${library}.${chunk}.rna1.fq\n\n    paste <(awk '{print \\$1, \\$3, \\$4}' ${fastq_table}) \\\n        <(awk '{print \\$5}' ${trim_table}) \\\n        <(head -n -1 ${cout_r1_br_for} | tail -n +2 | awk '{print \\$4}') \\\n        <(head -n -1 ${cout_r1_rev} | tail -n +2 | awk '{print \\$4}') \\\n        <(head -n -1 ${cout_compl_1} | tail -n +2 | awk '{print \\$5}') \\\n        | awk 'BEGIN{OFS=\"\\\\t\";} {bgn=1; if (\\$5<500) bgn=\\$5+${br_length}+1; \\\n        end=${read_length}; if (\\$6<end) end=\\$6; if (\\$7<end) end=\\$7; \\\n        print \\$1, bgn, end, end-bgn+1, \\$4, \\$4-bgn+1}' > ${library}.${chunk}.rna1.info.txt\n    \"\"\"\n}",
        "nb_lignes_process": 36,
        "string_script": "    def limit = params.run.min_substring_size\n\n    \"\"\"\n    paste <(awk '{print \\$1, \\$3, \\$4}' ${fastq_table}) <(awk '{print \\$5}' ${trim_table}) \\\n        <(head -n -1 ${cout_r1_br_for} | tail -n +2 | awk '{print \\$4}') \\\n        <(head -n -1 ${cout_r1_rev} | tail -n +2 | awk '{print \\$4}') \\\n        <(head -n -1 ${cout_compl_1} | tail -n +2 | awk '{print \\$5}') \\\n        | awk 'BEGIN{OFS=\"\\\\n\";} {bgn=\\$4; if (\\$5<500) bgn=\\$5+${br_length}+1; \\\n        end=\\$4; if (\\$6<end) end=\\$6; if (\\$7<end) end=\\$7; \\\n        if (end-bgn+1>=${limit}) print \\$1, substr(\\$2, bgn, end-bgn+1), \\\n        \"+\", substr(\\$3, bgn, end-bgn+1)}' > ${library}.${chunk}.rna1.fq\n\n    paste <(awk '{print \\$1, \\$3, \\$4}' ${fastq_table}) \\\n        <(awk '{print \\$5}' ${trim_table}) \\\n        <(head -n -1 ${cout_r1_br_for} | tail -n +2 | awk '{print \\$4}') \\\n        <(head -n -1 ${cout_r1_rev} | tail -n +2 | awk '{print \\$4}') \\\n        <(head -n -1 ${cout_compl_1} | tail -n +2 | awk '{print \\$5}') \\\n        | awk 'BEGIN{OFS=\"\\\\t\";} {bgn=1; if (\\$5<500) bgn=\\$5+${br_length}+1; \\\n        end=${read_length}; if (\\$6<end) end=\\$6; if (\\$7<end) end=\\$7; \\\n        print \\$1, bgn, end, end-bgn+1, \\$4, \\$4-bgn+1}' > ${library}.${chunk}.rna1.info.txt\n    \"\"\"",
        "nb_lignes_script": 20,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_FOR_SUBSTR_RNA1"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_SUBSTR_RNA1",
            "LIB_SUBSTR_RNA1_INFO"
        ],
        "nb_outputs": 2,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk}\"",
            "storeDir getOutputDir('filtered_fastq')"
        ],
        "when": "",
        "stub": ""
    },
    "GET_RNA2_FRAGMENTS_CHUNKS": {
        "name_process": "GET_RNA2_FRAGMENTS_CHUNKS",
        "string_process": "\nprocess GET_RNA2_FRAGMENTS_CHUNKS{\n    tag \"library:${library} chunk:${chunk}\"\n\n    storeDir getOutputDir('filtered_fastq')\n\n    input:\n    set val(library), val(chunk), file(fastq_table),\n        file(cout_r2_ggg), file(cout_r2_for), file(cout_r2_br_rev),\n        file(cout_compl_2), file(trim_table), val(read_length) from LIB_FOR_SUBSTR_RNA2\n\n    output:\n    set library, chunk, \"${library}.${chunk}.rna2.fq\" into LIB_SUBSTR_RNA2\n    set library, chunk, \"${library}.${chunk}.rna2.info.txt\" into LIB_SUBSTR_RNA2_INFO\n\n    script:\n    def limit = params.run.min_substring_size\n\n    \"\"\"\n    paste <(awk '{print \\$1, \\$5, \\$6}' ${fastq_table}) \\\n        <(head -n -1 ${cout_r2_ggg} | tail -n +2 | awk '{print \\$5+1}') \\\n        <(head -n -1 ${cout_r2_for} | tail -n +2 | awk '{print \\$5+1}') \\\n        <(awk '{print \\$7}' ${trim_table}) \\\n        <(head -n -1 ${cout_r2_br_rev} | tail -n +2 | awk '{print \\$4}') \\\n        <(head -n -1 ${cout_compl_2} | tail -n +2 | awk '{print \\$5}') \\\n        | awk 'BEGIN{OFS=\"\\\\n\";} {bgn=1; if (\\$4<500) bgn=\\$4; if (\\$5<500 && \\$5>bgn) bgn=\\$5; \\\n        end=\\$6; if (\\$7<end) end=\\$7; if (\\$8<end) end=\\$8; \\\n        if (end-bgn+1>=${limit}) print \\$1, substr(\\$2, bgn, end-bgn+1), \\\n        \"+\", substr(\\$3, bgn, end-bgn+1)}' > ${library}.${chunk}.rna2.fq\n\n    paste <(awk '{print \\$1, \\$5, \\$6}' ${fastq_table}) \\\n        <(head -n -1 ${cout_r2_ggg} | tail -n +2 | awk '{print \\$5+1}') \\\n        <(head -n -1 ${cout_r2_for} | tail -n +2 | awk '{print \\$5+1}') \\\n        <(awk '{print \\$7}' ${trim_table}) \\\n        <(head -n -1 ${cout_r2_br_rev} | tail -n +2 | awk '{print \\$4}') \\\n        <(head -n -1 ${cout_compl_2} | tail -n +2 | awk '{print \\$5}') \\\n        | awk 'BEGIN{OFS=\"\\\\t\";} {bgn=1; if (\\$4<500) bgn=\\$4; if (\\$5<500 && \\$5>bgn) bgn=\\$5; \\\n        end=${read_length}; if (\\$7<end) end=\\$7; if (\\$8<end) end=\\$8; \\\n        print \\$1, bgn, end, end-bgn+1, \\$6, \\$6-bgn+1}' > ${library}.${chunk}.rna2.info.txt\n    \"\"\"\n}",
        "nb_lignes_process": 39,
        "string_script": "    def limit = params.run.min_substring_size\n\n    \"\"\"\n    paste <(awk '{print \\$1, \\$5, \\$6}' ${fastq_table}) \\\n        <(head -n -1 ${cout_r2_ggg} | tail -n +2 | awk '{print \\$5+1}') \\\n        <(head -n -1 ${cout_r2_for} | tail -n +2 | awk '{print \\$5+1}') \\\n        <(awk '{print \\$7}' ${trim_table}) \\\n        <(head -n -1 ${cout_r2_br_rev} | tail -n +2 | awk '{print \\$4}') \\\n        <(head -n -1 ${cout_compl_2} | tail -n +2 | awk '{print \\$5}') \\\n        | awk 'BEGIN{OFS=\"\\\\n\";} {bgn=1; if (\\$4<500) bgn=\\$4; if (\\$5<500 && \\$5>bgn) bgn=\\$5; \\\n        end=\\$6; if (\\$7<end) end=\\$7; if (\\$8<end) end=\\$8; \\\n        if (end-bgn+1>=${limit}) print \\$1, substr(\\$2, bgn, end-bgn+1), \\\n        \"+\", substr(\\$3, bgn, end-bgn+1)}' > ${library}.${chunk}.rna2.fq\n\n    paste <(awk '{print \\$1, \\$5, \\$6}' ${fastq_table}) \\\n        <(head -n -1 ${cout_r2_ggg} | tail -n +2 | awk '{print \\$5+1}') \\\n        <(head -n -1 ${cout_r2_for} | tail -n +2 | awk '{print \\$5+1}') \\\n        <(awk '{print \\$7}' ${trim_table}) \\\n        <(head -n -1 ${cout_r2_br_rev} | tail -n +2 | awk '{print \\$4}') \\\n        <(head -n -1 ${cout_compl_2} | tail -n +2 | awk '{print \\$5}') \\\n        | awk 'BEGIN{OFS=\"\\\\t\";} {bgn=1; if (\\$4<500) bgn=\\$4; if (\\$5<500 && \\$5>bgn) bgn=\\$5; \\\n        end=${read_length}; if (\\$7<end) end=\\$7; if (\\$8<end) end=\\$8; \\\n        print \\$1, bgn, end, end-bgn+1, \\$6, \\$6-bgn+1}' > ${library}.${chunk}.rna2.info.txt\n    \"\"\"",
        "nb_lignes_script": 23,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_FOR_SUBSTR_RNA2"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_SUBSTR_RNA2",
            "LIB_SUBSTR_RNA2_INFO"
        ],
        "nb_outputs": 2,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk}\"",
            "storeDir getOutputDir('filtered_fastq')"
        ],
        "when": "",
        "stub": ""
    },
    "MAP_DNA_NONEXTENDED_CHUNKS": {
        "name_process": "MAP_DNA_NONEXTENDED_CHUNKS",
        "string_process": "\nprocess MAP_DNA_NONEXTENDED_CHUNKS{\n    tag \"library:${library} chunk:${chunk}\"\n\n    storeDir getOutputDir('sam')\n\n    input:\n    set val(library), val(chunk), file(input_dna), val(index_pref), file(genome_index) from LIB_FOR_DNA_MAPPING\n\n    output:\n    set library, chunk, \"${library}.${chunk}.dna_nonextended.sam\" into LIB_SAM_DNA\n\n    script:\n    \"\"\"\n    hisat2 -p ${task.cpus} -x ${index_pref} --no-spliced-alignment -k 100 \\\n           --no-softclip -U ${input_dna} > ${library}.${chunk}.dna_nonextended.sam\n    \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "    \"\"\"\n    hisat2 -p ${task.cpus} -x ${index_pref} --no-spliced-alignment -k 100 \\\n           --no-softclip -U ${input_dna} > ${library}.${chunk}.dna_nonextended.sam\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "HISAT2"
        ],
        "tools_url": [
            "https://bio.tools/hisat2"
        ],
        "tools_dico": [
            {
                "name": "HISAT2",
                "uri": "https://bio.tools/hisat2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Alignment program for mapping next-generation sequencing reads (both DNA and RNA) to a population of human genomes (as well as to a single reference genome).",
                "homepage": "https://ccb.jhu.edu/software/hisat2/index.shtml"
            }
        ],
        "inputs": [
            "LIB_FOR_DNA_MAPPING"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_SAM_DNA"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk}\"",
            "storeDir getOutputDir('sam')"
        ],
        "when": "",
        "stub": ""
    },
    "MAP_DNA_EXTENDED_CHUNKS": {
        "name_process": "MAP_DNA_EXTENDED_CHUNKS",
        "string_process": " process MAP_DNA_EXTENDED_CHUNKS{\n        tag \"library:${library} chunk:${chunk}\"\n\n        storeDir getOutputDir('sam')\n\n        input:\n        set val(library), val(chunk), file(input_dna), val(index_pref), file(genome_index) from LIB_FOR_DNA_MAPPING_EXT\n\n        output:\n        set library, chunk, \"${library}.${chunk}.dna.sam\" into LIB_SAM_DNA_EXT\n\n        script:\n        \"\"\"\n        hisat2 -p ${task.cpus} -x ${index_pref} --no-spliced-alignment -k 100 \\\n               --no-softclip -U ${input_dna} > ${library}.${chunk}.dna.sam\n        \"\"\"\n    }",
        "nb_lignes_process": 15,
        "string_script": "        \"\"\"\n        hisat2 -p ${task.cpus} -x ${index_pref} --no-spliced-alignment -k 100 \\\n               --no-softclip -U ${input_dna} > ${library}.${chunk}.dna.sam\n        \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "HISAT2"
        ],
        "tools_url": [
            "https://bio.tools/hisat2"
        ],
        "tools_dico": [
            {
                "name": "HISAT2",
                "uri": "https://bio.tools/hisat2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Alignment program for mapping next-generation sequencing reads (both DNA and RNA) to a population of human genomes (as well as to a single reference genome).",
                "homepage": "https://ccb.jhu.edu/software/hisat2/index.shtml"
            }
        ],
        "inputs": [
            "LIB_FOR_DNA_MAPPING_EXT"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_SAM_DNA_EXT"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk}\"",
            "storeDir getOutputDir('sam')"
        ],
        "when": "",
        "stub": ""
    },
    "MAP_RNA1_CHUNKS": {
        "name_process": "MAP_RNA1_CHUNKS",
        "string_process": "\nprocess MAP_RNA1_CHUNKS{\n    tag \"library:${library} chunk:${chunk}\"\n\n    storeDir getOutputDir('sam')\n\n    input:\n    set val(library), val(chunk), file(input_rna1), val(index_pref), file(genome_index), file(known_splicesites) from LIB_FOR_RNA1_MAPPING\n\n    output:\n    set library, chunk, \"${library}.${chunk}.rna1.sam\" into LIB_SAM_RNA1\n\n    script:\n    \"\"\"\n    hisat2 -p ${task.cpus} -x ${index_pref} -k 100 --no-softclip --known-splicesite-infile ${known_splicesites} \\\n        --dta-cufflinks --novel-splicesite-outfile ${library}.${chunk}.novel.splicesites.txt \\\n        -U ${input_rna1} > ${library}.${chunk}.rna1.sam\n    \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "    \"\"\"\n    hisat2 -p ${task.cpus} -x ${index_pref} -k 100 --no-softclip --known-splicesite-infile ${known_splicesites} \\\n        --dta-cufflinks --novel-splicesite-outfile ${library}.${chunk}.novel.splicesites.txt \\\n        -U ${input_rna1} > ${library}.${chunk}.rna1.sam\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "HISAT2"
        ],
        "tools_url": [
            "https://bio.tools/hisat2"
        ],
        "tools_dico": [
            {
                "name": "HISAT2",
                "uri": "https://bio.tools/hisat2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Alignment program for mapping next-generation sequencing reads (both DNA and RNA) to a population of human genomes (as well as to a single reference genome).",
                "homepage": "https://ccb.jhu.edu/software/hisat2/index.shtml"
            }
        ],
        "inputs": [
            "LIB_FOR_RNA1_MAPPING"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_SAM_RNA1"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk}\"",
            "storeDir getOutputDir('sam')"
        ],
        "when": "",
        "stub": ""
    },
    "MAP_RNA2_CHUNKS": {
        "name_process": "MAP_RNA2_CHUNKS",
        "string_process": "\nprocess MAP_RNA2_CHUNKS{\n    tag \"library:${library} chunk:${chunk}\"\n\n    storeDir getOutputDir('sam')\n\n    input:\n    set val(library), val(chunk), file(input_rna2), val(index_pref), file(genome_index), file(known_splicesites) from LIB_FOR_RNA2_MAPPING\n\n    output:\n    set library, chunk, \"${library}.${chunk}.rna2.sam\" into LIB_SAM_RNA2\n\n    script:\n    \"\"\"\n    hisat2 -p ${task.cpus} -x ${index_pref} -k 100 --no-softclip --known-splicesite-infile ${known_splicesites} \\\n        --dta-cufflinks --novel-splicesite-outfile ${library}.${chunk}.novel.splicesites.txt \\\n        -U ${input_rna2} > ${library}.${chunk}.rna2.sam\n    \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "    \"\"\"\n    hisat2 -p ${task.cpus} -x ${index_pref} -k 100 --no-softclip --known-splicesite-infile ${known_splicesites} \\\n        --dta-cufflinks --novel-splicesite-outfile ${library}.${chunk}.novel.splicesites.txt \\\n        -U ${input_rna2} > ${library}.${chunk}.rna2.sam\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "HISAT2"
        ],
        "tools_url": [
            "https://bio.tools/hisat2"
        ],
        "tools_dico": [
            {
                "name": "HISAT2",
                "uri": "https://bio.tools/hisat2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Alignment program for mapping next-generation sequencing reads (both DNA and RNA) to a population of human genomes (as well as to a single reference genome).",
                "homepage": "https://ccb.jhu.edu/software/hisat2/index.shtml"
            }
        ],
        "inputs": [
            "LIB_FOR_RNA2_MAPPING"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_SAM_RNA2"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk}\"",
            "storeDir getOutputDir('sam')"
        ],
        "when": "",
        "stub": ""
    },
    "SAM2BED_CHUNKS": {
        "name_process": "SAM2BED_CHUNKS",
        "string_process": "\nprocess SAM2BED_CHUNKS{\n    tag \"library:${library} chunk:${chunk} ${segment_name}\"\n\n    storeDir getOutputDir('bed')\n\n    input:\n    set val(library), val(chunk), val(segment_name), file(sam) from LIB_SAM2BED\n\n    output:\n    set library, chunk, segment_name, \"${library}.${chunk}.${segment_name}.bed\" into LIB_BED\n\n    script:\n                                      \n    \"\"\"\n    samtools view -Sh -F 4 ${sam} | grep -E 'XM:i:[0-2]\\\\s.*NH:i:1\\$|^@' | samtools view -Sbh - \\\n        | bedtools bamtobed -cigar -i stdin > ${library}.${chunk}.${segment_name}.bed\n    \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "    \"\"\"\n    samtools view -Sh -F 4 ${sam} | grep -E 'XM:i:[0-2]\\\\s.*NH:i:1\\$|^@' | samtools view -Sbh - \\\n        | bedtools bamtobed -cigar -i stdin > ${library}.${chunk}.${segment_name}.bed\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "SAMtools",
            "BEDTools"
        ],
        "tools_url": [
            "https://bio.tools/samtools",
            "https://bio.tools/bedtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            },
            {
                "name": "BEDTools",
                "uri": "https://bio.tools/bedtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Cartography"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "BEDTools is an extensive suite of utilities for comparing genomic features in BED format.",
                "homepage": "https://github.com/arq5x/bedtools2"
            }
        ],
        "inputs": [
            "LIB_SAM2BED"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_BED"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk} ${segment_name}\"",
            "storeDir getOutputDir('bed')"
        ],
        "when": "",
        "stub": ""
    },
    "ANNOTATE_RENZYMES_CHUNKS": {
        "name_process": "ANNOTATE_RENZYMES_CHUNKS",
        "string_process": " process ANNOTATE_RENZYMES_CHUNKS{\n        tag \"library:${library} ${chunk} ${segment_name} ${renz_key}${renz_strand}\"\n\n        storeDir getOutputDir('table')\n\n        input:\n        set library, chunk, segment_name, file(bed_file), renz_key, renz_strand, file(renz_file) from LIB_FOR_RESTR_RUN\n\n        output:\n        set library, chunk, segment_name, renz_key, renz_strand,\n            \"${library}.${chunk}.${segment_name}.${renz_key}${renz_strand}.distances.txt\" into LIB_DISTANCES\n\n        script:\n        def renz_strand_key = (renz_strand==\"+\") ? \"p\" : (renz_strand==\"-\") ? \"n\" : \"\"\n        def renz_strand_sub = (renz_strand==\"+\") ? \"+\" : (renz_strand==\"-\") ? \"-\" : \"+\"\n        def columns = [\n            \"${segment_name}_start_${renz_key}${renz_strand_key}_left\",\n            \"${segment_name}_start_${renz_key}${renz_strand_key}_right\",\n            \"${segment_name}_end_${renz_key}${renz_strand_key}_left\",\n            \"${segment_name}_end_${renz_key}${renz_strand_key}_right\"\n            ]\n        def header = ([\"id\"]+columns).join(\" \")\n        \"\"\"\n        echo \"${header}\" > ${library}.${chunk}.${segment_name}.${renz_key}${renz_strand}.distances.txt\n        get_closest_sites.py ${bed_file} ${renz_file} ${renz_strand_sub} ${library}.${chunk}.${segment_name}.${renz_key}${renz_strand}.distances.txt\n        \"\"\"\n    }",
        "nb_lignes_process": 25,
        "string_script": "        def renz_strand_key = (renz_strand==\"+\") ? \"p\" : (renz_strand==\"-\") ? \"n\" : \"\"\n        def renz_strand_sub = (renz_strand==\"+\") ? \"+\" : (renz_strand==\"-\") ? \"-\" : \"+\"\n        def columns = [\n            \"${segment_name}_start_${renz_key}${renz_strand_key}_left\",\n            \"${segment_name}_start_${renz_key}${renz_strand_key}_right\",\n            \"${segment_name}_end_${renz_key}${renz_strand_key}_left\",\n            \"${segment_name}_end_${renz_key}${renz_strand_key}_right\"\n            ]\n        def header = ([\"id\"]+columns).join(\" \")\n        \"\"\"\n        echo \"${header}\" > ${library}.${chunk}.${segment_name}.${renz_key}${renz_strand}.distances.txt\n        get_closest_sites.py ${bed_file} ${renz_file} ${renz_strand_sub} ${library}.${chunk}.${segment_name}.${renz_key}${renz_strand}.distances.txt\n        \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_FOR_RESTR_RUN"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_DISTANCES"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} ${chunk} ${segment_name} ${renz_key}${renz_strand}\"",
            "storeDir getOutputDir('table')"
        ],
        "when": "",
        "stub": ""
    },
    "COLLECT_DATA_CHUNKS": {
        "name_process": "COLLECT_DATA_CHUNKS",
        "string_process": "\nprocess COLLECT_DATA_CHUNKS{\n        tag \"library:${library} ${chunk}\"\n\n        storeDir getOutputDir('hdf5')\n\n        input:\n        set val(library), val(chunk), file(input) from LIB_COLLECT\n\n        output:\n        set library, chunk, \"${library}.${chunk}.data.hdf5\" into LIB_COLLECTED\n\n        script:\n        \"\"\"\n        collect_data.py ${library}.${chunk}.data.hdf5 ${input}\n        \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "        \"\"\"\n        collect_data.py ${library}.${chunk}.data.hdf5 ${input}\n        \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_COLLECT"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_COLLECTED"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} ${chunk}\"",
            "storeDir getOutputDir('hdf5')"
        ],
        "when": "",
        "stub": ""
    },
    "COLLECT_FILTERS_CHUNKS": {
        "name_process": "COLLECT_FILTERS_CHUNKS",
        "string_process": "\nprocess COLLECT_FILTERS_CHUNKS{\n        tag \"library:${library} ${chunk}\"\n\n        storeDir getOutputDir('hdf5')\n\n        input:\n        set val(library), val(chunk), file(input), file(genome_chromsizes), val(assembly) from LIB_COLLECTED_FOR_FILTERS\n\n        output:\n        set library, chunk, \"${library}.${chunk}.data.hdf5\", \"${library}.${chunk}.filters.hdf5\" into LIB_FILTERS\n\n        script:\n        def chrom_pattern = params.filters.canonical_chromosomes\n\n        \"\"\"\n        printf '${patterns}' > filters.txt\n        filter_data.py ${input} ${library}.${chunk}.filters.hdf5 \\\n            ${genome_chromsizes} \"${chrom_pattern}\" filters.txt\n        \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "        def chrom_pattern = params.filters.canonical_chromosomes\n\n        \"\"\"\n        printf '${patterns}' > filters.txt\n        filter_data.py ${input} ${library}.${chunk}.filters.hdf5 \\\n            ${genome_chromsizes} \"${chrom_pattern}\" filters.txt\n        \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_COLLECTED_FOR_FILTERS"
        ],
        "nb_inputs": 1,
        "outputs": [
            "LIB_FILTERS"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} ${chunk}\"",
            "storeDir getOutputDir('hdf5')"
        ],
        "when": "",
        "stub": ""
    },
    "WRITE_STATS_CHUNKS": {
        "name_process": "WRITE_STATS_CHUNKS",
        "string_process": "\nprocess WRITE_STATS_CHUNKS{\n        tag \"library:${library} chunk:${chunk}\"\n\n        storeDir getOutputDir('stats')\n\n        input:\n        set val(library), val(chunk), file(data_hdf5), file(filters_hdf5) from LIB_FILTERS_STATS\n\n        output:\n        set library, chunk, \"${library}.${chunk}.stats.txt\" into FILES_STATS\n\n        script:\n        def stats_str = '[\"'+ stats_list.join('\", \"')+'\"]'\n        \"\"\"\n        #!/usr/bin/env python\n\nimport h5py\nf = h5py.File(\"${filters_hdf5}\", 'r')\nwith open(\"${library}.${chunk}.stats.txt\", \"w\") as outfile:\n    for filt in ${stats_str}:\n        n = int(sum(f[filt][()]))\n        outfile.write(f\"{filt}\\\\t{n}\\\\n\")\nf.close()\n        \"\"\"\n}",
        "nb_lignes_process": 24,
        "string_script": "        def stats_str = '[\"'+ stats_list.join('\", \"')+'\"]'\n        \"\"\"\n        #!/usr/bin/env python\n\nimport h5py\nf = h5py.File(\"${filters_hdf5}\", 'r')\nwith open(\"${library}.${chunk}.stats.txt\", \"w\") as outfile:\n    for filt in ${stats_str}:\n        n = int(sum(f[filt][()]))\n        outfile.write(f\"{filt}\\\\t{n}\\\\n\")\nf.close()\n        \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_FILTERS_STATS"
        ],
        "nb_inputs": 1,
        "outputs": [
            "FILES_STATS"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk}\"",
            "storeDir getOutputDir('stats')"
        ],
        "when": "",
        "stub": ""
    },
    "MERGE_STATS": {
        "name_process": "MERGE_STATS",
        "string_process": "\nprocess MERGE_STATS{\n        tag \"library:${library}\"\n\n        storeDir getOutputDir('stats')\n\n        input:\n        set val(library), val(chunk), file(files_stats) from FILES_STATS_FOR_MERGE\n\n        output:\n        set library, \"${library}.stats.txt\" into FILES_STATS_MERGED\n\n        script:\n        def files_str = '[\"'+ files_stats.join('\", \"')+'\"]'\n        \"\"\"\n        #!/usr/bin/env python\n\nimport pandas as pd\nres = []\nfor f in ${files_str}:\n    tmp = pd.read_csv(f, sep='\\\\t', header=None).set_index(0)\n    if len(res)==0:\n        res = tmp.copy()\n    else:\n        res += tmp\nres.to_csv(\"${library}.stats.txt\", sep='\\t', header=False, index=True)\n        \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "        def files_str = '[\"'+ files_stats.join('\", \"')+'\"]'\n        \"\"\"\n        #!/usr/bin/env python\n\nimport pandas as pd\nres = []\nfor f in ${files_str}:\n    tmp = pd.read_csv(f, sep='\\\\t', header=None).set_index(0)\n    if len(res)==0:\n        res = tmp.copy()\n    else:\n        res += tmp\nres.to_csv(\"${library}.stats.txt\", sep='\\t', header=False, index=True)\n        \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "FILES_STATS_FOR_MERGE"
        ],
        "nb_inputs": 1,
        "outputs": [
            "FILES_STATS_MERGED"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library}\"",
            "storeDir getOutputDir('stats')"
        ],
        "when": "",
        "stub": ""
    },
    "WRITE_FINAL_TABLE_CHUNKS": {
        "name_process": "WRITE_FINAL_TABLE_CHUNKS",
        "string_process": " process WRITE_FINAL_TABLE_CHUNKS{\n            tag \"library:${library} chunk:${chunk} table:${table_name}\"\n\n            storeDir getOutputDir('final_table')\n\n            input:\n            set val(library), val(chunk), file(data_hdf5), file(filters_hdf5),\n                val(table_name), val(filter), val(header) from LIB_FOR_WRITING\n\n            output:\n            set library, chunk, table_name, \"${library}.${chunk}.${table_name}.tsv\" into FILES_TABLE\n\n            script:\n            def stats_str = '[\"'+ stats_list.join('\", \"')+'\"]'\n            \"\"\"\n            #!/usr/bin/env python\n\n    import numpy as np\n\n    import h5py\n    f_filt = h5py.File(\"${filters_hdf5}\", 'r')\n    f_data = h5py.File(\"${data_hdf5}\", 'r')\n\n    header = \"${header}\".split()\n    res = {}\n    for col in header+[\"${filter}\"]:\n        try:\n            res[col] = f_filt[col][()]\n        except Exception as e:\n            res[col] = f_data[col][()]\n    f_filt.close()\n    f_data.close()\n\n    def strand(x):\n        if x:\n            return \"+\"\n        return \"-\"\n\n    indexes = np.where(res[\"${filter}\"])[0]\n    with open(\"${library}.${chunk}.${table_name}.tsv\", \"w\") as outfile:\n        outfile.write( \"\\\\t\".join(header)+\"\\\\n\" )\n        for i in indexes:\n            line = [res[col][i] if isinstance(res[col][i], str) else\n                    strand(res[col][i]) if 'strand' in col else\n                    res[col][i].decode() if isinstance(res[col][i], np.bytes_) else\n                    str(res[col][i]) for col in header]\n            line = \"\\\\t\".join( line )+\"\\\\n\"\n            outfile.write( line )\n\n            \"\"\"\n    }",
        "nb_lignes_process": 49,
        "string_script": "            def stats_str = '[\"'+ stats_list.join('\", \"')+'\"]'\n            \"\"\"\n            #!/usr/bin/env python\n\n    import numpy as np\n\n    import h5py\n    f_filt = h5py.File(\"${filters_hdf5}\", 'r')\n    f_data = h5py.File(\"${data_hdf5}\", 'r')\n\n    header = \"${header}\".split()\n    res = {}\n    for col in header+[\"${filter}\"]:\n        try:\n            res[col] = f_filt[col][()]\n        except Exception as e:\n            res[col] = f_data[col][()]\n    f_filt.close()\n    f_data.close()\n\n    def strand(x):\n        if x:\n            return \"+\"\n        return \"-\"\n\n    indexes = np.where(res[\"${filter}\"])[0]\n    with open(\"${library}.${chunk}.${table_name}.tsv\", \"w\") as outfile:\n        outfile.write( \"\\\\t\".join(header)+\"\\\\n\" )\n        for i in indexes:\n            line = [res[col][i] if isinstance(res[col][i], str) else\n                    strand(res[col][i]) if 'strand' in col else\n                    res[col][i].decode() if isinstance(res[col][i], np.bytes_) else\n                    str(res[col][i]) for col in header]\n            line = \"\\\\t\".join( line )+\"\\\\n\"\n            outfile.write( line )\n\n            \"\"\"",
        "nb_lignes_script": 36,
        "language_script": "python",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "LIB_FOR_WRITING"
        ],
        "nb_inputs": 1,
        "outputs": [
            "FILES_TABLE"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} chunk:${chunk} table:${table_name}\"",
            "storeDir getOutputDir('final_table')"
        ],
        "when": "",
        "stub": ""
    },
    "MERGE_TABLE": {
        "name_process": "MERGE_TABLE",
        "string_process": " process MERGE_TABLE{\n            tag \"library:${library} table:${table_name}\"\n\n            storeDir getOutputDir('final_table')\n\n            input:\n            set val(library), val(chunk), val(table_name), file(files_stats) from FILES_TABLE_FOR_MERGE\n\n            output:\n            set library, table_name, \"${library}.${table_name}.tsv\" into FILES_TABLE_MERGED\n\n            script:\n            \"\"\"\n            head -n 1 \"${files_stats[0]}\" > ${library}.${table_name}.tsv\n            for FILE in ${files_stats}\n            do\n                tail -n +2 \\$FILE >> ${library}.${table_name}.tsv\n            done\n            \"\"\"\n    }",
        "nb_lignes_process": 18,
        "string_script": "            \"\"\"\n            head -n 1 \"${files_stats[0]}\" > ${library}.${table_name}.tsv\n            for FILE in ${files_stats}\n            do\n                tail -n +2 \\$FILE >> ${library}.${table_name}.tsv\n            done\n            \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "FILES_TABLE_FOR_MERGE"
        ],
        "nb_inputs": 1,
        "outputs": [
            "FILES_TABLE_MERGED"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library} table:${table_name}\"",
            "storeDir getOutputDir('final_table')"
        ],
        "when": "",
        "stub": ""
    },
    "WRITE_COOLER": {
        "name_process": "WRITE_COOLER",
        "string_process": " process WRITE_COOLER{\n            tag \"library:${library}\"\n\n            storeDir getOutputDir('cooler')\n\n            input:\n            set val(library),\n                val(table_name),\n                file(table),\n                file(chromsizes),\n                val(assembly) from TABLES_FOR_COOLER\n\n            output:\n            set library,\n                \"${assembly}.${resolution}.bins.txt\",\n                \"${library}.${params.output.cooler_properties.resolution}.cool\" into COOLERS\n\n            script:\n            resolution = params.output.cooler_properties.resolution\n            \"\"\"\n            cooler makebins ${chromsizes} 1000 > ${assembly}.${resolution}.bins.txt\n            cooler cload pairs --no-symmetric-upper \\\n              -c1 ${params.output.cooler_properties.c1} -c2 ${params.output.cooler_properties.c2} \\\n              -p1 ${params.output.cooler_properties.p1} -p2 ${params.output.cooler_properties.p2} \\\n              ${assembly}.${resolution}.bins.txt <(tail -n +2 $table) \"${library}.${resolution}.cool\"\n\n            \"\"\"\n    }",
        "nb_lignes_process": 26,
        "string_script": "            resolution = params.output.cooler_properties.resolution\n            \"\"\"\n            cooler makebins ${chromsizes} 1000 > ${assembly}.${resolution}.bins.txt\n            cooler cload pairs --no-symmetric-upper \\\n              -c1 ${params.output.cooler_properties.c1} -c2 ${params.output.cooler_properties.c2} \\\n              -p1 ${params.output.cooler_properties.p1} -p2 ${params.output.cooler_properties.p2} \\\n              ${assembly}.${resolution}.bins.txt <(tail -n +2 $table) \"${library}.${resolution}.cool\"\n\n            \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "TABLES_FOR_COOLER"
        ],
        "nb_inputs": 1,
        "outputs": [
            "COOLERS"
        ],
        "nb_outputs": 1,
        "name_workflow": "agalitsyna__RedClib",
        "directive": [
            "tag \"library:${library}\"",
            "storeDir getOutputDir('cooler')"
        ],
        "when": "",
        "stub": ""
    }
}