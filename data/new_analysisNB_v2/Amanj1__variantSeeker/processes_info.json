{
    "reformat_fastq_reads_PE": {
        "name_process": "reformat_fastq_reads_PE",
        "string_process": "\nprocess reformat_fastq_reads_PE{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/reformation\", mode:'link'\n\n  input:\n  set sample_id, reads from reformat_PE_in \n  \n  output:\n  set sample_id, \"${sample_id}_phase1_input_PE.fasta.gz\" into reformation_PE\n  \n  script:\n\"\"\" \nreformat.sh in1=${reads[0]} in2=${reads[1]} out=${sample_id}_phase1_input_PE.fasta addslash\nsleep 1s\ngzip ${sample_id}_phase1_input_PE.fasta\nsleep 1s\n\"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\" \nreformat.sh in1=${reads[0]} in2=${reads[1]} out=${sample_id}_phase1_input_PE.fasta addslash\nsleep 1s\ngzip ${sample_id}_phase1_input_PE.fasta\nsleep 1s\n\"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "reformat_PE_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "reformation_PE"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/reformation\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "reformat_fastq_reads_S": {
        "name_process": "reformat_fastq_reads_S",
        "string_process": "\nprocess reformat_fastq_reads_S{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/reformation\", mode:'link'\n\n  input:\n  set sample_id, reads from reformat_S_in \n  \n  output:\n  set sample_id, \"${sample_id}_phase1_input_S.fasta.gz\" into reformation_S\n  \n  script:\n\"\"\" \nreformat.sh in=${reads[2]} out=${sample_id}_phase1_input_S.fasta\nsleep 1s\ngzip ${sample_id}_phase1_input_S.fasta\nsleep 1s\n\"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\" \nreformat.sh in=${reads[2]} out=${sample_id}_phase1_input_S.fasta\nsleep 1s\ngzip ${sample_id}_phase1_input_S.fasta\nsleep 1s\n\"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "reformat_S_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "reformation_S"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/reformation\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "blastn_phase1_PE": {
        "name_process": "blastn_phase1_PE",
        "string_process": "\nprocess blastn_phase1_PE{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/blast_results\", mode:'link'\n\n  input:\n  set sample_id, reads from reformation_PE \n  \n  output:\n  set sample_id, \"${sample_id}_blastn_phase1_PE.out\" into blastn_phase1_PE\n  \n  script:\n\"\"\" \ncp ${reads} ./tmp.fasta.gz\nsleep 1s\ngunzip tmp.fasta.gz\nsleep 1s\nblastn -db ${params.small_db_nucl} -query tmp.fasta -num_threads ${task.cpus} -outfmt 6 > \"${sample_id}_blastn_phase1_PE.out\"\nsleep 1s\nrm tmp.fasta\n\"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\" \ncp ${reads} ./tmp.fasta.gz\nsleep 1s\ngunzip tmp.fasta.gz\nsleep 1s\nblastn -db ${params.small_db_nucl} -query tmp.fasta -num_threads ${task.cpus} -outfmt 6 > \"${sample_id}_blastn_phase1_PE.out\"\nsleep 1s\nrm tmp.fasta\n\"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "G-BLASTN"
        ],
        "tools_url": [
            "https://bio.tools/g-blastn"
        ],
        "tools_dico": [
            {
                "name": "G-BLASTN",
                "uri": "https://bio.tools/g-blastn",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acids"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0346",
                                    "term": "Sequence similarity search"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2976",
                                "term": "Protein sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0857",
                                "term": "Sequence search results"
                            }
                        ]
                    }
                ],
                "description": "GPU-accelerated nucleotide alignment tool based on the widely used NCBI-BLAST.",
                "homepage": "http://www.comp.hkbu.edu.hk/~chxw/software/G-BLASTN.html"
            }
        ],
        "inputs": [
            "reformation_PE"
        ],
        "nb_inputs": 1,
        "outputs": [
            "blastn_phase1_PE"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/blast_results\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "blastn_phase1_S": {
        "name_process": "blastn_phase1_S",
        "string_process": "\nprocess blastn_phase1_S{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/blast_results\", mode:'link'\n\n  input:\n  set sample_id, reads from reformation_S \n  \n  output:\n  set sample_id, \"${sample_id}_blastn_phase1_S.out\" into blastn_phase1_S\n  \n  script:\n\"\"\" \ncp ${reads} ./tmp.fasta.gz\nsleep 1s\ngunzip tmp.fasta.gz\nsleep 1s\nblastn -db ${params.small_db_nucl} -query tmp.fasta -num_threads ${task.cpus} -outfmt 6 > \"${sample_id}_blastn_phase1_S.out\"\nsleep 1s\nrm tmp.fasta\n\"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "\"\"\" \ncp ${reads} ./tmp.fasta.gz\nsleep 1s\ngunzip tmp.fasta.gz\nsleep 1s\nblastn -db ${params.small_db_nucl} -query tmp.fasta -num_threads ${task.cpus} -outfmt 6 > \"${sample_id}_blastn_phase1_S.out\"\nsleep 1s\nrm tmp.fasta\n\"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "G-BLASTN"
        ],
        "tools_url": [
            "https://bio.tools/g-blastn"
        ],
        "tools_dico": [
            {
                "name": "G-BLASTN",
                "uri": "https://bio.tools/g-blastn",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acids"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0346",
                                    "term": "Sequence similarity search"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2976",
                                "term": "Protein sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0857",
                                "term": "Sequence search results"
                            }
                        ]
                    }
                ],
                "description": "GPU-accelerated nucleotide alignment tool based on the widely used NCBI-BLAST.",
                "homepage": "http://www.comp.hkbu.edu.hk/~chxw/software/G-BLASTN.html"
            }
        ],
        "inputs": [
            "reformation_S"
        ],
        "nb_inputs": 1,
        "outputs": [
            "blastn_phase1_S"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/blast_results\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "extract_fastq_reads_for_phase1": {
        "name_process": "extract_fastq_reads_for_phase1",
        "string_process": "\nprocess extract_fastq_reads_for_phase1{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/phase1_fa_to_fq\", mode:'link'\n\n  input:\n  set sample_id, blastn_PE, blastn_S, reads from blastn_phase1_with_fq \n  \n  output:\n  set sample_id, \"${sample_id}_phase1_read_1.fq.gz\", \"${sample_id}_phase1_read_2.fq.gz\", \"${sample_id}_phase1_read_unpaired.fq.gz\" into phase1_fq_reads\n  script:\n\"\"\"\ncat ${blastn_PE} | awk '{ print \\$1 }' | awk '!seen[\\$0]++' > list_PE.txt\ncat ${blastn_S} | awk '{ print \\$1 }' | awk '!seen[\\$0]++' > list_S.txt\nseqtk subseq ${reads[0]} list_PE.txt | gzip > ${sample_id}_phase1_read_1.fq.gz\nseqtk subseq ${reads[1]} list_PE.txt | gzip > ${sample_id}_phase1_read_2.fq.gz\nseqtk subseq ${reads[2]} list_S.txt | gzip > ${sample_id}_phase1_read_unpaired.fq.gz\nsleep 10\nrm list_PE.txt list_S.txt\n\"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "\"\"\"\ncat ${blastn_PE} | awk '{ print \\$1 }' | awk '!seen[\\$0]++' > list_PE.txt\ncat ${blastn_S} | awk '{ print \\$1 }' | awk '!seen[\\$0]++' > list_S.txt\nseqtk subseq ${reads[0]} list_PE.txt | gzip > ${sample_id}_phase1_read_1.fq.gz\nseqtk subseq ${reads[1]} list_PE.txt | gzip > ${sample_id}_phase1_read_2.fq.gz\nseqtk subseq ${reads[2]} list_S.txt | gzip > ${sample_id}_phase1_read_unpaired.fq.gz\nsleep 10\nrm list_PE.txt list_S.txt\n\"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "seqtk"
        ],
        "tools_url": [
            "https://bio.tools/seqtk"
        ],
        "tools_dico": [
            {
                "name": "seqtk",
                "uri": "https://bio.tools/seqtk",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Biological databases"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Data management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Information systems"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Databases and information systems"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Data handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2121",
                                    "term": "Sequence file editing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Utility operation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File handling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "File processing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2409",
                                    "term": "Report handling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A tool for processing sequences in the FASTA or FASTQ format. It parses both FASTA and FASTQ files which can also be optionally compressed by gzip.",
                "homepage": "https://github.com/lh3/seqtk"
            }
        ],
        "inputs": [
            "blastn_phase1_with_fq"
        ],
        "nb_inputs": 1,
        "outputs": [
            "phase1_fq_reads"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/phase1_fa_to_fq\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "asm_metaviralspades": {
        "name_process": "asm_metaviralspades",
        "string_process": "\nprocess asm_metaviralspades{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/assembly\", mode:'link'\n\n  input:\n  set sample_id, read_1, read_2, read_un from asm_spades_in \n  \n  output:\n  set sample_id, \"${sample_id}_asm_metaspades.fasta\" into asm_spades_out\n  file \"spades_assembly\" optional true into asm_spades_dir\n  script:\n\"\"\" \n\\$assembler -1 ${read_1} -2 ${read_2} -s ${read_un} -t ${task.cpus} -m ${params.memory} -o spades_assembly\ncp ./spades_assembly/contigs.fasta ./${sample_id}_asm_metaspades.fasta\n\"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "\"\"\" \n\\$assembler -1 ${read_1} -2 ${read_2} -s ${read_un} -t ${task.cpus} -m ${params.memory} -o spades_assembly\ncp ./spades_assembly/contigs.fasta ./${sample_id}_asm_metaspades.fasta\n\"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "asm_spades_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "asm_spades_out",
            "asm_spades_dir"
        ],
        "nb_outputs": 2,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/assembly\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "asm_megahit": {
        "name_process": "asm_megahit",
        "string_process": "\nprocess asm_megahit{\n  tag { \"${sample_id}\" }\n  publishDir \"${params.publish_base_dir}/${sample_id}/assembly\", mode: 'link'\n\n  input:\n  set sample_id, read_1, read_2, read_un from asm_megahit_in\n\n  output:\n  set sample_id, \"${sample_id}_asm_megahit.fasta\" optional true into asm_megahit\n  file \"megahit_assembly\" optional true into asm_megahit_dir\n\n  script:\n  \"\"\"\n  megahit -t ${task.cpus} --presets meta-sensitive -1 ${read_1} -2 ${read_2} -r ${read_un} --cpu-only  -o megahit_assembly\n  if [ -s megahit_assembly/final.contigs.fa ]; then ln megahit_assembly/final.contigs.fa ${sample_id}_asm_megahit.fasta; fi\n  \"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "  \"\"\"\n  megahit -t ${task.cpus} --presets meta-sensitive -1 ${read_1} -2 ${read_2} -r ${read_un} --cpu-only  -o megahit_assembly\n  if [ -s megahit_assembly/final.contigs.fa ]; then ln megahit_assembly/final.contigs.fa ${sample_id}_asm_megahit.fasta; fi\n  \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "MEGAHIT"
        ],
        "tools_url": [
            "https://bio.tools/megahit"
        ],
        "tools_dico": [
            {
                "name": "MEGAHIT",
                "uri": "https://bio.tools/megahit",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0610",
                            "term": "Ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Single node assembler for large and complex metagenomics NGS reads, such as soil. It makes use of succinct de Bruijn graph to achieve low memory usage, whereas its goal is not to make memory usage as low as possible.",
                "homepage": "https://github.com/voutcn/megahit"
            }
        ],
        "inputs": [
            "asm_megahit_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "asm_megahit",
            "asm_megahit_dir"
        ],
        "nb_outputs": 2,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag { \"${sample_id}\" }",
            "publishDir \"${params.publish_base_dir}/${sample_id}/assembly\", mode: 'link'"
        ],
        "when": "",
        "stub": ""
    },
    "modify_asm_megahit": {
        "name_process": "modify_asm_megahit",
        "string_process": "\nprocess modify_asm_megahit{\n  tag { \"${sample_id}\" }\n  publishDir \"${params.publish_base_dir}/${sample_id}/assembly\", mode: 'link'\n\n  input:\n  set sample_id, asm from asm_megahit\n\n  output:\n  set sample_id, \"${sample_id}_asm_megahit_modified.fasta\" optional true into asm_megahit_mod_out\n\n  script:\n  \"\"\"\n  cat ${asm} | sed -e 's/\\\\s\\\\+/,/g' > \"${sample_id}_asm_megahit_modified.fasta\"\n  \"\"\"\n}",
        "nb_lignes_process": 14,
        "string_script": "  \"\"\"\n  cat ${asm} | sed -e 's/\\\\s\\\\+/,/g' > \"${sample_id}_asm_megahit_modified.fasta\"\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "asm_megahit"
        ],
        "nb_inputs": 1,
        "outputs": [
            "asm_megahit_mod_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag { \"${sample_id}\" }",
            "publishDir \"${params.publish_base_dir}/${sample_id}/assembly\", mode: 'link'"
        ],
        "when": "",
        "stub": ""
    },
    "map_reads_to_contigs": {
        "name_process": "map_reads_to_contigs",
        "string_process": "\nprocess map_reads_to_contigs{\n  tag { \"${sample_id}\" }\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/assembly/reads_mapped_to_contigs\", mode:'link'\n\n  input:\n  set sample_id, spades_contigs, megahit_contigs, read_PE_1, read_PE_2, read_single  from map_reads_to_contigs\n\n  output:\n  set sample_id, \"${sample_id}_metaspades_reads_to_contigs.sam.gz\", \"${sample_id}_megahit_reads_to_contigs.sam.gz\" into map_reads_to_contigs_out\n\n  script:\n  \"\"\"\n  bbwrap.sh ref=${spades_contigs} in=${read_PE_1},${read_single} in2=${read_PE_2},null out=${sample_id}_metaspades_reads_to_contigs.sam.gz usejni=t kfilter=22 subfilter=15 maxindel=80\n  sleep 5s\n  bbwrap.sh ref=${megahit_contigs} in=${read_PE_1},${read_single} in2=${read_PE_2},null out=${sample_id}_megahit_reads_to_contigs.sam.gz usejni=t kfilter=22 subfilter=15 maxindel=80\n  \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "  \"\"\"\n  bbwrap.sh ref=${spades_contigs} in=${read_PE_1},${read_single} in2=${read_PE_2},null out=${sample_id}_metaspades_reads_to_contigs.sam.gz usejni=t kfilter=22 subfilter=15 maxindel=80\n  sleep 5s\n  bbwrap.sh ref=${megahit_contigs} in=${read_PE_1},${read_single} in2=${read_PE_2},null out=${sample_id}_megahit_reads_to_contigs.sam.gz usejni=t kfilter=22 subfilter=15 maxindel=80\n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "map_reads_to_contigs"
        ],
        "nb_inputs": 1,
        "outputs": [
            "map_reads_to_contigs_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag { \"${sample_id}\" }",
            "publishDir \"${params.publish_base_dir}/${sample_id}/assembly/reads_mapped_to_contigs\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "asm_mapping_stats": {
        "name_process": "asm_mapping_stats",
        "string_process": "\nprocess asm_mapping_stats{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/assembly/reads_mapped_to_contigs\", mode:'link'\n\n  input:\n  set sample_id, spades_mapping, megahit_mapping from asm_mapping_stats_in\n\n  output:\n  set sample_id, \"${sample_id}_metaspades_flagstat.txt\", \"${sample_id}_megahit_flagstat.txt\" into asm_mapping_stats_out\n\n  script:\n  \"\"\"\n  samtools flagstat ${spades_mapping} > ${sample_id}_metaspades_flagstat.txt\n  sleep 5s\n  samtools flagstat ${megahit_mapping} > ${sample_id}_megahit_flagstat.txt\n  \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "  \"\"\"\n  samtools flagstat ${spades_mapping} > ${sample_id}_metaspades_flagstat.txt\n  sleep 5s\n  samtools flagstat ${megahit_mapping} > ${sample_id}_megahit_flagstat.txt\n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "asm_mapping_stats_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "asm_mapping_stats_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/assembly/reads_mapped_to_contigs\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "asm_per_ctg_coverage": {
        "name_process": "asm_per_ctg_coverage",
        "string_process": "\nprocess asm_per_ctg_coverage{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/assembly/reads_mapped_to_contigs\", mode:'link'\n\n  input:\n  set sample_id, spades_mapping, megahit_mapping from asm_per_ctg_coverage_in\n\n  output:\n  set sample_id, \"${sample_id}_metaspades_reads_to_contigs.cov.txt\", \"${sample_id}_megahit_reads_to_contigs.cov.txt\" into asm_per_ctg_coverage_out\n\n  script:\n  \"\"\"\n  pileup.sh in=${spades_mapping} out=${sample_id}_metaspades_reads_to_contigs.cov.txt\n  sleep 5s\n  pileup.sh in=${megahit_mapping} out=${sample_id}_megahit_reads_to_contigs.cov.txt\n  \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "  \"\"\"\n  pileup.sh in=${spades_mapping} out=${sample_id}_metaspades_reads_to_contigs.cov.txt\n  sleep 5s\n  pileup.sh in=${megahit_mapping} out=${sample_id}_megahit_reads_to_contigs.cov.txt\n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "asm_per_ctg_coverage_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "asm_per_ctg_coverage_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/assembly/reads_mapped_to_contigs\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "asm_bedtools_parse_reads_mapped_to_contigs": {
        "name_process": "asm_bedtools_parse_reads_mapped_to_contigs",
        "string_process": "\nprocess asm_bedtools_parse_reads_mapped_to_contigs{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/assembly/reads_mapped_to_contigs\", mode:'link'\n\n  input:\n  set sample_id, spades_mapping, megahit_mapping from parse_sam_mapping\n\n  output:\n  set sample_id, \"${sample_id}_bedtools_metaspades_reads_mapped_to_contigs.txt.gz\", \"${sample_id}_bedtools_megahit_reads_mapped_to_contigs.txt.gz\" into parse_sam_mapping_out\n\n  script:\n  \"\"\"\n  bedtools bamtobed -i ${spades_mapping} | gzip > ${sample_id}_bedtools_metaspades_reads_mapped_to_contigs.txt.gz\n  sleep 5s\n  bedtools bamtobed -i ${megahit_mapping} | gzip > ${sample_id}_bedtools_megahit_reads_mapped_to_contigs.txt.gz\n  \"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "  \"\"\"\n  bedtools bamtobed -i ${spades_mapping} | gzip > ${sample_id}_bedtools_metaspades_reads_mapped_to_contigs.txt.gz\n  sleep 5s\n  bedtools bamtobed -i ${megahit_mapping} | gzip > ${sample_id}_bedtools_megahit_reads_mapped_to_contigs.txt.gz\n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "BEDTools"
        ],
        "tools_url": [
            "https://bio.tools/bedtools"
        ],
        "tools_dico": [
            {
                "name": "BEDTools",
                "uri": "https://bio.tools/bedtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2429",
                                    "term": "Cartography"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "BEDTools is an extensive suite of utilities for comparing genomic features in BED format.",
                "homepage": "https://github.com/arq5x/bedtools2"
            }
        ],
        "inputs": [
            "parse_sam_mapping"
        ],
        "nb_inputs": 1,
        "outputs": [
            "parse_sam_mapping_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/assembly/reads_mapped_to_contigs\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "blastn_phase2_metaviralspades": {
        "name_process": "blastn_phase2_metaviralspades",
        "string_process": "\nprocess blastn_phase2_metaviralspades{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/blast_results\", mode:'link'\n\n  input:\n  set sample_id, reads from asm_spades \n  \n  output:\n  set sample_id, \"${sample_id}_blastn_phase2_spades.out\" into blastn_phase2_spades\n  \n  script:\n\"\"\" \nblastn -db ${params.large_db_nucl} -query ${reads} -num_threads ${task.cpus} -outfmt 6 > \"${sample_id}_blastn_phase2_spades.out\"\n\"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "\"\"\" \nblastn -db ${params.large_db_nucl} -query ${reads} -num_threads ${task.cpus} -outfmt 6 > \"${sample_id}_blastn_phase2_spades.out\"\n\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "G-BLASTN"
        ],
        "tools_url": [
            "https://bio.tools/g-blastn"
        ],
        "tools_dico": [
            {
                "name": "G-BLASTN",
                "uri": "https://bio.tools/g-blastn",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acids"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0346",
                                    "term": "Sequence similarity search"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2976",
                                "term": "Protein sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0857",
                                "term": "Sequence search results"
                            }
                        ]
                    }
                ],
                "description": "GPU-accelerated nucleotide alignment tool based on the widely used NCBI-BLAST.",
                "homepage": "http://www.comp.hkbu.edu.hk/~chxw/software/G-BLASTN.html"
            }
        ],
        "inputs": [
            "asm_spades"
        ],
        "nb_inputs": 1,
        "outputs": [
            "blastn_phase2_spades"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/blast_results\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "blastn_phase2_megahit": {
        "name_process": "blastn_phase2_megahit",
        "string_process": "\nprocess blastn_phase2_megahit{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/blast_results\", mode:'link'\n\n  input:\n  set sample_id, reads from asm_megahit_mod \n  \n  output:\n  set sample_id, \"${sample_id}_blastn_phase2_megahit.out\" into blastn_phase2_megahit\n  \n  script:\n\"\"\" \nblastn -db ${params.large_db_nucl} -query ${reads} -num_threads ${task.cpus} -outfmt 6 > \"${sample_id}_blastn_phase2_megahit.out\"\n\"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "\"\"\" \nblastn -db ${params.large_db_nucl} -query ${reads} -num_threads ${task.cpus} -outfmt 6 > \"${sample_id}_blastn_phase2_megahit.out\"\n\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "G-BLASTN"
        ],
        "tools_url": [
            "https://bio.tools/g-blastn"
        ],
        "tools_dico": [
            {
                "name": "G-BLASTN",
                "uri": "https://bio.tools/g-blastn",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acids"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0346",
                                    "term": "Sequence similarity search"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2976",
                                "term": "Protein sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0857",
                                "term": "Sequence search results"
                            }
                        ]
                    }
                ],
                "description": "GPU-accelerated nucleotide alignment tool based on the widely used NCBI-BLAST.",
                "homepage": "http://www.comp.hkbu.edu.hk/~chxw/software/G-BLASTN.html"
            }
        ],
        "inputs": [
            "asm_megahit_mod"
        ],
        "nb_inputs": 1,
        "outputs": [
            "blastn_phase2_megahit"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/blast_results\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "process_phase2_results": {
        "name_process": "process_phase2_results",
        "string_process": "\nprocess process_phase2_results{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/blast_results\", mode:'link'\n\n  input:\n  set sample_id, spades_res, megahit_res from blastn_phase2_results \n  \n  output:\n  set sample_id, \"${sample_id}_blastn_phase2_spades_datamash_filt.txt\", \"${sample_id}_blastn_phase2_megahit_datamash_filt.txt\", \"${sample_id}_phase2_accession_num_list.txt\" into phase2_processed_results\n  \n  script:\n\"\"\" \ncat ${spades_res} | tr '.' ',' | datamash -g 1 min 11 -f | awk '{print \\$1 \"\\t\" \\$2 \"\\t\" \\$3 \"\\t\" \\$4 \"\\t\" \\$5 \"\\t\" \\$6 \"\\t\" \\$7 \"\\t\" \\$8 \"\\t\" \\$9 \"\\t\" \\$10 \"\\t\" \\$11 \"\\t\" \\$12}' | tr ',' '.' > ${sample_id}_blastn_phase2_spades_datamash_filt.txt\n\ncat ${megahit_res} | tr '.' ',' | datamash -g 1 min 11 -f | awk '{print \\$1 \"\\t\" \\$2 \"\\t\" \\$3 \"\\t\" \\$4 \"\\t\" \\$5 \"\\t\" \\$6 \"\\t\" \\$7 \"\\t\" \\$8 \"\\t\" \\$9 \"\\t\" \\$10 \"\\t\" \\$11 \"\\t\" \\$12}' | tr ',' '.' > ${sample_id}_blastn_phase2_megahit_datamash_filt.txt\n\ncat ${sample_id}_blastn_phase2_megahit_datamash_filt.txt ${sample_id}_blastn_phase2_spades_datamash_filt.txt | awk '{print \\$2}' | awk '!seen[\\$0]++' > ${sample_id}_phase2_accession_num_list.txt\n\n\"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "\"\"\" \ncat ${spades_res} | tr '.' ',' | datamash -g 1 min 11 -f | awk '{print \\$1 \"\\t\" \\$2 \"\\t\" \\$3 \"\\t\" \\$4 \"\\t\" \\$5 \"\\t\" \\$6 \"\\t\" \\$7 \"\\t\" \\$8 \"\\t\" \\$9 \"\\t\" \\$10 \"\\t\" \\$11 \"\\t\" \\$12}' | tr ',' '.' > ${sample_id}_blastn_phase2_spades_datamash_filt.txt\n\ncat ${megahit_res} | tr '.' ',' | datamash -g 1 min 11 -f | awk '{print \\$1 \"\\t\" \\$2 \"\\t\" \\$3 \"\\t\" \\$4 \"\\t\" \\$5 \"\\t\" \\$6 \"\\t\" \\$7 \"\\t\" \\$8 \"\\t\" \\$9 \"\\t\" \\$10 \"\\t\" \\$11 \"\\t\" \\$12}' | tr ',' '.' > ${sample_id}_blastn_phase2_megahit_datamash_filt.txt\n\ncat ${sample_id}_blastn_phase2_megahit_datamash_filt.txt ${sample_id}_blastn_phase2_spades_datamash_filt.txt | awk '{print \\$2}' | awk '!seen[\\$0]++' > ${sample_id}_phase2_accession_num_list.txt\n\n\"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "blastn_phase2_results"
        ],
        "nb_inputs": 1,
        "outputs": [
            "phase2_processed_results"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/blast_results\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "fetch_fasta_headers_local_DB": {
        "name_process": "fetch_fasta_headers_local_DB",
        "string_process": "\nprocess fetch_fasta_headers_local_DB{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/taxonomy/local_DB\", mode:'link'\n\n  input:\n  set sample_id, spades_filt, megahit_filt, acc_list from fetch_names_local_DB\n \n  output:\n  set sample_id, \"${sample_id}_organism_names_local_DB.tsv\", \"${sample_id}_acc_nr_not_found.txt\", \"${sample_id}_acc_nr_exist_for_esearch.txt\" into local_DB_organism_name_fetched\n  script:\n\"\"\"\ncount=1\necho \"Accession_nr\tTitle\tOrganism\" > tmp.tsv\necho \"Accession_numbers_not_found_in_nr_db\" > acc_nr_not_found.txt\necho \"Existing acession number for esearch\" > acc_nr_exist_for_esearch.txt\nfor line in \\$(cat ${acc_list})\ndo\n\ttitle_blast=\\$(blastdbcmd -entry \\$line -db ${params.large_db_nucl} -range 1-1)\n\ttitle=\\$(echo \\$title_blast|awk -F \\$line '{print \\$2}'| awk -F '>' '{print \\$1}')\n\tif [ -z \"\\$title\" ]\n\tthen\n\t\tif [ -z \"\\$title_blast\" ]\n\t\tthen\n\t\t\techo \"\\$line\" >> acc_nr_not_found.txt\n\t\telse\n\t\t\techo \"\\$line\" >> acc_nr_exist_for_esearch.txt\n\t\tfi\n\telse\n\t\torganism=\\$(echo \"\\$title\"|awk -F '[' '{print \\$2}'|awk -F ']' '{print \\$1}')\n\t\torganism=\\${organism// /_}\n\t\ttitle=\\${title// /_}\n\t\ttitle=\\${title//:1-1_/}\n\t\techo \\$line\t\\$title\t\\$organism >>tmp.tsv\n\tfi\ndone\ntr ' ' '\\t' <tmp.tsv >\"${sample_id}_organism_names_local_DB.tsv\"\nrm tmp.tsv\nmv acc_nr_not_found.txt ${sample_id}_acc_nr_not_found.txt\nmv acc_nr_exist_for_esearch.txt ${sample_id}_acc_nr_exist_for_esearch.txt\n\n\"\"\"\n}",
        "nb_lignes_process": 42,
        "string_script": "\"\"\"\ncount=1\necho \"Accession_nr\tTitle\tOrganism\" > tmp.tsv\necho \"Accession_numbers_not_found_in_nr_db\" > acc_nr_not_found.txt\necho \"Existing acession number for esearch\" > acc_nr_exist_for_esearch.txt\nfor line in \\$(cat ${acc_list})\ndo\n\ttitle_blast=\\$(blastdbcmd -entry \\$line -db ${params.large_db_nucl} -range 1-1)\n\ttitle=\\$(echo \\$title_blast|awk -F \\$line '{print \\$2}'| awk -F '>' '{print \\$1}')\n\tif [ -z \"\\$title\" ]\n\tthen\n\t\tif [ -z \"\\$title_blast\" ]\n\t\tthen\n\t\t\techo \"\\$line\" >> acc_nr_not_found.txt\n\t\telse\n\t\t\techo \"\\$line\" >> acc_nr_exist_for_esearch.txt\n\t\tfi\n\telse\n\t\torganism=\\$(echo \"\\$title\"|awk -F '[' '{print \\$2}'|awk -F ']' '{print \\$1}')\n\t\torganism=\\${organism// /_}\n\t\ttitle=\\${title// /_}\n\t\ttitle=\\${title//:1-1_/}\n\t\techo \\$line\t\\$title\t\\$organism >>tmp.tsv\n\tfi\ndone\ntr ' ' '\\t' <tmp.tsv >\"${sample_id}_organism_names_local_DB.tsv\"\nrm tmp.tsv\nmv acc_nr_not_found.txt ${sample_id}_acc_nr_not_found.txt\nmv acc_nr_exist_for_esearch.txt ${sample_id}_acc_nr_exist_for_esearch.txt\n\n\"\"\"",
        "nb_lignes_script": 30,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fetch_names_local_DB"
        ],
        "nb_inputs": 1,
        "outputs": [
            "local_DB_organism_name_fetched"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/taxonomy/local_DB\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "fetch_taxonomy_esearch": {
        "name_process": "fetch_taxonomy_esearch",
        "string_process": "\nprocess fetch_taxonomy_esearch{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/taxonomy/esearch\", mode:'link'\n\n  input:\n  set sample_id, spades_filt, megahit_filt, acc_list from fetch_names_esearch\n \n  output:\n  set sample_id, \"${sample_id}_phase2_esearch_taxonomy.tsv\" into esearch_taxonomy_fetched\n  script:\n\"\"\"\nAPI_KEY=\"${params.api_key}\"\ntouch test.tsv\necho \"Accession_nr\tTitle\tOrganism\tDivision\tRank\" > header.txt\nfor line in \\$(cat ${acc_list})\ndo\n\tesearch  -db nucleotide -query \\$line -api_key=\\$API_KEY|esummary > temp.txt\n\tsleep 1s\n\ttitle=\\$(cat temp.txt | grep Title| awk -F '</Title>' '{print \\$1}' | awk -F '<Title>' '{print \\$2}')\n\torganism=\\$(cat temp.txt | grep Organism| awk -F '</Organism>' '{print \\$1}'|awk -F '<Organism>' '{print \\$2}')\n\tesearch  -db taxonomy -query \"\\$organism\" -api_key=\\$API_KEY|esummary > temp.txt\n\tsleep 1s\n\trank=\\$(cat temp.txt | grep Rank| awk -F '</Rank>' '{print \\$1}' | awk -F '<Rank>' '{print \\$2}')\n\tdiv=\\$(cat temp.txt | grep Division| awk -F '</Division>' '{print \\$1}'|awk -F '<Division>' '{print \\$2}')\n\torganism=\\${organism// /_}\n\ttitle=\\${title// /_}\n\trank=\\${rank// /_}\n\tdiv=\\${div// /_}\n\techo -e \\$line\"\\t\"\\$title\"\\t\"\\$organism\"\\t\"\\$div\"\\t\"\\$rank >> test.tsv\n\tsleep 3s   \ndone\ncat header.txt test.tsv > \"${sample_id}_phase2_esearch_taxonomy.tsv\"\nrm temp.txt test.tsv header.txt\n\n\"\"\"\n}",
        "nb_lignes_process": 36,
        "string_script": "\"\"\"\nAPI_KEY=\"${params.api_key}\"\ntouch test.tsv\necho \"Accession_nr\tTitle\tOrganism\tDivision\tRank\" > header.txt\nfor line in \\$(cat ${acc_list})\ndo\n\tesearch  -db nucleotide -query \\$line -api_key=\\$API_KEY|esummary > temp.txt\n\tsleep 1s\n\ttitle=\\$(cat temp.txt | grep Title| awk -F '</Title>' '{print \\$1}' | awk -F '<Title>' '{print \\$2}')\n\torganism=\\$(cat temp.txt | grep Organism| awk -F '</Organism>' '{print \\$1}'|awk -F '<Organism>' '{print \\$2}')\n\tesearch  -db taxonomy -query \"\\$organism\" -api_key=\\$API_KEY|esummary > temp.txt\n\tsleep 1s\n\trank=\\$(cat temp.txt | grep Rank| awk -F '</Rank>' '{print \\$1}' | awk -F '<Rank>' '{print \\$2}')\n\tdiv=\\$(cat temp.txt | grep Division| awk -F '</Division>' '{print \\$1}'|awk -F '<Division>' '{print \\$2}')\n\torganism=\\${organism// /_}\n\ttitle=\\${title// /_}\n\trank=\\${rank// /_}\n\tdiv=\\${div// /_}\n\techo -e \\$line\"\\t\"\\$title\"\\t\"\\$organism\"\\t\"\\$div\"\\t\"\\$rank >> test.tsv\n\tsleep 3s   \ndone\ncat header.txt test.tsv > \"${sample_id}_phase2_esearch_taxonomy.tsv\"\nrm temp.txt test.tsv header.txt\n\n\"\"\"",
        "nb_lignes_script": 24,
        "language_script": "bash",
        "tools": [
            "QResearch"
        ],
        "tools_url": [
            "https://bio.tools/QResearch"
        ],
        "tools_dico": [
            {
                "name": "QResearch",
                "uri": "https://bio.tools/QResearch",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatric medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3335",
                            "term": "Cardiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "https://en.wikipedia.org/wiki/Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3399",
                            "term": "Geriatrics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3335",
                            "term": "Cardiovascular medicine"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3283",
                                    "term": "Anonymisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Imputation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3283",
                                    "term": "Data anonymisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Data imputation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "General practice database for research.",
                "homepage": "http://www.qresearch.org"
            }
        ],
        "inputs": [
            "fetch_names_esearch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "esearch_taxonomy_fetched"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/taxonomy/esearch\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "preprocess_classification_data": {
        "name_process": "preprocess_classification_data",
        "string_process": "\nprocess preprocess_classification_data{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/taxonomy/classification\", mode:'link'\n\n  input:\n  set sample_id, taxonomy, spades, megahit, acc_list from classification_data\n \n  output:\n  set sample_id, \"${sample_id}_both_assemblers_blastn_results_sorted_by_acc_num.txt\", \"${sample_id}_phase2_esearch_taxonomy_sorted.tsv\" into preprocessed_classification\n  script:\n\"\"\"\ncat ${spades} ${megahit} > tmp.txt\ncat tmp.txt | sort -k2 > \"${sample_id}_both_assemblers_blastn_results_sorted_by_acc_num.txt\"\ncat ${taxonomy} | sort -k1 > \"${sample_id}_phase2_esearch_taxonomy_sorted.tsv\"\nrm tmp.txt\n\"\"\"\n}",
        "nb_lignes_process": 17,
        "string_script": "\"\"\"\ncat ${spades} ${megahit} > tmp.txt\ncat tmp.txt | sort -k2 > \"${sample_id}_both_assemblers_blastn_results_sorted_by_acc_num.txt\"\ncat ${taxonomy} | sort -k1 > \"${sample_id}_phase2_esearch_taxonomy_sorted.tsv\"\nrm tmp.txt\n\"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "classification_data"
        ],
        "nb_inputs": 1,
        "outputs": [
            "preprocessed_classification"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/taxonomy/classification\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "classified_asm_results": {
        "name_process": "classified_asm_results",
        "string_process": "\nprocess classified_asm_results{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/tables/tsv_tables\", mode:'link'\n\n  input:\n  set sample_id, asm_res, tax_list from preprocessed_classification\n \n  output:\n  set sample_id, \"${sample_id}_all_asm_results.tsv\", \"${sample_id}_viral_asm_results.tsv\" into classified_asm\n  script:\n\"\"\"\n#!/usr/bin/python3\nf1 = open(\"${tax_list}\",\"r+\")\ntaxonomy = f1.readlines()\nf2 = open(\"${asm_res}\",\"r+\")\nasm_results = f2.readlines()\nf1.close()\nf2.close()\nheader = \"sample_id\\\\tseq_id\\\\tref_id\\\\ttitle\\\\tscientific_name\\\\trank\\\\tdivison\\\\tperc_of_identical_matches\\\\tevalue\\\\tmismatches\\\\talignment_len\\\\tseq_len\\\\tassembler\\\\n\"\nf1 = open(\"${sample_id}_all_asm_results.tsv\",\"a+\")\nf2 = open(\"${sample_id}_viral_asm_results.tsv\",\"a+\")\nf1.write(header)\nf2.write(header)\nc = ['-'] * 13 #container\nfor tax in taxonomy:\n    for asm_r in asm_results:\n        tmp = asm_r.split()\n        tmp2 = tax.split()\n        if str(tmp2[0]) == str(tmp[1]):\n            c[0] = \"${sample_id}\"\n            if \"node\" in tmp[0].lower(): #adding sequence id to first column\n                c[1] = tmp[0].split(\"_len\")[0]\n                # sequence len\n                c[11] = tmp[0].split(\"_length_\")[1].split(\"_cov\")[0]\n                # assembler\n                c[12] = \"metaviralspades\"\n            else:\n                c[1] = tmp[0].split(\".flag\")[0]\n                # sequence len\n                c[11] = tmp[0].split(\".len=\")[1]\n                # assembler\n                c[12] = \"megahit\"\n           # adding reference id to 2nd column\n            c[2] = tmp[1]\n            # title\n            c[3] = tmp2[1]\n            # scientific name\n            c[4] = tmp2[2]      \n            if len(tmp2) == 5:\n                # rank\n                c[5] = tmp2[4]\n                # divison\n                c[6] = tmp2[3] \n            # identical\n            c[7] = tmp[2]\n            c[8] = tmp[10]\n            # mismatches\n            c[9] = tmp[4] \n            # alignment length\n            c[10] = tmp[3]\n            str_tax = str(c[0])+\"\\\\t\"+str(c[1])+\"\\\\t\"+str(c[2])+\"\\\\t\"+str(c[3])+\"\\\\t\"+str(c[4])+\"\\\\t\"+str(c[5])+\"\\\\t\"+str(c[6])+\"\\\\t\"+str(c[7])+\"\\\\t\"+str(c[8])+\"\\\\t\"+str(c[9])+\"\\\\t\"+str(c[10])+\"\\\\t\"+str(c[11])+\"\\\\t\"+str(c[12])+\"\\\\n\"\n            f1.write(str_tax)\n            str_cont=\"\"\n            str_cont = str_cont.join(c[:-1])\n            if \"virus\" in str_cont.lower() or \"viral\" in str_cont.lower() or \"phage\" in str_cont.lower():\n                f2.write(str_tax)\n            c = ['-'] * 13\nf1.close()\nf2.close()\n\"\"\"\n}",
        "nb_lignes_process": 71,
        "string_script": "\"\"\"\n#!/usr/bin/python3\nf1 = open(\"${tax_list}\",\"r+\")\ntaxonomy = f1.readlines()\nf2 = open(\"${asm_res}\",\"r+\")\nasm_results = f2.readlines()\nf1.close()\nf2.close()\nheader = \"sample_id\\\\tseq_id\\\\tref_id\\\\ttitle\\\\tscientific_name\\\\trank\\\\tdivison\\\\tperc_of_identical_matches\\\\tevalue\\\\tmismatches\\\\talignment_len\\\\tseq_len\\\\tassembler\\\\n\"\nf1 = open(\"${sample_id}_all_asm_results.tsv\",\"a+\")\nf2 = open(\"${sample_id}_viral_asm_results.tsv\",\"a+\")\nf1.write(header)\nf2.write(header)\nc = ['-'] * 13 #container\nfor tax in taxonomy:\n    for asm_r in asm_results:\n        tmp = asm_r.split()\n        tmp2 = tax.split()\n        if str(tmp2[0]) == str(tmp[1]):\n            c[0] = \"${sample_id}\"\n            if \"node\" in tmp[0].lower(): #adding sequence id to first column\n                c[1] = tmp[0].split(\"_len\")[0]\n                # sequence len\n                c[11] = tmp[0].split(\"_length_\")[1].split(\"_cov\")[0]\n                # assembler\n                c[12] = \"metaviralspades\"\n            else:\n                c[1] = tmp[0].split(\".flag\")[0]\n                # sequence len\n                c[11] = tmp[0].split(\".len=\")[1]\n                # assembler\n                c[12] = \"megahit\"\n           # adding reference id to 2nd column\n            c[2] = tmp[1]\n            # title\n            c[3] = tmp2[1]\n            # scientific name\n            c[4] = tmp2[2]      \n            if len(tmp2) == 5:\n                # rank\n                c[5] = tmp2[4]\n                # divison\n                c[6] = tmp2[3] \n            # identical\n            c[7] = tmp[2]\n            c[8] = tmp[10]\n            # mismatches\n            c[9] = tmp[4] \n            # alignment length\n            c[10] = tmp[3]\n            str_tax = str(c[0])+\"\\\\t\"+str(c[1])+\"\\\\t\"+str(c[2])+\"\\\\t\"+str(c[3])+\"\\\\t\"+str(c[4])+\"\\\\t\"+str(c[5])+\"\\\\t\"+str(c[6])+\"\\\\t\"+str(c[7])+\"\\\\t\"+str(c[8])+\"\\\\t\"+str(c[9])+\"\\\\t\"+str(c[10])+\"\\\\t\"+str(c[11])+\"\\\\t\"+str(c[12])+\"\\\\n\"\n            f1.write(str_tax)\n            str_cont=\"\"\n            str_cont = str_cont.join(c[:-1])\n            if \"virus\" in str_cont.lower() or \"viral\" in str_cont.lower() or \"phage\" in str_cont.lower():\n                f2.write(str_tax)\n            c = ['-'] * 13\nf1.close()\nf2.close()\n\"\"\"",
        "nb_lignes_script": 59,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "preprocessed_classification"
        ],
        "nb_inputs": 1,
        "outputs": [
            "classified_asm"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/tables/tsv_tables\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "TSV_to_HTML_each_sample": {
        "name_process": "TSV_to_HTML_each_sample",
        "string_process": "\nprocess TSV_to_HTML_each_sample{\n  tag {\"${sample_id}\"}\n\n  publishDir \"${params.publish_base_dir}/${sample_id}/tables/html_tables\", mode:'link'\n\n  input:\n  set sample_id, all, viral, hvalue, html from html_data_each\n \n  output:\n  set sample_id, \"${sample_id}_all_asm_results.html\", \"${sample_id}_viral_asm_results.html\" into html_each_out\n  script:\n\"\"\"\n#!/usr/bin/python3\nimport string\nimport csv\nimport json\nimport collections\n\nOrderedDict = collections.OrderedDict\n\ndef HTML_table(src, header, htmlEndStr, name):\n    \n    f = open(src)\n    lines = f.readlines()\n    f.close()\n    f1 = open(header)\n    header_lines = f1.readlines()\n    f2 = open(htmlEndStr)\n    end_lines = f2.readlines()\n    f1.close()\n    f2.close()\n    \n    with open(name, 'w') as f:\n        for h_line in header_lines:\n            f.write(h_line)\n        for line in lines:\n            f.write(line)\n        for e_line in end_lines:\n            f.write(e_line)\n    f.close()\n    return None\n\ndef TSV_file_into_JSON(src, dst, header):\n    data = []\n    with open(src, 'r') as csvfile:\n        reader = csv.reader(csvfile, delimiter='\\\\t', quotechar='\"')\n        for row in reader:\n            if row[0] == 'sample_id':\n                print(\"\\\\n\")\n            else:\n                if row[0].strip()[0] == '#':  #\n                    continue\n                row = filter(None, row)\n                data.append(OrderedDict(zip(header, row)))\n\n    with open(dst, 'w') as jsonfile:\n        json.dump(data, jsonfile, indent=2)\n    return None\n\nheader = ['sample_id', 'seq_id', 'ref_id', 'title', 'scientific_name', 'rank', 'divison', 'perc_of_identical_matches', 'evalue', 'mismatches', 'alignment_len', 'seq_len', 'assembler']\nsrc = \"${all}\"\ndst = \"all.json\"  \nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${sample_id}_all_asm_results.html\")\n\nsrc = \"${viral}\"\ndst = \"viral.json\"  \nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${sample_id}_viral_asm_results.html\")\n\n\"\"\"\n}",
        "nb_lignes_process": 71,
        "string_script": "\"\"\"\n#!/usr/bin/python3\nimport string\nimport csv\nimport json\nimport collections\n\nOrderedDict = collections.OrderedDict\n\ndef HTML_table(src, header, htmlEndStr, name):\n    \n    f = open(src)\n    lines = f.readlines()\n    f.close()\n    f1 = open(header)\n    header_lines = f1.readlines()\n    f2 = open(htmlEndStr)\n    end_lines = f2.readlines()\n    f1.close()\n    f2.close()\n    \n    with open(name, 'w') as f:\n        for h_line in header_lines:\n            f.write(h_line)\n        for line in lines:\n            f.write(line)\n        for e_line in end_lines:\n            f.write(e_line)\n    f.close()\n    return None\n\ndef TSV_file_into_JSON(src, dst, header):\n    data = []\n    with open(src, 'r') as csvfile:\n        reader = csv.reader(csvfile, delimiter='\\\\t', quotechar='\"')\n        for row in reader:\n            if row[0] == 'sample_id':\n                print(\"\\\\n\")\n            else:\n                if row[0].strip()[0] == '#':  #\n                    continue\n                row = filter(None, row)\n                data.append(OrderedDict(zip(header, row)))\n\n    with open(dst, 'w') as jsonfile:\n        json.dump(data, jsonfile, indent=2)\n    return None\n\nheader = ['sample_id', 'seq_id', 'ref_id', 'title', 'scientific_name', 'rank', 'divison', 'perc_of_identical_matches', 'evalue', 'mismatches', 'alignment_len', 'seq_len', 'assembler']\nsrc = \"${all}\"\ndst = \"all.json\"  \nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${sample_id}_all_asm_results.html\")\n\nsrc = \"${viral}\"\ndst = \"viral.json\"  \nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${sample_id}_viral_asm_results.html\")\n\n\"\"\"",
        "nb_lignes_script": 59,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "html_data_each"
        ],
        "nb_inputs": 1,
        "outputs": [
            "html_each_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"${sample_id}\"}",
            "publishDir \"${params.publish_base_dir}/${sample_id}/tables/html_tables\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "collect_all_tsv_tables": {
        "name_process": "collect_all_tsv_tables",
        "string_process": "\nprocess collect_all_tsv_tables{\n  tag {\"All\"}\n\n  publishDir \"${params.publish_base_dir}/All/tables/tsv_tables\", mode:'link'\n\n  input:\n  file \"tsv_table\" from combine_all_tsv.map{it[1]}.collect()\n \n  output:\n  file \"${params.project_id}_all_asm_results.tsv\" into all_tsv_collected\n  script:\n\"\"\"\ntouch tmp.tsv\necho -e \"sample_id\\\\tseq_id\\\\tref_id\\\\ttitle\\\\tscientific_name\\\\trank\\\\tdivison\\\\tperc_of_identical_matches\\\\tevalue\\\\tmismatches\\\\talignment_len\\\\tseq_len\\\\tassembler\" > header.tsv\n\nfor sample_file in ${tsv_table}\ndo\n\tcat \\$sample_file | sed '1d' >> tmp.tsv\ndone\n\ncat tmp.tsv | sort -k1,1 -k3,3 > tmp2.tsv\ncat header.tsv tmp2.tsv > \"${params.project_id}_all_asm_results.tsv\"\nrm tmp.tsv tmp2.tsv header.tsv\n\"\"\"\n}",
        "nb_lignes_process": 24,
        "string_script": "\"\"\"\ntouch tmp.tsv\necho -e \"sample_id\\\\tseq_id\\\\tref_id\\\\ttitle\\\\tscientific_name\\\\trank\\\\tdivison\\\\tperc_of_identical_matches\\\\tevalue\\\\tmismatches\\\\talignment_len\\\\tseq_len\\\\tassembler\" > header.tsv\n\nfor sample_file in ${tsv_table}\ndo\n\tcat \\$sample_file | sed '1d' >> tmp.tsv\ndone\n\ncat tmp.tsv | sort -k1,1 -k3,3 > tmp2.tsv\ncat header.tsv tmp2.tsv > \"${params.project_id}_all_asm_results.tsv\"\nrm tmp.tsv tmp2.tsv header.tsv\n\"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "combine_all_tsv"
        ],
        "nb_inputs": 1,
        "outputs": [
            "all_tsv_collected"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"All\"}",
            "publishDir \"${params.publish_base_dir}/All/tables/tsv_tables\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "collect_viral_tsv_tables": {
        "name_process": "collect_viral_tsv_tables",
        "string_process": "\nprocess collect_viral_tsv_tables{\n  tag {\"All\"}\n\n  publishDir \"${params.publish_base_dir}/All/tables/tsv_tables\", mode:'link'\n\n  input:\n  file \"tsv_table\" from combine_viral_tsv.map{it[2]}.collect()\n \n  output:\n  file \"${params.project_id}_viral_asm_results.tsv\" into viral_tsv_collected\n  script:\n\"\"\"\n#!/bin/bash\ntouch tmp.tsv\necho -e \"sample_id\\\\tseq_id\\\\tref_id\\\\ttitle\\\\tscientific_name\\\\trank\\\\tdivison\\\\tperc_of_identical_matches\\\\tevalue\\\\tmismatches\\\\talignment_len\\\\tseq_len\\\\tassembler\" > header.tsv\n\nfor sample_file in ${tsv_table}\ndo\n\tcat \\$sample_file | sed '1d' >> tmp.tsv\ndone\n\ncat tmp.tsv | sort -k1,1 -k3,3 > tmp2.tsv\ncat header.tsv tmp2.tsv > \"${params.project_id}_viral_asm_results.tsv\"\nrm tmp.tsv tmp2.tsv header.tsv\n\"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "\"\"\"\n#!/bin/bash\ntouch tmp.tsv\necho -e \"sample_id\\\\tseq_id\\\\tref_id\\\\ttitle\\\\tscientific_name\\\\trank\\\\tdivison\\\\tperc_of_identical_matches\\\\tevalue\\\\tmismatches\\\\talignment_len\\\\tseq_len\\\\tassembler\" > header.tsv\n\nfor sample_file in ${tsv_table}\ndo\n\tcat \\$sample_file | sed '1d' >> tmp.tsv\ndone\n\ncat tmp.tsv | sort -k1,1 -k3,3 > tmp2.tsv\ncat header.tsv tmp2.tsv > \"${params.project_id}_viral_asm_results.tsv\"\nrm tmp.tsv tmp2.tsv header.tsv\n\"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "combine_viral_tsv"
        ],
        "nb_inputs": 1,
        "outputs": [
            "viral_tsv_collected"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"All\"}",
            "publishDir \"${params.publish_base_dir}/All/tables/tsv_tables\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    },
    "TSV_to_HTML_all_collected": {
        "name_process": "TSV_to_HTML_all_collected",
        "string_process": "\nprocess TSV_to_HTML_all_collected{\n  tag {\"All\"}\n\n  publishDir \"${params.publish_base_dir}/All/tables/html_tables\", mode:'link'\n\n  input:\n  set \"all\", \"viral\", \"hvalue\", \"html\" from tsv_collection_into_html\n \n  output:\n  set \"${params.project_id}_all_asm_results.html\", \"${params.project_id}_viral_asm_results.html\" into html_all_out\n  script:\n\"\"\"\n#!/usr/bin/python3\nimport string\nimport csv\nimport json\nimport collections\n\nOrderedDict = collections.OrderedDict\n\ndef HTML_table(src, header, htmlEndStr, name):\n    \n    f = open(src)\n    lines = f.readlines()\n    f.close()\n    f1 = open(header)\n    header_lines = f1.readlines()\n    f2 = open(htmlEndStr)\n    end_lines = f2.readlines()\n    f1.close()\n    f2.close()\n    \n    with open(name, 'w') as f:\n        for h_line in header_lines:\n            f.write(h_line)\n        for line in lines:\n            f.write(line)\n        for e_line in end_lines:\n            f.write(e_line)\n    f.close()\n    return None\n\ndef TSV_file_into_JSON(src, dst, header):\n    data = []\n    with open(src, 'r') as csvfile:\n        reader = csv.reader(csvfile, delimiter='\\\\t', quotechar='\"')\n        for row in reader:\n            if row[0] == 'sample_id':\n                print(\"\\\\n\")\n            else:\n                if row[0].strip()[0] == '#':  #\n                    continue\n                row = filter(None, row)\n                data.append(OrderedDict(zip(header, row)))\n\n    with open(dst, 'w') as jsonfile:\n        json.dump(data, jsonfile, indent=2)\n    return None\n\nheader = ['sample_id', 'seq_id', 'ref_id', 'title', 'scientific_name', 'rank', 'divison', 'perc_of_identical_matches', 'evalue', 'mismatches', 'alignment_len', 'seq_len', 'assembler']\nsrc = \"${all}\"\ndst = \"all.json\"  \nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_all_asm_results.html\")\n\nsrc = \"${viral}\"\ndst = \"viral.json\"  \nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_viral_asm_results.html\")\n\n\"\"\"\n}",
        "nb_lignes_process": 71,
        "string_script": "\"\"\"\n#!/usr/bin/python3\nimport string\nimport csv\nimport json\nimport collections\n\nOrderedDict = collections.OrderedDict\n\ndef HTML_table(src, header, htmlEndStr, name):\n    \n    f = open(src)\n    lines = f.readlines()\n    f.close()\n    f1 = open(header)\n    header_lines = f1.readlines()\n    f2 = open(htmlEndStr)\n    end_lines = f2.readlines()\n    f1.close()\n    f2.close()\n    \n    with open(name, 'w') as f:\n        for h_line in header_lines:\n            f.write(h_line)\n        for line in lines:\n            f.write(line)\n        for e_line in end_lines:\n            f.write(e_line)\n    f.close()\n    return None\n\ndef TSV_file_into_JSON(src, dst, header):\n    data = []\n    with open(src, 'r') as csvfile:\n        reader = csv.reader(csvfile, delimiter='\\\\t', quotechar='\"')\n        for row in reader:\n            if row[0] == 'sample_id':\n                print(\"\\\\n\")\n            else:\n                if row[0].strip()[0] == '#':  #\n                    continue\n                row = filter(None, row)\n                data.append(OrderedDict(zip(header, row)))\n\n    with open(dst, 'w') as jsonfile:\n        json.dump(data, jsonfile, indent=2)\n    return None\n\nheader = ['sample_id', 'seq_id', 'ref_id', 'title', 'scientific_name', 'rank', 'divison', 'perc_of_identical_matches', 'evalue', 'mismatches', 'alignment_len', 'seq_len', 'assembler']\nsrc = \"${all}\"\ndst = \"all.json\"  \nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_all_asm_results.html\")\n\nsrc = \"${viral}\"\ndst = \"viral.json\"  \nTSV_file_into_JSON(src, dst, header)\nHTML_table(dst, \"${html[1]}\", \"${html[0]}\", \"${params.project_id}_viral_asm_results.html\")\n\n\"\"\"",
        "nb_lignes_script": 59,
        "language_script": "python3",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "tsv_collection_into_html"
        ],
        "nb_inputs": 1,
        "outputs": [
            "html_all_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "Amanj1__variantSeeker",
        "directive": [
            "tag {\"All\"}",
            "publishDir \"${params.publish_base_dir}/All/tables/html_tables\", mode:'link'"
        ],
        "when": "",
        "stub": ""
    }
}