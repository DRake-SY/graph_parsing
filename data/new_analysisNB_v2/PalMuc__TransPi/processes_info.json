{
    "fasqc": {
        "name_process": "fasqc",
        "string_process": " process fasqc {\n\n                label 'low_cpus'\n\n                tag \"${sample_id}\"\n\n                publishDir \"${params.outdir}/fastqc\", mode: \"copy\", overwrite: true\n                publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n                conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::fastqc=0.11.9=0\" : null)\n                if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\" : \"quay.io/biocontainers/fastqc:0.11.9--0\")\n                }\n\n                input:\n                    tuple sample_id, file(reads) from reads_qc_ch\n\n                output:\n                    tuple sample_id, file(\"*_fastqc.{zip,html}\") into fastqc_results\n                    file(\"fastqc.version.txt\") into fastqc_version\n\n                script:\n                    \"\"\"\n                    fastqc --quiet --threads $task.cpus $reads\n                    v=\\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n                    echo \"FastQC: \\$v\" >fastqc.version.txt\n                    \"\"\"\n            }",
        "nb_lignes_process": 26,
        "string_script": "                    \"\"\"\n                    fastqc --quiet --threads $task.cpus $reads\n                    v=\\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n                    echo \"FastQC: \\$v\" >fastqc.version.txt\n                    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [
            "FastQC"
        ],
        "tools_url": [
            "https://bio.tools/fastqc"
        ],
        "tools_dico": [
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            }
        ],
        "inputs": [
            "reads_qc_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fastqc_results",
            "fastqc_version"
        ],
        "nb_outputs": 2,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'low_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/fastqc\", mode: \"copy\", overwrite: true",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::fastqc=0.11.9=0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\" : \"quay.io/biocontainers/fastqc:0.11.9--0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "fastp": {
        "name_process": "fastp",
        "string_process": " process fastp {\n\n                label 'med_cpus'\n\n                tag \"${sample_id}\"\n\n                publishDir \"${params.outdir}/filter\", mode: \"copy\", overwrite: true, pattern: \"*.fastp.{json,html}\"\n                publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n                conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"bioconda::fastp=0.20.1=h8b12597_0\" : null)\n                if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/fastp:0.20.1--h8b12597_0\" : \"quay.io/biocontainers/fastp:0.20.1--h8b12597_0\")\n                }\n\n                input:\n                    tuple sample_id, file(reads) from reads_ch\n\n                output:\n                    tuple sample_id, file(\"*.fastp.{json,html}\") into fastp_results\n                    tuple sample_id, file(\"${sample_id}.fastp.json\") into fastp_stats_ch\n                    tuple sample_id, file(\"*${sample_id}.filter.fq\") into reads_rna_ch\n                    tuple sample_id, file(\"left-${sample_id}.filter.fq\"), file(\"right-${sample_id}.filter.fq\") into save_filter_reads\n                    file(\"fastp.version.txt\") into fastp_version\n\n                script:\n                    \"\"\"\n                    fastp -i ${reads[0]} -I ${reads[1]} -o left-${sample_id}.filter.fq -O right-${sample_id}.filter.fq --detect_adapter_for_pe \\\n                    --average_qual ${params.minQual} --overrepresentation_analysis --html ${sample_id}.fastp.html --json ${sample_id}.fastp.json \\\n                    --thread ${task.cpus} --report_title ${sample_id}\n\n                    v=\\$( fastp --version 2>&1 | awk '{print \\$2}' )\n                    echo \"fastp: \\$v\" >fastp.version.txt\n                    \"\"\"\n            }",
        "nb_lignes_process": 32,
        "string_script": "                    \"\"\"\n                    fastp -i ${reads[0]} -I ${reads[1]} -o left-${sample_id}.filter.fq -O right-${sample_id}.filter.fq --detect_adapter_for_pe \\\n                    --average_qual ${params.minQual} --overrepresentation_analysis --html ${sample_id}.fastp.html --json ${sample_id}.fastp.json \\\n                    --thread ${task.cpus} --report_title ${sample_id}\n\n                    v=\\$( fastp --version 2>&1 | awk '{print \\$2}' )\n                    echo \"fastp: \\$v\" >fastp.version.txt\n                    \"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [
            "fastPHASE"
        ],
        "tools_url": [
            "https://bio.tools/fastphase"
        ],
        "tools_dico": [
            {
                "name": "fastPHASE",
                "uri": "https://bio.tools/fastphase",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3056",
                            "term": "Population genetics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3454",
                                    "term": "Phasing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Imputation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Data imputation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "fastPHASE is a program to estimate missing genotypes and unobserved haplotypes. It is an implementation of the model described in Scheet & Stephens (2006). This is a cluster-based model for haplotype variation, and gains its utility from implicitly modeling the genealogy of chromosomes in a random sample from a population as a tree but summarizing all haplotype variation in the \"tips\" of the trees.",
                "homepage": "http://scheet.org/software.html"
            }
        ],
        "inputs": [
            "reads_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fastp_results",
            "fastp_stats_ch",
            "reads_rna_ch",
            "save_filter_reads",
            "fastp_version"
        ],
        "nb_outputs": 5,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/filter\", mode: \"copy\", overwrite: true, pattern: \"*.fastp.{json,html}\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"bioconda::fastp=0.20.1=h8b12597_0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/fastp:0.20.1--h8b12597_0\" : \"quay.io/biocontainers/fastp:0.20.1--h8b12597_0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "fastp_stats": {
        "name_process": "fastp_stats",
        "string_process": " process fastp_stats {\n\n                label 'low_cpus'\n\n                tag \"${sample_id}\"\n\n                publishDir \"${params.outdir}/filter\", mode: \"copy\", overwrite: true, pattern: \"*.fastp.{json,html}\"\n\n                conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"conda-forge::jq=1.6=h14c3975_1000 \" : null)\n                if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/jq:1.6\" : \"quay.io/biocontainers/jq:1.6\")\n                }\n\n                input:\n                    tuple sample_id, file(json) from fastp_stats_ch\n\n                output:\n                    tuple sample_id, file(\"*.csv\") into fastp_csv\n\n                script:\n                    \"\"\"\n                    echo ${sample_id}\n                    bash get_readstats.sh ${json}\n                    bash get_readqual.sh ${json}\n                    \"\"\"\n\n            }",
        "nb_lignes_process": 25,
        "string_script": "                    \"\"\"\n                    echo ${sample_id}\n                    bash get_readstats.sh ${json}\n                    bash get_readqual.sh ${json}\n                    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "fastp_stats_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fastp_csv"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'low_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/filter\", mode: \"copy\", overwrite: true, pattern: \"*.fastp.{json,html}\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"conda-forge::jq=1.6=h14c3975_1000 \" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/jq:1.6\" : \"quay.io/biocontainers/jq:1.6\") }"
        ],
        "when": "",
        "stub": ""
    },
    "save_filter_reads": {
        "name_process": "save_filter_reads",
        "string_process": " process save_filter_reads {\n\n                    label 'med_cpus'\n\n                    tag \"${sample_id}\"\n\n                    publishDir \"${params.outdir}/saveReads/filter\", mode: \"copy\", overwrite: true, pattern: \"*_R{1,2}.filter.fq.gz\"\n\n                    conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"conda-forge::pigz=2.3.4=hed695b0_1\" : null)\n                    if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                    container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/pigz:2.3.4\" : \"quay.io/biocontainers/pigz:2.3.4\")\n                    }\n\n                    input:\n                        tuple sample_id, file(r1), file(r2) from save_filter_reads\n\n                    output:\n                        tuple sample_id, file(\"*.filter.fq.gz\") into save_filter_reads_out\n\n                    script:\n                        \"\"\"\n                        cat $r1 >${sample_id}_filter.R1.fq\n                        cat $r2 >${sample_id}_filter.R2.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_filter.R1.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_filter.R2.fq\n                        \"\"\"\n                }",
        "nb_lignes_process": 25,
        "string_script": "                        \"\"\"\n                        cat $r1 >${sample_id}_filter.R1.fq\n                        cat $r2 >${sample_id}_filter.R2.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_filter.R1.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_filter.R2.fq\n                        \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "save_filter_reads"
        ],
        "nb_inputs": 1,
        "outputs": [
            "save_filter_reads_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/saveReads/filter\", mode: \"copy\", overwrite: true, pattern: \"*_R{1,2}.filter.fq.gz\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"conda-forge::pigz=2.3.4=hed695b0_1\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/pigz:2.3.4\" : \"quay.io/biocontainers/pigz:2.3.4\") }"
        ],
        "when": "",
        "stub": ""
    },
    "remove_rrna": {
        "name_process": "remove_rrna",
        "string_process": " process remove_rrna {\n\n                label 'med_mem'\n\n                tag \"${sample_id}\"\n\n                publishDir \"${params.outdir}/rRNA_reads\", mode: \"copy\", overwrite: true, pattern: \"*.log\"\n                publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n                conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"bioconda::sortmerna=4.2.0\" : null)\n                if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/sortmerna:4.2.0--0\" : \"quay.io/biocontainers/sortmerna:4.2.0--0\")\n                }\n\n                input:\n                    tuple sample_id, file(reads) from reads_rna_ch\n\n                output:\n                    tuple sample_id, file(\"*rRNA.R*.fq\") into reads_rna_ass\n                    tuple sample_id, file(\"${sample_id}_rRNA_reads.R1.fq\"), file(\"${sample_id}_rRNA_reads.R2.fq\"), file(\"${sample_id}_no_rRNA.R1.fq\"), file(\"${sample_id}_no_rRNA.R2.fq\") into save_rrna_reads\n                    tuple sample_id, file(\"${sample_id}_remove_rRNA.log\") into remove_rrna_sum\n                    file(\"sortmerna.version.txt\") into sortmerna_version\n\n                script:\n                if (!params.skipFilter) {\n                    \"\"\"\n                    sortmerna --ref ${params.rRNAdb} --reads ${reads[0]} --reads ${reads[1]} --threads ${task.cpus} --aligned rRNAreads --other nonrRNAreads --paired_in --out2 --fastx --workdir .\n                    mv rRNAreads.log ${sample_id}_remove_rRNA.log\n                    mv rRNAreads_fwd* ${sample_id}_rRNA_reads.R1.fq\n                    mv rRNAreads_rev* ${sample_id}_rRNA_reads.R2.fq\n                    mv nonrRNAreads_fwd* ${sample_id}_no_rRNA.R1.fq\n                    mv nonrRNAreads_rev* ${sample_id}_no_rRNA.R2.fq\n                    v=\\$( sortmerna -version | grep \"SortMeRNA version\" | awk '{print \\$3}' )\n                    echo \"SortMeRNA: \\$v\" >sortmerna.version.txt\n                    \"\"\"\n                } else {\n                    \"\"\"\n                    sortmerna --ref ${params.rRNAdb} --reads ${reads[0]} --reads ${reads[1]} --threads ${task.cpus} --aligned rRNAreads --other nonrRNAreads --paired_in --out2 --fastx --workdir .\n                    mv rRNAreads.log ${sample_id}_remove_rRNA.log\n                    mv rRNAreads_fwd* ${sample_id}_rRNA_reads.R1.fq\n                    mv rRNAreads_rev* ${sample_id}_rRNA_reads.R2.fq\n                    mv nonrRNAreads_fwd* ${sample_id}_no_rRNA.R1.fq\n                    mv nonrRNAreads_rev* ${sample_id}_no_rRNA.R2.fq\n                    v=\\$( sortmerna -version | grep \"SortMeRNA version\" | awk '{print \\$3}' )\n                    echo \"SortMeRNA: \\$v\" >sortmerna.version.txt\n                    \"\"\"\n                }\n            }",
        "nb_lignes_process": 46,
        "string_script": "                if (!params.skipFilter) {\n                    \"\"\"\n                    sortmerna --ref ${params.rRNAdb} --reads ${reads[0]} --reads ${reads[1]} --threads ${task.cpus} --aligned rRNAreads --other nonrRNAreads --paired_in --out2 --fastx --workdir .\n                    mv rRNAreads.log ${sample_id}_remove_rRNA.log\n                    mv rRNAreads_fwd* ${sample_id}_rRNA_reads.R1.fq\n                    mv rRNAreads_rev* ${sample_id}_rRNA_reads.R2.fq\n                    mv nonrRNAreads_fwd* ${sample_id}_no_rRNA.R1.fq\n                    mv nonrRNAreads_rev* ${sample_id}_no_rRNA.R2.fq\n                    v=\\$( sortmerna -version | grep \"SortMeRNA version\" | awk '{print \\$3}' )\n                    echo \"SortMeRNA: \\$v\" >sortmerna.version.txt\n                    \"\"\"\n                } else {\n                    \"\"\"\n                    sortmerna --ref ${params.rRNAdb} --reads ${reads[0]} --reads ${reads[1]} --threads ${task.cpus} --aligned rRNAreads --other nonrRNAreads --paired_in --out2 --fastx --workdir .\n                    mv rRNAreads.log ${sample_id}_remove_rRNA.log\n                    mv rRNAreads_fwd* ${sample_id}_rRNA_reads.R1.fq\n                    mv rRNAreads_rev* ${sample_id}_rRNA_reads.R2.fq\n                    mv nonrRNAreads_fwd* ${sample_id}_no_rRNA.R1.fq\n                    mv nonrRNAreads_rev* ${sample_id}_no_rRNA.R2.fq\n                    v=\\$( sortmerna -version | grep \"SortMeRNA version\" | awk '{print \\$3}' )\n                    echo \"SortMeRNA: \\$v\" >sortmerna.version.txt\n                    \"\"\"\n                }",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [
            "SortMeRna"
        ],
        "tools_url": [
            "https://bio.tools/sortmerna"
        ],
        "tools_dico": [
            {
                "name": "SortMeRna",
                "uri": "https://bio.tools/sortmerna",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2451",
                                    "term": "Sequence comparison"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0346",
                                    "term": "Sequence similarity search"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0258",
                                    "term": "Sequence alignment analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Sequence analysis tool for filtering, mapping and OTU-picking NGS reads.",
                "homepage": "http://bioinfo.lifl.fr/RNA/sortmerna/"
            }
        ],
        "inputs": [
            "reads_rna_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "reads_rna_ass",
            "save_rrna_reads",
            "remove_rrna_sum",
            "sortmerna_version"
        ],
        "nb_outputs": 4,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_mem'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/rRNA_reads\", mode: \"copy\", overwrite: true, pattern: \"*.log\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"bioconda::sortmerna=4.2.0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/sortmerna:4.2.0--0\" : \"quay.io/biocontainers/sortmerna:4.2.0--0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "save_rrna_reads": {
        "name_process": "save_rrna_reads",
        "string_process": " process save_rrna_reads {\n\n                    label 'med_cpus'\n\n                    tag \"${sample_id}\"\n\n                    publishDir \"${params.outdir}/saveReads/rRNA\", mode: \"copy\", overwrite: true, pattern: \"*.gz\"\n\n                    conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"conda-forge::pigz=2.3.4=hed695b0_1\" : null)\n                    if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                    container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/pigz:2.3.4\" : \"quay.io/biocontainers/pigz:2.3.4\")\n                    }\n\n                    input:\n                        tuple sample_id, file(r1), file(r2), file(r3), file (r4) from save_rrna_reads\n\n                    output:\n                        tuple sample_id, file(\"*.gz\") into save_rrna_reads_out\n\n                    script:\n                        \"\"\"\n                        cat $r1 >${sample_id}_rRNA_reads.R1.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_rRNA_reads.R1.fq\n                        cat $r2 >${sample_id}_rRNA_reads.R2.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_rRNA_reads.R2.fq\n                        cat $r3 >${sample_id}_no_rRNA.R1.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_no_rRNA.R1.fq\n                        cat $r4 >${sample_id}_no_rRNA.R2.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_no_rRNA.R2.fq\n                        \"\"\"\n                }",
        "nb_lignes_process": 29,
        "string_script": "                        \"\"\"\n                        cat $r1 >${sample_id}_rRNA_reads.R1.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_rRNA_reads.R1.fq\n                        cat $r2 >${sample_id}_rRNA_reads.R2.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_rRNA_reads.R2.fq\n                        cat $r3 >${sample_id}_no_rRNA.R1.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_no_rRNA.R1.fq\n                        cat $r4 >${sample_id}_no_rRNA.R2.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_no_rRNA.R2.fq\n                        \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "save_rrna_reads"
        ],
        "nb_inputs": 1,
        "outputs": [
            "save_rrna_reads_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/saveReads/rRNA\", mode: \"copy\", overwrite: true, pattern: \"*.gz\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"conda-forge::pigz=2.3.4=hed695b0_1\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/pigz:2.3.4\" : \"quay.io/biocontainers/pigz:2.3.4\") }"
        ],
        "when": "",
        "stub": ""
    },
    "skip_rrna_removal": {
        "name_process": "skip_rrna_removal",
        "string_process": " process skip_rrna_removal {\n                tag \"${sample_id}\"\n\n                input:\n                    tuple sample_id, file(files) from skip_rna_ch\n\n                output:\n                    tuple sample_id, file(\"rrna_removal.txt\") into remove_rrna_sum\n\n                script:\n                    \"\"\"\n                    echo \"rRNA removal step was skipped\" >rrna_removal.txt\n                    \"\"\"\n            }",
        "nb_lignes_process": 12,
        "string_script": "                    \"\"\"\n                    echo \"rRNA removal step was skipped\" >rrna_removal.txt\n                    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "skip_rna_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "remove_rrna_sum"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "normalize_reads": {
        "name_process": "normalize_reads",
        "string_process": " process normalize_reads {\n\n                label 'med_mem'\n\n                tag \"${sample_id}\"\n\n                conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda trinity=2.9.1=h8b12597_1\" : null)\n                if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/trinity:2.9.1--h8b12597_1\" : \"quay.io/biocontainers/trinity:2.9.1--h8b12597_1\")\n                }\n\n                input:\n                    tuple sample_id, file(reads) from reads_rna_ass\n\n                output:\n                    tuple sample_id, file(\"left-${sample_id}.norm.fq\"), file(\"right-${sample_id}.norm.fq\") into ( norm_reads_soap, norm_reads_velvet, norm_reads_trinity, norm_reads_spades, norm_reads_transabyss, reads_rna_quast )\n                    tuple sample_id, file(\"left-${sample_id}.norm.fq\"), file(\"right-${sample_id}.norm.fq\") into ( mapping_reads_trinity, mapping_reads_evi, mapping_symbiont )\n                    tuple sample_id, file(\"left-${sample_id}.norm.fq\"), file(\"right-${sample_id}.norm.fq\") into save_norm_reads\n                    tuple sample_id, file(\"${sample_id}_normStats.txt\") into norm_report\n\n                script:\n                                           \n                                                       \n                if (!params.skipFilter) {\n                    \"\"\"\n                    echo ${sample_id}\n\n                    echo -e \"\\\\n-- Starting Normalization --\\\\n\"\n\n                    mem=\\$( echo ${task.memory} | cut -f 1 -d \" \" )\n\n                    insilico_read_normalization.pl --seqType fq -JM \\${mem}G --max_cov ${params.normMaxCov} --min_cov ${params.normMinCov} --left ${reads[0]} --right ${reads[1]} --pairs_together --PARALLEL_STATS --CPU ${task.cpus}\n\n                    echo -e \"\\\\n-- DONE with Normalization --\\\\n\"\n\n                    cat .command.out | grep \"stats_file\" -A 3 | tail -n 3 >${sample_id}_normStats.txt\n\n                    cp left.norm.fq left-\"${sample_id}\".norm.fq\n                    cp right.norm.fq right-\"${sample_id}\".norm.fq\n\n                    rm \\$(readlink -e left.norm.fq) \\$(readlink -e right.norm.fq ) left.norm.fq right.norm.fq\n                    \"\"\"\n                } else if (params.skipFilter && !params.rRNAfilter) {\n                    \"\"\"\n                    echo ${sample_id}\n                    zcat ${reads[0]} >left-${sample_id}.fq &\n                    zcat ${reads[1]} >right-${sample_id}.fq\n\n                    echo -e \"\\\\n-- Starting Normalization --\\\\n\"\n\n                    mem=\\$( echo ${task.memory} | cut -f 1 -d \" \" )\n\n                    insilico_read_normalization.pl --seqType fq -JM \\${mem}G --max_cov ${params.normMaxCov} --min_cov ${params.normMinCov} --left left-${sample_id}.fq --right right-${sample_id}.fq --pairs_together --PARALLEL_STATS --CPU ${task.cpus}\n\n                    echo -e \"\\\\n-- DONE with Normalization --\\\\n\"\n\n                    cat .command.out | grep \"stats_file\" -A 3 | tail -n 3 >${sample_id}_normStats.txt\n\n                    cp left.norm.fq left-\"${sample_id}\".norm.fq\n                    cp right.norm.fq right-\"${sample_id}\".norm.fq\n\n                    rm \\$(readlink -e left.norm.fq) \\$(readlink -e right.norm.fq ) left.norm.fq right.norm.fq\n                    \"\"\"\n                }\n            }",
        "nb_lignes_process": 63,
        "string_script": "                if (!params.skipFilter) {\n                    \"\"\"\n                    echo ${sample_id}\n\n                    echo -e \"\\\\n-- Starting Normalization --\\\\n\"\n\n                    mem=\\$( echo ${task.memory} | cut -f 1 -d \" \" )\n\n                    insilico_read_normalization.pl --seqType fq -JM \\${mem}G --max_cov ${params.normMaxCov} --min_cov ${params.normMinCov} --left ${reads[0]} --right ${reads[1]} --pairs_together --PARALLEL_STATS --CPU ${task.cpus}\n\n                    echo -e \"\\\\n-- DONE with Normalization --\\\\n\"\n\n                    cat .command.out | grep \"stats_file\" -A 3 | tail -n 3 >${sample_id}_normStats.txt\n\n                    cp left.norm.fq left-\"${sample_id}\".norm.fq\n                    cp right.norm.fq right-\"${sample_id}\".norm.fq\n\n                    rm \\$(readlink -e left.norm.fq) \\$(readlink -e right.norm.fq ) left.norm.fq right.norm.fq\n                    \"\"\"\n                } else if (params.skipFilter && !params.rRNAfilter) {\n                    \"\"\"\n                    echo ${sample_id}\n                    zcat ${reads[0]} >left-${sample_id}.fq &\n                    zcat ${reads[1]} >right-${sample_id}.fq\n\n                    echo -e \"\\\\n-- Starting Normalization --\\\\n\"\n\n                    mem=\\$( echo ${task.memory} | cut -f 1 -d \" \" )\n\n                    insilico_read_normalization.pl --seqType fq -JM \\${mem}G --max_cov ${params.normMaxCov} --min_cov ${params.normMinCov} --left left-${sample_id}.fq --right right-${sample_id}.fq --pairs_together --PARALLEL_STATS --CPU ${task.cpus}\n\n                    echo -e \"\\\\n-- DONE with Normalization --\\\\n\"\n\n                    cat .command.out | grep \"stats_file\" -A 3 | tail -n 3 >${sample_id}_normStats.txt\n\n                    cp left.norm.fq left-\"${sample_id}\".norm.fq\n                    cp right.norm.fq right-\"${sample_id}\".norm.fq\n\n                    rm \\$(readlink -e left.norm.fq) \\$(readlink -e right.norm.fq ) left.norm.fq right.norm.fq\n                    \"\"\"\n                }",
        "nb_lignes_script": 40,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "reads_rna_ass"
        ],
        "nb_inputs": 1,
        "outputs": [
            "",
            "",
            "save_norm_reads",
            "norm_report"
        ],
        "nb_outputs": 4,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_mem'",
            "tag \"${sample_id}\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda trinity=2.9.1=h8b12597_1\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/trinity:2.9.1--h8b12597_1\" : \"quay.io/biocontainers/trinity:2.9.1--h8b12597_1\") }"
        ],
        "when": "",
        "stub": ""
    },
    "save_norm_reads": {
        "name_process": "save_norm_reads",
        "string_process": " process save_norm_reads {\n\n                    label 'med_cpus'\n\n                    tag \"${sample_id}\"\n\n                    publishDir \"${params.outdir}/saveReads/normalization\", mode: \"copy\", overwrite: true, pattern: \"*.gz\"\n\n                    conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"conda-forge::pigz=2.3.4=hed695b0_1\" : null)\n                    if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                    container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/pigz:2.3.4\" : \"quay.io/biocontainers/pigz:2.3.4\")\n                    }\n\n                    input:\n                        tuple sample_id, file(r1), file(r2) from save_norm_reads\n\n                    output:\n                        tuple sample_id, file(\"*.gz\") into save_norm_reads_out\n\n                    script:\n                        \"\"\"\n                        cat $r1 >${sample_id}_norm.R1.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_norm.R1.fq\n                        cat $r2 >${sample_id}_norm.R2.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_norm.R2.fq\n                        \"\"\"\n                }",
        "nb_lignes_process": 25,
        "string_script": "                        \"\"\"\n                        cat $r1 >${sample_id}_norm.R1.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_norm.R1.fq\n                        cat $r2 >${sample_id}_norm.R2.fq\n                        pigz --best --force -p ${task.cpus} -r ${sample_id}_norm.R2.fq\n                        \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "save_norm_reads"
        ],
        "nb_inputs": 1,
        "outputs": [
            "save_norm_reads_out"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/saveReads/normalization\", mode: \"copy\", overwrite: true, pattern: \"*.gz\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"conda-forge::pigz=2.3.4=hed695b0_1\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/pigz:2.3.4\" : \"quay.io/biocontainers/pigz:2.3.4\") }"
        ],
        "when": "",
        "stub": ""
    },
    "prepare_reads": {
        "name_process": "prepare_reads",
        "string_process": " process prepare_reads {\n\n                label 'low_cpus'\n\n                tag \"${sample_id}\"\n\n                input:\n                    tuple sample_id, file(reads) from reads_rna_ass\n\n                output:\n                    tuple sample_id, file(\"left-${sample_id}.fq\"), file(\"right-${sample_id}.fq\") into ( norm_reads_soap, norm_reads_velvet, norm_reads_trinity, norm_reads_spades, norm_reads_transabyss, reads_rna_quast )\n                    tuple sample_id, file(\"left-${sample_id}.fq\"), file(\"right-${sample_id}.fq\") into ( mapping_reads_trinity, mapping_reads_evi, mapping_symbiont )\n\n                script:\n                if (hasExtension(params.reads, 'gz')) {\n                    \"\"\"\n                    echo ${sample_id}\n                    zcat ${reads[0]} >left-${sample_id}.fq &\n                    zcat ${reads[1]} >right-${sample_id}.fq\n                    \"\"\"\n                } else {\n                    \"\"\"\n                    echo ${sample_id}\n                    cat ${reads[0]} >left-${sample_id}.fq &\n                    cat ${reads[1]} >right-${sample_id}.fq\n                    \"\"\"\n                }\n            }",
        "nb_lignes_process": 26,
        "string_script": "                if (hasExtension(params.reads, 'gz')) {\n                    \"\"\"\n                    echo ${sample_id}\n                    zcat ${reads[0]} >left-${sample_id}.fq &\n                    zcat ${reads[1]} >right-${sample_id}.fq\n                    \"\"\"\n                } else {\n                    \"\"\"\n                    echo ${sample_id}\n                    cat ${reads[0]} >left-${sample_id}.fq &\n                    cat ${reads[1]} >right-${sample_id}.fq\n                    \"\"\"\n                }",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "reads_rna_ass"
        ],
        "nb_inputs": 1,
        "outputs": [
            "",
            ""
        ],
        "nb_outputs": 2,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'low_cpus'",
            "tag \"${sample_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "skip_filter": {
        "name_process": "skip_filter",
        "string_process": " process skip_filter {\n                tag \"${sample_id}\"\n\n                input:\n                    tuple sample_id, file(files) from skip_filter_ch\n\n                output:\n                    tuple sample_id, file(\"filter_reads.txt\") into fastp_csv\n\n                script:\n                    \"\"\"\n                    echo \"Filter step was skipped\" >filter_reads.txt\n                    \"\"\"\n            }",
        "nb_lignes_process": 12,
        "string_script": "                    \"\"\"\n                    echo \"Filter step was skipped\" >filter_reads.txt\n                    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "skip_filter_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fastp_csv"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "skip_normalization": {
        "name_process": "skip_normalization",
        "string_process": " process skip_normalization {\n                tag \"${sample_id}\"\n\n                input:\n                    tuple sample_id, file(files) from skip_norm_ch\n\n                output:\n                    tuple sample_id, file(\"norm_reads.txt\") into norm_report\n\n                script:\n                    \"\"\"\n                    echo \"Normalization step was skipped\" >norm_reads.txt\n                    \"\"\"\n            }",
        "nb_lignes_process": 12,
        "string_script": "                    \"\"\"\n                    echo \"Normalization step was skipped\" >norm_reads.txt\n                    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "skip_norm_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "norm_report"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "skip_normalization_only": {
        "name_process": "skip_normalization_only",
        "string_process": " process skip_normalization_only {\n                tag \"${sample_id}\"\n\n                input:\n                    tuple sample_id, file(reads) from reads_rna_ass\n\n                output:\n                    tuple sample_id, file(\"norm_reads.txt\") into norm_report\n                    tuple sample_id, file(\"left-${sample_id}.fq\"), file(\"right-${sample_id}.fq\") into norm_reads_soap, norm_reads_velvet, norm_reads_trinity, norm_reads_spades, norm_reads_transabyss, reads_rna_quast, mapping_reads_trinity, mapping_reads_evi, mapping_symbiont\n\n                script:\n                    \"\"\"\n                    echo \"Normalization step was skipped\" >norm_reads.txt\n                    echo ${sample_id}\n                    cat ${reads[0]} >left-${sample_id}.fq &\n                    cat ${reads[1]} >right-${sample_id}.fq\n                    \"\"\"\n            }",
        "nb_lignes_process": 16,
        "string_script": "                    \"\"\"\n                    echo \"Normalization step was skipped\" >norm_reads.txt\n                    echo ${sample_id}\n                    cat ${reads[0]} >left-${sample_id}.fq &\n                    cat ${reads[1]} >right-${sample_id}.fq\n                    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "reads_rna_ass"
        ],
        "nb_inputs": 1,
        "outputs": [
            "norm_report",
            "norm_reads_soap",
            "norm_reads_velvet",
            "norm_reads_trinity",
            "norm_reads_spades",
            "norm_reads_transabyss",
            "reads_rna_quast",
            "mapping_reads_trinity",
            "mapping_reads_evi",
            "mapping_symbiont"
        ],
        "nb_outputs": 10,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "skip_filter_only": {
        "name_process": "skip_filter_only",
        "string_process": " process skip_filter_only {\n                tag \"${sample_id}\"\n\n                input:\n                    tuple sample_id, file(files) from skip_filter_only_ch\n\n                output:\n                    tuple sample_id, file(\"filter_reads.txt\") into fastp_csv\n\n                script:\n                    \"\"\"\n                    echo \"Filter step was skipped\" >filter_reads.txt\n                    \"\"\"\n            }",
        "nb_lignes_process": 12,
        "string_script": "                    \"\"\"\n                    echo \"Filter step was skipped\" >filter_reads.txt\n                    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "skip_filter_only_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "fastp_csv"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "trinity_assembly": {
        "name_process": "trinity_assembly",
        "string_process": " process trinity_assembly {\n\n            label 'med_mem'\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/assemblies\", mode: \"copy\", overwrite: true, pattern: \"*.fa\"\n            publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::trinity=2.9.1=h8b12597_1\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/trinity:2.9.1--h8b12597_1\" : \"quay.io/biocontainers/trinity:2.9.1--h8b12597_1\")\n            }\n\n            input:\n                tuple sample_id, file(left), file(right) from norm_reads_trinity\n\n            output:\n                tuple sample_id, file(\"${sample_id}.Trinity.fa\") into ( assemblies_ch_trinity, busco4_ch_trinity, assemblies_ch_trinity_busco4, mapping_trinity )\n                file(\"trinity.version.txt\") into trinity_version\n\n            script:\n                \"\"\"\n                mem=\\$( echo ${task.memory} | cut -f 1 -d \" \" )\n\n                Trinity --max_memory \\${mem}G --seqType fq --left ${left} --right ${right} --CPU ${task.cpus} --no_normalize_reads --full_cleanup --output trinity_out_dir\n\n                mv trinity_out_dir.Trinity.fasta ${sample_id}.Trinity.fa\n\n                v=\\$( Trinity --version | grep \"version\" | head -n 1 | cut -f 2 -d \"-\" | tr -d \"v\" )\n                echo \"Trinity: \\$v\" >trinity.version.txt\n                \"\"\"\n        }",
        "nb_lignes_process": 31,
        "string_script": "                \"\"\"\n                mem=\\$( echo ${task.memory} | cut -f 1 -d \" \" )\n\n                Trinity --max_memory \\${mem}G --seqType fq --left ${left} --right ${right} --CPU ${task.cpus} --no_normalize_reads --full_cleanup --output trinity_out_dir\n\n                mv trinity_out_dir.Trinity.fasta ${sample_id}.Trinity.fa\n\n                v=\\$( Trinity --version | grep \"version\" | head -n 1 | cut -f 2 -d \"-\" | tr -d \"v\" )\n                echo \"Trinity: \\$v\" >trinity.version.txt\n                \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [
            "Trinity"
        ],
        "tools_url": [
            "https://bio.tools/trinity"
        ],
        "tools_dico": [
            {
                "name": "Trinity",
                "uri": "https://bio.tools/trinity",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "Gene transcripts"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "mRNA features"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3258",
                                    "term": "Transcriptome assembly"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Trinity is a transcriptome assembler which relies on three different tools, inchworm an assembler, chrysalis which pools contigs and butterfly which amongst others compacts a graph resulting from butterfly with reads.",
                "homepage": "https://github.com/trinityrnaseq/trinityrnaseq/wiki"
            }
        ],
        "inputs": [
            "norm_reads_trinity"
        ],
        "nb_inputs": 1,
        "outputs": [
            "",
            "trinity_version"
        ],
        "nb_outputs": 2,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_mem'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/assemblies\", mode: \"copy\", overwrite: true, pattern: \"*.fa\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::trinity=2.9.1=h8b12597_1\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/trinity:2.9.1--h8b12597_1\" : \"quay.io/biocontainers/trinity:2.9.1--h8b12597_1\") }"
        ],
        "when": "",
        "stub": ""
    },
    "soap_assembly": {
        "name_process": "soap_assembly",
        "string_process": " process soap_assembly {\n\n            label 'med_mem'\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/assemblies\", mode: \"copy\", overwrite: true, pattern: \"*.fa\"\n            publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::soapdenovo-trans=1.04=ha92aebf_2\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/soapdenovo-trans:1.04--ha92aebf_2\" : \"quay.io/biocontainers/soapdenovo-trans:1.04--ha92aebf_2\")\n            }\n\n            input:\n                val k from \"${params.k}\"\n                tuple sample_id, file(left), file(right) from norm_reads_soap\n\n            output:\n                tuple sample_id, file(\"${sample_id}.SOAP.fa\") into assemblies_ch_soap\n                tuple sample_id, file(\"${sample_id}.SOAP.k*\") into assemblies_ch_soap_busco4\n                file(\"soap.version.txt\") into soap_version\n\n            script:\n                \"\"\"\n                echo -e \"\\\\n-- Generating SOAP config file --\\\\n\"\n                echo \"maxReadLen=\"${params.maxReadLen} >>config.txt\n                echo \"[LIB]\" >>config.txt\n                echo \"rd_len_cutof=\"${params.rd_len_cutof} >>config.txt\n                #echo \"avg_ins=\"${params.avg_ins} >>config.txt\n                echo \"reverse_seq=\"${params.reverse_seq} >>config.txt\n                echo \"asm_flags=\"${params.asm_flags} >>config.txt\n                echo \"map_len=\"${params.map_len} >>config.txt\n                echo \"q1=\"${left} >>config.txt\n                echo \"q2=\"${right} >>config.txt\n\n                echo -e \"\\\\n-- Starting SOAP assemblies --\\\\n\"\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    echo -e \"\\\\n-- SOAP k\\${x} --\\\\n\"\n                    SOAPdenovo-Trans-127mer all -s config.txt -K \\${x} -o output\\${x} -p ${task.cpus}\n                    sed -i \"s/>/>SOAP.k\\${x}./g\" output\\${x}.scafSeq\n                done\n\n                echo -e \"\\\\n-- Finished with the assemblies --\\\\n\"\n\n                cat output*.scafSeq >${sample_id}.SOAP.fa\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    cp output\\${x}.scafSeq ${sample_id}.SOAP.k\\${x}.fa\n                done\n\n                rm -rf output*\n                v=\\$( SOAPdenovo-Trans-127mer --version | grep \"version\" | awk '{print \\$2,\\$3}' | cut -f 1 -d \":\" | cut -f 2 -d \" \" )\n                echo \"SOAP: \\$v\" >soap.version.txt\n                \"\"\"\n        }",
        "nb_lignes_process": 55,
        "string_script": "                \"\"\"\n                echo -e \"\\\\n-- Generating SOAP config file --\\\\n\"\n                echo \"maxReadLen=\"${params.maxReadLen} >>config.txt\n                echo \"[LIB]\" >>config.txt\n                echo \"rd_len_cutof=\"${params.rd_len_cutof} >>config.txt\n                #echo \"avg_ins=\"${params.avg_ins} >>config.txt\n                echo \"reverse_seq=\"${params.reverse_seq} >>config.txt\n                echo \"asm_flags=\"${params.asm_flags} >>config.txt\n                echo \"map_len=\"${params.map_len} >>config.txt\n                echo \"q1=\"${left} >>config.txt\n                echo \"q2=\"${right} >>config.txt\n\n                echo -e \"\\\\n-- Starting SOAP assemblies --\\\\n\"\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    echo -e \"\\\\n-- SOAP k\\${x} --\\\\n\"\n                    SOAPdenovo-Trans-127mer all -s config.txt -K \\${x} -o output\\${x} -p ${task.cpus}\n                    sed -i \"s/>/>SOAP.k\\${x}./g\" output\\${x}.scafSeq\n                done\n\n                echo -e \"\\\\n-- Finished with the assemblies --\\\\n\"\n\n                cat output*.scafSeq >${sample_id}.SOAP.fa\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    cp output\\${x}.scafSeq ${sample_id}.SOAP.k\\${x}.fa\n                done\n\n                rm -rf output*\n                v=\\$( SOAPdenovo-Trans-127mer --version | grep \"version\" | awk '{print \\$2,\\$3}' | cut -f 1 -d \":\" | cut -f 2 -d \" \" )\n                echo \"SOAP: \\$v\" >soap.version.txt\n                \"\"\"",
        "nb_lignes_script": 31,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "\"${params",
            "norm_reads_soap"
        ],
        "nb_inputs": 2,
        "outputs": [
            "assemblies_ch_soap",
            "assemblies_ch_soap_busco4",
            "soap_version"
        ],
        "nb_outputs": 3,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_mem'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/assemblies\", mode: \"copy\", overwrite: true, pattern: \"*.fa\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::soapdenovo-trans=1.04=ha92aebf_2\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/soapdenovo-trans:1.04--ha92aebf_2\" : \"quay.io/biocontainers/soapdenovo-trans:1.04--ha92aebf_2\") }"
        ],
        "when": "",
        "stub": ""
    },
    "velvet_oases_assembly": {
        "name_process": "velvet_oases_assembly",
        "string_process": " process velvet_oases_assembly {\n\n            label 'med_mem'\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/assemblies\", mode: \"copy\", overwrite: true, pattern: \"*.fa\"\n            publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda velvet=1.2.10 oases=0.2.09\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-8ce10492777ba3fb1db6e6e13fa9b78ac116db2f:f54a9246f1216443f2e0f6de9ec5908ca882f710-0\" : \"quay.io/biocontainers/mulled-v2-8ce10492777ba3fb1db6e6e13fa9b78ac116db2f:f54a9246f1216443f2e0f6de9ec5908ca882f710-0\")\n            }\n\n            input:\n                val k from \"${params.k}\"\n                tuple sample_id, file(left), file(right) from norm_reads_velvet\n\n            output:\n                tuple sample_id, file(\"${sample_id}.Velvet.fa\") into assemblies_ch_velvet\n                tuple sample_id, file(\"${sample_id}.Velvet.k*\") into assemblies_ch_velvet_busco4\n                file(\"velvet_oases.versions.txt\") into velvet_version\n\n            script:\n                \"\"\"\n        \t    echo -e \"\\\\n-- Starting with Velveth --\\\\n\"\n                for x in `echo $k | tr \",\" \" \"`;do\n                    echo -e \"\\\\n-- k\\${x} --\\\\n\"\n                    velveth oases.\\${x} \\${x} -shortPaired -fastq -separate ${left} ${right}\n                done\n\n                echo -e \"\\\\n-- Starting with Velvetg --\\\\n\"\n                for x in `echo $k | tr \",\" \" \"`;do\n                    echo -e \"\\\\n-- vg \\${x} --\\\\n\"\n                    velvetg oases.\\${x} -read_trkg yes\n                done\n\n                echo -e \"\\\\n-- Starting with Oases --\\\\n\"\n                for x in `echo $k | tr \",\" \" \"`;do\n                    echo -e \"\\\\n-- oases \\${x} --\\\\n\"\n                    oases oases.\\${x}\n                done\n\n                echo -e \"\\\\n-- Finished with Velvet/Oases assemblies --\\\\n\"\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    sed -i \"s/>/>Velvet.k\\${x}./g\" oases.\\${x}/contigs.fa\n                done\n\n                cat oases.*/contigs.fa >${sample_id}.Velvet.fa\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    cp oases.\\${x}/contigs.fa ${sample_id}.Velvet.k\\${x}.fa\n                done\n\n                rm -rf oases.*\n\n                v=\\$( velveth | grep \"Version\" | cut -f 2 -d \" \" )\n                echo \"Velveth: \\$v\" >velvet_oases.versions.txt\n                v=\\$( velvetg | grep \"Version\" | cut -f 2 -d \" \" )\n                echo \"Velvetg: \\$v\" >>velvet_oases.versions.txt\n                v=\\$( oases | grep \"Version\" | cut -f 2 -d \" \" )\n                echo \"Oases: \\$v\" >>velvet_oases.versions.txt\n                \"\"\"\n        }",
        "nb_lignes_process": 63,
        "string_script": "                \"\"\"\n        \t    echo -e \"\\\\n-- Starting with Velveth --\\\\n\"\n                for x in `echo $k | tr \",\" \" \"`;do\n                    echo -e \"\\\\n-- k\\${x} --\\\\n\"\n                    velveth oases.\\${x} \\${x} -shortPaired -fastq -separate ${left} ${right}\n                done\n\n                echo -e \"\\\\n-- Starting with Velvetg --\\\\n\"\n                for x in `echo $k | tr \",\" \" \"`;do\n                    echo -e \"\\\\n-- vg \\${x} --\\\\n\"\n                    velvetg oases.\\${x} -read_trkg yes\n                done\n\n                echo -e \"\\\\n-- Starting with Oases --\\\\n\"\n                for x in `echo $k | tr \",\" \" \"`;do\n                    echo -e \"\\\\n-- oases \\${x} --\\\\n\"\n                    oases oases.\\${x}\n                done\n\n                echo -e \"\\\\n-- Finished with Velvet/Oases assemblies --\\\\n\"\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    sed -i \"s/>/>Velvet.k\\${x}./g\" oases.\\${x}/contigs.fa\n                done\n\n                cat oases.*/contigs.fa >${sample_id}.Velvet.fa\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    cp oases.\\${x}/contigs.fa ${sample_id}.Velvet.k\\${x}.fa\n                done\n\n                rm -rf oases.*\n\n                v=\\$( velveth | grep \"Version\" | cut -f 2 -d \" \" )\n                echo \"Velveth: \\$v\" >velvet_oases.versions.txt\n                v=\\$( velvetg | grep \"Version\" | cut -f 2 -d \" \" )\n                echo \"Velvetg: \\$v\" >>velvet_oases.versions.txt\n                v=\\$( oases | grep \"Version\" | cut -f 2 -d \" \" )\n                echo \"Oases: \\$v\" >>velvet_oases.versions.txt\n                \"\"\"",
        "nb_lignes_script": 39,
        "language_script": "bash",
        "tools": [
            "Oases"
        ],
        "tools_url": [
            "https://bio.tools/oases"
        ],
        "tools_dico": [
            {
                "name": "Oases",
                "uri": "https://bio.tools/oases",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Whole genome sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Transcriptome profiling"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA-Seq analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small RNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Small-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "Whole transcriptome shotgun sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "RNA sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3170",
                            "term": "WTSS"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Genome sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "WGS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3258",
                                    "term": "Transcriptome assembly"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3436",
                                    "term": "Aggregation"
                                }
                            ],
                            []
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0925",
                                "term": "Sequence assembly"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3181",
                                "term": "Sequence assembly report"
                            },
                            {
                                "uri": "http://edamontology.org/data_0925",
                                "term": "Sequence assembly"
                            },
                            {
                                "uri": "http://edamontology.org/data_3495",
                                "term": "RNA sequence"
                            }
                        ]
                    }
                ],
                "description": "De novo transcriptome assembler designed to produce transcripts from short read sequencing technologies. It uploads a preliminary assembly produced by Velvet, and clusters the contigs into small groups, called loci. It then exploits the paired-end read and long read information, when available, to construct transcript isoforms.",
                "homepage": "https://github.com/dzerbino/oases"
            }
        ],
        "inputs": [
            "\"${params",
            "norm_reads_velvet"
        ],
        "nb_inputs": 2,
        "outputs": [
            "assemblies_ch_velvet",
            "assemblies_ch_velvet_busco4",
            "velvet_version"
        ],
        "nb_outputs": 3,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_mem'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/assemblies\", mode: \"copy\", overwrite: true, pattern: \"*.fa\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda velvet=1.2.10 oases=0.2.09\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-8ce10492777ba3fb1db6e6e13fa9b78ac116db2f:f54a9246f1216443f2e0f6de9ec5908ca882f710-0\" : \"quay.io/biocontainers/mulled-v2-8ce10492777ba3fb1db6e6e13fa9b78ac116db2f:f54a9246f1216443f2e0f6de9ec5908ca882f710-0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "rna_spades_assembly": {
        "name_process": "rna_spades_assembly",
        "string_process": " process rna_spades_assembly {\n\n            label 'med_mem'\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/assemblies\", mode: \"copy\", overwrite: true, pattern: \"*.fa\"\n            publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::spades=3.15.3=h95f258a_1\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/spades:3.15.3--h95f258a_1\" : \"quay.io/biocontainers/spades:3.15.3--h95f258a_1\")\n            }\n\n            input:\n                val k from \"${params.k}\"\n                tuple sample_id, file(left), file(right) from norm_reads_spades\n\n            output:\n                tuple sample_id, file(\"${sample_id}.SPADES.fa\") into assemblies_ch_spades\n                tuple sample_id, file(\"${sample_id}.SPADES.k*\") into assemblies_ch_spades_busco4\n                file(\"rna_spades.version.txt\") into rnaspades_version\n\n            script:\n                \"\"\"\n                echo -e \"\\\\n-- Starting rnaSPADES assemblies --\\\\n\"\n\n                mem=\\$( echo ${task.memory} | cut -f 1 -d \" \" )\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    echo -e \"\\\\n-- rnaSPADES k\\${x} --\\\\n\"\n                    rnaspades.py -1 ${left} -2 ${right} -o ${sample_id}_spades_\\${x} -t ${task.cpus} -k \\${x} -m \\${mem}\n                done\n\n                echo -e \"\\\\n-- Finished with the assemblies --\\\\n\"\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    sed -i \"s/>/>SPADES.k\\${x}./g\" ${sample_id}_spades_\\${x}/transcripts.fasta\n                done\n\n                cat ${sample_id}_spades_*/transcripts.fasta >${sample_id}.SPADES.fa\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    cp ${sample_id}_spades_\\${x}/transcripts.fasta ${sample_id}.SPADES.k\\${x}.fa\n                done\n\n                rm -rf ${sample_id}_spades_*\n\n                v=\\$( rnaspades.py -v 2>&1 | awk '{print \\$4}' | tr -d \"v\" )\n                echo \"rna-SPADES: \\$v\" >rna_spades.version.txt\n                \"\"\"\n        }",
        "nb_lignes_process": 50,
        "string_script": "                \"\"\"\n                echo -e \"\\\\n-- Starting rnaSPADES assemblies --\\\\n\"\n\n                mem=\\$( echo ${task.memory} | cut -f 1 -d \" \" )\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    echo -e \"\\\\n-- rnaSPADES k\\${x} --\\\\n\"\n                    rnaspades.py -1 ${left} -2 ${right} -o ${sample_id}_spades_\\${x} -t ${task.cpus} -k \\${x} -m \\${mem}\n                done\n\n                echo -e \"\\\\n-- Finished with the assemblies --\\\\n\"\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    sed -i \"s/>/>SPADES.k\\${x}./g\" ${sample_id}_spades_\\${x}/transcripts.fasta\n                done\n\n                cat ${sample_id}_spades_*/transcripts.fasta >${sample_id}.SPADES.fa\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    cp ${sample_id}_spades_\\${x}/transcripts.fasta ${sample_id}.SPADES.k\\${x}.fa\n                done\n\n                rm -rf ${sample_id}_spades_*\n\n                v=\\$( rnaspades.py -v 2>&1 | awk '{print \\$4}' | tr -d \"v\" )\n                echo \"rna-SPADES: \\$v\" >rna_spades.version.txt\n                \"\"\"",
        "nb_lignes_script": 26,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "\"${params",
            "norm_reads_spades"
        ],
        "nb_inputs": 2,
        "outputs": [
            "assemblies_ch_spades",
            "assemblies_ch_spades_busco4",
            "rnaspades_version"
        ],
        "nb_outputs": 3,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_mem'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/assemblies\", mode: \"copy\", overwrite: true, pattern: \"*.fa\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::spades=3.15.3=h95f258a_1\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/spades:3.15.3--h95f258a_1\" : \"quay.io/biocontainers/spades:3.15.3--h95f258a_1\") }"
        ],
        "when": "",
        "stub": ""
    },
    "transabyss_assembly": {
        "name_process": "transabyss_assembly",
        "string_process": " process transabyss_assembly {\n\n            label 'med_mem'\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/assemblies\", mode: \"copy\", overwrite: true, pattern: \"*.fa\"\n            publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::transabyss=2.0.1=pyh864c0ab_7\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/transabyss:2.0.1--pyh864c0ab_7\" : \"quay.io/biocontainers/transabyss:2.0.1--pyh864c0ab_7\")\n            }\n\n            input:\n                val k from \"${params.k}\"\n                tuple sample_id, file(left), file(right) from norm_reads_transabyss\n\n            output:\n                tuple sample_id, file(\"${sample_id}.TransABySS.fa\") into assemblies_ch_transabyss\n                tuple sample_id, file(\"${sample_id}.TransABySS.k*\") into assemblies_ch_transabyss_busco4\n                file(\"transabyss.version.txt\") into transabyss_version\n\n            script:\n                \"\"\"\n                echo -e \"\\\\n-- Starting Trans-ABySS assemblies --\\\\n\"\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    echo -e \"\\\\n-- Trans-ABySS k\\${x} --\\\\n\"\n                    transabyss -k \\${x} --pe ${left} ${right} --outdir ${sample_id}_transabyss_\\${x} --name k\\${x}.transabyss.fa --threads ${task.cpus} -c 12 --length 200\n                done\n\n                echo -e \"\\\\n-- Finished with the assemblies --\\\\n\"\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    sed -i \"s/>/>TransABySS.k\\${x}./g\" ${sample_id}_transabyss_\\${x}/k\\${x}.transabyss.fa-final.fa\n                done\n\n                cat ${sample_id}_transabyss_*/k*.transabyss.fa-final.fa >${sample_id}.TransABySS.fa\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    cp ${sample_id}_transabyss_\\${x}/k\\${x}.transabyss.fa-final.fa ${sample_id}.TransABySS.k\\${x}.fa\n                done\n\n                rm -rf ${sample_id}_transabyss_*\n\n                v=\\$( transabyss --version )\n                echo \"Trans-ABySS: \\$v\" >transabyss.version.txt\n                \"\"\"\n\n        }",
        "nb_lignes_process": 49,
        "string_script": "                \"\"\"\n                echo -e \"\\\\n-- Starting Trans-ABySS assemblies --\\\\n\"\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    echo -e \"\\\\n-- Trans-ABySS k\\${x} --\\\\n\"\n                    transabyss -k \\${x} --pe ${left} ${right} --outdir ${sample_id}_transabyss_\\${x} --name k\\${x}.transabyss.fa --threads ${task.cpus} -c 12 --length 200\n                done\n\n                echo -e \"\\\\n-- Finished with the assemblies --\\\\n\"\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    sed -i \"s/>/>TransABySS.k\\${x}./g\" ${sample_id}_transabyss_\\${x}/k\\${x}.transabyss.fa-final.fa\n                done\n\n                cat ${sample_id}_transabyss_*/k*.transabyss.fa-final.fa >${sample_id}.TransABySS.fa\n\n                for x in `echo $k | tr \",\" \" \"`;do\n                    cp ${sample_id}_transabyss_\\${x}/k\\${x}.transabyss.fa-final.fa ${sample_id}.TransABySS.k\\${x}.fa\n                done\n\n                rm -rf ${sample_id}_transabyss_*\n\n                v=\\$( transabyss --version )\n                echo \"Trans-ABySS: \\$v\" >transabyss.version.txt\n                \"\"\"",
        "nb_lignes_script": 24,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "\"${params",
            "norm_reads_transabyss"
        ],
        "nb_inputs": 2,
        "outputs": [
            "assemblies_ch_transabyss",
            "assemblies_ch_transabyss_busco4",
            "transabyss_version"
        ],
        "nb_outputs": 3,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_mem'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/assemblies\", mode: \"copy\", overwrite: true, pattern: \"*.fa\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::transabyss=2.0.1=pyh864c0ab_7\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/transabyss:2.0.1--pyh864c0ab_7\" : \"quay.io/biocontainers/transabyss:2.0.1--pyh864c0ab_7\") }"
        ],
        "when": "",
        "stub": ""
    },
    "evigene": {
        "name_process": "evigene",
        "string_process": " process evigene {\n\n            label 'med_mem'\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/evigene\", mode: \"copy\", overwrite: true, pattern: \"*.fa\"\n            publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::cd-hit=4.8.1 bioconda::exonerate=2.4 bioconda::blast=2.11.0\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-962eae98c9ff8d5b31e1df7e41a355a99e1152c4:0ed9db56fd54cfea67041f80bdd8b8fac575112f-0\" : \"quay.io/biocontainers/mulled-v2-962eae98c9ff8d5b31e1df7e41a355a99e1152c4:0ed9db56fd54cfea67041f80bdd8b8fac575112f-0\")\n            }\n\n            input:\n                tuple sample_id, file(assemblies) from evigene_ch\n\n            output:\n                tuple sample_id, file(\"${sample_id}.combined.okay.fa\") into ( evigene_ch_busco4, annotation_ch_transdecoder, annotation_ch_transdecoderB, evigene_ch_rnammer, evigene_ch_trinotate, evi_dist, evi_filt, evigene_ch_rna_quast, mapping_evi )\n                tuple sample_id, file(\"${sample_id}.combined.fa\"), file(\"${sample_id}.combined.okay.fa\") into evigene_summary\n                file(\"evigene.version.txt\") into evigene_version\n\n            script:\n                def mem_MB=(task.memory.toMega())\n                if ((workflow.containerEngine == 'singularity' || workflow.containerEngine == 'docker') && params.oneContainer) {\n                    \"\"\"\n                    echo -e \"\\\\n-- Starting EviGene --\\\\n\"\n\n                    cat ${assemblies} >${sample_id}.combined.fa\n\n                    tr2aacds.pl -tidy -NCPU ${task.cpus} -MAXMEM ${mem_MB} -log -cdna ${sample_id}.combined.fa\n\n                    echo -e \"\\\\n-- DONE with EviGene --\\\\n\"\n\n                    cp okayset/*combined.okay*.fa ${sample_id}.combined.okay.fa\n                    cp okayset/*combined.okay*.cds ${sample_id}.combined.okay.cds\n\n                    if [ -d tmpfiles/ ];then\n                        rm -rf tmpfiles/\n                    fi\n\n                    v=\\$( echo \"2019.05.14\" )\n                    echo \"EvidentialGene: \\$v\" >evigene.version.txt\n                    v=\\$( blastn -version | head -n1 | awk '{print \\$2}' )\n                    echo \"Blast: \\$v\" >>evigene.version.txt\n                    v=\\$( cd-hit -h | head -n1 | cut -f 1 -d \"(\" | cut -f 2 -d \"n\" )\n                    echo \"CD-HIT: \\$v\" >>evigene.version.txt\n                    v=\\$( exonerate -v | head -n1 | cut -f 5 -d \" \" )\n                    echo \"Exonerate: \\$v\" >>evigene.version.txt\n                    \"\"\"\n                } else {\n                    \"\"\"\n                    echo -e \"\\\\n-- Starting EviGene --\\\\n\"\n\n                    cat ${assemblies} >${sample_id}.combined.fa\n\n                    $evi/scripts/prot/tr2aacds.pl -tidy -NCPU ${task.cpus} -MAXMEM ${mem_MB} -log -cdna ${sample_id}.combined.fa\n\n                    echo -e \"\\\\n-- DONE with EviGene --\\\\n\"\n\n                    cp okayset/*combined.okay*.fa ${sample_id}.combined.okay.fa\n                    cp okayset/*combined.okay*.cds ${sample_id}.combined.okay.cds\n\n                    if [ -d tmpfiles/ ];then\n                        rm -rf tmpfiles/\n                    fi\n\n                    v=\\$( echo \"2019.05.14\" )\n                    echo \"EvidentialGene: \\$v\" >evigene.version.txt\n                    v=\\$( blastn -version | head -n1 | awk '{print \\$2}' )\n                    echo \"Blast: \\$v\" >>evigene.version.txt\n                    v=\\$( cd-hit -h | head -n1 | cut -f 1 -d \"(\" | cut -f 2 -d \"n\" )\n                    echo \"CD-HIT: \\$v\" >>evigene.version.txt\n                    v=\\$( exonerate -v | head -n1 | cut -f 5 -d \" \" )\n                    echo \"Exonerate: \\$v\" >>evigene.version.txt\n                \t\"\"\"\n                }\n        }",
        "nb_lignes_process": 76,
        "string_script": "                def mem_MB=(task.memory.toMega())\n                if ((workflow.containerEngine == 'singularity' || workflow.containerEngine == 'docker') && params.oneContainer) {\n                    \"\"\"\n                    echo -e \"\\\\n-- Starting EviGene --\\\\n\"\n\n                    cat ${assemblies} >${sample_id}.combined.fa\n\n                    tr2aacds.pl -tidy -NCPU ${task.cpus} -MAXMEM ${mem_MB} -log -cdna ${sample_id}.combined.fa\n\n                    echo -e \"\\\\n-- DONE with EviGene --\\\\n\"\n\n                    cp okayset/*combined.okay*.fa ${sample_id}.combined.okay.fa\n                    cp okayset/*combined.okay*.cds ${sample_id}.combined.okay.cds\n\n                    if [ -d tmpfiles/ ];then\n                        rm -rf tmpfiles/\n                    fi\n\n                    v=\\$( echo \"2019.05.14\" )\n                    echo \"EvidentialGene: \\$v\" >evigene.version.txt\n                    v=\\$( blastn -version | head -n1 | awk '{print \\$2}' )\n                    echo \"Blast: \\$v\" >>evigene.version.txt\n                    v=\\$( cd-hit -h | head -n1 | cut -f 1 -d \"(\" | cut -f 2 -d \"n\" )\n                    echo \"CD-HIT: \\$v\" >>evigene.version.txt\n                    v=\\$( exonerate -v | head -n1 | cut -f 5 -d \" \" )\n                    echo \"Exonerate: \\$v\" >>evigene.version.txt\n                    \"\"\"\n                } else {\n                    \"\"\"\n                    echo -e \"\\\\n-- Starting EviGene --\\\\n\"\n\n                    cat ${assemblies} >${sample_id}.combined.fa\n\n                    $evi/scripts/prot/tr2aacds.pl -tidy -NCPU ${task.cpus} -MAXMEM ${mem_MB} -log -cdna ${sample_id}.combined.fa\n\n                    echo -e \"\\\\n-- DONE with EviGene --\\\\n\"\n\n                    cp okayset/*combined.okay*.fa ${sample_id}.combined.okay.fa\n                    cp okayset/*combined.okay*.cds ${sample_id}.combined.okay.cds\n\n                    if [ -d tmpfiles/ ];then\n                        rm -rf tmpfiles/\n                    fi\n\n                    v=\\$( echo \"2019.05.14\" )\n                    echo \"EvidentialGene: \\$v\" >evigene.version.txt\n                    v=\\$( blastn -version | head -n1 | awk '{print \\$2}' )\n                    echo \"Blast: \\$v\" >>evigene.version.txt\n                    v=\\$( cd-hit -h | head -n1 | cut -f 1 -d \"(\" | cut -f 2 -d \"n\" )\n                    echo \"CD-HIT: \\$v\" >>evigene.version.txt\n                    v=\\$( exonerate -v | head -n1 | cut -f 5 -d \" \" )\n                    echo \"Exonerate: \\$v\" >>evigene.version.txt\n                \t\"\"\"\n                }",
        "nb_lignes_script": 53,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "evigene_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "",
            "evigene_summary",
            "evigene_version"
        ],
        "nb_outputs": 3,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_mem'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/evigene\", mode: \"copy\", overwrite: true, pattern: \"*.fa\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::cd-hit=4.8.1 bioconda::exonerate=2.4 bioconda::blast=2.11.0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-962eae98c9ff8d5b31e1df7e41a355a99e1152c4:0ed9db56fd54cfea67041f80bdd8b8fac575112f-0\" : \"quay.io/biocontainers/mulled-v2-962eae98c9ff8d5b31e1df7e41a355a99e1152c4:0ed9db56fd54cfea67041f80bdd8b8fac575112f-0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "rna_quast": {
        "name_process": "rna_quast",
        "string_process": " process rna_quast {\n\n            label 'med_mem'\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/rnaQuast\", mode: \"copy\", overwrite: true, pattern: \"*.{rna_quast,csv}\"\n            publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::rnaquast=2.2.1=h9ee0642_0\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/rnaquast:2.2.1--h9ee0642_0\" : \"quay.io/biocontainers/rnaquast:2.2.1--h9ee0642_0\")\n            }\n\n            input:\n                tuple sample_id, file(r1), file(r2), file(assembly) from rna_quast\n\n            output:\n                tuple sample_id, file(\"${sample_id}.rna_quast\") into rna_quast_sum\n                tuple sample_id, file(\"${sample_id}_rnaQUAST.csv\") into rna_quast_report\n                file(\"rnaquast.version.txt\") into rnaquast_version\n\n            script:\n                if (params.onlyEvi){\n                \"\"\"\n                rnaQUAST.py --transcripts ${r1} -o ${sample_id}.rna_quast -t ${task.cpus} --blat\n                echo \"Category,Value\" >${sample_id}_rnaQUAST.csv\n                cat ${sample_id}.rna_quast/*_output/basic_metrics.txt | grep -v \"METRICS\" |  sed 's/\\\\(\\\\ \\\\)* \\\\([0-9]\\\\)/,\\\\2/g' | sed 's/>,/>/g' | grep [0-9] >>${sample_id}_rnaQUAST.csv\n                cat ${sample_id}.rna_quast/*_output/basic_metrics.txt | grep \"Genes\" | sed 's/\\\\(\\\\ \\\\)* \\\\([0-9]\\\\)/,\\\\2/g' >>${sample_id}_rnaQUAST.csv\n                v=\\$( rnaQUAST.py | grep \"QUALITY ASSESSMENT\" | head -n1 | awk -F \" v.\" '{print \\$2}' )\n                echo \"rnaQUAST: \\$v\" >rnaquast.version.txt\n                \"\"\"\n                } else {\n                \"\"\"\n                rnaQUAST.py --transcripts ${assembly} -1 ${r1} -2 ${r2} -o ${sample_id}.rna_quast -t ${task.cpus} --blat\n                echo \"Category,Value\" >${sample_id}_rnaQUAST.csv\n                cat ${sample_id}.rna_quast/*_output/basic_metrics.txt | grep -v \"METRICS\" |  sed 's/\\\\(\\\\ \\\\)* \\\\([0-9]\\\\)/,\\\\2/g' | sed 's/>,/>/g' | grep [0-9] >>${sample_id}_rnaQUAST.csv\n                cat ${sample_id}.rna_quast/*_output/basic_metrics.txt | grep \"Genes\" | sed 's/\\\\(\\\\ \\\\)* \\\\([0-9]\\\\)/,\\\\2/g' >>${sample_id}_rnaQUAST.csv\n                v=\\$( rnaQUAST.py | grep \"QUALITY ASSESSMENT\" | head -n1 | awk -F \" v.\" '{print \\$2}' )\n                echo \"rnaQUAST: \\$v\" >rnaquast.version.txt\n                \"\"\"\n                }\n        }",
        "nb_lignes_process": 41,
        "string_script": "                if (params.onlyEvi){\n                \"\"\"\n                rnaQUAST.py --transcripts ${r1} -o ${sample_id}.rna_quast -t ${task.cpus} --blat\n                echo \"Category,Value\" >${sample_id}_rnaQUAST.csv\n                cat ${sample_id}.rna_quast/*_output/basic_metrics.txt | grep -v \"METRICS\" |  sed 's/\\\\(\\\\ \\\\)* \\\\([0-9]\\\\)/,\\\\2/g' | sed 's/>,/>/g' | grep [0-9] >>${sample_id}_rnaQUAST.csv\n                cat ${sample_id}.rna_quast/*_output/basic_metrics.txt | grep \"Genes\" | sed 's/\\\\(\\\\ \\\\)* \\\\([0-9]\\\\)/,\\\\2/g' >>${sample_id}_rnaQUAST.csv\n                v=\\$( rnaQUAST.py | grep \"QUALITY ASSESSMENT\" | head -n1 | awk -F \" v.\" '{print \\$2}' )\n                echo \"rnaQUAST: \\$v\" >rnaquast.version.txt\n                \"\"\"\n                } else {\n                \"\"\"\n                rnaQUAST.py --transcripts ${assembly} -1 ${r1} -2 ${r2} -o ${sample_id}.rna_quast -t ${task.cpus} --blat\n                echo \"Category,Value\" >${sample_id}_rnaQUAST.csv\n                cat ${sample_id}.rna_quast/*_output/basic_metrics.txt | grep -v \"METRICS\" |  sed 's/\\\\(\\\\ \\\\)* \\\\([0-9]\\\\)/,\\\\2/g' | sed 's/>,/>/g' | grep [0-9] >>${sample_id}_rnaQUAST.csv\n                cat ${sample_id}.rna_quast/*_output/basic_metrics.txt | grep \"Genes\" | sed 's/\\\\(\\\\ \\\\)* \\\\([0-9]\\\\)/,\\\\2/g' >>${sample_id}_rnaQUAST.csv\n                v=\\$( rnaQUAST.py | grep \"QUALITY ASSESSMENT\" | head -n1 | awk -F \" v.\" '{print \\$2}' )\n                echo \"rnaQUAST: \\$v\" >rnaquast.version.txt\n                \"\"\"\n                }",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "rna_quast"
        ],
        "nb_inputs": 1,
        "outputs": [
            "rna_quast_sum",
            "rna_quast_report",
            "rnaquast_version"
        ],
        "nb_outputs": 3,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_mem'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/rnaQuast\", mode: \"copy\", overwrite: true, pattern: \"*.{rna_quast,csv}\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::rnaquast=2.2.1=h9ee0642_0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/rnaquast:2.2.1--h9ee0642_0\" : \"quay.io/biocontainers/rnaquast:2.2.1--h9ee0642_0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "mapping_evigene": {
        "name_process": "mapping_evigene",
        "string_process": " process mapping_evigene {\n\n                label 'big_cpus'\n\n                tag \"${sample_id}\"\n\n                publishDir \"${params.outdir}/mapping\", mode: \"copy\", overwrite: true\n\n                conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::bowtie2=2.4.2=py36hff7a194_2 bioconda::samtools=1.11=h6270b1f_0\" : null)\n                if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-c742dccc9d8fabfcff2af0d8d6799dbc711366cf:b42a120d4ad2b0f6626127964d8bcc9ada05ed03-0\" : \"quay.io/biocontainers/mulled-v2-c742dccc9d8fabfcff2af0d8d6799dbc711366cf:b42a120d4ad2b0f6626127964d8bcc9ada05ed03-0\")\n                }\n\n                input:\n                    tuple sample_id, file(files), file(files2) from mapping_evi_in\n\n                output:\n                    tuple sample_id, file(\"log*\") into mapping_evi_results\n                    tuple sample_id, file(\"*\") into mapping_evi_results_bam\n\n                script:\n                if (params.saveBam) {\n                    \"\"\"\n                    a=\\$( echo $files $files2 )\n                    ass=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \".combined.okay.fa\" )\n                    r1=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \"left-\" )\n                    r2=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \"right-\" )\n                    bowtie2-build \\${ass} \\${ass} --threads ${task.cpus}\n                    bowtie2 -x \\${ass} -1 \\${r1} -2 \\${r2} -p ${task.cpus} 2>log_\\${ass}.txt | samtools view -@ ${task.cpus} -bS - >\\${ass}.bam\n                    rm *.bt2\n                    \"\"\"\n                } else {\n                    \"\"\"\n                    a=\\$( echo $files $files2 )\n                    ass=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \".combined.okay.fa\" )\n                    r1=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \"left-\" )\n                    r2=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \"right-\" )\n                    bowtie2-build \\${ass} \\${ass} --threads ${task.cpus}\n                    bowtie2 -x \\${ass} -1 \\${r1} -2 \\${r2} -p ${task.cpus} 2>log_\\${ass}.txt | samtools view -@ ${task.cpus} -bS - >\\${ass}.bam\n                    rm *.bam *.bt2\n                    \"\"\"\n                }\n            }",
        "nb_lignes_process": 41,
        "string_script": "                if (params.saveBam) {\n                    \"\"\"\n                    a=\\$( echo $files $files2 )\n                    ass=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \".combined.okay.fa\" )\n                    r1=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \"left-\" )\n                    r2=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \"right-\" )\n                    bowtie2-build \\${ass} \\${ass} --threads ${task.cpus}\n                    bowtie2 -x \\${ass} -1 \\${r1} -2 \\${r2} -p ${task.cpus} 2>log_\\${ass}.txt | samtools view -@ ${task.cpus} -bS - >\\${ass}.bam\n                    rm *.bt2\n                    \"\"\"\n                } else {\n                    \"\"\"\n                    a=\\$( echo $files $files2 )\n                    ass=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \".combined.okay.fa\" )\n                    r1=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \"left-\" )\n                    r2=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \"right-\" )\n                    bowtie2-build \\${ass} \\${ass} --threads ${task.cpus}\n                    bowtie2 -x \\${ass} -1 \\${r1} -2 \\${r2} -p ${task.cpus} 2>log_\\${ass}.txt | samtools view -@ ${task.cpus} -bS - >\\${ass}.bam\n                    rm *.bam *.bt2\n                    \"\"\"\n                }",
        "nb_lignes_script": 20,
        "language_script": "bash",
        "tools": [
            "Rbowtie2",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/rbowtie2",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "Rbowtie2",
                "uri": "https://bio.tools/rbowtie2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence merging"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence splicing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package provides an R wrapper of the popular bowtie2 sequencing reads aligner and AdapterRemoval, a convenient tool for rapid adapter trimming, identification, and read merging.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/Rbowtie2.html"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "mapping_evi_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "mapping_evi_results",
            "mapping_evi_results_bam"
        ],
        "nb_outputs": 2,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'big_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/mapping\", mode: \"copy\", overwrite: true",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::bowtie2=2.4.2=py36hff7a194_2 bioconda::samtools=1.11=h6270b1f_0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-c742dccc9d8fabfcff2af0d8d6799dbc711366cf:b42a120d4ad2b0f6626127964d8bcc9ada05ed03-0\" : \"quay.io/biocontainers/mulled-v2-c742dccc9d8fabfcff2af0d8d6799dbc711366cf:b42a120d4ad2b0f6626127964d8bcc9ada05ed03-0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "busco4": {
        "name_process": "busco4",
        "string_process": " process busco4 {\n\n            label 'med_cpus'\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/busco4\", mode: \"copy\", overwrite: true, pattern: \"*.{bus4,bus4.txt,tsv}\"\n            publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n                                                      \n            conda (params.condaActivate && params.myConda ? params.cenv : params.condaActivate ? \"-c conda-forge bioconda::busco=4.1.4=py_2\" : null)\n            if (params.oneContainer){ container \"${params.v4container}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/busco:4.1.4--py_2\" : \"quay.io/biocontainers/busco:4.1.4--py_2\")\n            }\n\n            input:\n                tuple sample_id, file(\"${sample_id}.combined.okay.fa\") from evigene_ch_busco4\n\n            output:\n                tuple sample_id, file(\"${sample_id}.TransPi.bus4\") into busco4_ch\n                tuple sample_id, file(\"*${sample_id}.TransPi.bus4.txt\") into ( busco4_summary, busco4_comp_1 )\n                tuple sample_id, file(\"*tsv\") into busco4_transpi_tsv\n                file(\"busco4.version.txt\") into busco4_version\n\n            script:\n                \"\"\"\n                echo -e \"\\\\n-- Starting BUSCO --\\\\n\"\n\n                busco -i ${sample_id}.combined.okay.fa -o ${sample_id}.TransPi.bus4 -l ${params.busco4db} -m tran -c ${task.cpus} --offline\n\n                echo -e \"\\\\n-- DONE with BUSCO --\\\\n\"\n\n                cp ${sample_id}.TransPi.bus4/short_summary.*.${sample_id}.TransPi.bus4.txt .\n                cp ${sample_id}.TransPi.bus4/run_*/full_table.tsv full_table_${sample_id}.TransPi.bus4.tsv\n\n                v=\\$( busco -v | cut -f 2 -d \" \" )\n                echo \"BUSCO4: \\$v\" >busco4.version.txt\n                \"\"\"\n        }",
        "nb_lignes_process": 37,
        "string_script": "                \"\"\"\n                echo -e \"\\\\n-- Starting BUSCO --\\\\n\"\n\n                busco -i ${sample_id}.combined.okay.fa -o ${sample_id}.TransPi.bus4 -l ${params.busco4db} -m tran -c ${task.cpus} --offline\n\n                echo -e \"\\\\n-- DONE with BUSCO --\\\\n\"\n\n                cp ${sample_id}.TransPi.bus4/short_summary.*.${sample_id}.TransPi.bus4.txt .\n                cp ${sample_id}.TransPi.bus4/run_*/full_table.tsv full_table_${sample_id}.TransPi.bus4.tsv\n\n                v=\\$( busco -v | cut -f 2 -d \" \" )\n                echo \"BUSCO4: \\$v\" >busco4.version.txt\n                \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "BUSCO"
        ],
        "tools_url": [
            "https://bio.tools/busco"
        ],
        "tools_dico": [
            {
                "name": "BUSCO",
                "uri": "https://bio.tools/busco",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly validation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly quality evaluation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Assembly QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Assembly quality evaluation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly QC"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_1234",
                                "term": "Sequence set (nucleic acid)"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "Provides measures for quantitative assessment of genome assembly, gene set, and transcriptome completeness based on evolutionarily informed expectations of gene content from near-universal single-copy orthologs.",
                "homepage": "http://busco.ezlab.org/"
            }
        ],
        "inputs": [
            "evigene_ch_busco4"
        ],
        "nb_inputs": 1,
        "outputs": [
            "busco4_ch",
            "",
            "busco4_transpi_tsv",
            "busco4_version"
        ],
        "nb_outputs": 4,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/busco4\", mode: \"copy\", overwrite: true, pattern: \"*.{bus4,bus4.txt,tsv}\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.cenv : params.condaActivate ? \"-c conda-forge bioconda::busco=4.1.4=py_2\" : null) if (params.oneContainer){ container \"${params.v4container}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/busco:4.1.4--py_2\" : \"quay.io/biocontainers/busco:4.1.4--py_2\") }"
        ],
        "when": "",
        "stub": ""
    },
    "mapping_trinity": {
        "name_process": "mapping_trinity",
        "string_process": " process mapping_trinity {\n\n            label 'big_cpus'\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/mapping\", mode: \"copy\", overwrite: true, pattern: \"log*\"\n            publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::bowtie2=2.4.2=py36hff7a194_2 bioconda::samtools=1.11=h6270b1f_0\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-c742dccc9d8fabfcff2af0d8d6799dbc711366cf:b42a120d4ad2b0f6626127964d8bcc9ada05ed03-0\" : \"quay.io/biocontainers/mulled-v2-c742dccc9d8fabfcff2af0d8d6799dbc711366cf:b42a120d4ad2b0f6626127964d8bcc9ada05ed03-0\")\n            }\n\n            input:\n                tuple sample_id, file(files), file(files2) from mapping_trinity_in\n\n            output:\n                tuple sample_id, file(\"log*\") into mapping_trinity_results\n                file(\"bowtie2.version.txt\") into bowtie_version\n\n            script:\n                \"\"\"\n                a=\\$( echo $files $files2 )\n                ass=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \".Trinity.fa\" )\n                r1=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \"left-\" )\n                r2=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \"right-\" )\n                bowtie2-build \\${ass} \\${ass} --threads ${task.cpus}\n                bowtie2 -x \\${ass} -1 \\${r1} -2 \\${r2} -p ${task.cpus} 2>log_\\${ass}.txt | samtools view -@ ${task.cpus} -bS - >\\${ass}.bam\n                rm *.bam\n                v=\\$( bowtie2 --version | head -n1 | cut -f 3 -d \" \" )\n                echo \"Bowtie2: \\$v\" >bowtie2.version.txt\n                \"\"\"\n        }",
        "nb_lignes_process": 32,
        "string_script": "                \"\"\"\n                a=\\$( echo $files $files2 )\n                ass=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \".Trinity.fa\" )\n                r1=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \"left-\" )\n                r2=\\$( echo \\$a | tr \" \" \"\\\\n\" | grep \"right-\" )\n                bowtie2-build \\${ass} \\${ass} --threads ${task.cpus}\n                bowtie2 -x \\${ass} -1 \\${r1} -2 \\${r2} -p ${task.cpus} 2>log_\\${ass}.txt | samtools view -@ ${task.cpus} -bS - >\\${ass}.bam\n                rm *.bam\n                v=\\$( bowtie2 --version | head -n1 | cut -f 3 -d \" \" )\n                echo \"Bowtie2: \\$v\" >bowtie2.version.txt\n                \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [
            "Rbowtie2",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/rbowtie2",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "Rbowtie2",
                "uri": "https://bio.tools/rbowtie2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence merging"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence splicing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package provides an R wrapper of the popular bowtie2 sequencing reads aligner and AdapterRemoval, a convenient tool for rapid adapter trimming, identification, and read merging.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/Rbowtie2.html"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "mapping_trinity_in"
        ],
        "nb_inputs": 1,
        "outputs": [
            "mapping_trinity_results",
            "bowtie_version"
        ],
        "nb_outputs": 2,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'big_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/mapping\", mode: \"copy\", overwrite: true, pattern: \"log*\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::bowtie2=2.4.2=py36hff7a194_2 bioconda::samtools=1.11=h6270b1f_0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-c742dccc9d8fabfcff2af0d8d6799dbc711366cf:b42a120d4ad2b0f6626127964d8bcc9ada05ed03-0\" : \"quay.io/biocontainers/mulled-v2-c742dccc9d8fabfcff2af0d8d6799dbc711366cf:b42a120d4ad2b0f6626127964d8bcc9ada05ed03-0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "summary_evigene_individual": {
        "name_process": "summary_evigene_individual",
        "string_process": " process summary_evigene_individual {\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/stats\", mode: \"copy\", overwrite: true\n\n            input:\n                tuple sample_id, file(\"${sample_id}.combined.fa\"), file(\"${sample_id}.combined.okay.fa\") from evigene_summary\n\n            output:\n                tuple sample_id, file(\"${sample_id}_sum_preEG.txt\"), file(\"${sample_id}_sum_EG.txt\") into final_sum_1\n                tuple sample_id, file(\"*.csv\") into summary_evi_csv\n\n            script:\n                \"\"\"\n                #Summary of total number of transcripts\n                echo -e \"- Number of transcripts before Evidential Genes\\\\n\" >>${sample_id}_sum_preEG.txt\n                echo -e \"- Individual ${sample_id} \\\\n\" >>${sample_id}_sum_preEG.txt\n                echo -e \"\\\\t Total transcripts:\" >>${sample_id}_sum_preEG.txt\n                num=\\$( cat ${sample_id}.combined.fa | grep -c \">\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_preEG.txt\n                echo -e \"\\\\t Trinity\" >>${sample_id}_sum_preEG.txt\n                num=\\$( cat ${sample_id}.combined.fa | grep -c \">TRINITY\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_preEG.txt\n                echo -e \"\\\\t SOAP\" >>${sample_id}_sum_preEG.txt\n                num=\\$( cat ${sample_id}.combined.fa | grep -c \">SOAP\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_preEG.txt\n                echo -e \"\\\\t Velvet/Oases\" >>${sample_id}_sum_preEG.txt\n                num=\\$( cat ${sample_id}.combined.fa | grep -c \">Velvet\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_preEG.txt\n                echo -e \"\\\\t rna-SPADES\" >>${sample_id}_sum_preEG.txt\n                num=\\$( cat ${sample_id}.combined.fa | grep -c \">SPADES\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_preEG.txt\n                echo -e \"\\\\t Trans-ABySS\" >>${sample_id}_sum_preEG.txt\n                num=\\$( cat ${sample_id}.combined.fa | grep -c \">TransABySS\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_preEG.txt\n\n                # csv report\n                echo \"Total,Trinity,SOAP,Velvet,SPADES,TransBySS\" >${sample_id}_sum_preEG.csv\n                total=\\$( cat ${sample_id}.combined.fa | grep -c \">\" )\n                trinity=\\$( cat ${sample_id}.combined.fa | grep -c \">TRINITY\" )\n                soap=\\$( cat ${sample_id}.combined.fa | grep -c \">SOAP\" )\n                velvet=\\$( cat ${sample_id}.combined.fa | grep -c \">Velvet\" )\n                spades=\\$( cat ${sample_id}.combined.fa | grep -c \">SPADES\" )\n                transabyss=\\$( cat ${sample_id}.combined.fa | grep -c \">TransABySS\" )\n                echo \"\\${total},\\${trinity},\\${soap},\\${velvet},\\${spades},\\${transabyss}\" >>${sample_id}_sum_preEG.csv\n\n                #Summary of transcripts after EvidentialGenes\n                echo -e \"- Number of transcripts by individual after EvidentialGenes\\\\n\" >>${sample_id}_sum_EG.txt\n                echo -e \"- Individual ${sample_id} \\\\n\" >>${sample_id}_sum_EG.txt\n                echo -e \"\\\\t Total transcripts:\" >>${sample_id}_sum_EG.txt\n                num=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_EG.txt\n                echo -e \"\\\\t Trinity\" >>${sample_id}_sum_EG.txt\n                num=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">TRINITY\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_EG.txt\n                echo -e \"\\\\t SOAP\" >>${sample_id}_sum_EG.txt\n                num=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">SOAP\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_EG.txt\n                echo -e \"\\\\t Velvet/Oases\" >>${sample_id}_sum_EG.txt\n                num=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">Velvet\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_EG.txt\n                echo -e \"\\\\t rna-SPADES\" >>${sample_id}_sum_EG.txt\n                num=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">SPADES\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_EG.txt\n                echo -e \"\\\\t Trans-ABySS\" >>${sample_id}_sum_EG.txt\n                num=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">TransABySS\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_EG.txt\n\n                # csv report after evigene\n                echo \"Total,Trinity,SOAP,Velvet,SPADES,TransBySS\" >${sample_id}_sum_EG.csv\n                total=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">\" )\n                trinity=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">TRINITY\" )\n                soap=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">SOAP\" )\n                velvet=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">Velvet\" )\n                spades=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">SPADES\" )\n                transabyss=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">TransABySS\" )\n                echo \"\\${total},\\${trinity},\\${soap},\\${velvet},\\${spades},\\${transabyss}\" >>${sample_id}_sum_EG.csv\n                \"\"\"\n        }",
        "nb_lignes_process": 78,
        "string_script": "                \"\"\"\n                #Summary of total number of transcripts\n                echo -e \"- Number of transcripts before Evidential Genes\\\\n\" >>${sample_id}_sum_preEG.txt\n                echo -e \"- Individual ${sample_id} \\\\n\" >>${sample_id}_sum_preEG.txt\n                echo -e \"\\\\t Total transcripts:\" >>${sample_id}_sum_preEG.txt\n                num=\\$( cat ${sample_id}.combined.fa | grep -c \">\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_preEG.txt\n                echo -e \"\\\\t Trinity\" >>${sample_id}_sum_preEG.txt\n                num=\\$( cat ${sample_id}.combined.fa | grep -c \">TRINITY\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_preEG.txt\n                echo -e \"\\\\t SOAP\" >>${sample_id}_sum_preEG.txt\n                num=\\$( cat ${sample_id}.combined.fa | grep -c \">SOAP\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_preEG.txt\n                echo -e \"\\\\t Velvet/Oases\" >>${sample_id}_sum_preEG.txt\n                num=\\$( cat ${sample_id}.combined.fa | grep -c \">Velvet\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_preEG.txt\n                echo -e \"\\\\t rna-SPADES\" >>${sample_id}_sum_preEG.txt\n                num=\\$( cat ${sample_id}.combined.fa | grep -c \">SPADES\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_preEG.txt\n                echo -e \"\\\\t Trans-ABySS\" >>${sample_id}_sum_preEG.txt\n                num=\\$( cat ${sample_id}.combined.fa | grep -c \">TransABySS\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_preEG.txt\n\n                # csv report\n                echo \"Total,Trinity,SOAP,Velvet,SPADES,TransBySS\" >${sample_id}_sum_preEG.csv\n                total=\\$( cat ${sample_id}.combined.fa | grep -c \">\" )\n                trinity=\\$( cat ${sample_id}.combined.fa | grep -c \">TRINITY\" )\n                soap=\\$( cat ${sample_id}.combined.fa | grep -c \">SOAP\" )\n                velvet=\\$( cat ${sample_id}.combined.fa | grep -c \">Velvet\" )\n                spades=\\$( cat ${sample_id}.combined.fa | grep -c \">SPADES\" )\n                transabyss=\\$( cat ${sample_id}.combined.fa | grep -c \">TransABySS\" )\n                echo \"\\${total},\\${trinity},\\${soap},\\${velvet},\\${spades},\\${transabyss}\" >>${sample_id}_sum_preEG.csv\n\n                #Summary of transcripts after EvidentialGenes\n                echo -e \"- Number of transcripts by individual after EvidentialGenes\\\\n\" >>${sample_id}_sum_EG.txt\n                echo -e \"- Individual ${sample_id} \\\\n\" >>${sample_id}_sum_EG.txt\n                echo -e \"\\\\t Total transcripts:\" >>${sample_id}_sum_EG.txt\n                num=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_EG.txt\n                echo -e \"\\\\t Trinity\" >>${sample_id}_sum_EG.txt\n                num=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">TRINITY\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_EG.txt\n                echo -e \"\\\\t SOAP\" >>${sample_id}_sum_EG.txt\n                num=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">SOAP\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_EG.txt\n                echo -e \"\\\\t Velvet/Oases\" >>${sample_id}_sum_EG.txt\n                num=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">Velvet\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_EG.txt\n                echo -e \"\\\\t rna-SPADES\" >>${sample_id}_sum_EG.txt\n                num=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">SPADES\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_EG.txt\n                echo -e \"\\\\t Trans-ABySS\" >>${sample_id}_sum_EG.txt\n                num=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">TransABySS\" )\n                echo -e \"\\\\t\\\\t \\$num \\\\n\" >>${sample_id}_sum_EG.txt\n\n                # csv report after evigene\n                echo \"Total,Trinity,SOAP,Velvet,SPADES,TransBySS\" >${sample_id}_sum_EG.csv\n                total=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">\" )\n                trinity=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">TRINITY\" )\n                soap=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">SOAP\" )\n                velvet=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">Velvet\" )\n                spades=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">SPADES\" )\n                transabyss=\\$( cat ${sample_id}.combined.okay.fa | grep -c \">TransABySS\" )\n                echo \"\\${total},\\${trinity},\\${soap},\\${velvet},\\${spades},\\${transabyss}\" >>${sample_id}_sum_EG.csv\n                \"\"\"",
        "nb_lignes_script": 64,
        "language_script": "bash",
        "tools": [
            "Trinity",
            "SOAP",
            "Velvet",
            "SPAdes"
        ],
        "tools_url": [
            "https://bio.tools/trinity",
            "https://bio.tools/soap",
            "https://bio.tools/velvet",
            "https://bio.tools/spades"
        ],
        "tools_dico": [
            {
                "name": "Trinity",
                "uri": "https://bio.tools/trinity",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "Gene transcripts"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "mRNA features"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3258",
                                    "term": "Transcriptome assembly"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Trinity is a transcriptome assembler which relies on three different tools, inchworm an assembler, chrysalis which pools contigs and butterfly which amongst others compacts a graph resulting from butterfly with reads.",
                "homepage": "https://github.com/trinityrnaseq/trinityrnaseq/wiki"
            },
            {
                "name": "SOAP",
                "uri": "https://bio.tools/soap",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0292",
                                    "term": "Sequence alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Short Oligonucleotide Analysis Package. Efficient gapped and ungapped alignment of short oligonucleotides onto reference sequences and full solution to next generation sequencing data analysis.",
                "homepage": "http://soap.genomics.org.cn/"
            },
            {
                "name": "Velvet",
                "uri": "https://bio.tools/velvet",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0524",
                                    "term": "De-novo assembly"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0524",
                                    "term": "De Bruijn graph"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0524",
                                    "term": "Sequence assembly (de-novo assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2337",
                                "term": "Resource metadata"
                            },
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0925",
                                "term": "Sequence assembly"
                            },
                            {
                                "uri": "http://edamontology.org/data_2044",
                                "term": "Sequence"
                            },
                            {
                                "uri": "http://edamontology.org/data_2337",
                                "term": "Resource metadata"
                            }
                        ]
                    }
                ],
                "description": "A de novo genomic assembler specially designed for short read sequencing technologies, such as Solexa or 454 or SOLiD.",
                "homepage": "https://github.com/dzerbino/velvet"
            },
            {
                "name": "SPAdes",
                "uri": "https://bio.tools/spades",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            },
                            {
                                "uri": "http://edamontology.org/data_0006",
                                "term": "Data"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0006",
                                "term": "Data"
                            },
                            {
                                "uri": "http://edamontology.org/data_0863",
                                "term": "Sequence alignment"
                            }
                        ]
                    }
                ],
                "description": "St. Petersburg genome assembler \u2013 is intended for both standard isolates and single-cell MDA bacteria assemblies. SPAdes 3.9 works with Illumina or IonTorrent reads and is capable of providing hybrid assemblies using PacBio, Oxford Nanopore and Sanger reads. Additional contigs can be provided and can be used as long reads.",
                "homepage": "http://cab.spbu.ru/software/spades/"
            }
        ],
        "inputs": [
            "evigene_summary"
        ],
        "nb_inputs": 1,
        "outputs": [
            "final_sum_1",
            "summary_evi_csv"
        ],
        "nb_outputs": 2,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/stats\", mode: \"copy\", overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "busco4_all": {
        "name_process": "busco4_all",
        "string_process": " process busco4_all {\n\n                label 'med_cpus'\n\n                tag \"${sample_id}\"\n\n                publishDir \"${params.outdir}/busco4_all\", mode: \"copy\", overwrite: true, pattern: \"*.{tsv,txt,bus4}\"\n\n                                                          \n                conda (params.condaActivate && params.myConda ? params.cenv : params.condaActivate ? \"-c conda-forge bioconda::busco=4.1.4=py_2\" : null)\n                if (params.oneContainer){ container \"${params.v4container}\" } else {\n                container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/busco:4.1.4--py_2\" : \"ezlabgva/busco:v4.0.5_cv1\")\n                }\n\n                input:\n                    tuple sample_id, file(files) from busco4_all\n\n                output:\n                    tuple sample_id, file(\"*.bus4\") into busco4_all_ch\n                    tuple sample_id, file(\"*.Trinity.bus4.txt\") into ( busco4_ch_trinity_sum, busco4_comp_2 )\n                    tuple sample_id, file(\"*.txt\"), file(\"*.tsv\") into busco4_all_sum_ch\n                    tuple sample_id, file(\"${sample_id}_all_busco4.tsv\"), file(\"${sample_id}_all_assemblers.fa\") into busco4_all_tsv\n\n                script:\n                    \"\"\"\n                    cat input.1 | sed 's/, /\\\\n/g' | tr -d \"[\" | tr \"]\" \"\\\\n\" >list.txt\n                    cat input.2 | sed 's/, /\\\\n/g' | tr -d \"[\" | tr \"]\" \"\\\\n\" >>list.txt\n                    cat input.3 | sed 's/, /\\\\n/g' | tr -d \"[\" | tr \"]\" \"\\\\n\" >>list.txt\n                    cat input.4 | sed 's/, /\\\\n/g' | tr -d \"[\" | tr \"]\" \"\\\\n\" >>list.txt\n\n                    for x in `cat list.txt`;do\n                        ln -s \\$x \\$( basename \\$x )\n                    done\n\n                    find . -maxdepth 1 -type l -ls | grep \"Trinity\" | awk -F \"-> \" '{print \\$2}' >>list.txt\n\n                    for x in `cat list.txt`;do\n\n                        name=\\$( basename \\$x .fa )\n\n                        echo -e \"\\\\n-- Starting BUSCO --\\\\n\"\n\n                        busco -i \\${name}.fa -o \\${name}.bus4 -l ${params.busco4db} -m tran -c ${task.cpus} --offline\n\n                        cp \\${name}.bus4/short_summary.* .\n                        cp \\${name}.bus4/run_*/full_table.tsv full_table_\\${name}.tsv\n\n                        echo -e \"\\\\n-- DONE with BUSCO --\\\\n\"\n\n                    done\n\n                    echo \"Busco_id,Status,Sequence,Score,Length\" >.header.txt\n                    cat full_table_*.tsv | grep -v \"#\" | tr \"\\t\" \",\" >.busco_names.txt\n                    cat .header.txt .busco_names.txt >${sample_id}_all_busco4.tsv\n                    rm .header.txt .busco_names.txt\n\n                    cat *.fa >${sample_id}_all_assemblers.fa\n                    \"\"\"\n            }",
        "nb_lignes_process": 57,
        "string_script": "                    \"\"\"\n                    cat input.1 | sed 's/, /\\\\n/g' | tr -d \"[\" | tr \"]\" \"\\\\n\" >list.txt\n                    cat input.2 | sed 's/, /\\\\n/g' | tr -d \"[\" | tr \"]\" \"\\\\n\" >>list.txt\n                    cat input.3 | sed 's/, /\\\\n/g' | tr -d \"[\" | tr \"]\" \"\\\\n\" >>list.txt\n                    cat input.4 | sed 's/, /\\\\n/g' | tr -d \"[\" | tr \"]\" \"\\\\n\" >>list.txt\n\n                    for x in `cat list.txt`;do\n                        ln -s \\$x \\$( basename \\$x )\n                    done\n\n                    find . -maxdepth 1 -type l -ls | grep \"Trinity\" | awk -F \"-> \" '{print \\$2}' >>list.txt\n\n                    for x in `cat list.txt`;do\n\n                        name=\\$( basename \\$x .fa )\n\n                        echo -e \"\\\\n-- Starting BUSCO --\\\\n\"\n\n                        busco -i \\${name}.fa -o \\${name}.bus4 -l ${params.busco4db} -m tran -c ${task.cpus} --offline\n\n                        cp \\${name}.bus4/short_summary.* .\n                        cp \\${name}.bus4/run_*/full_table.tsv full_table_\\${name}.tsv\n\n                        echo -e \"\\\\n-- DONE with BUSCO --\\\\n\"\n\n                    done\n\n                    echo \"Busco_id,Status,Sequence,Score,Length\" >.header.txt\n                    cat full_table_*.tsv | grep -v \"#\" | tr \"\\t\" \",\" >.busco_names.txt\n                    cat .header.txt .busco_names.txt >${sample_id}_all_busco4.tsv\n                    rm .header.txt .busco_names.txt\n\n                    cat *.fa >${sample_id}_all_assemblers.fa\n                    \"\"\"",
        "nb_lignes_script": 33,
        "language_script": "bash",
        "tools": [
            "BUSCO"
        ],
        "tools_url": [
            "https://bio.tools/busco"
        ],
        "tools_dico": [
            {
                "name": "BUSCO",
                "uri": "https://bio.tools/busco",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly validation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly quality evaluation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Assembly QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Assembly quality evaluation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly QC"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_1234",
                                "term": "Sequence set (nucleic acid)"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "Provides measures for quantitative assessment of genome assembly, gene set, and transcriptome completeness based on evolutionarily informed expectations of gene content from near-universal single-copy orthologs.",
                "homepage": "http://busco.ezlab.org/"
            }
        ],
        "inputs": [
            "busco4_all"
        ],
        "nb_inputs": 1,
        "outputs": [
            "busco4_all_ch",
            "",
            "busco4_all_sum_ch",
            "busco4_all_tsv"
        ],
        "nb_outputs": 4,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/busco4_all\", mode: \"copy\", overwrite: true, pattern: \"*.{tsv,txt,bus4}\"",
            "conda (params.condaActivate && params.myConda ? params.cenv : params.condaActivate ? \"-c conda-forge bioconda::busco=4.1.4=py_2\" : null) if (params.oneContainer){ container \"${params.v4container}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/busco:4.1.4--py_2\" : \"ezlabgva/busco:v4.0.5_cv1\") }"
        ],
        "when": "",
        "stub": ""
    },
    "busco4_tri": {
        "name_process": "busco4_tri",
        "string_process": " process busco4_tri {\n\n                label 'med_cpus'\n\n                tag \"${sample_id}\"\n\n                publishDir \"${params.outdir}/busco4\", mode: \"copy\", overwrite: true\n\n                                                          \n                conda (params.condaActivate && params.myConda ? params.cenv : params.condaActivate ? \"-c conda-forge bioconda::busco=4.1.4=py_2\" : null)\n                if (params.oneContainer){ container \"${params.v4container}\" } else {\n                container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/busco:4.1.4--py_2\" : \"ezlabgva/busco:v4.0.5_cv1\")\n                }\n\n                input:\n                    tuple sample_id, file(\"${sample_id}.Trinity.fa\") from busco4_ch_trinity\n\n                output:\n                    tuple sample_id, file(\"*${sample_id}.Trinity.bus4.txt\") into ( busco4_ch_trinity_sum, busco4_comp_2 )\n                    file(\"${sample_id}.Trinity.bus4\")\n                    tuple sample_id, file(\"*tsv\") into busco4_trinity_rescue\n\n                script:\n                    \"\"\"\n                    echo -e \"\\\\n-- Starting BUSCO --\\\\n\"\n\n                    busco -i ${sample_id}.Trinity.fa -o ${sample_id}.Trinity.bus4 -l ${params.busco4db} -m tran -c ${task.cpus} --offline\n\n                    echo -e \"\\\\n-- DONE with BUSCO --\\\\n\"\n\n                    cp ${sample_id}.Trinity.bus4/short_summary.*.${sample_id}.Trinity.bus4.txt .\n                    cp ${sample_id}.Trinity.bus4/run_*/full_table.tsv full_table_${sample_id}.Trinity.bus4.tsv\n                    \"\"\"\n            }",
        "nb_lignes_process": 32,
        "string_script": "                    \"\"\"\n                    echo -e \"\\\\n-- Starting BUSCO --\\\\n\"\n\n                    busco -i ${sample_id}.Trinity.fa -o ${sample_id}.Trinity.bus4 -l ${params.busco4db} -m tran -c ${task.cpus} --offline\n\n                    echo -e \"\\\\n-- DONE with BUSCO --\\\\n\"\n\n                    cp ${sample_id}.Trinity.bus4/short_summary.*.${sample_id}.Trinity.bus4.txt .\n                    cp ${sample_id}.Trinity.bus4/run_*/full_table.tsv full_table_${sample_id}.Trinity.bus4.tsv\n                    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [
            "BUSCO"
        ],
        "tools_url": [
            "https://bio.tools/busco"
        ],
        "tools_dico": [
            {
                "name": "BUSCO",
                "uri": "https://bio.tools/busco",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly validation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly quality evaluation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Assembly QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Assembly quality evaluation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly QC"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_1234",
                                "term": "Sequence set (nucleic acid)"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "Provides measures for quantitative assessment of genome assembly, gene set, and transcriptome completeness based on evolutionarily informed expectations of gene content from near-universal single-copy orthologs.",
                "homepage": "http://busco.ezlab.org/"
            }
        ],
        "inputs": [
            "busco4_ch_trinity"
        ],
        "nb_inputs": 1,
        "outputs": [
            "",
            "busco4_trinity_rescue"
        ],
        "nb_outputs": 2,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/busco4\", mode: \"copy\", overwrite: true",
            "conda (params.condaActivate && params.myConda ? params.cenv : params.condaActivate ? \"-c conda-forge bioconda::busco=4.1.4=py_2\" : null) if (params.oneContainer){ container \"${params.v4container}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/busco:4.1.4--py_2\" : \"ezlabgva/busco:v4.0.5_cv1\") }"
        ],
        "when": "",
        "stub": ""
    },
    "busco4_dist": {
        "name_process": "busco4_dist",
        "string_process": " process busco4_dist {\n\n                label 'low_cpus'\n\n                tag \"${sample_id}\"\n\n                publishDir \"${params.outdir}/busco4_dist\", mode: \"copy\", overwrite: true, pattern: \"*.{tsv,fasta}\"\n\n                conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda biopython=1.78 pandas=1.1.2 numpy=1.18.1\" : null)\n                if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-1e9d4f78feac0eb2c8d8246367973b3f6358defc:41ffac721ff9b03ca1121742e969d0e7d78e589f-0\" : \"quay.io/biocontainers/mulled-v2-1e9d4f78feac0eb2c8d8246367973b3f6358defc:41ffac721ff9b03ca1121742e969d0e7d78e589f-0\")\n                }\n\n                input:\n                    tuple sample_id, file(transpi_tsv), file(all_busco), file(assembly) from busco4_dist_ch\n\n                output:\n                    tuple sample_id, file(\"*.fasta\"), file(\"*_table.tsv\") into busco4_dist_sum\n                    tuple sample_id, file(\"*_table.tsv\") into busco4_heatmap\n\n                script:\n                    \"\"\"\n                    cat $transpi_tsv | grep -v \"#\" | tr \"\\\\t\" \",\" >>$all_busco\n                    SOS_busco.py -input_file_busco $all_busco -input_file_fasta $assembly -min ${params.minPerc} -kmers ${params.k}\n                    mv Complete_comparison_table ${sample_id}_complete_BUSCO4_table.tsv\n                    mv TransPi_comparison_table ${sample_id}_TransPi_missing_BUSCO4_table.tsv\n                    if [ -e sequences_to_add.fasta ];then\n                        mv sequences_to_add.fasta ${sample_id}_rescued_BUSCO4.fasta\n                    else\n                        touch ${sample_id}_rescued_BUSCO4.fasta\n                    fi\n                    \"\"\"\n\n            }",
        "nb_lignes_process": 32,
        "string_script": "                    \"\"\"\n                    cat $transpi_tsv | grep -v \"#\" | tr \"\\\\t\" \",\" >>$all_busco\n                    SOS_busco.py -input_file_busco $all_busco -input_file_fasta $assembly -min ${params.minPerc} -kmers ${params.k}\n                    mv Complete_comparison_table ${sample_id}_complete_BUSCO4_table.tsv\n                    mv TransPi_comparison_table ${sample_id}_TransPi_missing_BUSCO4_table.tsv\n                    if [ -e sequences_to_add.fasta ];then\n                        mv sequences_to_add.fasta ${sample_id}_rescued_BUSCO4.fasta\n                    else\n                        touch ${sample_id}_rescued_BUSCO4.fasta\n                    fi\n                    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "busco4_dist_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "busco4_dist_sum",
            "busco4_heatmap"
        ],
        "nb_outputs": 2,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'low_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/busco4_dist\", mode: \"copy\", overwrite: true, pattern: \"*.{tsv,fasta}\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda biopython=1.78 pandas=1.1.2 numpy=1.18.1\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-1e9d4f78feac0eb2c8d8246367973b3f6358defc:41ffac721ff9b03ca1121742e969d0e7d78e589f-0\" : \"quay.io/biocontainers/mulled-v2-1e9d4f78feac0eb2c8d8246367973b3f6358defc:41ffac721ff9b03ca1121742e969d0e7d78e589f-0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "skip_busco_dist": {
        "name_process": "skip_busco_dist",
        "string_process": " process skip_busco_dist {\n                tag \"${sample_id}\"\n\n                input:\n                    tuple sample_id, file(files) from skip_busco_dist\n\n                output:\n                    tuple sample_id, file(\"busco4_dist.txt\") into busco4_heatmap\n\n                script:\n                    \"\"\"\n                    echo \"BUSCO4 distribution analysis was skipped\" >busco4_dist.txt\n                    \"\"\"\n            }",
        "nb_lignes_process": 12,
        "string_script": "                    \"\"\"\n                    echo \"BUSCO4 distribution analysis was skipped\" >busco4_dist.txt\n                    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "skip_busco_dist"
        ],
        "nb_inputs": 1,
        "outputs": [
            "busco4_heatmap"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "summary_busco4_individual": {
        "name_process": "summary_busco4_individual",
        "string_process": " process summary_busco4_individual {\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/stats\", mode: \"copy\", overwrite: true\n\n            input:\n                tuple sample_id, file(files) from busco4_sum\n\n            output:\n                tuple sample_id, file(\"${sample_id}.sum_busco4.txt\") into final_sum_2v4\n\n            script:\n                \"\"\"\n                tri=\\$( echo $files | tr \" \" \"\\\\n\" | grep \".Trinity.bus4.txt\" )\n                trans=\\$( echo $files | tr \" \" \"\\\\n\" | grep \".TransPi.bus4.txt\" )\n                #Summary of BUSCO scores for the final_assemblies\n                echo -e \"Summary of BUSCO V4 \\\\n\" >>${sample_id}.sum_busco4.txt\n                echo \"-- TransPi BUSCO V4 scores -- \" >>${sample_id}.sum_busco4.txt\n                cat \\${trans} >>${sample_id}.sum_busco4.txt\n                echo -e \"\\\\n-- Trinity BUSCO V4 scores --\" >>${sample_id}.sum_busco4.txt\n                cat \\${tri} >>${sample_id}.sum_busco4.txt\n                \"\"\"\n        }",
        "nb_lignes_process": 22,
        "string_script": "                \"\"\"\n                tri=\\$( echo $files | tr \" \" \"\\\\n\" | grep \".Trinity.bus4.txt\" )\n                trans=\\$( echo $files | tr \" \" \"\\\\n\" | grep \".TransPi.bus4.txt\" )\n                #Summary of BUSCO scores for the final_assemblies\n                echo -e \"Summary of BUSCO V4 \\\\n\" >>${sample_id}.sum_busco4.txt\n                echo \"-- TransPi BUSCO V4 scores -- \" >>${sample_id}.sum_busco4.txt\n                cat \\${trans} >>${sample_id}.sum_busco4.txt\n                echo -e \"\\\\n-- Trinity BUSCO V4 scores --\" >>${sample_id}.sum_busco4.txt\n                cat \\${tri} >>${sample_id}.sum_busco4.txt\n                \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "busco4_sum"
        ],
        "nb_inputs": 1,
        "outputs": [
            "final_sum_2v4"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/stats\", mode: \"copy\", overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "get_busco4_comparison": {
        "name_process": "get_busco4_comparison",
        "string_process": " process get_busco4_comparison {\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/figures/BUSCO4\", mode: \"copy\", overwrite: true\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda r-reshape2=1.4.4 r-plotly=4.9.2.1 plotly-orca=3.4.2 r-ggplot2=3.3.0 r-svglite=1.2.3 r-ggthemes=4.2.0 r-knitr=1.29 r-rmarkdown=2.3 r-kableextra=1.1.0\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\" : \"quay.io/biocontainers/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\")\n            }\n\n            input:\n                tuple sample_id, file(files) from busco4_comp\n\n            output:\n                tuple sample_id, file(\"${sample_id}_BUSCO4_comparison.pdf\"), file(\"${sample_id}_BUSCO4_comparison.svg\") into busco4_fig\n                tuple sample_id, file(\"*.csv\") into busco4_csv\n\n            script:\n                \"\"\"\n                set +e\n                tri=\\$( echo $files | tr \" \" \"\\\\n\" | grep \".Trinity.bus4.txt\" )\n                trans=\\$( echo $files | tr \" \" \"\\\\n\" | grep \".TransPi.bus4.txt\" )\n                bash get_busco_val.sh \\${tri} \\${trans} v4 ${sample_id}\n                cp ${params.pipeInstall}/bin/busco_comparison.R .\n                a=\\$( cat final_spec )\n                sed -i \"s/MYSPEC/\\${a}/\" busco_comparison.R\n                b=\\$( cat final_perc )\n                sed -i \"s/MYPERC/\\${b}/\" busco_comparison.R\n                c=\\$( cat final_num )\n                sed -i \"s/MYVAL/\\${c}/\" busco_comparison.R\n                Rscript busco_comparison.R ${sample_id}\n                mv ${sample_id}_BUSCO_comparison.pdf ${sample_id}_BUSCO4_comparison.pdf\n                mv ${sample_id}_BUSCO_comparison.svg ${sample_id}_BUSCO4_comparison.svg\n                # csv\n                sed -i 's/\\$/\\\\n/g' final_*\n                cat final_spec final_perc final_num | tr -d \"'\" >${sample_id}_busco4.csv\n                \"\"\"\n        }",
        "nb_lignes_process": 37,
        "string_script": "                \"\"\"\n                set +e\n                tri=\\$( echo $files | tr \" \" \"\\\\n\" | grep \".Trinity.bus4.txt\" )\n                trans=\\$( echo $files | tr \" \" \"\\\\n\" | grep \".TransPi.bus4.txt\" )\n                bash get_busco_val.sh \\${tri} \\${trans} v4 ${sample_id}\n                cp ${params.pipeInstall}/bin/busco_comparison.R .\n                a=\\$( cat final_spec )\n                sed -i \"s/MYSPEC/\\${a}/\" busco_comparison.R\n                b=\\$( cat final_perc )\n                sed -i \"s/MYPERC/\\${b}/\" busco_comparison.R\n                c=\\$( cat final_num )\n                sed -i \"s/MYVAL/\\${c}/\" busco_comparison.R\n                Rscript busco_comparison.R ${sample_id}\n                mv ${sample_id}_BUSCO_comparison.pdf ${sample_id}_BUSCO4_comparison.pdf\n                mv ${sample_id}_BUSCO_comparison.svg ${sample_id}_BUSCO4_comparison.svg\n                # csv\n                sed -i 's/\\$/\\\\n/g' final_*\n                cat final_spec final_perc final_num | tr -d \"'\" >${sample_id}_busco4.csv\n                \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "busco4_comp"
        ],
        "nb_inputs": 1,
        "outputs": [
            "busco4_fig",
            "busco4_csv"
        ],
        "nb_outputs": 2,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/figures/BUSCO4\", mode: \"copy\", overwrite: true",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda r-reshape2=1.4.4 r-plotly=4.9.2.1 plotly-orca=3.4.2 r-ggplot2=3.3.0 r-svglite=1.2.3 r-ggthemes=4.2.0 r-knitr=1.29 r-rmarkdown=2.3 r-kableextra=1.1.0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\" : \"quay.io/biocontainers/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "transdecoder_short": {
        "name_process": "transdecoder_short",
        "string_process": " process transdecoder_short {\n\n                label 'med_cpus'\n\n                tag \"${sample_id}\"\n\n                publishDir \"${params.outdir}/transdecoder\", mode: \"copy\", overwrite: true, pattern: \"*.{csv,stats,cds,gff,bed,pep}\"\n                publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n                conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::transdecoder=5.5.0=pl526_2\" : null)\n                if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/transdecoder:5.5.0--pl526_2\" : \"quay.io/biocontainers/transdecoder:5.5.0--pl526_2\")\n                }\n\n                input:\n                    tuple sample_id, file(assembly) from annotation_ch_transdecoder\n\n                output:\n                    tuple sample_id, file(\"${sample_id}*.transdecoder.pep\") into ( transdecoder_ch_hmmer, transdecoder_ch_signalp, transdecoder_ch_tmhmm, transdecoder_ch_trinotate )\n                    tuple sample_id, file(\"${assembly}\"), file(\"${sample_id}*.transdecoder.pep\") into ( transdecoder_ch_diamond, transdecoder_ch_diamond_custom )\n                    tuple sample_id, file(\"${sample_id}_transdecoder.stats\") into transdecoder_summary\n                    tuple sample_id, file(\"${sample_id}_transdecoder.csv\") into transdecoder_csv\n                    tuple sample_id, file(\"${sample_id}*.transdecoder.{cds,gff,bed}\") into transdecoder_files\n                    file(\"transdecoder.version.txt\") into transdecoder_version\n\n                script:\n                    \"\"\"\n                    echo -e \"\\\\n-- TransDecoder.LongOrfs... --\\\\n\"\n\n                    TransDecoder.LongOrfs -t ${assembly} --output_dir ${sample_id}.transdecoder_dir -G ${params.genCode}\n\n                    echo -e \"\\\\n-- Done with TransDecoder.LongOrfs --\\\\n\"\n\n                    echo -e \"\\\\n-- TransDecoder.Predict... --\\\\n\"\n\n                    TransDecoder.Predict -t ${assembly} --output_dir ${sample_id}.transdecoder_dir -G ${params.genCode}\n\n                    echo -e \"\\\\n-- Done with TransDecoder.Predict --\\\\n\"\n\n                    echo -e \"\\\\n-- Calculating statistics... --\\\\n\"\n                    #Calculate statistics of Transdecoder\n                    echo \"- Transdecoder (short,no homolgy) stats for ${sample_id}\" >${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \">\" )\n                    echo -e \"Total number of ORFs: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep| grep -c \"ORF type:complete\" )\n                    echo -e \"\\\\t ORFs type=complete: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep| grep -c \"ORF type:5prime_partial\" )\n                    echo -e \"\\\\t ORFs type=5prime_partial: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \"ORF type:3prime_partial\" )\n                    echo -e \"\\\\t ORFs type=3prime_partial: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \"ORF type:internal\" )\n                    echo -e \"\\\\t ORFs type=internal: \\$orfnum \\\\n\">>${sample_id}_transdecoder.stats\n                    # csv for report\n                    echo \"Sample,Total_orf,orf_complete,orf_5prime_partial,orf_3prime_partial,orf_internal\" >${sample_id}_transdecoder.csv\n                    total=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \">\" )\n                    complete=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:complete\" )\n                    n5prime=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:5prime_partial\" )\n                    n3prime=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:3prime_partial\" )\n                    internal=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:internal\" )\n                    echo \"${sample_id},\\${total},\\${complete},\\${n5prime},\\${n3prime},\\${internal}\" >>${sample_id}_transdecoder.csv\n                    echo -e \"\\\\n-- Done with statistics --\\\\n\"\n\n                    cp ${assembly} ${assembly}.tmp\n                    rm ${assembly}\n                    mv ${assembly}.tmp ${assembly}\n\n                    echo -e \"\\\\n-- DONE with TransDecoder --\\\\n\"\n\n                    v=\\$( TransDecoder.LongOrfs --version | cut -f 2 -d \" \" )\n                    echo \"Transdecoder: \\$v\" >transdecoder.version.txt\n                    \"\"\"\n                }",
        "nb_lignes_process": 70,
        "string_script": "                    \"\"\"\n                    echo -e \"\\\\n-- TransDecoder.LongOrfs... --\\\\n\"\n\n                    TransDecoder.LongOrfs -t ${assembly} --output_dir ${sample_id}.transdecoder_dir -G ${params.genCode}\n\n                    echo -e \"\\\\n-- Done with TransDecoder.LongOrfs --\\\\n\"\n\n                    echo -e \"\\\\n-- TransDecoder.Predict... --\\\\n\"\n\n                    TransDecoder.Predict -t ${assembly} --output_dir ${sample_id}.transdecoder_dir -G ${params.genCode}\n\n                    echo -e \"\\\\n-- Done with TransDecoder.Predict --\\\\n\"\n\n                    echo -e \"\\\\n-- Calculating statistics... --\\\\n\"\n                    #Calculate statistics of Transdecoder\n                    echo \"- Transdecoder (short,no homolgy) stats for ${sample_id}\" >${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \">\" )\n                    echo -e \"Total number of ORFs: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep| grep -c \"ORF type:complete\" )\n                    echo -e \"\\\\t ORFs type=complete: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep| grep -c \"ORF type:5prime_partial\" )\n                    echo -e \"\\\\t ORFs type=5prime_partial: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \"ORF type:3prime_partial\" )\n                    echo -e \"\\\\t ORFs type=3prime_partial: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \"ORF type:internal\" )\n                    echo -e \"\\\\t ORFs type=internal: \\$orfnum \\\\n\">>${sample_id}_transdecoder.stats\n                    # csv for report\n                    echo \"Sample,Total_orf,orf_complete,orf_5prime_partial,orf_3prime_partial,orf_internal\" >${sample_id}_transdecoder.csv\n                    total=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \">\" )\n                    complete=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:complete\" )\n                    n5prime=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:5prime_partial\" )\n                    n3prime=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:3prime_partial\" )\n                    internal=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:internal\" )\n                    echo \"${sample_id},\\${total},\\${complete},\\${n5prime},\\${n3prime},\\${internal}\" >>${sample_id}_transdecoder.csv\n                    echo -e \"\\\\n-- Done with statistics --\\\\n\"\n\n                    cp ${assembly} ${assembly}.tmp\n                    rm ${assembly}\n                    mv ${assembly}.tmp ${assembly}\n\n                    echo -e \"\\\\n-- DONE with TransDecoder --\\\\n\"\n\n                    v=\\$( TransDecoder.LongOrfs --version | cut -f 2 -d \" \" )\n                    echo \"Transdecoder: \\$v\" >transdecoder.version.txt\n                    \"\"\"",
        "nb_lignes_script": 44,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "annotation_ch_transdecoder"
        ],
        "nb_inputs": 1,
        "outputs": [
            "",
            "",
            "transdecoder_summary",
            "transdecoder_csv",
            "transdecoder_files",
            "transdecoder_version"
        ],
        "nb_outputs": 6,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/transdecoder\", mode: \"copy\", overwrite: true, pattern: \"*.{csv,stats,cds,gff,bed,pep}\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::transdecoder=5.5.0=pl526_2\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/transdecoder:5.5.0--pl526_2\" : \"quay.io/biocontainers/transdecoder:5.5.0--pl526_2\") }"
        ],
        "when": "",
        "stub": ""
    },
    "transdecoder_longorf": {
        "name_process": "transdecoder_longorf",
        "string_process": " process transdecoder_longorf {\n\n                label 'med_cpus'\n\n                tag \"${sample_id}\"\n\n                publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n                conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::transdecoder=5.5.0=pl526_2\" : null)\n                if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/transdecoder:5.5.0--pl526_2\" : \"quay.io/biocontainers/transdecoder:5.5.0--pl526_2\")\n                }\n\n                input:\n                    tuple sample_id, file(assembly) from annotation_ch_transdecoder\n\n                output:\n                    tuple sample_id, file(\"${sample_id}.longest_orfs.pep\") into transdecoder_diamond, transdecoder_hmmer\n                    file(\"transdecoder.version.txt\") into transdecoder_version\n\n                script:\n                    \"\"\"\n                    cp ${assembly} ${sample_id}_asssembly.fasta\n\n                    echo -e \"\\\\n-- TransDecoder.LongOrfs... --\\\\n\"\n\n                    TransDecoder.LongOrfs -t ${assembly} --output_dir ${sample_id}.transdecoder_dir -G ${params.genCode}\n\n                    cp ${sample_id}.transdecoder_dir/longest_orfs.pep ${sample_id}.longest_orfs.pep\n\n                    echo -e \"\\\\n-- Done with TransDecoder.LongOrfs --\\\\n\"\n\n                    v=\\$( TransDecoder.LongOrfs --version | cut -f 2 -d \" \" )\n                    echo \"Transdecoder: \\$v\" >transdecoder.version.txt\n                    \"\"\"\n            }",
        "nb_lignes_process": 34,
        "string_script": "                    \"\"\"\n                    cp ${assembly} ${sample_id}_asssembly.fasta\n\n                    echo -e \"\\\\n-- TransDecoder.LongOrfs... --\\\\n\"\n\n                    TransDecoder.LongOrfs -t ${assembly} --output_dir ${sample_id}.transdecoder_dir -G ${params.genCode}\n\n                    cp ${sample_id}.transdecoder_dir/longest_orfs.pep ${sample_id}.longest_orfs.pep\n\n                    echo -e \"\\\\n-- Done with TransDecoder.LongOrfs --\\\\n\"\n\n                    v=\\$( TransDecoder.LongOrfs --version | cut -f 2 -d \" \" )\n                    echo \"Transdecoder: \\$v\" >transdecoder.version.txt\n                    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "annotation_ch_transdecoder"
        ],
        "nb_inputs": 1,
        "outputs": [
            "transdecoder_diamond",
            "transdecoder_hmmer",
            "transdecoder_version"
        ],
        "nb_outputs": 3,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::transdecoder=5.5.0=pl526_2\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/transdecoder:5.5.0--pl526_2\" : \"quay.io/biocontainers/transdecoder:5.5.0--pl526_2\") }"
        ],
        "when": "",
        "stub": ""
    },
    "transdecoder_diamond": {
        "name_process": "transdecoder_diamond",
        "string_process": " process transdecoder_diamond {\n\n                label 'med_cpus'\n\n                tag \"${sample_id}\"\n\n                publishDir \"${params.outdir}/transdecoder\", mode: \"copy\", overwrite: true\n\n                conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::diamond=0.9.30=h56fc30b_0\" : null)\n                if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/diamond:0.9.30--h56fc30b_0\" : \"quay.io/biocontainers/diamond:0.9.30--h56fc30b_0\")\n                }\n\n                input:\n                    tuple sample_id, file(pep) from transdecoder_diamond\n\n                output:\n                    tuple sample_id, file(\"${sample_id}.diamond_blastp.outfmt6\") into transdecoder_predict_diamond\n\n                script:\n                    \"\"\"\n                    dbPATH=${params.pipeInstall}/DBs/uniprot_db/\n                    echo -e \"\\\\n-- Starting Diamond (blastp) --\\\\n\"\n                    if [ ! -d \\${dbPATH} ];then\n                        echo \"Directory \\${dbPATH} not found. Run the precheck to fix this issue\"\n                        exit 0\n                    elif [ -d \\${dbPATH} ];then\n                        if [ ! -e \\${dbPATH}/${params.uniname} ];then\n                            echo \"File \\${dbPATH}/${params.uniname} not found. Run the precheck to fix this issue\"\n                            exit 0\n                        elif [ -e \\${dbPATH}/${params.uniname} ];then\n                            if [ ! -e \\${dbPATH}/${params.uniname}.dmnd ];then\n                                cp \\${dbPATH}/${params.uniname} .\n                                diamond makedb --in ${params.uniname} -d ${params.uniname} -p ${task.cpus}\n                                diamond blastp -d ${params.uniname}.dmnd -q ${pep} -p ${task.cpus} -f 6 -k 1 -e 0.00001 >${sample_id}.diamond_blastp.outfmt6\n                            elif [ -e \\${dbPATH}/${params.uniname}.dmnd ];then\n                                cp \\${dbPATH}/${params.uniname}.dmnd .\n                                diamond blastp -d ${params.uniname}.dmnd -q ${pep} -p ${task.cpus} -f 6 -k 1 -e 0.00001 >${sample_id}.diamond_blastp.outfmt6\n                            fi\n                        fi\n                    fi\n                    echo -e \"\\\\n-- Done with Diamond (blastp) --\\\\n\"\n                    \"\"\"\n            }",
        "nb_lignes_process": 42,
        "string_script": "                    \"\"\"\n                    dbPATH=${params.pipeInstall}/DBs/uniprot_db/\n                    echo -e \"\\\\n-- Starting Diamond (blastp) --\\\\n\"\n                    if [ ! -d \\${dbPATH} ];then\n                        echo \"Directory \\${dbPATH} not found. Run the precheck to fix this issue\"\n                        exit 0\n                    elif [ -d \\${dbPATH} ];then\n                        if [ ! -e \\${dbPATH}/${params.uniname} ];then\n                            echo \"File \\${dbPATH}/${params.uniname} not found. Run the precheck to fix this issue\"\n                            exit 0\n                        elif [ -e \\${dbPATH}/${params.uniname} ];then\n                            if [ ! -e \\${dbPATH}/${params.uniname}.dmnd ];then\n                                cp \\${dbPATH}/${params.uniname} .\n                                diamond makedb --in ${params.uniname} -d ${params.uniname} -p ${task.cpus}\n                                diamond blastp -d ${params.uniname}.dmnd -q ${pep} -p ${task.cpus} -f 6 -k 1 -e 0.00001 >${sample_id}.diamond_blastp.outfmt6\n                            elif [ -e \\${dbPATH}/${params.uniname}.dmnd ];then\n                                cp \\${dbPATH}/${params.uniname}.dmnd .\n                                diamond blastp -d ${params.uniname}.dmnd -q ${pep} -p ${task.cpus} -f 6 -k 1 -e 0.00001 >${sample_id}.diamond_blastp.outfmt6\n                            fi\n                        fi\n                    fi\n                    echo -e \"\\\\n-- Done with Diamond (blastp) --\\\\n\"\n                    \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [
            "Diamond"
        ],
        "tools_url": [
            "https://bio.tools/diamond"
        ],
        "tools_dico": [
            {
                "name": "Diamond",
                "uri": "https://bio.tools/diamond",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0258",
                                    "term": "Sequence alignment analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Sequence aligner for protein and translated DNA searches and functions as a drop-in replacement for the NCBI BLAST software tools. It is suitable for protein-protein search as well as DNA-protein search on short reads and longer sequences including contigs and assemblies, providing a speedup of BLAST ranging up to x20,000.",
                "homepage": "https://github.com/bbuchfink/diamond"
            }
        ],
        "inputs": [
            "transdecoder_diamond"
        ],
        "nb_inputs": 1,
        "outputs": [
            "transdecoder_predict_diamond"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/transdecoder\", mode: \"copy\", overwrite: true",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::diamond=0.9.30=h56fc30b_0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/diamond:0.9.30--h56fc30b_0\" : \"quay.io/biocontainers/diamond:0.9.30--h56fc30b_0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "transdecoder_hmmer": {
        "name_process": "transdecoder_hmmer",
        "string_process": " process transdecoder_hmmer {\n\n                label 'low_cpus'\n\n                tag \"${sample_id}\"\n\n                publishDir \"${params.outdir}/transdecoder\", mode: \"copy\", overwrite: true\n\n                conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::hmmer=3.3=he1b5a44_0\" : null)\n                if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/hmmer:3.3--he1b5a44_0\" : \"quay.io/biocontainers/hmmer:3.3--he1b5a44_0\")\n                }\n\n                input:\n                    tuple sample_id, file(pep) from transdecoder_hmmer\n\n                output:\n                    tuple sample_id, file(\"${sample_id}.pfam.domtblout\") into transdecoder_predict_hmmer\n\n                script:\n                    \"\"\"\n                    dbPATH=${params.pipeInstall}/DBs/hmmerdb/\n                    echo -e \"\\\\n-- Starting HMMER --\\\\n\"\n                    if [ ! -d \\${dbPATH} ];then\n                        echo \"Directory \\${dbPATH} not found. Run the precheck to fix this issue\"\n                        exit 0\n                    elif [ -d \\${dbPATH} ];then\n                        if [ ! -e \\${dbPATH}/${params.pfname} ];then\n                            echo \"File \\${dbPATH}/${params.pfname} not found. Run the precheck to fix this issue\"\n                            exit 0\n                        elif [ -e \\${dbPATH}/${params.pfname} ];then\n                            if [ ! -e \\${dbPATH}/${params.pfname}.h3f ] && [ ! -e \\${dbPATH}/${params.pfname}.h3i ] && [ ! -e \\${dbPATH}/${params.pfname}.h3m ] && [ ! -e \\${dbPATH}/${params.pfname}.h3p ];then\n                                cp \\${dbPATH}/${params.pfname} .\n                                hmmpress ${params.pfname}\n                                hmmscan --cpu ${task.cpus} --domtblout ${sample_id}.pfam.domtblout ${params.pfname} ${pep}\n                            elif [ -s \\${dbPATH}/${params.pfname}.h3f ] && [ -s \\${dbPATH}/${params.pfname}.h3i ] && [ -s \\${dbPATH}/${params.pfname}.h3m ] && [ -s \\${dbPATH}/${params.pfname}.h3p ];then\n                                cp \\${dbPATH}/${params.pfname}.* .\n                                hmmscan --cpu ${task.cpus} --domtblout ${sample_id}.pfam.domtblout ${params.pfname} ${pep}\n                            else\n                                cp \\${dbPATH}/${params.pfname} .\n                                hmmpress ${params.pfname}\n                                hmmscan --cpu ${task.cpus} --domtblout ${sample_id}.pfam.domtblout ${params.pfname} ${pep}\n                            fi\n                        fi\n                    fi\n                    echo -e \"\\\\n-- Done with HMMER --\\\\n\"\n                    \"\"\"\n            }",
        "nb_lignes_process": 46,
        "string_script": "                    \"\"\"\n                    dbPATH=${params.pipeInstall}/DBs/hmmerdb/\n                    echo -e \"\\\\n-- Starting HMMER --\\\\n\"\n                    if [ ! -d \\${dbPATH} ];then\n                        echo \"Directory \\${dbPATH} not found. Run the precheck to fix this issue\"\n                        exit 0\n                    elif [ -d \\${dbPATH} ];then\n                        if [ ! -e \\${dbPATH}/${params.pfname} ];then\n                            echo \"File \\${dbPATH}/${params.pfname} not found. Run the precheck to fix this issue\"\n                            exit 0\n                        elif [ -e \\${dbPATH}/${params.pfname} ];then\n                            if [ ! -e \\${dbPATH}/${params.pfname}.h3f ] && [ ! -e \\${dbPATH}/${params.pfname}.h3i ] && [ ! -e \\${dbPATH}/${params.pfname}.h3m ] && [ ! -e \\${dbPATH}/${params.pfname}.h3p ];then\n                                cp \\${dbPATH}/${params.pfname} .\n                                hmmpress ${params.pfname}\n                                hmmscan --cpu ${task.cpus} --domtblout ${sample_id}.pfam.domtblout ${params.pfname} ${pep}\n                            elif [ -s \\${dbPATH}/${params.pfname}.h3f ] && [ -s \\${dbPATH}/${params.pfname}.h3i ] && [ -s \\${dbPATH}/${params.pfname}.h3m ] && [ -s \\${dbPATH}/${params.pfname}.h3p ];then\n                                cp \\${dbPATH}/${params.pfname}.* .\n                                hmmscan --cpu ${task.cpus} --domtblout ${sample_id}.pfam.domtblout ${params.pfname} ${pep}\n                            else\n                                cp \\${dbPATH}/${params.pfname} .\n                                hmmpress ${params.pfname}\n                                hmmscan --cpu ${task.cpus} --domtblout ${sample_id}.pfam.domtblout ${params.pfname} ${pep}\n                            fi\n                        fi\n                    fi\n                    echo -e \"\\\\n-- Done with HMMER --\\\\n\"\n                    \"\"\"",
        "nb_lignes_script": 26,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "transdecoder_hmmer"
        ],
        "nb_inputs": 1,
        "outputs": [
            "transdecoder_predict_hmmer"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'low_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/transdecoder\", mode: \"copy\", overwrite: true",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::hmmer=3.3=he1b5a44_0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/hmmer:3.3--he1b5a44_0\" : \"quay.io/biocontainers/hmmer:3.3--he1b5a44_0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "transdecoder_predict": {
        "name_process": "transdecoder_predict",
        "string_process": " process transdecoder_predict {\n\n                label 'med_cpus'\n\n                tag \"${sample_id}\"\n\n                publishDir \"${params.outdir}/transdecoder\", mode: \"copy\", overwrite: true, pattern: \"*.{csv,stats,cds,gff,bed,pep}\"\n\n                conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::transdecoder=5.5.0=pl526_2\" : null)\n                if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/transdecoder:5.5.0--pl526_2\" : \"quay.io/biocontainers/transdecoder:5.5.0--pl526_2\")\n                }\n\n                input:\n                    tuple sample_id, file(files) from transdecoder_predict_ch\n\n                output:\n                    tuple sample_id, file(\"${sample_id}*.transdecoder.pep\") into ( transdecoder_ch_hmmer, transdecoder_ch_signalp, transdecoder_ch_tmhmm, transdecoder_ch_trinotate )\n                    tuple sample_id, file(\"${sample_id}_assembly.fasta\"), file(\"${sample_id}*.transdecoder.pep\") into ( transdecoder_ch_diamond, transdecoder_ch_diamond_custom )\n                    tuple sample_id, file(\"${sample_id}_transdecoder.stats\") into transdecoder_summary\n                    tuple sample_id, file(\"${sample_id}_transdecoder.csv\") into transdecoder_csv\n                    tuple sample_id, file(\"${sample_id}*.transdecoder.{cds,gff,bed}\") into transdecoder_files\n\n                script:\n                    \"\"\"\n                    ass=\\$( echo $files | tr \" \" \"\\\\n\" | grep -v \".diamond_blastp.outfmt6\" | grep -v \".pfam.domtblout\" | grep \".fa\" )\n                    dia=\\$( echo $files | tr \" \" \"\\\\n\" | grep \".diamond_blastp.outfmt6\" )\n                    pfa=\\$( echo $files | tr \" \" \"\\\\n\" | grep \".pfam.domtblout\" )\n\n                    echo -e \"\\\\n-- TransDecoder.LongOrfs... --\\\\n\"\n\n                    TransDecoder.LongOrfs -t \\${ass} --output_dir ${sample_id}.transdecoder_dir -G ${params.genCode}\n\n                    echo -e \"\\\\n-- Done with TransDecoder.LongOrfs --\\\\n\"\n\n                    echo -e \"\\\\n-- TransDecoder.Predict... --\\\\n\"\n\n                    TransDecoder.Predict -t \\${ass} --retain_pfam_hits \\${pfa} --retain_blastp_hits \\${dia} --output_dir ${sample_id}.transdecoder_dir -G ${params.genCode}\n\n                    echo -e \"\\\\n-- Done with TransDecoder.Predict --\\\\n\"\n\n                    echo -e \"\\\\n-- Calculating statistics... --\\\\n\"\n                    #Calculate statistics of Transdecoder\n                    echo \"- Transdecoder (long, with homology) stats for ${sample_id}\" >${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \">\" )\n                    echo -e \"Total number of ORFs: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    echo -e \"\\\\t Of these ORFs\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep \">\" | grep -c \"|\" )\n                    echo -e \"\\\\t\\\\t with annotations: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep \">\" | grep -v \"|\" | grep -c \">\" )\n                    echo -e \"\\\\t\\\\t no annotation: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \"ORF type:complete\" )\n                    echo -e \"\\\\t ORFs type=complete: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep \"ORF type:complete\" | grep -c \"|\" )\n                    echo -e \"\\\\t\\\\t with annotations: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \"ORF type:5prime_partial\" )\n                    echo -e \"\\\\t ORFs type=5prime_partial: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep \"ORF type:5prime_partial\" | grep -c \"|\" )\n                    echo -e \"\\\\t\\\\t with annotations: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \"ORF type:3prime_partial\" )\n                    echo -e \"\\\\t ORFs type=3prime_partial: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep \"ORF type:3prime_partial\" | grep -c \"|\" )\n                    echo -e \"\\\\t\\\\t with annotations: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \"ORF type:internal\" )\n                    echo -e \"\\\\t ORFs type=internal: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep \"ORF type:internal\" | grep -c \"|\" )\n                    echo -e \"\\\\t\\\\t with annotations: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    # csv for report\n                    echo \"Sample,Total_orf,orf_complete,orf_5prime_partial,orf_3prime_partial,orf_internal\" >${sample_id}_transdecoder.csv\n                    total=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \">\" )\n                    complete=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:complete\" )\n                    n5prime=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:5prime_partial\" )\n                    n3prime=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:3prime_partial\" )\n                    internal=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:internal\" )\n                    echo \"${sample_id},\\${total},\\${complete},\\${n5prime},\\${n3prime},\\${internal}\" >>${sample_id}_transdecoder.csv\n                    echo -e \"\\\\n-- Done with statistics --\\\\n\"\n\n                    mv \\${ass} ${sample_id}_assembly.fasta\n\n                    echo -e \"\\\\n-- DONE with TransDecoder --\\\\n\"\n                    \"\"\"\n            }",
        "nb_lignes_process": 80,
        "string_script": "                    \"\"\"\n                    ass=\\$( echo $files | tr \" \" \"\\\\n\" | grep -v \".diamond_blastp.outfmt6\" | grep -v \".pfam.domtblout\" | grep \".fa\" )\n                    dia=\\$( echo $files | tr \" \" \"\\\\n\" | grep \".diamond_blastp.outfmt6\" )\n                    pfa=\\$( echo $files | tr \" \" \"\\\\n\" | grep \".pfam.domtblout\" )\n\n                    echo -e \"\\\\n-- TransDecoder.LongOrfs... --\\\\n\"\n\n                    TransDecoder.LongOrfs -t \\${ass} --output_dir ${sample_id}.transdecoder_dir -G ${params.genCode}\n\n                    echo -e \"\\\\n-- Done with TransDecoder.LongOrfs --\\\\n\"\n\n                    echo -e \"\\\\n-- TransDecoder.Predict... --\\\\n\"\n\n                    TransDecoder.Predict -t \\${ass} --retain_pfam_hits \\${pfa} --retain_blastp_hits \\${dia} --output_dir ${sample_id}.transdecoder_dir -G ${params.genCode}\n\n                    echo -e \"\\\\n-- Done with TransDecoder.Predict --\\\\n\"\n\n                    echo -e \"\\\\n-- Calculating statistics... --\\\\n\"\n                    #Calculate statistics of Transdecoder\n                    echo \"- Transdecoder (long, with homology) stats for ${sample_id}\" >${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \">\" )\n                    echo -e \"Total number of ORFs: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    echo -e \"\\\\t Of these ORFs\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep \">\" | grep -c \"|\" )\n                    echo -e \"\\\\t\\\\t with annotations: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep \">\" | grep -v \"|\" | grep -c \">\" )\n                    echo -e \"\\\\t\\\\t no annotation: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \"ORF type:complete\" )\n                    echo -e \"\\\\t ORFs type=complete: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep \"ORF type:complete\" | grep -c \"|\" )\n                    echo -e \"\\\\t\\\\t with annotations: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \"ORF type:5prime_partial\" )\n                    echo -e \"\\\\t ORFs type=5prime_partial: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep \"ORF type:5prime_partial\" | grep -c \"|\" )\n                    echo -e \"\\\\t\\\\t with annotations: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \"ORF type:3prime_partial\" )\n                    echo -e \"\\\\t ORFs type=3prime_partial: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep \"ORF type:3prime_partial\" | grep -c \"|\" )\n                    echo -e \"\\\\t\\\\t with annotations: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep -c \"ORF type:internal\" )\n                    echo -e \"\\\\t ORFs type=internal: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    orfnum=\\$( cat ${sample_id}*.transdecoder.pep | grep \"ORF type:internal\" | grep -c \"|\" )\n                    echo -e \"\\\\t\\\\t with annotations: \\$orfnum \\\\n\" >>${sample_id}_transdecoder.stats\n                    # csv for report\n                    echo \"Sample,Total_orf,orf_complete,orf_5prime_partial,orf_3prime_partial,orf_internal\" >${sample_id}_transdecoder.csv\n                    total=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \">\" )\n                    complete=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:complete\" )\n                    n5prime=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:5prime_partial\" )\n                    n3prime=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:3prime_partial\" )\n                    internal=\\$( cat ${sample_id}*.transdecoder.pep  | grep -c \"ORF type:internal\" )\n                    echo \"${sample_id},\\${total},\\${complete},\\${n5prime},\\${n3prime},\\${internal}\" >>${sample_id}_transdecoder.csv\n                    echo -e \"\\\\n-- Done with statistics --\\\\n\"\n\n                    mv \\${ass} ${sample_id}_assembly.fasta\n\n                    echo -e \"\\\\n-- DONE with TransDecoder --\\\\n\"\n                    \"\"\"",
        "nb_lignes_script": 56,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "transdecoder_predict_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "",
            "",
            "transdecoder_summary",
            "transdecoder_csv",
            "transdecoder_files"
        ],
        "nb_outputs": 5,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/transdecoder\", mode: \"copy\", overwrite: true, pattern: \"*.{csv,stats,cds,gff,bed,pep}\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::transdecoder=5.5.0=pl526_2\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/transdecoder:5.5.0--pl526_2\" : \"quay.io/biocontainers/transdecoder:5.5.0--pl526_2\") }"
        ],
        "when": "",
        "stub": ""
    },
    "swiss_diamond_trinotate": {
        "name_process": "swiss_diamond_trinotate",
        "string_process": " process swiss_diamond_trinotate {\n\n            label 'big_cpus'\n\n            tag \"${sample_id}\"\n\n            publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::diamond=0.9.30=h56fc30b_0\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/diamond:0.9.30--h56fc30b_0\" : \"quay.io/biocontainers/diamond:0.9.30--h56fc30b_0\")\n            }\n\n            input:\n                tuple sample_id, file(assembly), file(transdecoder) from transdecoder_ch_diamond\n\n            output:\n                tuple sample_id, file(\"${sample_id}.diamond_blastx.outfmt6\") into trinotate_ch_diamondX\n                tuple sample_id, file(\"${sample_id}.diamond_blastp.outfmt6\") into trinotate_ch_diamondP\n                file(\"diamond.version.txt\") into diamond_version\n\n            script:\n                \"\"\"\n                dbPATH=${params.pipeInstall}/DBs/sqlite_db/\n                echo -e \"\\\\n-- Starting Diamond --\\\\n\"\n                if [ ! -d \\${dbPATH} ];then\n                    echo \"Directory \\${dbPATH} not found. Run the precheck to fix this issue\"\n                    exit 0\n                elif [ -d \\${dbPATH} ];then\n                    if [ ! -e \\${dbPATH}/uniprot_sprot.pep ];then\n                        echo \"File \\${dbPATH}/uniprot_sprot.pep not found. Run the precheck to fix this issue\"\n                        exit 0\n                    elif [ -e \\${dbPATH}/uniprot_sprot.pep ];then\n                        if [ ! -e \\${dbPATH}/uniprot_sprot.pep.dmnd ];then\n                            cp \\${dbPATH}/uniprot_sprot.pep .\n                            diamond makedb --in uniprot_sprot.pep -d uniprot_sprot.pep -p ${task.cpus}\n                            echo -e \"\\\\n-- Starting with Diamond (blastx) --\\\\n\"\n                            diamond blastx -d uniprot_sprot.pep.dmnd -q ${assembly} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.diamond_blastx.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastx) --\\\\n\"\n                            echo -e \"\\\\n-- Starting with Diamond (blastp) --\\\\n\"\n                            diamond blastp -d uniprot_sprot.pep.dmnd -q ${transdecoder} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.diamond_blastp.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastp)  --\\\\n\"\n                        elif [ -e \\${dbPATH}/uniprot_sprot.pep.dmnd ];then\n                            cp \\${dbPATH}/uniprot_sprot.pep.dmnd .\n                            echo -e \"\\\\n-- Starting with Diamond (blastx) --\\\\n\"\n                            diamond blastx -d uniprot_sprot.pep.dmnd -q ${assembly} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.diamond_blastx.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastx) --\\\\n\"\n                            echo -e \"\\\\n-- Starting with Diamond (blastp) --\\\\n\"\n                            diamond blastp -d uniprot_sprot.pep.dmnd -q ${transdecoder} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.diamond_blastp.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastp)  --\\\\n\"\n                        fi\n                    fi\n                fi\n                echo -e \"\\\\n-- Done with Diamond --\\\\n\"\n\n                v=\\$( diamond --version 2>&1 | tail -n 1 | cut -f 3 -d \" \" )\n                echo \"Diamond: \\$v\" >diamond.version.txt\n                \"\"\"\n        }",
        "nb_lignes_process": 57,
        "string_script": "                \"\"\"\n                dbPATH=${params.pipeInstall}/DBs/sqlite_db/\n                echo -e \"\\\\n-- Starting Diamond --\\\\n\"\n                if [ ! -d \\${dbPATH} ];then\n                    echo \"Directory \\${dbPATH} not found. Run the precheck to fix this issue\"\n                    exit 0\n                elif [ -d \\${dbPATH} ];then\n                    if [ ! -e \\${dbPATH}/uniprot_sprot.pep ];then\n                        echo \"File \\${dbPATH}/uniprot_sprot.pep not found. Run the precheck to fix this issue\"\n                        exit 0\n                    elif [ -e \\${dbPATH}/uniprot_sprot.pep ];then\n                        if [ ! -e \\${dbPATH}/uniprot_sprot.pep.dmnd ];then\n                            cp \\${dbPATH}/uniprot_sprot.pep .\n                            diamond makedb --in uniprot_sprot.pep -d uniprot_sprot.pep -p ${task.cpus}\n                            echo -e \"\\\\n-- Starting with Diamond (blastx) --\\\\n\"\n                            diamond blastx -d uniprot_sprot.pep.dmnd -q ${assembly} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.diamond_blastx.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastx) --\\\\n\"\n                            echo -e \"\\\\n-- Starting with Diamond (blastp) --\\\\n\"\n                            diamond blastp -d uniprot_sprot.pep.dmnd -q ${transdecoder} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.diamond_blastp.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastp)  --\\\\n\"\n                        elif [ -e \\${dbPATH}/uniprot_sprot.pep.dmnd ];then\n                            cp \\${dbPATH}/uniprot_sprot.pep.dmnd .\n                            echo -e \"\\\\n-- Starting with Diamond (blastx) --\\\\n\"\n                            diamond blastx -d uniprot_sprot.pep.dmnd -q ${assembly} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.diamond_blastx.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastx) --\\\\n\"\n                            echo -e \"\\\\n-- Starting with Diamond (blastp) --\\\\n\"\n                            diamond blastp -d uniprot_sprot.pep.dmnd -q ${transdecoder} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.diamond_blastp.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastp)  --\\\\n\"\n                        fi\n                    fi\n                fi\n                echo -e \"\\\\n-- Done with Diamond --\\\\n\"\n\n                v=\\$( diamond --version 2>&1 | tail -n 1 | cut -f 3 -d \" \" )\n                echo \"Diamond: \\$v\" >diamond.version.txt\n                \"\"\"",
        "nb_lignes_script": 35,
        "language_script": "bash",
        "tools": [
            "Diamond"
        ],
        "tools_url": [
            "https://bio.tools/diamond"
        ],
        "tools_dico": [
            {
                "name": "Diamond",
                "uri": "https://bio.tools/diamond",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0258",
                                    "term": "Sequence alignment analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Sequence aligner for protein and translated DNA searches and functions as a drop-in replacement for the NCBI BLAST software tools. It is suitable for protein-protein search as well as DNA-protein search on short reads and longer sequences including contigs and assemblies, providing a speedup of BLAST ranging up to x20,000.",
                "homepage": "https://github.com/bbuchfink/diamond"
            }
        ],
        "inputs": [
            "transdecoder_ch_diamond"
        ],
        "nb_inputs": 1,
        "outputs": [
            "trinotate_ch_diamondX",
            "trinotate_ch_diamondP",
            "diamond_version"
        ],
        "nb_outputs": 3,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'big_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::diamond=0.9.30=h56fc30b_0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/diamond:0.9.30--h56fc30b_0\" : \"quay.io/biocontainers/diamond:0.9.30--h56fc30b_0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "custom_diamond_trinotate": {
        "name_process": "custom_diamond_trinotate",
        "string_process": " process custom_diamond_trinotate {\n\n            label 'big_cpus'\n\n            tag \"${sample_id}\"\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::diamond=0.9.30=h56fc30b_0\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/diamond:0.9.30--h56fc30b_0\" : \"quay.io/biocontainers/diamond:0.9.30--h56fc30b_0\")\n            }\n\n            input:\n                tuple sample_id, file(assembly), file(transdecoder) from transdecoder_ch_diamond_custom\n\n            output:\n                tuple sample_id, file(\"${sample_id}.custom.diamond_blastx.outfmt6\") into trinotate_ch_diamondX_custom\n                tuple sample_id, file(\"${sample_id}.custom.diamond_blastp.outfmt6\") into trinotate_ch_diamondP_custom\n\n            script:\n                \"\"\"\n                dbPATH=${params.pipeInstall}/DBs/uniprot_db/\n                echo -e \"\\\\n-- Starting Diamond --\\\\n\"\n                if [ ! -d \\${dbPATH} ];then\n                    echo \"Directory \\${dbPATH} not found. Run the precheck to fix this issue\"\n                    exit 0\n                elif [ -d \\${dbPATH} ];then\n                    if [ ! -e \\${dbPATH}/${params.uniname} ];then\n                        echo \"File \\${dbPATH}/${params.uniname} not found. Run the precheck to fix this issue\"\n                        exit 0\n                    elif [ -e \\${dbPATH}/${params.uniname} ];then\n                        if [ ! -e \\${dbPATH}/${params.uniname}.dmnd ];then\n                            cp \\${dbPATH}/${params.uniname} .\n                            diamond makedb --in ${params.uniname} -d ${params.uniname} -p ${task.cpus}\n                            echo -e \"\\\\n-- Starting with Diamond (blastx) --\\\\n\"\n                            diamond blastx -d ${params.uniname}.dmnd -q ${assembly} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.custom.diamond_blastx.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastx) --\\\\n\"\n                            echo -e \"\\\\n-- Starting with Diamond (blastp) --\\\\n\"\n                            diamond blastp -d ${params.uniname}.dmnd -q ${transdecoder} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.custom.diamond_blastp.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastp)  --\\\\n\"\n                        elif [ -e \\${dbPATH}/${params.uniname}.dmnd ];then\n                            cp \\${dbPATH}/${params.uniname}.dmnd .\n                            echo -e \"\\\\n-- Starting with Diamond (blastx) --\\\\n\"\n                            diamond blastx -d ${params.uniname}.dmnd -q ${assembly} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.custom.diamond_blastx.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastx) --\\\\n\"\n                            echo -e \"\\\\n-- Starting with Diamond (blastp) --\\\\n\"\n                            diamond blastp -d ${params.uniname}.dmnd -q ${transdecoder} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.custom.diamond_blastp.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastp)  --\\\\n\"\n                        fi\n                    fi\n                fi\n                echo -e \"\\\\n-- Done with Diamond --\\\\n\"\n                \"\"\"\n        }",
        "nb_lignes_process": 51,
        "string_script": "                \"\"\"\n                dbPATH=${params.pipeInstall}/DBs/uniprot_db/\n                echo -e \"\\\\n-- Starting Diamond --\\\\n\"\n                if [ ! -d \\${dbPATH} ];then\n                    echo \"Directory \\${dbPATH} not found. Run the precheck to fix this issue\"\n                    exit 0\n                elif [ -d \\${dbPATH} ];then\n                    if [ ! -e \\${dbPATH}/${params.uniname} ];then\n                        echo \"File \\${dbPATH}/${params.uniname} not found. Run the precheck to fix this issue\"\n                        exit 0\n                    elif [ -e \\${dbPATH}/${params.uniname} ];then\n                        if [ ! -e \\${dbPATH}/${params.uniname}.dmnd ];then\n                            cp \\${dbPATH}/${params.uniname} .\n                            diamond makedb --in ${params.uniname} -d ${params.uniname} -p ${task.cpus}\n                            echo -e \"\\\\n-- Starting with Diamond (blastx) --\\\\n\"\n                            diamond blastx -d ${params.uniname}.dmnd -q ${assembly} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.custom.diamond_blastx.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastx) --\\\\n\"\n                            echo -e \"\\\\n-- Starting with Diamond (blastp) --\\\\n\"\n                            diamond blastp -d ${params.uniname}.dmnd -q ${transdecoder} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.custom.diamond_blastp.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastp)  --\\\\n\"\n                        elif [ -e \\${dbPATH}/${params.uniname}.dmnd ];then\n                            cp \\${dbPATH}/${params.uniname}.dmnd .\n                            echo -e \"\\\\n-- Starting with Diamond (blastx) --\\\\n\"\n                            diamond blastx -d ${params.uniname}.dmnd -q ${assembly} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.custom.diamond_blastx.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastx) --\\\\n\"\n                            echo -e \"\\\\n-- Starting with Diamond (blastp) --\\\\n\"\n                            diamond blastp -d ${params.uniname}.dmnd -q ${transdecoder} -p ${task.cpus} -f 6 -k 1 -e 0.001 >${sample_id}.custom.diamond_blastp.outfmt6\n                            echo -e \"\\\\n-- Done with Diamond (blastp)  --\\\\n\"\n                        fi\n                    fi\n                fi\n                echo -e \"\\\\n-- Done with Diamond --\\\\n\"\n                \"\"\"",
        "nb_lignes_script": 32,
        "language_script": "bash",
        "tools": [
            "Diamond"
        ],
        "tools_url": [
            "https://bio.tools/diamond"
        ],
        "tools_dico": [
            {
                "name": "Diamond",
                "uri": "https://bio.tools/diamond",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0258",
                                    "term": "Sequence alignment analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Sequence aligner for protein and translated DNA searches and functions as a drop-in replacement for the NCBI BLAST software tools. It is suitable for protein-protein search as well as DNA-protein search on short reads and longer sequences including contigs and assemblies, providing a speedup of BLAST ranging up to x20,000.",
                "homepage": "https://github.com/bbuchfink/diamond"
            }
        ],
        "inputs": [
            "transdecoder_ch_diamond_custom"
        ],
        "nb_inputs": 1,
        "outputs": [
            "trinotate_ch_diamondX_custom",
            "trinotate_ch_diamondP_custom"
        ],
        "nb_outputs": 2,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'big_cpus'",
            "tag \"${sample_id}\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::diamond=0.9.30=h56fc30b_0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/diamond:0.9.30--h56fc30b_0\" : \"quay.io/biocontainers/diamond:0.9.30--h56fc30b_0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "hmmer_trinotate": {
        "name_process": "hmmer_trinotate",
        "string_process": " process hmmer_trinotate {\n\n            label 'low_cpus'\n\n            tag \"${sample_id}\"\n\n            publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::hmmer=3.3=he1b5a44_0\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/hmmer:3.3--he1b5a44_0\" : \"quay.io/biocontainers/hmmer:3.3--he1b5a44_0\")\n            }\n\n            input:\n                tuple sample_id, file(transdecoder_pep) from transdecoder_ch_hmmer\n\n            output:\n                tuple sample_id, file(\"${sample_id}.TrinotatePFAM.out\") into trinotate_ch_hmmer\n                file(\"hmmer.version.txt\") into hmmer_version\n\n            script:\n                \"\"\"\n                dbPATH=${params.pipeInstall}/DBs/hmmerdb/\n                echo -e \"\\\\n-- Starting HMMER --\\\\n\"\n                if [ ! -d \\${dbPATH} ];then\n                    echo \"Directory \\${dbPATH} not found. Run the precheck to fix this issue\"\n                    exit 0\n                elif [ -d \\${dbPATH} ];then\n                    if [ ! -e \\${dbPATH}/${params.pfname} ];then\n                        echo \"File \\${dbPATH}/${params.pfname} not found. Run the precheck to fix this issue\"\n                        exit 0\n                    elif [ -e \\${dbPATH}/${params.pfname} ];then\n                        if [ ! -e \\${dbPATH}/${params.pfname}.h3f ] && [ ! -e \\${dbPATH}/${params.pfname}.h3i ] && [ ! -e \\${dbPATH}/${params.pfname}.h3m ] && [ ! -e \\${dbPATH}/${params.pfname}.h3p ];then\n                            cp \\${dbPATH}/${params.pfname} .\n                            hmmpress ${params.pfname}\n                            hmmscan --cpu ${task.cpus} --domtblout ${sample_id}.TrinotatePFAM.out ${params.pfname} ${transdecoder_pep} >pfam.log\n                        elif [ -s \\${dbPATH}/${params.pfname}.h3f ] && [ -s \\${dbPATH}/${params.pfname}.h3i ] && [ -s \\${dbPATH}/${params.pfname}.h3m ] && [ -s \\${dbPATH}/${params.pfname}.h3p ];then\n                            cp \\${dbPATH}/${params.pfname}.* .\n                            hmmscan --cpu ${task.cpus} --domtblout ${sample_id}.TrinotatePFAM.out ${params.pfname} ${transdecoder_pep} >pfam.log\n                        else\n                            cp \\${dbPATH}/${params.pfname} .\n                            hmmpress ${params.pfname}\n                            hmmscan --cpu ${task.cpus} --domtblout ${sample_id}.TrinotatePFAM.out ${params.pfname} ${transdecoder_pep} >pfam.log\n                        fi\n                    fi\n                fi\n                echo -e \"\\\\n-- Done with HMMER --\\\\n\"\n\n                v=\\$( hmmsearch -h | head -n 2 | cut -f 3 -d \" \" | grep [0-9] )\n                echo \"HMMER: \\$v\" >hmmer.version.txt\n                \"\"\"\n        }",
        "nb_lignes_process": 50,
        "string_script": "                \"\"\"\n                dbPATH=${params.pipeInstall}/DBs/hmmerdb/\n                echo -e \"\\\\n-- Starting HMMER --\\\\n\"\n                if [ ! -d \\${dbPATH} ];then\n                    echo \"Directory \\${dbPATH} not found. Run the precheck to fix this issue\"\n                    exit 0\n                elif [ -d \\${dbPATH} ];then\n                    if [ ! -e \\${dbPATH}/${params.pfname} ];then\n                        echo \"File \\${dbPATH}/${params.pfname} not found. Run the precheck to fix this issue\"\n                        exit 0\n                    elif [ -e \\${dbPATH}/${params.pfname} ];then\n                        if [ ! -e \\${dbPATH}/${params.pfname}.h3f ] && [ ! -e \\${dbPATH}/${params.pfname}.h3i ] && [ ! -e \\${dbPATH}/${params.pfname}.h3m ] && [ ! -e \\${dbPATH}/${params.pfname}.h3p ];then\n                            cp \\${dbPATH}/${params.pfname} .\n                            hmmpress ${params.pfname}\n                            hmmscan --cpu ${task.cpus} --domtblout ${sample_id}.TrinotatePFAM.out ${params.pfname} ${transdecoder_pep} >pfam.log\n                        elif [ -s \\${dbPATH}/${params.pfname}.h3f ] && [ -s \\${dbPATH}/${params.pfname}.h3i ] && [ -s \\${dbPATH}/${params.pfname}.h3m ] && [ -s \\${dbPATH}/${params.pfname}.h3p ];then\n                            cp \\${dbPATH}/${params.pfname}.* .\n                            hmmscan --cpu ${task.cpus} --domtblout ${sample_id}.TrinotatePFAM.out ${params.pfname} ${transdecoder_pep} >pfam.log\n                        else\n                            cp \\${dbPATH}/${params.pfname} .\n                            hmmpress ${params.pfname}\n                            hmmscan --cpu ${task.cpus} --domtblout ${sample_id}.TrinotatePFAM.out ${params.pfname} ${transdecoder_pep} >pfam.log\n                        fi\n                    fi\n                fi\n                echo -e \"\\\\n-- Done with HMMER --\\\\n\"\n\n                v=\\$( hmmsearch -h | head -n 2 | cut -f 3 -d \" \" | grep [0-9] )\n                echo \"HMMER: \\$v\" >hmmer.version.txt\n                \"\"\"",
        "nb_lignes_script": 29,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "transdecoder_ch_hmmer"
        ],
        "nb_inputs": 1,
        "outputs": [
            "trinotate_ch_hmmer",
            "hmmer_version"
        ],
        "nb_outputs": 2,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'low_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::hmmer=3.3=he1b5a44_0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/hmmer:3.3--he1b5a44_0\" : \"quay.io/biocontainers/hmmer:3.3--he1b5a44_0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "signalP_trinotate": {
        "name_process": "signalP_trinotate",
        "string_process": " process signalP_trinotate {\n\n                label 'low_cpus'\n\n                tag \"${sample_id}\"\n\n                input:\n                    tuple sample_id, file(transdecoder_pep) from transdecoder_ch_signalp\n\n                output:\n                    tuple sample_id, file(\"${sample_id}.signalp.out\") into trinotate_ch_signalp\n\n                script:\n                    \"\"\"\n                    #signalP to predict signal peptides\n\n                    echo -e \"\\\\n-- Starting with SignalP --\\\\n\"\n\n                    ${params.signalp} -f short -n ${sample_id}.signalp.out ${transdecoder_pep}\n\n                    echo -e \"\\\\n-- Done with SignalP --\\\\n\"\n                    \"\"\"\n            }",
        "nb_lignes_process": 21,
        "string_script": "                    \"\"\"\n                    #signalP to predict signal peptides\n\n                    echo -e \"\\\\n-- Starting with SignalP --\\\\n\"\n\n                    ${params.signalp} -f short -n ${sample_id}.signalp.out ${transdecoder_pep}\n\n                    echo -e \"\\\\n-- Done with SignalP --\\\\n\"\n                    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "transdecoder_ch_signalp"
        ],
        "nb_inputs": 1,
        "outputs": [
            "trinotate_ch_signalp"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'low_cpus'",
            "tag \"${sample_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "skip_signalP": {
        "name_process": "skip_signalP",
        "string_process": " process skip_signalP {\n                tag \"${sample_id}\"\n\n                input:\n                    tuple sample_id, file(transdecoder_pep) from transdecoder_ch_signalp\n\n                output:\n                    tuple sample_id, file(\"${sample_id}.signalp.out\") into trinotate_ch_signalp\n\n                script:\n                    \"\"\"\n                    touch ${sample_id}.signalp.out\n                    \"\"\"\n            }",
        "nb_lignes_process": 12,
        "string_script": "                    \"\"\"\n                    touch ${sample_id}.signalp.out\n                    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "transdecoder_ch_signalp"
        ],
        "nb_inputs": 1,
        "outputs": [
            "trinotate_ch_signalp"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "tmhmm_trinotate": {
        "name_process": "tmhmm_trinotate",
        "string_process": " process tmhmm_trinotate {\n\n                label 'low_cpus'\n\n                tag \"${sample_id}\"\n\n                input:\n                    tuple sample_id, file(transdecoder_pep) from transdecoder_ch_tmhmm\n\n                output:\n                    tuple sample_id, file(\"${sample_id}.tmhmm.out\") into trinotate_ch_tmhmm\n\n                script:\n                    \"\"\"\n                    #tmHMM to predict transmembrane regions\n\n                    echo -e \"\\\\n-- Starting with tmHMM --\\\\n\"\n\n                    ${params.tmhmm} --short < ${transdecoder_pep} >${sample_id}.tmhmm.out\n\n                    echo -e \"\\\\n-- Done with tmHMM --\\\\n\"\n                    \"\"\"\n            }",
        "nb_lignes_process": 21,
        "string_script": "                    \"\"\"\n                    #tmHMM to predict transmembrane regions\n\n                    echo -e \"\\\\n-- Starting with tmHMM --\\\\n\"\n\n                    ${params.tmhmm} --short < ${transdecoder_pep} >${sample_id}.tmhmm.out\n\n                    echo -e \"\\\\n-- Done with tmHMM --\\\\n\"\n                    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "transdecoder_ch_tmhmm"
        ],
        "nb_inputs": 1,
        "outputs": [
            "trinotate_ch_tmhmm"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'low_cpus'",
            "tag \"${sample_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "skip_tmhmm": {
        "name_process": "skip_tmhmm",
        "string_process": " process skip_tmhmm {\n                tag \"${sample_id}\"\n\n                input:\n                    tuple sample_id, file(transdecoder_pep) from transdecoder_ch_tmhmm\n\n                output:\n                    tuple sample_id, file(\"${sample_id}.tmhmm.out\") into trinotate_ch_tmhmm\n\n                script:\n                    \"\"\"\n                    touch ${sample_id}.tmhmm.out\n                    \"\"\"\n            }",
        "nb_lignes_process": 12,
        "string_script": "                    \"\"\"\n                    touch ${sample_id}.tmhmm.out\n                    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "transdecoder_ch_tmhmm"
        ],
        "nb_inputs": 1,
        "outputs": [
            "trinotate_ch_tmhmm"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "rnammer_trinotate": {
        "name_process": "rnammer_trinotate",
        "string_process": " process rnammer_trinotate {\n\n                label 'low_cpus'\n\n                tag \"${sample_id}\"\n\n                input:\n                    tuple sample_id, file(transcriptome) from evigene_ch_rnammer\n\n                output:\n                    tuple sample_id, file(\"${sample_id}.rnammer.gff\") into trinotate_ch_rnammer\n\n                script:\n                    \"\"\"\n                    set +e\n                    #RNAMMER to identify rRNA transcripts\n\n                    echo -e \"\\\\n-- Starting with RNAMMER --\\\\n\"\n\n                    RnammerTranscriptome.pl --transcriptome ${transcriptome} --path_to_rnammer ${params.rnam}\n\n                    mv ${sample_id}.combined.okay.fa.rnammer.gff ${sample_id}.rnammer.gff\n\n                    echo -e \"\\\\n-- Done with RNAMMER --\\\\n\"\n                    \"\"\"\n            }",
        "nb_lignes_process": 24,
        "string_script": "                    \"\"\"\n                    set +e\n                    #RNAMMER to identify rRNA transcripts\n\n                    echo -e \"\\\\n-- Starting with RNAMMER --\\\\n\"\n\n                    RnammerTranscriptome.pl --transcriptome ${transcriptome} --path_to_rnammer ${params.rnam}\n\n                    mv ${sample_id}.combined.okay.fa.rnammer.gff ${sample_id}.rnammer.gff\n\n                    echo -e \"\\\\n-- Done with RNAMMER --\\\\n\"\n                    \"\"\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "evigene_ch_rnammer"
        ],
        "nb_inputs": 1,
        "outputs": [
            "trinotate_ch_rnammer"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'low_cpus'",
            "tag \"${sample_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "skip_rnammer": {
        "name_process": "skip_rnammer",
        "string_process": " process skip_rnammer {\n                tag \"${sample_id}\"\n\n                input:\n                    tuple sample_id, file(transcriptome) from evigene_ch_rnammer\n\n                output:\n                    tuple sample_id, file(\"${sample_id}.rnammer.gff\") into trinotate_ch_rnammer\n\n                script:\n                    \"\"\"\n                    touch ${sample_id}.rnammer.gff\n                    \"\"\"\n            }",
        "nb_lignes_process": 12,
        "string_script": "                    \"\"\"\n                    touch ${sample_id}.rnammer.gff\n                    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "evigene_ch_rnammer"
        ],
        "nb_inputs": 1,
        "outputs": [
            "trinotate_ch_rnammer"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "trinotate": {
        "name_process": "trinotate",
        "string_process": " process trinotate {\n\n            label 'low_cpus'\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/trinotate\", mode: \"copy\", overwrite: true, pattern: \"*.{terms.txt,xls}\"\n            publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::trinotate=3.2.2=pl5262hdfd78af_0\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/trinotate:3.2.2--pl5262hdfd78af_0\" : \"quay.io/biocontainers/trinotate:3.2.2--pl5262hdfd78af_0\")\n            }\n\n            input:\n                tuple sample_id, file(files) from trinotate_ch\n\n            output:\n                tuple sample_id, file(\"${sample_id}.GO.terms.txt\") into trinotate_summary\n                tuple sample_id, file(\"${sample_id}.trinotate_annotation_report.xls\") into ( trinotate_out_ch, custom_uniprot_ch )\n                tuple sample_id, file(\"*.terms.txt\") into other_files\n                tuple sample_id, file(\"${sample_id}.KEGG.terms.txt\") into kegg_paths\n                file(\"trinotate.version.txt\") into trinotate_version\n                file(\"perl.version.txt\") into perl_version\n\n            script:\n                \"\"\"\n                for x in `echo ${files}`;do\n                    echo \\${x} >>.vars.txt\n                done\n\n                assembly=\\$( cat .vars.txt | grep -E \"${sample_id}.*.fa\" | grep -v \".transdecoder.pep\" )\n                transdecoder=\\$( cat .vars.txt | grep -E \"${sample_id}.*.transdecoder.pep\" )\n                diamond_blastx=\\$( cat .vars.txt | grep \"${sample_id}.diamond_blastx.outfmt6\" )\n                diamond_blastp=\\$( cat .vars.txt | grep \"${sample_id}.diamond_blastp.outfmt6\" )\n                custom_blastx=\\$( cat .vars.txt | grep \"${sample_id}.custom.diamond_blastx.outfmt6\" )\n                custom_blastp=\\$( cat .vars.txt | grep \"${sample_id}.custom.diamond_blastp.outfmt6\" )\n                pfam=\\$( cat .vars.txt | grep \"${sample_id}.TrinotatePFAM.out\" )\n                signalp=\\$( cat .vars.txt | grep \"${sample_id}.signalp.out\" )\n                tmhmm=\\$( cat .vars.txt | grep \"${sample_id}.tmhmm.out\" )\n                rnammer=\\$( cat .vars.txt | grep \"${sample_id}.rnammer.gff\" )\n\n                #Generate gene_trans_map\n                #Not using get_Trinity_gene_to_trans_map.pl since all the names are uniq\n                cat \\${assembly} | awk '{print \\$1}' | grep \">\" | cut -c 2- >a.txt\n\n                #paste a.txt a.txt >\\${assembly}.gene_trans_map - does not work in container\n                touch \\${assembly}.gene_trans_map\n                for x in `cat a.txt`;do echo -e \\${x}\"\\\\t\"\\${x} >>\\${assembly}.gene_trans_map ;done\n\n                #Get Trinotate.sqlite from folder (original)\n                cp ${params.Tsql} .\n                sqlname=`echo ${params.Tsql} | tr \"\\\\/\" \"\\\\n\" | grep \"\\\\.sqlite\"`\n\n                echo -e \"\\\\n-- Running Trinotate --\\\\n\"\n\n                Trinotate \\$sqlname init --gene_trans_map \\${assembly}.gene_trans_map --transcript_fasta \\${assembly} --transdecoder_pep \\${transdecoder}\n\n                echo -e \"\\\\n-- Ending run of Trinotate --\\\\n\"\n\n                echo -e \"\\\\n-- Loading hits and predictions to sqlite database... --\\\\n\"\n\n                #Load protein hits\n                Trinotate \\$sqlname LOAD_swissprot_blastp \\${diamond_blastp}\n\n                #Load transcript hits\n                Trinotate \\$sqlname LOAD_swissprot_blastx \\${diamond_blastx}\n\n                #Load custom protein hits\n                Trinotate \\$sqlname LOAD_custom_blast --outfmt6 \\${custom_blastp} --prog blastp --dbtype ${params.uniname}\n\n                #Load custom transcript hits\n                Trinotate \\$sqlname LOAD_custom_blast --outfmt6 \\${custom_blastx} --prog blastx --dbtype ${params.uniname}\n\n                #Load Pfam domain entries\n                Trinotate \\$sqlname LOAD_pfam \\${pfam}\n\n                #Load transmembrane domains\n                if [ -s \\${tmhmm} ];then\n                    Trinotate \\$sqlname LOAD_tmhmm \\${tmhmm}\n                else\n                    echo \"No transmembrane domains (tmhmm)\"\n                fi\n\n                #Load signal peptide predictions\n                if [ -s \\${signalp} ];then\n                    Trinotate \\$sqlname LOAD_signalp \\${signalp}\n                else\n                    echo \"No Signal-P\"\n                fi\n\n                #Load rnammer results\n                if [ -s \\${rnammer} ];then\n                    Trinotate \\$sqlname LOAD_rnammer \\${rnammer}\n                else\n                    echo \"No rnammer results\"\n                fi\n\n                echo -e \"\\\\n-- Loading finished --\\\\n\"\n\n                #Report\n\n                echo -e \"\\\\n-- Generating report... --\\\\n\"\n\n                Trinotate \\$sqlname report >${sample_id}.trinotate_annotation_report.xls\n\n                echo -e \"\\\\n-- Report generated --\\\\n\"\n\n                #Extract info from XLS file\n\n                echo -e \"\\\\n-- Creating GO file from XLS... --\\\\n\"\n\n                extract_GO_assignments_from_Trinotate_xls.pl --Trinotate_xls ${sample_id}.trinotate_annotation_report.xls --trans >${sample_id}.GO.terms.txt\n\n                echo -e \"\\\\n-- Done with the GO --\\\\n\"\n\n                echo -e \"\\\\n-- Creating KEGG file from XLS... --\\\\n\"\n\n                cat ${sample_id}.trinotate_annotation_report.xls | cut -f 1,14 | grep \"KEGG\" | tr \"\\\\`\" \";\" | grep \"KO:K\" | sed 's/\\\\tKEGG/\\\\t#KEGG/g' | sed 's/KO:/KO:#/g' | cut -f 1,3 -d \"#\" | tr -d \"#\" >${sample_id}.KEGG.terms.txt\n\n                echo -e \"\\\\n-- Done with the KEGG --\\\\n\"\n\n                echo -e \"\\\\n-- Creating eggNOG file from XLS... --\\\\n\"\n\n                cat ${sample_id}.trinotate_annotation_report.xls | cut -f 1,13 | grep \"OG\" | tr \"\\\\`\" \";\" | sed 's/^/#/g' | sed 's/;/\\\\n;/g' | cut -f 1 -d \"^\" | tr -d \"\\\\n\" | tr \"#\" \"\\\\n\" | grep \"OG\" >${sample_id}.eggNOG_COG.terms.txt\n\n                echo -e \"\\\\n-- Done with the eggNOG --\\\\n\"\n\n                echo -e \"\\\\n-- Creating PFAM file from XLS... --\\\\n\"\n\n                cat ${sample_id}.trinotate_annotation_report.xls | cut -f 1,10 | grep \"PF\" | tr \"\\\\`\" \";\" | sed 's/^/#/g' | sed 's/;PF/\\\\n;PF/g' | cut -f 1 -d \"^\" | tr -d \"\\\\n\" | tr \"#\" \"\\\\n\" | grep \"PF\" | tr \";\" \",\" >${sample_id}.PFAM.terms.txt\n\n                echo -e \"\\\\n-- Done with the PFAM --\\\\n\"\n\n                echo -e \"\\\\n-- DONE with Trinotate --\\\\n\"\n\n                v=\\$( echo 3.2.1 )\n                echo \"Trinotate: \\$v\" >trinotate.version.txt\n\n                v=\\$( perl -v | head -n2 | grep version | cut -f 1 -d \")\" | cut -f 2 -d \"(\" | tr -d \"v\" )\n                echo \"Perl: \\$v\" >perl.version.txt\n                \"\"\"\n        }",
        "nb_lignes_process": 141,
        "string_script": "                \"\"\"\n                for x in `echo ${files}`;do\n                    echo \\${x} >>.vars.txt\n                done\n\n                assembly=\\$( cat .vars.txt | grep -E \"${sample_id}.*.fa\" | grep -v \".transdecoder.pep\" )\n                transdecoder=\\$( cat .vars.txt | grep -E \"${sample_id}.*.transdecoder.pep\" )\n                diamond_blastx=\\$( cat .vars.txt | grep \"${sample_id}.diamond_blastx.outfmt6\" )\n                diamond_blastp=\\$( cat .vars.txt | grep \"${sample_id}.diamond_blastp.outfmt6\" )\n                custom_blastx=\\$( cat .vars.txt | grep \"${sample_id}.custom.diamond_blastx.outfmt6\" )\n                custom_blastp=\\$( cat .vars.txt | grep \"${sample_id}.custom.diamond_blastp.outfmt6\" )\n                pfam=\\$( cat .vars.txt | grep \"${sample_id}.TrinotatePFAM.out\" )\n                signalp=\\$( cat .vars.txt | grep \"${sample_id}.signalp.out\" )\n                tmhmm=\\$( cat .vars.txt | grep \"${sample_id}.tmhmm.out\" )\n                rnammer=\\$( cat .vars.txt | grep \"${sample_id}.rnammer.gff\" )\n\n                #Generate gene_trans_map\n                #Not using get_Trinity_gene_to_trans_map.pl since all the names are uniq\n                cat \\${assembly} | awk '{print \\$1}' | grep \">\" | cut -c 2- >a.txt\n\n                #paste a.txt a.txt >\\${assembly}.gene_trans_map - does not work in container\n                touch \\${assembly}.gene_trans_map\n                for x in `cat a.txt`;do echo -e \\${x}\"\\\\t\"\\${x} >>\\${assembly}.gene_trans_map ;done\n\n                #Get Trinotate.sqlite from folder (original)\n                cp ${params.Tsql} .\n                sqlname=`echo ${params.Tsql} | tr \"\\\\/\" \"\\\\n\" | grep \"\\\\.sqlite\"`\n\n                echo -e \"\\\\n-- Running Trinotate --\\\\n\"\n\n                Trinotate \\$sqlname init --gene_trans_map \\${assembly}.gene_trans_map --transcript_fasta \\${assembly} --transdecoder_pep \\${transdecoder}\n\n                echo -e \"\\\\n-- Ending run of Trinotate --\\\\n\"\n\n                echo -e \"\\\\n-- Loading hits and predictions to sqlite database... --\\\\n\"\n\n                #Load protein hits\n                Trinotate \\$sqlname LOAD_swissprot_blastp \\${diamond_blastp}\n\n                #Load transcript hits\n                Trinotate \\$sqlname LOAD_swissprot_blastx \\${diamond_blastx}\n\n                #Load custom protein hits\n                Trinotate \\$sqlname LOAD_custom_blast --outfmt6 \\${custom_blastp} --prog blastp --dbtype ${params.uniname}\n\n                #Load custom transcript hits\n                Trinotate \\$sqlname LOAD_custom_blast --outfmt6 \\${custom_blastx} --prog blastx --dbtype ${params.uniname}\n\n                #Load Pfam domain entries\n                Trinotate \\$sqlname LOAD_pfam \\${pfam}\n\n                #Load transmembrane domains\n                if [ -s \\${tmhmm} ];then\n                    Trinotate \\$sqlname LOAD_tmhmm \\${tmhmm}\n                else\n                    echo \"No transmembrane domains (tmhmm)\"\n                fi\n\n                #Load signal peptide predictions\n                if [ -s \\${signalp} ];then\n                    Trinotate \\$sqlname LOAD_signalp \\${signalp}\n                else\n                    echo \"No Signal-P\"\n                fi\n\n                #Load rnammer results\n                if [ -s \\${rnammer} ];then\n                    Trinotate \\$sqlname LOAD_rnammer \\${rnammer}\n                else\n                    echo \"No rnammer results\"\n                fi\n\n                echo -e \"\\\\n-- Loading finished --\\\\n\"\n\n                #Report\n\n                echo -e \"\\\\n-- Generating report... --\\\\n\"\n\n                Trinotate \\$sqlname report >${sample_id}.trinotate_annotation_report.xls\n\n                echo -e \"\\\\n-- Report generated --\\\\n\"\n\n                #Extract info from XLS file\n\n                echo -e \"\\\\n-- Creating GO file from XLS... --\\\\n\"\n\n                extract_GO_assignments_from_Trinotate_xls.pl --Trinotate_xls ${sample_id}.trinotate_annotation_report.xls --trans >${sample_id}.GO.terms.txt\n\n                echo -e \"\\\\n-- Done with the GO --\\\\n\"\n\n                echo -e \"\\\\n-- Creating KEGG file from XLS... --\\\\n\"\n\n                cat ${sample_id}.trinotate_annotation_report.xls | cut -f 1,14 | grep \"KEGG\" | tr \"\\\\`\" \";\" | grep \"KO:K\" | sed 's/\\\\tKEGG/\\\\t#KEGG/g' | sed 's/KO:/KO:#/g' | cut -f 1,3 -d \"#\" | tr -d \"#\" >${sample_id}.KEGG.terms.txt\n\n                echo -e \"\\\\n-- Done with the KEGG --\\\\n\"\n\n                echo -e \"\\\\n-- Creating eggNOG file from XLS... --\\\\n\"\n\n                cat ${sample_id}.trinotate_annotation_report.xls | cut -f 1,13 | grep \"OG\" | tr \"\\\\`\" \";\" | sed 's/^/#/g' | sed 's/;/\\\\n;/g' | cut -f 1 -d \"^\" | tr -d \"\\\\n\" | tr \"#\" \"\\\\n\" | grep \"OG\" >${sample_id}.eggNOG_COG.terms.txt\n\n                echo -e \"\\\\n-- Done with the eggNOG --\\\\n\"\n\n                echo -e \"\\\\n-- Creating PFAM file from XLS... --\\\\n\"\n\n                cat ${sample_id}.trinotate_annotation_report.xls | cut -f 1,10 | grep \"PF\" | tr \"\\\\`\" \";\" | sed 's/^/#/g' | sed 's/;PF/\\\\n;PF/g' | cut -f 1 -d \"^\" | tr -d \"\\\\n\" | tr \"#\" \"\\\\n\" | grep \"PF\" | tr \";\" \",\" >${sample_id}.PFAM.terms.txt\n\n                echo -e \"\\\\n-- Done with the PFAM --\\\\n\"\n\n                echo -e \"\\\\n-- DONE with Trinotate --\\\\n\"\n\n                v=\\$( echo 3.2.1 )\n                echo \"Trinotate: \\$v\" >trinotate.version.txt\n\n                v=\\$( perl -v | head -n2 | grep version | cut -f 1 -d \")\" | cut -f 2 -d \"(\" | tr -d \"v\" )\n                echo \"Perl: \\$v\" >perl.version.txt\n                \"\"\"",
        "nb_lignes_script": 115,
        "language_script": "bash",
        "tools": [
            "Trinotate"
        ],
        "tools_url": [
            "https://bio.tools/trinotate"
        ],
        "tools_dico": [
            {
                "name": "Trinotate",
                "uri": "https://bio.tools/trinotate",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3672",
                                    "term": "Gene functional annotation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3672",
                                    "term": "Sequence functional annotation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Comprehensive annotation suite designed for automatic functional annotation of transcriptomes, particularly de novo assembled transcriptomes, from model or non-model organisms.",
                "homepage": "https://github.com/Trinotate/Trinotate.github.io"
            }
        ],
        "inputs": [
            "trinotate_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "trinotate_summary",
            "",
            "other_files",
            "kegg_paths",
            "trinotate_version",
            "perl_version"
        ],
        "nb_outputs": 6,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'low_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/trinotate\", mode: \"copy\", overwrite: true, pattern: \"*.{terms.txt,xls}\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::trinotate=3.2.2=pl5262hdfd78af_0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/trinotate:3.2.2--pl5262hdfd78af_0\" : \"quay.io/biocontainers/trinotate:3.2.2--pl5262hdfd78af_0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "get_GO_comparison": {
        "name_process": "get_GO_comparison",
        "string_process": " process get_GO_comparison {\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/figures/GO\", mode: \"copy\", overwrite: true, pattern: \"*.{svg,pdf,txt}\"\n            publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda r-reshape2=1.4.4 r-plotly=4.9.2.1 plotly-orca=3.4.2 r-ggplot2=3.3.0 r-svglite=1.2.3 r-ggthemes=4.2.0 r-knitr=1.29 r-rmarkdown=2.3 r-kableextra=1.1.0\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\" : \"quay.io/biocontainers/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\")\n            }\n\n            input:\n                tuple sample_id, file(\"${sample_id}.trinotate_annotation_report.xls\") from trinotate_out_ch\n\n            output:\n                tuple sample_id, file(\"*.svg\"), file(\"*.pdf\") , file(\"*.txt\") into go_fig\n                tuple sample_id, file(\"*.csv\") into go_csv\n                file(\"r.version.txt\") into r_version\n\n            script:\n                \"\"\"\n                set +e\n    \t        cp ${params.pipeInstall}/bin/GO_plots.R .\n\n                cat ${sample_id}.trinotate_annotation_report.xls | awk 'FS=\"\\\\t\",OFS=\"#\" {print \\$1,\\$15,\\$16,\\$17}' | grep -v \"gene_id\" >all_GOs.txt\n\n                touch final_GOs.txt\n\n                while read line;do\n                    echo \\${line} | cut -f 2,3,4 -d \"#\" | grep \"GO:\" | tr \"#\" \"\\\\n\" | tr \"\\\\`\" \"\\\\n\" | sed 's/\\\\. /,/g' | tr \",\" \"\\\\n\" | grep \"GO:\" | sort -u >>final_GOs.txt\n                done<all_GOs.txt\n\n                cat final_GOs.txt | tr [a-z] [A-Z] | grep \"CELLULAR_COMPONENT\" | cut -f 3 -d \"^\" | sort | uniq -c | sort -nr | head -n 15 | sed 's/^ *//g' \\\n                | sed 's/\\\\([0-9] \\\\)/\\\\1#/g' | tr \"#\" \"\\\\t\" >GO_cellular.txt\n\n                cat final_GOs.txt | tr [a-z] [A-Z] | grep \"BIOLOGICAL_PROCESS\" | cut -f 3 -d \"^\" | sort | uniq -c | sort -nr | head -n 15 | sed 's/^ *//g' \\\n                | sed 's/\\\\([0-9] \\\\)/\\\\1#/g' | tr \"#\" \"\\\\t\" >GO_biological.txt\n\n                cat final_GOs.txt | tr [a-z] [A-Z] | grep \"MOLECULAR_FUNCTION\" | cut -f 3 -d \"^\" | sort | uniq -c | sort -nr | head -n 15 | sed 's/^ *//g' \\\n                | sed 's/\\\\([0-9] \\\\)/\\\\1#/g' | tr \"#\" \"\\\\t\" >GO_molecular.txt\n\n                Rscript GO_plots.R ${sample_id}\n\n                rm all_GOs.txt final_GOs.txt\n\n                mv GO_cellular.txt ${sample_id}_GO_cellular.txt\n                mv GO_biological.txt ${sample_id}_GO_biological.txt\n                mv GO_molecular.txt ${sample_id}_GO_molecular.txt\n\n                cat ${sample_id}_GO_cellular.txt | sed -r 's/^[^0-9]*([0-9]+)/\\\\1,/g' >${sample_id}_GO_cellular.csv\n                cat ${sample_id}_GO_biological.txt | sed -r 's/^[^0-9]*([0-9]+)/\\\\1,/g' >${sample_id}_GO_biological.csv\n                cat ${sample_id}_GO_molecular.txt | sed -r 's/^[^0-9]*([0-9]+)/\\\\1,/g' >${sample_id}_GO_molecular.csv\n\n                v=\\$( R --version | grep \"R version\" | awk '{print \\$3}' )\n                echo \"R: \\$v\" >r.version.txt\n                \"\"\"\n        }",
        "nb_lignes_process": 56,
        "string_script": "                \"\"\"\n                set +e\n    \t        cp ${params.pipeInstall}/bin/GO_plots.R .\n\n                cat ${sample_id}.trinotate_annotation_report.xls | awk 'FS=\"\\\\t\",OFS=\"#\" {print \\$1,\\$15,\\$16,\\$17}' | grep -v \"gene_id\" >all_GOs.txt\n\n                touch final_GOs.txt\n\n                while read line;do\n                    echo \\${line} | cut -f 2,3,4 -d \"#\" | grep \"GO:\" | tr \"#\" \"\\\\n\" | tr \"\\\\`\" \"\\\\n\" | sed 's/\\\\. /,/g' | tr \",\" \"\\\\n\" | grep \"GO:\" | sort -u >>final_GOs.txt\n                done<all_GOs.txt\n\n                cat final_GOs.txt | tr [a-z] [A-Z] | grep \"CELLULAR_COMPONENT\" | cut -f 3 -d \"^\" | sort | uniq -c | sort -nr | head -n 15 | sed 's/^ *//g' \\\n                | sed 's/\\\\([0-9] \\\\)/\\\\1#/g' | tr \"#\" \"\\\\t\" >GO_cellular.txt\n\n                cat final_GOs.txt | tr [a-z] [A-Z] | grep \"BIOLOGICAL_PROCESS\" | cut -f 3 -d \"^\" | sort | uniq -c | sort -nr | head -n 15 | sed 's/^ *//g' \\\n                | sed 's/\\\\([0-9] \\\\)/\\\\1#/g' | tr \"#\" \"\\\\t\" >GO_biological.txt\n\n                cat final_GOs.txt | tr [a-z] [A-Z] | grep \"MOLECULAR_FUNCTION\" | cut -f 3 -d \"^\" | sort | uniq -c | sort -nr | head -n 15 | sed 's/^ *//g' \\\n                | sed 's/\\\\([0-9] \\\\)/\\\\1#/g' | tr \"#\" \"\\\\t\" >GO_molecular.txt\n\n                Rscript GO_plots.R ${sample_id}\n\n                rm all_GOs.txt final_GOs.txt\n\n                mv GO_cellular.txt ${sample_id}_GO_cellular.txt\n                mv GO_biological.txt ${sample_id}_GO_biological.txt\n                mv GO_molecular.txt ${sample_id}_GO_molecular.txt\n\n                cat ${sample_id}_GO_cellular.txt | sed -r 's/^[^0-9]*([0-9]+)/\\\\1,/g' >${sample_id}_GO_cellular.csv\n                cat ${sample_id}_GO_biological.txt | sed -r 's/^[^0-9]*([0-9]+)/\\\\1,/g' >${sample_id}_GO_biological.csv\n                cat ${sample_id}_GO_molecular.txt | sed -r 's/^[^0-9]*([0-9]+)/\\\\1,/g' >${sample_id}_GO_molecular.csv\n\n                v=\\$( R --version | grep \"R version\" | awk '{print \\$3}' )\n                echo \"R: \\$v\" >r.version.txt\n                \"\"\"",
        "nb_lignes_script": 35,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "trinotate_out_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "go_fig",
            "go_csv",
            "r_version"
        ],
        "nb_outputs": 3,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/figures/GO\", mode: \"copy\", overwrite: true, pattern: \"*.{svg,pdf,txt}\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda r-reshape2=1.4.4 r-plotly=4.9.2.1 plotly-orca=3.4.2 r-ggplot2=3.3.0 r-svglite=1.2.3 r-ggthemes=4.2.0 r-knitr=1.29 r-rmarkdown=2.3 r-kableextra=1.1.0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\" : \"quay.io/biocontainers/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "summary_custom_uniprot": {
        "name_process": "summary_custom_uniprot",
        "string_process": " process summary_custom_uniprot {\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/figures/CustomUniProt\", mode: \"copy\", overwrite: true\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda r-reshape2=1.4.4 r-plotly=4.9.2.1 plotly-orca=3.4.2 r-ggplot2=3.3.0 r-svglite=1.2.3 r-ggthemes=4.2.0 r-knitr=1.29 r-rmarkdown=2.3 r-kableextra=1.1.0\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\" : \"quay.io/biocontainers/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\")\n            }\n\n            input:\n                tuple sample_id, file(\"${sample_id}.trinotate_annotation_report.xls\") from custom_uniprot_ch\n\n            output:\n                tuple sample_id, file(\"${sample_id}_custom_uniprot_hits.txt\") into custom_uniprot_sum\n                tuple sample_id, file(\"${sample_id}_custom_uniprot_hits.svg\"), file(\"${sample_id}_custom_uniprot_hits.pdf\") into custom_uniprot_fig\n                tuple sample_id, file(\"*.csv\") into uniprot_csv\n\n            script:\n                \"\"\"\n                #get custom blast hits\n                cat ${sample_id}.trinotate_annotation_report.xls | cut -f 8 | grep [A-Z] | grep \"|\" | tr \"\\\\`\" \"\\\\n\" | \\\n                    cut -f 1 -d \"^\" | cut -f 3 -d \"|\" | cut -f 2 -d \"_\" >a.txt\n\n                cat ${sample_id}.trinotate_annotation_report.xls | cut -f 9 | grep [A-Z] | grep \"|\" | tr \"\\\\`\" \"\\\\n\" | \\\n                    cut -f 1 -d \"^\" | cut -f 3 -d \"|\" | cut -f 2 -d \"_\" >b.txt\n\n                cat a.txt b.txt | sort | uniq -c | sort -nr | head -n 20 | awk 'OFS=\",\" {print \\$1,\\$2}' >${sample_id}_custom_uniprot_hits.txt\n\n                rm a.txt b.txt\n\n                cp ${params.pipeInstall}/conf/uni_tax.txt .\n\n                cp ${sample_id}_custom_uniprot_hits.txt ${sample_id}_custom_uniprot_hits\n\n                while read line;do\n                    a=\\$( echo \\${line} | cut -f 2 -d \",\" )\n                    b=\\$( cat uni_tax.txt | grep \"\\${a}\" | cut -f 2 -d \",\" | wc -l )\n                    if [ \"\\${b}\" == \"1\" ];then\n                        c=\\$( cat uni_tax.txt | grep \"\\${a}\" | cut -f 2 -d \",\" )\n                        sed -i \"s/\\${a}/\\${c}/\" ${sample_id}_custom_uniprot_hits\n                    fi\n                done <${sample_id}_custom_uniprot_hits.txt\n\n                rm ${sample_id}_custom_uniprot_hits.txt uni_tax.txt\n                mv ${sample_id}_custom_uniprot_hits ${sample_id}_custom_uniprot_hits.txt\n\n                cp ${params.pipeInstall}/bin/custom_uniprot_hits.R .\n                Rscript custom_uniprot_hits.R ${sample_id}\n\n                cp ${sample_id}_custom_uniprot_hits.txt ${sample_id}_custom_uniprot_hits.csv\n                \"\"\"\n        }",
        "nb_lignes_process": 52,
        "string_script": "                \"\"\"\n                #get custom blast hits\n                cat ${sample_id}.trinotate_annotation_report.xls | cut -f 8 | grep [A-Z] | grep \"|\" | tr \"\\\\`\" \"\\\\n\" | \\\n                    cut -f 1 -d \"^\" | cut -f 3 -d \"|\" | cut -f 2 -d \"_\" >a.txt\n\n                cat ${sample_id}.trinotate_annotation_report.xls | cut -f 9 | grep [A-Z] | grep \"|\" | tr \"\\\\`\" \"\\\\n\" | \\\n                    cut -f 1 -d \"^\" | cut -f 3 -d \"|\" | cut -f 2 -d \"_\" >b.txt\n\n                cat a.txt b.txt | sort | uniq -c | sort -nr | head -n 20 | awk 'OFS=\",\" {print \\$1,\\$2}' >${sample_id}_custom_uniprot_hits.txt\n\n                rm a.txt b.txt\n\n                cp ${params.pipeInstall}/conf/uni_tax.txt .\n\n                cp ${sample_id}_custom_uniprot_hits.txt ${sample_id}_custom_uniprot_hits\n\n                while read line;do\n                    a=\\$( echo \\${line} | cut -f 2 -d \",\" )\n                    b=\\$( cat uni_tax.txt | grep \"\\${a}\" | cut -f 2 -d \",\" | wc -l )\n                    if [ \"\\${b}\" == \"1\" ];then\n                        c=\\$( cat uni_tax.txt | grep \"\\${a}\" | cut -f 2 -d \",\" )\n                        sed -i \"s/\\${a}/\\${c}/\" ${sample_id}_custom_uniprot_hits\n                    fi\n                done <${sample_id}_custom_uniprot_hits.txt\n\n                rm ${sample_id}_custom_uniprot_hits.txt uni_tax.txt\n                mv ${sample_id}_custom_uniprot_hits ${sample_id}_custom_uniprot_hits.txt\n\n                cp ${params.pipeInstall}/bin/custom_uniprot_hits.R .\n                Rscript custom_uniprot_hits.R ${sample_id}\n\n                cp ${sample_id}_custom_uniprot_hits.txt ${sample_id}_custom_uniprot_hits.csv\n                \"\"\"",
        "nb_lignes_script": 32,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "custom_uniprot_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "custom_uniprot_sum",
            "custom_uniprot_fig",
            "uniprot_csv"
        ],
        "nb_outputs": 3,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/figures/CustomUniProt\", mode: \"copy\", overwrite: true",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda r-reshape2=1.4.4 r-plotly=4.9.2.1 plotly-orca=3.4.2 r-ggplot2=3.3.0 r-svglite=1.2.3 r-ggthemes=4.2.0 r-knitr=1.29 r-rmarkdown=2.3 r-kableextra=1.1.0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\" : \"quay.io/biocontainers/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "get_kegg": {
        "name_process": "get_kegg",
        "string_process": " process get_kegg {\n\n                tag \"${sample_id}\"\n\n                publishDir \"${params.outdir}/figures/kegg\", mode: \"copy\", overwrite: true\n\n                input:\n                    tuple sample_id, file(kegg) from kegg_paths\n\n                output:\n                    tuple sample_id, file(\"${sample_id}_kegg.svg\") into kegg_report\n\n                script:\n                    \"\"\"\n                    awk '{print \\$2}' ${kegg} >kegg_terms\n                    curl -X POST --data-urlencode \"selection@kegg_terms\" -d \"export_type=svg\" -d \"default_opacity=.5\" -d \"default_width=2\" -d \"default_radius=5\" https://pathways.embl.de/mapping.cgi >${sample_id}_kegg.svg\n                    \"\"\"\n            }",
        "nb_lignes_process": 16,
        "string_script": "                    \"\"\"\n                    awk '{print \\$2}' ${kegg} >kegg_terms\n                    curl -X POST --data-urlencode \"selection@kegg_terms\" -d \"export_type=svg\" -d \"default_opacity=.5\" -d \"default_width=2\" -d \"default_radius=5\" https://pathways.embl.de/mapping.cgi >${sample_id}_kegg.svg\n                    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [
            "CURLS"
        ],
        "tools_url": [
            "https://bio.tools/CURLS"
        ],
        "tools_dico": [
            {
                "name": "CURLS",
                "uri": "https://bio.tools/CURLS",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3335",
                            "term": "Cardiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "Public health and epidemiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3421",
                            "term": "Surgery"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "Pathology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3335",
                            "term": "Cardiovascular medicine"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Public_health"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3305",
                            "term": "https://en.wikipedia.org/wiki/Epidemiology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3421",
                            "term": "https://en.wikipedia.org/wiki/Surgery"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "Disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0634",
                            "term": "https://en.wikipedia.org/wiki/Pathology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "towards a wider use of basic echo applications in Africa.\n\nBACKGROUND:Point-of-care ultrasound is increasingly being used as a diagnostic tool in resource-limited settings. The majority of existing ultrasound protocols have been developed and implemented in high-resource settings. In sub-Saharan Africa (SSA), patients with heart failure of various etiologies commonly present late in the disease process, with a similar syndrome of dyspnea, edema and cardiomegaly on chest X-ray. The causes of heart failure in SSA differ from those in high-resource settings. Point-of-care ultrasound has the potential to identify the underlying etiology of heart failure, and lead to targeted therapy.\n\n||| HOMEPAGE MISSING!.\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'ultrasound', 'Cardiac ultrasound resource-limited settings', 'high-resource', 'cardiomegaly SSA'",
                "homepage": "https://www.ncbi.nlm.nih.gov/pubmed/?term=31883027"
            }
        ],
        "inputs": [
            "kegg_paths"
        ],
        "nb_inputs": 1,
        "outputs": [
            "kegg_report"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/figures/kegg\", mode: \"copy\", overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "skip_kegg": {
        "name_process": "skip_kegg",
        "string_process": " process skip_kegg {\n                tag \"${sample_id}\"\n\n                input:\n                    tuple sample_id, file(kegg) from kegg_paths\n\n                output:\n                    tuple sample_id, file(\"${sample_id}_kegg.svg\") into kegg_report\n\n                script:\n                    \"\"\"\n                    touch ${sample_id}_kegg.svg\n                    \"\"\"\n            }",
        "nb_lignes_process": 12,
        "string_script": "                    \"\"\"\n                    touch ${sample_id}_kegg.svg\n                    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "kegg_paths"
        ],
        "nb_inputs": 1,
        "outputs": [
            "kegg_report"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\""
        ],
        "when": "",
        "stub": ""
    },
    "get_transcript_dist": {
        "name_process": "get_transcript_dist",
        "string_process": " process get_transcript_dist {\n\n            tag \"${sample_id}\"\n\n            publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda biopython=1.78 pandas=1.1.2 numpy=1.18.1\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-1e9d4f78feac0eb2c8d8246367973b3f6358defc:41ffac721ff9b03ca1121742e969d0e7d78e589f-0\" : \"quay.io/biocontainers/mulled-v2-1e9d4f78feac0eb2c8d8246367973b3f6358defc:41ffac721ff9b03ca1121742e969d0e7d78e589f-0\")\n            }\n\n            input:\n                tuple sample_id, file(dist) from evi_dist\n\n            output:\n                tuple sample_id, file(\"${sample_id}_sizes.txt\") into size_dist\n                file(\"python.version.txt\") into python_version\n\n            script:\n                \"\"\"\n                len.py ${dist} >all_transcript_sizes.txt\n                mv all_transcript_sizes.txt ${sample_id}_sizes.txt\n\n                v=\\$( python --version | cut -f 2 -d \" \" )\n                echo \"Python: \\$v\" >python.version.txt\n                \"\"\"\n        }",
        "nb_lignes_process": 25,
        "string_script": "                \"\"\"\n                len.py ${dist} >all_transcript_sizes.txt\n                mv all_transcript_sizes.txt ${sample_id}_sizes.txt\n\n                v=\\$( python --version | cut -f 2 -d \" \" )\n                echo \"Python: \\$v\" >python.version.txt\n                \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "evi_dist"
        ],
        "nb_inputs": 1,
        "outputs": [
            "size_dist",
            "python_version"
        ],
        "nb_outputs": 2,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda biopython=1.78 pandas=1.1.2 numpy=1.18.1\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-1e9d4f78feac0eb2c8d8246367973b3f6358defc:41ffac721ff9b03ca1121742e969d0e7d78e589f-0\" : \"quay.io/biocontainers/mulled-v2-1e9d4f78feac0eb2c8d8246367973b3f6358defc:41ffac721ff9b03ca1121742e969d0e7d78e589f-0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "summary_transdecoder_individual": {
        "name_process": "summary_transdecoder_individual",
        "string_process": " process summary_transdecoder_individual {\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/stats\", mode: \"copy\", overwrite: true\n\n            input:\n                tuple sample_id, file(\"${sample_id}_transdecoder.stats\") from transdecoder_summary\n\n            output:\n                tuple sample_id, file(\"${sample_id}.sum_transdecoder.txt\") into final_sum_3\n\n            script:\n                \"\"\"\n                #Summary of Transdecoder stats\n                echo -e \"Summary of Transdecoder \\\\n\" >>${sample_id}.sum_transdecoder.txt\n                cat ${sample_id}_transdecoder.stats >>${sample_id}.sum_transdecoder.txt\n                echo -e \"##### \\\\n\" >>${sample_id}.sum_transdecoder.txt\n                \"\"\"\n        }",
        "nb_lignes_process": 18,
        "string_script": "                \"\"\"\n                #Summary of Transdecoder stats\n                echo -e \"Summary of Transdecoder \\\\n\" >>${sample_id}.sum_transdecoder.txt\n                cat ${sample_id}_transdecoder.stats >>${sample_id}.sum_transdecoder.txt\n                echo -e \"##### \\\\n\" >>${sample_id}.sum_transdecoder.txt\n                \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "transdecoder_summary"
        ],
        "nb_inputs": 1,
        "outputs": [
            "final_sum_3"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/stats\", mode: \"copy\", overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "summary_trinotate_individual": {
        "name_process": "summary_trinotate_individual",
        "string_process": " process summary_trinotate_individual {\n\n            tag \"${sample_id}\"\n\n            publishDir \"${params.outdir}/stats\", mode: \"copy\", overwrite: true\n\n            input:\n                tuple sample_id, file(\"${sample_id}.GO.terms.txt\") from trinotate_summary\n\n            output:\n                tuple sample_id, file(\"${sample_id}.sum_GO.txt\") into final_sum_4\n\n            script:\n                \"\"\"\n                #Summary of Trinotate (Gene Ontologies)\n                echo -e \"Summary of Trinotate/Gene Ontologies \\\\n\" >>${sample_id}.sum_GO.txt\n                echo \"- Individual \"${sample_id} >>${sample_id}.sum_GO.txt\n                echo -e \"\\\\t Total transcripts with GO:\" >>${sample_id}.sum_GO.txt\n                num=\\$( cat ${sample_id}.GO.terms.txt | wc -l )\n                echo -e \"\\\\t\\\\t \\$num\" >>${sample_id}.sum_GO.txt\n                tnum=\\$num\n\n                echo -e \"\\\\t Total transcripts with only one GO:\" >>${sample_id}.sum_GO.txt\n                a=0\n                while read lines;do\n                    if [[ `echo \\$lines | awk '{print \\$2}' | tr \",\" \"\\\\n\" | wc -l` -eq 1 ]];then\n                        a=\\$((a+1))\n                    fi\n                done <${sample_id}.GO.terms.txt\n                num=\\$a\n                echo -e \"\\\\t\\\\t \\$num\" >>${sample_id}.sum_GO.txt\n                onum=\\$num\n\n                echo -e \"\\\\t Total transcripts with multiple GO:\" >>${sample_id}.sum_GO.txt\n                num=\\$( echo \\$tnum-\\$onum | bc )\n                echo -e \"\\\\t\\\\t \\$num\" >>${sample_id}.sum_GO.txt\n\n                echo -e \"\\\\t Total GO in the file:\" >>${sample_id}.sum_GO.txt\n                num=\\$( cat ${sample_id}.GO.terms.txt | awk '{print \\$2}' | tr \",\" \"\\\\n\" | wc -l )\n                echo -e \"\\\\t\\\\t \\$num\" >>${sample_id}.sum_GO.txt\n\n                echo -e \"\\\\t Total uniq GO in the file:\" >>${sample_id}.sum_GO.txt\n                num=\\$( cat ${sample_id}.GO.terms.txt | awk '{print \\$2}' | tr \",\" \"\\\\n\" | sort --parallel=10 | uniq | wc -l )\n                echo -e \"\\\\t\\\\t \\$num\" >>${sample_id}.sum_GO.txt\n                \"\"\"\n        }",
        "nb_lignes_process": 44,
        "string_script": "                \"\"\"\n                #Summary of Trinotate (Gene Ontologies)\n                echo -e \"Summary of Trinotate/Gene Ontologies \\\\n\" >>${sample_id}.sum_GO.txt\n                echo \"- Individual \"${sample_id} >>${sample_id}.sum_GO.txt\n                echo -e \"\\\\t Total transcripts with GO:\" >>${sample_id}.sum_GO.txt\n                num=\\$( cat ${sample_id}.GO.terms.txt | wc -l )\n                echo -e \"\\\\t\\\\t \\$num\" >>${sample_id}.sum_GO.txt\n                tnum=\\$num\n\n                echo -e \"\\\\t Total transcripts with only one GO:\" >>${sample_id}.sum_GO.txt\n                a=0\n                while read lines;do\n                    if [[ `echo \\$lines | awk '{print \\$2}' | tr \",\" \"\\\\n\" | wc -l` -eq 1 ]];then\n                        a=\\$((a+1))\n                    fi\n                done <${sample_id}.GO.terms.txt\n                num=\\$a\n                echo -e \"\\\\t\\\\t \\$num\" >>${sample_id}.sum_GO.txt\n                onum=\\$num\n\n                echo -e \"\\\\t Total transcripts with multiple GO:\" >>${sample_id}.sum_GO.txt\n                num=\\$( echo \\$tnum-\\$onum | bc )\n                echo -e \"\\\\t\\\\t \\$num\" >>${sample_id}.sum_GO.txt\n\n                echo -e \"\\\\t Total GO in the file:\" >>${sample_id}.sum_GO.txt\n                num=\\$( cat ${sample_id}.GO.terms.txt | awk '{print \\$2}' | tr \",\" \"\\\\n\" | wc -l )\n                echo -e \"\\\\t\\\\t \\$num\" >>${sample_id}.sum_GO.txt\n\n                echo -e \"\\\\t Total uniq GO in the file:\" >>${sample_id}.sum_GO.txt\n                num=\\$( cat ${sample_id}.GO.terms.txt | awk '{print \\$2}' | tr \",\" \"\\\\n\" | sort --parallel=10 | uniq | wc -l )\n                echo -e \"\\\\t\\\\t \\$num\" >>${sample_id}.sum_GO.txt\n                \"\"\"",
        "nb_lignes_script": 31,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "trinotate_summary"
        ],
        "nb_inputs": 1,
        "outputs": [
            "final_sum_4"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/stats\", mode: \"copy\", overwrite: true"
        ],
        "when": "",
        "stub": ""
    },
    "update_host_ids": {
        "name_process": "update_host_ids",
        "string_process": " process update_host_ids {\n\n                    label 'med_cpus'\n\n                \tinput:\n                \t   file(host) from host_sequences\n\n                \toutput:\n                \t   file(\"updated_host.fasta\") into updated_host\n\n                    script:\n                    if (hasExtension(host_sequences, 'gz')) {\n                        \"\"\"\n                    \tzcat ${host} | awk '{if (\\$1 ~ /^ *>/ ) {print substr(\\$1,0,1)\"species1_\" substr(\\$1,2,length(\\$1))} else {print \\$1} }' >updated_host.fasta\n                    \t\"\"\"\n                    } else {\n                    \t\"\"\"\n                    \tawk '{if (\\$1 ~ /^ *>/ ) {print substr(\\$1,0,1)\"species1_\" substr(\\$1,2,length(\\$1))} else {print \\$1} }' < ${host} >updated_host.fasta\n                    \t\"\"\"\n                    }\n                }",
        "nb_lignes_process": 19,
        "string_script": "                    if (hasExtension(host_sequences, 'gz')) {\n                        \"\"\"\n                    \tzcat ${host} | awk '{if (\\$1 ~ /^ *>/ ) {print substr(\\$1,0,1)\"species1_\" substr(\\$1,2,length(\\$1))} else {print \\$1} }' >updated_host.fasta\n                    \t\"\"\"\n                    } else {\n                    \t\"\"\"\n                    \tawk '{if (\\$1 ~ /^ *>/ ) {print substr(\\$1,0,1)\"species1_\" substr(\\$1,2,length(\\$1))} else {print \\$1} }' < ${host} >updated_host.fasta\n                    \t\"\"\"\n                    }",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "host_sequences"
        ],
        "nb_inputs": 1,
        "outputs": [
            "updated_host"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'"
        ],
        "when": "",
        "stub": ""
    },
    "update_symbio_ids": {
        "name_process": "update_symbio_ids",
        "string_process": " process update_symbio_ids {\n\n                    label 'med_cpus'\n\n                    input:\n                        file(symbiont) from symbiont_sequences\n\n                    output:\n                        file(\"updated_symbio.fasta\") into updated_symbiont\n\n                    script:\n                    if (hasExtension(symbiont_sequences, 'gz')) {\n                        \"\"\"\n                        zcat ${symbiont} | awk '{if (\\$1 ~ /^ *>/ ) {print substr(\\$1,0,1)\"species2_\" substr(\\$1,2,length(\\$1))} else {print \\$1} }' >updated_symbio.fasta\n                        \"\"\"\n                    } else {\n                        \"\"\"\n                        awk '{if (\\$1 ~ /^ *>/ ) {print substr(\\$1,0,1)\"species2_\" substr(\\$1,2,length(\\$1))} else {print \\$1} }' < ${symbiont} >updated_symbio.fasta\n                        \"\"\"\n                    }\n                }",
        "nb_lignes_process": 19,
        "string_script": "                    if (hasExtension(symbiont_sequences, 'gz')) {\n                        \"\"\"\n                        zcat ${symbiont} | awk '{if (\\$1 ~ /^ *>/ ) {print substr(\\$1,0,1)\"species2_\" substr(\\$1,2,length(\\$1))} else {print \\$1} }' >updated_symbio.fasta\n                        \"\"\"\n                    } else {\n                        \"\"\"\n                        awk '{if (\\$1 ~ /^ *>/ ) {print substr(\\$1,0,1)\"species2_\" substr(\\$1,2,length(\\$1))} else {print \\$1} }' < ${symbiont} >updated_symbio.fasta\n                        \"\"\"\n                    }",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "symbiont_sequences"
        ],
        "nb_inputs": 1,
        "outputs": [
            "updated_symbiont"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'"
        ],
        "when": "",
        "stub": ""
    },
    "concatenate_sets": {
        "name_process": "concatenate_sets",
        "string_process": " process concatenate_sets {\n\n                    label 'exlow_cpus'\n\n                \tinput:\n                    \tfile(host) from updated_host\n                    \tfile(symbiont) from updated_symbiont\n\n                \toutput:\n                \t   file(\"concatenated_file.fasta\") into file_for_database\n\n                    script:\n                    \t\"\"\"\n                    \tcat ${host} ${symbiont} >concatenated_file.fasta\n                    \t\"\"\"\n                }",
        "nb_lignes_process": 14,
        "string_script": "                    \t\"\"\"\n                    \tcat ${host} ${symbiont} >concatenated_file.fasta\n                    \t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "updated_host",
            "updated_symbiont"
        ],
        "nb_inputs": 2,
        "outputs": [
            "file_for_database"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'exlow_cpus'"
        ],
        "when": "",
        "stub": ""
    },
    "create_diamond_db": {
        "name_process": "create_diamond_db",
        "string_process": " process create_diamond_db {\n\n                    label 'low_mem'\n\n                    conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::diamond=0.9.30=h56fc30b_0\" : null)\n                    if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                    container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/diamond:0.9.30--h56fc30b_0\" : \"quay.io/biocontainers/diamond:0.9.30--h56fc30b_0\")\n                    }\n\n                \tinput:\n                \t   file(seqs) from file_for_database\n\n                \toutput:\n                \t   file(\"*.dmnd\") into diamond_db\n\n                    script:\n                    \t\"\"\"\n                    \tdiamond makedb --in ${seqs} -d diamond_database -p ${task.cpus}\n                    \t\"\"\"\n                }",
        "nb_lignes_process": 18,
        "string_script": "                    \t\"\"\"\n                    \tdiamond makedb --in ${seqs} -d diamond_database -p ${task.cpus}\n                    \t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "Diamond"
        ],
        "tools_url": [
            "https://bio.tools/diamond"
        ],
        "tools_dico": [
            {
                "name": "Diamond",
                "uri": "https://bio.tools/diamond",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0258",
                                    "term": "Sequence alignment analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Sequence aligner for protein and translated DNA searches and functions as a drop-in replacement for the NCBI BLAST software tools. It is suitable for protein-protein search as well as DNA-protein search on short reads and longer sequences including contigs and assemblies, providing a speedup of BLAST ranging up to x20,000.",
                "homepage": "https://github.com/bbuchfink/diamond"
            }
        ],
        "inputs": [
            "file_for_database"
        ],
        "nb_inputs": 1,
        "outputs": [
            "diamond_db"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'low_mem'",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::diamond=0.9.30=h56fc30b_0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/diamond:0.9.30--h56fc30b_0\" : \"quay.io/biocontainers/diamond:0.9.30--h56fc30b_0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "diamond_run": {
        "name_process": "diamond_run",
        "string_process": " process diamond_run {\n\n                    label 'med_cpus'\n\n                    tag \"${sample_id}\"\n\n                    conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::diamond=0.9.30=h56fc30b_0\" : null)\n                    if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                    container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/diamond:0.9.30--h56fc30b_0\" : \"quay.io/biocontainers/diamond:0.9.30--h56fc30b_0\")\n                    }\n\n                \tinput:\n                    \ttuple sample_id, file(transcriptome) from transcriptome_sequences1\n                    \tfile(database) from diamond_db\n\n                \toutput:\n                \t   file(\"*.tabular\") into diamond_output\n\n                    script:\n                    \t\"\"\"\n                    \tdiamond blastx -d ${database} -q ${transcriptome} -p ${task.cpus} -f 6 -o diamond_output.tabular\n                    \t\"\"\"\n                }",
        "nb_lignes_process": 21,
        "string_script": "                    \t\"\"\"\n                    \tdiamond blastx -d ${database} -q ${transcriptome} -p ${task.cpus} -f 6 -o diamond_output.tabular\n                    \t\"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [
            "Diamond"
        ],
        "tools_url": [
            "https://bio.tools/diamond"
        ],
        "tools_dico": [
            {
                "name": "Diamond",
                "uri": "https://bio.tools/diamond",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Proteins"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0078",
                            "term": "Protein informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0258",
                                    "term": "Sequence alignment analysis"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Sequence aligner for protein and translated DNA searches and functions as a drop-in replacement for the NCBI BLAST software tools. It is suitable for protein-protein search as well as DNA-protein search on short reads and longer sequences including contigs and assemblies, providing a speedup of BLAST ranging up to x20,000.",
                "homepage": "https://github.com/bbuchfink/diamond"
            }
        ],
        "inputs": [
            "transcriptome_sequences1",
            "diamond_db"
        ],
        "nb_inputs": 2,
        "outputs": [
            "diamond_output"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'",
            "tag \"${sample_id}\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge bioconda::diamond=0.9.30=h56fc30b_0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/diamond:0.9.30--h56fc30b_0\" : \"quay.io/biocontainers/diamond:0.9.30--h56fc30b_0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "psytrans_run": {
        "name_process": "psytrans_run",
        "string_process": " process psytrans_run {\n\n                    label 'med_cpus'\n\n                    tag \"${sample_id}\"\n\n                \tpublishDir \"${params.outdir}/psytrans_output/\", mode: \"copy\", overwrite: true, pattern: \"*.{fasta}\"\n                    publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n                    conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda biopython=1.78 pandas=1.1.2 numpy=1.18.1\" : null)\n                    if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n                    container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-1e9d4f78feac0eb2c8d8246367973b3f6358defc:41ffac721ff9b03ca1121742e969d0e7d78e589f-0\" : \"quay.io/biocontainers/mulled-v2-1e9d4f78feac0eb2c8d8246367973b3f6358defc:41ffac721ff9b03ca1121742e969d0e7d78e589f-0\")\n                    }\n\n                \tinput:\n                    \ttuple sample_id, file(transcriptome) from transcriptome_sequences2\n                    \tfile(host) from updated_host\n                    \tfile(symbiont) from updated_symbiont\n                    \tfile(database) from diamond_output\n\n                \toutput:\n                \t   file(\"*.fasta\") into psytrans\n                       file(\"psytrans.version.txt\") into psytrans_version\n\n                    script:\n                    \t\"\"\"\n                    \tmkdir temp\n                    \tpsytrans.py ${transcriptome} -A ${host} -B ${symbiont} -b ${database} -t temp -n ${params.psyval}\n                        mv species1_${transcriptome} ${sample_id}_only.fasta\n                        mv species2_${transcriptome} ${sample_id}_removed.fasta\n                        v=\\$( echo 1.0.0 )\n                        echo \"Psytrans: \\$v\" >psytrans.version.txt\n                    \t\"\"\"\n                }",
        "nb_lignes_process": 32,
        "string_script": "                    \t\"\"\"\n                    \tmkdir temp\n                    \tpsytrans.py ${transcriptome} -A ${host} -B ${symbiont} -b ${database} -t temp -n ${params.psyval}\n                        mv species1_${transcriptome} ${sample_id}_only.fasta\n                        mv species2_${transcriptome} ${sample_id}_removed.fasta\n                        v=\\$( echo 1.0.0 )\n                        echo \"Psytrans: \\$v\" >psytrans.version.txt\n                    \t\"\"\"",
        "nb_lignes_script": 7,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "transcriptome_sequences2",
            "updated_host",
            "updated_symbiont",
            "diamond_output"
        ],
        "nb_inputs": 4,
        "outputs": [
            "psytrans",
            "psytrans_version"
        ],
        "nb_outputs": 2,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/psytrans_output/\", mode: \"copy\", overwrite: true, pattern: \"*.{fasta}\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda biopython=1.78 pandas=1.1.2 numpy=1.18.1\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-1e9d4f78feac0eb2c8d8246367973b3f6358defc:41ffac721ff9b03ca1121742e969d0e7d78e589f-0\" : \"quay.io/biocontainers/mulled-v2-1e9d4f78feac0eb2c8d8246367973b3f6358defc:41ffac721ff9b03ca1121742e969d0e7d78e589f-0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "add_annotation_info": {
        "name_process": "add_annotation_info",
        "string_process": " process add_annotation_info {\n\n                label 'med_cpus'\n\n                tag \"${sample_id}\"\n\n                publishDir \"${params.outdir}/addAnnotation/\", mode: \"copy\", overwrite: true, pattern: \"*.{fasta}\"\n                publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\"\n\n                input:\n                    tuple file(assembly), file(annotation) from\n\n                output:\n                    file(\"*.fasta\") into\n\n                script:\n                    \"\"\"\n\n                    \"\"\"\n            }",
        "nb_lignes_process": 18,
        "string_script": "                    \"\"\"\n\n                    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "assembly",
            "annotation"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'med_cpus'",
            "tag \"${sample_id}\"",
            "publishDir \"${params.outdir}/addAnnotation/\", mode: \"copy\", overwrite: true, pattern: \"*.{fasta}\"",
            "publishDir \"${workDir}/.versions\", mode: \"copy\", overwrite: true, pattern: \"*.version.txt\""
        ],
        "when": "",
        "stub": ""
    },
    "get_report": {
        "name_process": "get_report",
        "string_process": " process get_report {\n\n            label 'low_cpus'\n\n            publishDir \"${params.outdir}/report\", mode: \"copy\", overwrite: true, pattern: \"*.{html,pdf}\"\n\n            conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda r-reshape2=1.4.4 r-plotly=4.9.2.1 plotly-orca=3.4.2 r-ggplot2=3.3.0 r-svglite=1.2.3 r-ggthemes=4.2.0 r-knitr=1.29 r-rmarkdown=2.3 r-kableextra=1.1.0\" : null)\n            if (params.oneContainer){ container \"${params.TPcontainer}\" } else {\n            container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\" : \"quay.io/biocontainers/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\")\n            }\n\n            input:\n                file(files) from report_ch\n                    .map{ it.flatten() }\n\n            output:\n                file(\"*html\") into final_report\n\n            script:\n                \"\"\"\n                sample_id=\\$( cat input.1 )\n                cp ${params.pipeInstall}/bin/TransPi_Report_Ind.Rmd .\n                Rscript -e \"rmarkdown::render('TransPi_Report_Ind.Rmd',output_file='TransPi_Report_\\${sample_id}.html')\" \\${sample_id} ${params.skipFilter} ${params.skipNormalization} ${params.rRNAfilter} ${params.buscoDist} ${params.allBuscos} ${params.skipKegg}\n                \"\"\"\n            }",
        "nb_lignes_process": 23,
        "string_script": "                \"\"\"\n                sample_id=\\$( cat input.1 )\n                cp ${params.pipeInstall}/bin/TransPi_Report_Ind.Rmd .\n                Rscript -e \"rmarkdown::render('TransPi_Report_Ind.Rmd',output_file='TransPi_Report_\\${sample_id}.html')\" \\${sample_id} ${params.skipFilter} ${params.skipNormalization} ${params.rRNAfilter} ${params.buscoDist} ${params.allBuscos} ${params.skipKegg}\n                \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "report_ch"
        ],
        "nb_inputs": 1,
        "outputs": [
            "final_report"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "label 'low_cpus'",
            "publishDir \"${params.outdir}/report\", mode: \"copy\", overwrite: true, pattern: \"*.{html,pdf}\"",
            "conda (params.condaActivate && params.myConda ? params.localConda : params.condaActivate ? \"-c conda-forge -c bioconda r-reshape2=1.4.4 r-plotly=4.9.2.1 plotly-orca=3.4.2 r-ggplot2=3.3.0 r-svglite=1.2.3 r-ggthemes=4.2.0 r-knitr=1.29 r-rmarkdown=2.3 r-kableextra=1.1.0\" : null) if (params.oneContainer){ container \"${params.TPcontainer}\" } else { container (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container ? \"https://depot.galaxyproject.org/singularity/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\" : \"quay.io/biocontainers/mulled-v2-3f431f5f8e54df68ea0029c209fce3b154f6e186:94cad00b5306639ceab6aaf211f45740560abb90-0\") }"
        ],
        "when": "",
        "stub": ""
    },
    "get_run_info": {
        "name_process": "get_run_info",
        "string_process": " process get_run_info {\n\n            publishDir \"${params.outdir}/\", mode: \"copy\", overwrite: true\n\n            output:\n               file(\".runInfo.txt\") into run_info\n\n            script:\n                \"\"\"\n                echo -e \"\n                ========================================================\n                    TransPi - Transcriptome Analysis Pipeline v${workflow.manifest.version}\n                ========================================================\\\\n\" >.runInfo.txt\n                echo -e \"\\\\t\\\\t\\\\tRUN INFO\\\\n\" >>.runInfo.txt\n                echo -e \"\\\\n-- Nextflow info --\" >>.runInfo.txt\n                echo \"Script Id          : $workflow.scriptId\n                Script name        : $workflow.scriptName\n                Script File        : $workflow.scriptFile\n                Repository         : $workflow.repository\n                Project directory  : $workflow.projectDir\n                Launch directory   : $workflow.launchDir\n                Work directory     : $workflow.workDir\n                Home directory     : $workflow.homeDir\n                User name          : $workflow.userName\n                Config Files       : $workflow.configFiles\n                Container Engine   : $workflow.containerEngine\n                Cmd line           : $workflow.commandLine\n                Profile            : $workflow.profile\n                Run name           : $workflow.runName\n                Session ID         : $workflow.sessionId\" >>.runInfo.txt\n\n                echo -e \"\\\\n-- Kmers used --\" >>.runInfo.txt\n                echo ${params.k} >>.runInfo.txt\n\n                echo -e \"\\\\n-- Databases name and last update --\" >>.runInfo.txt\n\n                v=\\$( echo ${params.uniname} )\n                echo \"Uniprot_DB: \\$v\" >>.runInfo.txt\n\n                if [ -f ${params.pipeInstall}/DBs/uniprot_db/.lastrun.txt ];then\n                    v=\\$( cat ${params.pipeInstall}/DBs/uniprot_db/.lastrun.txt )\n                else\n                    v=\"No info available. Check Instructions on README.\"\n                fi\n                echo -e \"Uniprot_DB last update: \\$v \\\\n\" >>.runInfo.txt\n\n                if [ -f ${params.pipeInstall}/DBs/hmmerdb/.lastrun.txt ];then\n                    v=\\$( cat ${params.pipeInstall}/DBs/hmmerdb/.lastrun.txt )\n                else\n                    v=\"No info available. Check Instructions on README.\"\n                fi\n                echo -e \"PfamA last update: \\$v \\\\n\" >>.runInfo.txt\n\n                v=\\$( echo ${params.busco4db} | tr \"/\" \"\\\\n\" | tail -n 1 )\n                echo \"BUSCO_v4_DB: \\$v\" >>.runInfo.txt\n\n                echo -e \"\\\\n-- Program versions --\" >>.runInfo.txt\n                \"\"\"\n        }",
        "nb_lignes_process": 57,
        "string_script": "                \"\"\"\n                echo -e \"\n                ========================================================\n                    TransPi - Transcriptome Analysis Pipeline v${workflow.manifest.version}\n                ========================================================\\\\n\" >.runInfo.txt\n                echo -e \"\\\\t\\\\t\\\\tRUN INFO\\\\n\" >>.runInfo.txt\n                echo -e \"\\\\n-- Nextflow info --\" >>.runInfo.txt\n                echo \"Script Id          : $workflow.scriptId\n                Script name        : $workflow.scriptName\n                Script File        : $workflow.scriptFile\n                Repository         : $workflow.repository\n                Project directory  : $workflow.projectDir\n                Launch directory   : $workflow.launchDir\n                Work directory     : $workflow.workDir\n                Home directory     : $workflow.homeDir\n                User name          : $workflow.userName\n                Config Files       : $workflow.configFiles\n                Container Engine   : $workflow.containerEngine\n                Cmd line           : $workflow.commandLine\n                Profile            : $workflow.profile\n                Run name           : $workflow.runName\n                Session ID         : $workflow.sessionId\" >>.runInfo.txt\n\n                echo -e \"\\\\n-- Kmers used --\" >>.runInfo.txt\n                echo ${params.k} >>.runInfo.txt\n\n                echo -e \"\\\\n-- Databases name and last update --\" >>.runInfo.txt\n\n                v=\\$( echo ${params.uniname} )\n                echo \"Uniprot_DB: \\$v\" >>.runInfo.txt\n\n                if [ -f ${params.pipeInstall}/DBs/uniprot_db/.lastrun.txt ];then\n                    v=\\$( cat ${params.pipeInstall}/DBs/uniprot_db/.lastrun.txt )\n                else\n                    v=\"No info available. Check Instructions on README.\"\n                fi\n                echo -e \"Uniprot_DB last update: \\$v \\\\n\" >>.runInfo.txt\n\n                if [ -f ${params.pipeInstall}/DBs/hmmerdb/.lastrun.txt ];then\n                    v=\\$( cat ${params.pipeInstall}/DBs/hmmerdb/.lastrun.txt )\n                else\n                    v=\"No info available. Check Instructions on README.\"\n                fi\n                echo -e \"PfamA last update: \\$v \\\\n\" >>.runInfo.txt\n\n                v=\\$( echo ${params.busco4db} | tr \"/\" \"\\\\n\" | tail -n 1 )\n                echo \"BUSCO_v4_DB: \\$v\" >>.runInfo.txt\n\n                echo -e \"\\\\n-- Program versions --\" >>.runInfo.txt\n                \"\"\"",
        "nb_lignes_script": 49,
        "language_script": "bash",
        "tools": [
            "3Dscript",
            "FlowRepositoryR",
            "project",
            "MetWork",
            "HOME",
            "gauseR",
            "haploconfig",
            "BioContainers",
            "SCMD",
            "gProfileR",
            "Faerun"
        ],
        "tools_url": [
            "https://bio.tools/3Dscript",
            "https://bio.tools/flowrepositoryr",
            "https://bio.tools/project",
            "https://bio.tools/MetWork",
            "https://bio.tools/HOME",
            "https://bio.tools/gauser",
            "https://bio.tools/haploconfig",
            "https://bio.tools/biocontainers",
            "https://bio.tools/scmd",
            "https://bio.tools/gprofile_r",
            "https://bio.tools/Faerun"
        ],
        "tools_dico": [
            {
                "name": "3Dscript",
                "uri": "https://bio.tools/3Dscript",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0176",
                            "term": "Molecular dynamics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3382",
                            "term": "Imaging"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0250",
                                    "term": "Protein property calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0250",
                                    "term": "Protein property rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Creates high-quality 3D/4D animations using a natural-language based syntax.",
                "homepage": "https://imagej.net/3Dscript"
            },
            {
                "name": "FlowRepositoryR",
                "uri": "https://bio.tools/flowrepositoryr",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_2229",
                            "term": "Cell biology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0226",
                                    "term": "Annotation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Query and retrieval"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Database retrieval"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package provides an interface to search and download data and annotations from FlowRepository (flowrepository.org). It uses the FlowRepository programming interface to communicate with a FlowRepository server.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/FlowRepositoryR.html"
            },
            {
                "name": "project",
                "uri": "https://bio.tools/project",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acids"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0077",
                            "term": "Nucleic acid informatics"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Nucleic acid sequence analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Sequence analysis (nucleic acid)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "project is a program that projects genomic features onto their sequences. Please contact Sarah Djebali (sarah dot djebali at crg dot es for any question).",
                "homepage": "http://big.crg.cat/services/project"
            },
            {
                "name": "MetWork",
                "uri": "https://bio.tools/MetWork",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3343",
                            "term": "Compound libraries and screening"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3172",
                            "term": "Metabolomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3627",
                                    "term": "Mass spectra calibration"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3629",
                                    "term": "Deisotoping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3803",
                                    "term": "Natural product identification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3629",
                                    "term": "Deconvolution"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Computer-Assisted Natural Products Anticipation | a web server for natural products anticipation | Currently running beta version 0.4.2 | \"Essence is a function of the relationship.\" Bachelard 1929 | MetWork used for the identification of bromotryptamine derivatives",
                "homepage": "https://metwork.pharmacie.parisdescartes.fr"
            },
            {
                "name": "HOME",
                "uri": "https://bio.tools/HOME",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3674",
                            "term": "Methylated DNA immunoprecipitation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3295",
                            "term": "Epigenetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3674",
                            "term": "MeDIP-seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3674",
                            "term": "mDIP"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3674",
                            "term": "MeDIP-chip"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0654",
                            "term": "DNA analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3207",
                                    "term": "Gene methylation analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3206",
                                    "term": "Whole genome methylation analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3186",
                                    "term": "Bisulfite mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3207",
                                    "term": "Gene-specific methylation analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3207",
                                    "term": "Methylation level analysis (gene-specific)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3206",
                                    "term": "Global methylation analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3206",
                                    "term": "Methylation level analysis (global)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3206",
                                    "term": "Genome methylation analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3186",
                                    "term": "Bisulfite sequence mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3186",
                                    "term": "Bisulfite sequence alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3186",
                                    "term": "Bisulfite read mapping"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "HOME (histogram of methylation) is a python package for differential methylation region (DMR) identification.",
                "homepage": "https://github.com/ListerLab/HOME"
            },
            {
                "name": "gauseR",
                "uri": "https://bio.tools/gauser",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3361",
                            "term": "Laboratory techniques"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0610",
                            "term": "Ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2269",
                            "term": "Statistics and probability"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3361",
                            "term": "Experimental techniques"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3361",
                            "term": "Lab techniques"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3361",
                            "term": "Lab method"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3361",
                            "term": "Laboratory method"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3659",
                                    "term": "Regression analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3946",
                                    "term": "Ecological modelling"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3435",
                                    "term": "Standardisation and normalisation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Simple methods for fitting Lotka-Volterra models describing Gause\u2019s \u201cStruggle for Existence\u201d.\n\nPackage repository for the gauseR package.",
                "homepage": "http://github.com/adamtclark/gauseR"
            },
            {
                "name": "haploconfig",
                "uri": "https://bio.tools/haploconfig",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3056",
                            "term": "Population genetics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype and phenotype"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3053",
                            "term": "Genetics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype and phenotype resources"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype-phenotype analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0625",
                            "term": "Genotype-phenotype"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0487",
                                    "term": "Haplotype mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0487",
                                    "term": "Haplotype reconstruction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0487",
                                    "term": "Haplotype map generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0487",
                                    "term": "Haplotype inference"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Program that can be used to implement tests of neutrality based on the frequency distribution of haplotypes in a sample of DNA sequences (the \u201chaplotype configuration\u201d) and the number of segregating sites. The neutrality tests can be performed conditional on the standard neutral coalescent model with or without recombination, exponential population growth, or island migration.",
                "homepage": "http://www.stanford.edu/group/rosenberglab/haploconfig.html"
            },
            {
                "name": "BioContainers",
                "uri": "https://bio.tools/biocontainers",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0091",
                            "term": "Bioinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2258",
                            "term": "Cheminformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Biological databases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0607",
                            "term": "Laboratory information management"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_2258",
                            "term": "Chemoinformatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2258",
                            "term": "Chemical informatics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Data management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Information systems"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3071",
                            "term": "Databases and information systems"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Query and retrieval"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0224",
                                    "term": "Database retrieval"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Open-source and community-driven framework for software standardization.",
                "homepage": "http://biocontainers.pro/"
            },
            {
                "name": "SCMD",
                "uri": "https://bio.tools/scmd",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3382",
                            "term": "Imaging"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0218",
                            "term": "Natural language processing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3473",
                            "term": "Data mining"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0218",
                            "term": "NLP"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0306",
                                    "term": "Text mining"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0306",
                                    "term": "Text data mining"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0306",
                                    "term": "Literature mining"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0306",
                                    "term": "Text analytics"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Collection of micrographs of budding yeast mutants, visualization and data mining tools are provided.",
                "homepage": "http://scmd.gi.k.u-tokyo.ac.jp/"
            },
            {
                "name": "gProfileR",
                "uri": "https://bio.tools/gprofile_r",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0602",
                            "term": "Molecular interactions, pathways and networks"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_1775",
                            "term": "Function analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0089",
                            "term": "Ontology and terminology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_1775",
                            "term": "Functional analysis"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3672",
                                    "term": "Gene functional annotation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2436",
                                    "term": "Gene-set enrichment analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2497",
                                    "term": "Pathway or network analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3672",
                                    "term": "Sequence functional annotation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Data extraction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2422",
                                    "term": "Retrieval"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2436",
                                    "term": "GSEA"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2436",
                                    "term": "Functional enrichment analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2436",
                                    "term": "Gene-set over-represenation analysis"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_2295",
                                "term": "Gene ID"
                            },
                            {
                                "uri": "http://edamontology.org/data_3021",
                                "term": "UniProt accession"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2600",
                                "term": "Pathway or network"
                            }
                        ]
                    }
                ],
                "description": "gProfileR performs functional enrichment analysis, gene identifier conversion and mapping homologous genes across related organisms via the 'g:Profiler' toolkit. The tool performs statistical enrichment analysis to find over-representation of information like Gene Ontology terms, biological pathways, regulatory DNA elements, human disease gene annotations, and protein-protein interaction networks. The basic input is a list of genes.",
                "homepage": "https://cran.r-project.org/web/packages/gProfileR/index.html"
            },
            {
                "name": "Faerun",
                "uri": "https://bio.tools/Faerun",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0154",
                            "term": "Small molecules"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0218",
                            "term": "Natural language processing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3047",
                            "term": "Molecular biology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0218",
                            "term": "NLP"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "PubChem and ChEMBL Beyond Lipinski.",
                "homepage": "http://faerun.gdb.tools/"
            }
        ],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [
            "run_info"
        ],
        "nb_outputs": 1,
        "name_workflow": "PalMuc__TransPi",
        "directive": [
            "publishDir \"${params.outdir}/\", mode: \"copy\", overwrite: true"
        ],
        "when": "",
        "stub": ""
    }
}