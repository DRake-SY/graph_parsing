{
    "retrieve_sra_metadata": {
        "name_process": "retrieve_sra_metadata",
        "string_process": "\nprocess retrieve_sra_metadata {\n  publishDir params.outdir, mode: params.publish_dir_mode, pattern: \"failed_runs.metadata.txt\"\n  label \"retrieve_sra_metadata\"\n\n  input:\n    file srr_file from SRR_FILE\n    file skip_samples from SKIP_SAMPLES_FILE\n\n  output:\n    stdout REMOTE_SAMPLES_LIST\n    file \"failed_runs.metadata.txt\" into METADATA_FAILED_RUNS\n\n  script:\n  if (skip_samples != 'NA') {\n      skip_arg = \"--skip_file ${skip_samples}\"\n  }\n  \"\"\"\n  >&2 echo \"#TRACE n_remote_run_ids=`cat ${srr_file} | wc -l`\"\n  retrieve_sra_metadata.py \\\n      --run_id_file ${srr_file} \\\n      --meta_dir ${workflow.workDir}/transcript_files \\\n      ${skip_arg}\n  \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "  if (skip_samples != 'NA') {\n      skip_arg = \"--skip_file ${skip_samples}\"\n  }\n  \"\"\"\n  >&2 echo \"#TRACE n_remote_run_ids=`cat ${srr_file} | wc -l`\"\n  retrieve_sra_metadata.py \\\n      --run_id_file ${srr_file} \\\n      --meta_dir ${workflow.workDir}/transcript_files \\\n      ${skip_arg}\n  \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "SRR_FILE",
            "SKIP_SAMPLES_FILE"
        ],
        "nb_inputs": 2,
        "outputs": [
            "REMOTE_SAMPLES_LIST",
            "METADATA_FAILED_RUNS"
        ],
        "nb_outputs": 2,
        "name_workflow": "SystemsGenetics__transcript_corral",
        "directive": [
            "publishDir params.outdir, mode: params.publish_dir_mode, pattern: \"failed_runs.metadata.txt\"",
            "label \"retrieve_sra_metadata\""
        ],
        "when": "",
        "stub": ""
    },
    "write_sample_files": {
        "name_process": "write_sample_files",
        "string_process": "\nprocess write_sample_files {\n  tag { sample_id }\n  label \"local\"\n\n  input:\n    set val(sample_id), val(run_files_or_ids), val(sample_type) from ALL_SAMPLES\n\n  output:\n    file '*.sample.csv' into SAMPLE_FILES\n\n  exec:\n                              \n    skip_samples = []\n    if (params.skip_samples) {\n        skip_file = file(\"${params.skip_samples}\")\n        skip_file.eachLine { line ->\n            skip_samples << line.trim()\n        }\n    }\n\n                                                   \n    if (skip_samples.intersect([sample_id]) == []) {\n                                        \n      sample_file = file(\"${task.workDir}\" + '/' + sample_id + '.sample.csv')\n      sample_file.withWriter {\n                                   \n        if (sample_type.equals('local')) {\n          it.writeLine '\"' + sample_id + '\",\"' + run_files_or_ids.join('::') + '\",\"' + sample_type + '\"'\n        }\n                                    \n        else {\n          it.writeLine '\"' + sample_id + '\",\"' + run_files_or_ids + '\",\"' + sample_type + '\"'\n        }\n      }\n    }\n}",
        "nb_lignes_process": 35,
        "string_script": "    skip_samples = []\n    if (params.skip_samples) {\n        skip_file = file(\"${params.skip_samples}\")\n        skip_file.eachLine { line ->\n            skip_samples << line.trim()\n        }\n    }\n\n                                                   \n    if (skip_samples.intersect([sample_id]) == []) {\n                                        \n      sample_file = file(\"${task.workDir}\" + '/' + sample_id + '.sample.csv')\n      sample_file.withWriter {\n                                   \n        if (sample_type.equals('local')) {\n          it.writeLine '\"' + sample_id + '\",\"' + run_files_or_ids.join('::') + '\",\"' + sample_type + '\"'\n        }\n                                    \n        else {\n          it.writeLine '\"' + sample_id + '\",\"' + run_files_or_ids + '\",\"' + sample_type + '\"'\n        }\n      }\n    }",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "ALL_SAMPLES"
        ],
        "nb_inputs": 1,
        "outputs": [
            "SAMPLE_FILES"
        ],
        "nb_outputs": 1,
        "name_workflow": "SystemsGenetics__transcript_corral",
        "directive": [
            "tag { sample_id }",
            "label \"local\""
        ],
        "when": "",
        "stub": ""
    },
    "get_software_versions": {
        "name_process": "get_software_versions",
        "string_process": "\nprocess get_software_versions {\n    publishDir \"${params.outdir}/pipeline_info\", mode: 'copy',\n        saveAs: { filename ->\n                      if (filename.indexOf(\".csv\") > 0) filename\n                      else null\n                }\n\n    output:\n    file 'software_versions_mqc.yaml' into ch_software_versions_yaml\n    file \"software_versions.csv\"\n\n    script:\n                                                                     \n    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastq-dump --version > v_fastq_dump.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "    \"\"\"\n    echo $workflow.manifest.version > v_pipeline.txt\n    echo $workflow.nextflow.version > v_nextflow.txt\n    fastq-dump --version > v_fastq_dump.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [
            "ch_software_versions_yaml"
        ],
        "nb_outputs": 1,
        "name_workflow": "SystemsGenetics__transcript_corral",
        "directive": [
            "publishDir \"${params.outdir}/pipeline_info\", mode: 'copy' , saveAs: { filename -> if (filename.indexOf(\".csv\") > 0) filename else null }"
        ],
        "when": "",
        "stub": ""
    },
    "read_sample_file": {
        "name_process": "read_sample_file",
        "string_process": "\nprocess read_sample_file {\n  tag { sample_file }\n  label \"local\"\n  cache false\n\n  input:\n    file(sample_file) from SAMPLE_FILES\n\n  output:\n    stdout SAMPLE_FILE_CONTENTS\n\n  script:\n  \"\"\"\n  cat ${sample_file}\n  \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "  \"\"\"\n  cat ${sample_file}\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "SAMPLE_FILES"
        ],
        "nb_inputs": 1,
        "outputs": [
            "SAMPLE_FILE_CONTENTS"
        ],
        "nb_outputs": 1,
        "name_workflow": "SystemsGenetics__transcript_corral",
        "directive": [
            "tag { sample_file }",
            "label \"local\"",
            "cache false"
        ],
        "when": "",
        "stub": ""
    },
    "download_runs": {
        "name_process": "download_runs",
        "string_process": "\nprocess download_runs {\n  publishDir params.outdir, mode: params.publish_dir_mode, pattern: '*.failed_runs.download.txt', saveAs: { \"Samples/${sample_id}/${it}\" }\n\n  tag { sample_id }\n  label \"download_runs\"\n\n  input:\n    set val(sample_id), val(run_ids), val(type) from REMOTE_SAMPLES\n\n  output:\n    set val(sample_id), file(\"*.sra\") optional true into SRA_TO_EXTRACT    \n    set val(sample_id), file('*.failed_runs.download.txt') into DOWNLOAD_FAILED_RUNS\n\n  script:\n  \"\"\"\n  echo \"#TRACE n_remote_run_ids=${run_ids.tokenize(' ').size()}\"\n  retrieve_sra.py --sample ${sample_id} --run_ids ${run_ids} --akey \\$ASPERA_KEY\n  \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "  \"\"\"\n  echo \"#TRACE n_remote_run_ids=${run_ids.tokenize(' ').size()}\"\n  retrieve_sra.py --sample ${sample_id} --run_ids ${run_ids} --akey \\$ASPERA_KEY\n  \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "REMOTE_SAMPLES"
        ],
        "nb_inputs": 1,
        "outputs": [
            "SRA_TO_EXTRACT",
            "DOWNLOAD_FAILED_RUNS"
        ],
        "nb_outputs": 2,
        "name_workflow": "SystemsGenetics__transcript_corral",
        "directive": [
            "publishDir params.outdir, mode: params.publish_dir_mode, pattern: '*.failed_runs.download.txt', saveAs: { \"Samples/${sample_id}/${it}\" }",
            "tag { sample_id }",
            "label \"download_runs\""
        ],
        "when": "",
        "stub": ""
    },
    "fastq_dump": {
        "name_process": "fastq_dump",
        "string_process": "\nprocess fastq_dump {\n  publishDir params.outdir, mode: params.publish_dir_mode, pattern: publish_pattern_fastq_dump, saveAs: { \"Samples/${sample_id}/${it}\" }\n  publishDir params.outdir, mode: params.publish_dir_mode, pattern: '*.failed_runs.fastq-dump.txt', saveAs: { \"Samples/${sample_id}/${it}\" }\n  tag { sample_id }\n  label \"fastq_dump\"\n\n  input:\n    set val(sample_id), file(sra_files) from SRA_TO_EXTRACT\n\n  output:\n    set val(sample_id), file(\"*.fastq\") optional true into DOWNLOADED_FASTQ_FOR_MERGING     \n    set val(sample_id), file('*.failed_runs.fastq-dump.txt') into FASTQ_DUMP_FAILED_RUNS\n\n  script:\n  \"\"\"\n  echo \"#TRACE sample_id=${sample_id}\"\n  echo \"#TRACE sra_bytes=`stat -Lc '%s' *.sra | awk '{sum += \\$1} END {print sum}'`\"\n  sra2fastq.py --sample ${sample_id} --sra_files ${sra_files}\n  \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "  \"\"\"\n  echo \"#TRACE sample_id=${sample_id}\"\n  echo \"#TRACE sra_bytes=`stat -Lc '%s' *.sra | awk '{sum += \\$1} END {print sum}'`\"\n  sra2fastq.py --sample ${sample_id} --sra_files ${sra_files}\n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "SRA_TO_EXTRACT"
        ],
        "nb_inputs": 1,
        "outputs": [
            "DOWNLOADED_FASTQ_FOR_MERGING",
            "FASTQ_DUMP_FAILED_RUNS"
        ],
        "nb_outputs": 2,
        "name_workflow": "SystemsGenetics__transcript_corral",
        "directive": [
            "publishDir params.outdir, mode: params.publish_dir_mode, pattern: publish_pattern_fastq_dump, saveAs: { \"Samples/${sample_id}/${it}\" }",
            "publishDir params.outdir, mode: params.publish_dir_mode, pattern: '*.failed_runs.fastq-dump.txt', saveAs: { \"Samples/${sample_id}/${it}\" }",
            "tag { sample_id }",
            "label \"fastq_dump\""
        ],
        "when": "",
        "stub": ""
    },
    "fastq_merge": {
        "name_process": "fastq_merge",
        "string_process": "\nprocess fastq_merge {\n  tag { sample_id }\n\n  input:\n    set val(sample_id), file(fastq_files) from DOWNLOADED_FASTQ_FOR_MERGING\n\n  output:\n    set val(sample_id), file(\"${sample_id}_?.fastq\") into MERGED_SAMPLES_FOR_TRIMMING\n\n  script:\n  \"\"\"\n  echo \"#TRACE sample_id=${sample_id}\"\n  echo \"#TRACE fastq_lines=`cat *.fastq | wc -l`\"\n  fastq_merge.sh ${sample_id}\n  \"\"\"\n}",
        "nb_lignes_process": 15,
        "string_script": "  \"\"\"\n  echo \"#TRACE sample_id=${sample_id}\"\n  echo \"#TRACE fastq_lines=`cat *.fastq | wc -l`\"\n  fastq_merge.sh ${sample_id}\n  \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "DOWNLOADED_FASTQ_FOR_MERGING"
        ],
        "nb_inputs": 1,
        "outputs": [
            "MERGED_SAMPLES_FOR_TRIMMING"
        ],
        "nb_outputs": 1,
        "name_workflow": "SystemsGenetics__transcript_corral",
        "directive": [
            "tag { sample_id }"
        ],
        "when": "",
        "stub": ""
    },
    "failed_run_report": {
        "name_process": "failed_run_report",
        "string_process": "\nprocess failed_run_report {\n  label \"reports\"\n  publishDir \"${params.outdir}/reports\", mode: params.publish_dir_mode\n\n  input:\n    file metadata_failed_runs from METADATA_FAILED_RUNS\n    file download_failed_runs from DOWNLOAD_FAILED_RUNS.collect()\n    file fastq_dump_failed_runs from FASTQ_DUMP_FAILED_RUNS.collect()\n    file failed_run_template from FAILED_RUN_TEMPLATE\n\n  output:\n    file \"failed_SRA_run_report.html\" into FAILED_RUN_REPORT\n\n  script:\n  \"\"\"\n  failed_runs_report.py --template ${failed_run_template}\n  \"\"\"\n\n}",
        "nb_lignes_process": 18,
        "string_script": "  \"\"\"\n  failed_runs_report.py --template ${failed_run_template}\n  \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "METADATA_FAILED_RUNS",
            "DOWNLOAD_FAILED_RUNS",
            "FASTQ_DUMP_FAILED_RUNS",
            "FAILED_RUN_TEMPLATE"
        ],
        "nb_inputs": 4,
        "outputs": [
            "FAILED_RUN_REPORT"
        ],
        "nb_outputs": 1,
        "name_workflow": "SystemsGenetics__transcript_corral",
        "directive": [
            "label \"reports\"",
            "publishDir \"${params.outdir}/reports\", mode: params.publish_dir_mode"
        ],
        "when": "",
        "stub": ""
    },
    "trim_adapters": {
        "name_process": "trim_adapters",
        "string_process": "\nprocess trim_adapters {\n   input:\n   set val(sample_id), path(raw_fastq_files) from COMBINED_SAMPLES\n\n   output:\n   tuple val(sample_id), file(\"*_val_1.fq\"), file(\"*_val_2.fq\") into TRIMMED_PAIRS\n\n   script:\n   \"\"\"\n   echo \"Trimming adapters from $raw_fastq_files\"\n\n   trim_galore --paired --trim-n --length 36 -q 5 --suppress_warn $raw_fastq_files\n   \"\"\"\n}",
        "nb_lignes_process": 13,
        "string_script": "   \"\"\"\n   echo \"Trimming adapters from $raw_fastq_files\"\n\n   trim_galore --paired --trim-n --length 36 -q 5 --suppress_warn $raw_fastq_files\n   \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "COMBINED_SAMPLES"
        ],
        "nb_inputs": 1,
        "outputs": [
            "TRIMMED_PAIRS"
        ],
        "nb_outputs": 1,
        "name_workflow": "SystemsGenetics__transcript_corral",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "error_correction": {
        "name_process": "error_correction",
        "string_process": "\nprocess error_correction {\n   input:\n   tuple val(sample_id), file(forward_reads), file(reverse_reads) from TRIMMED_PAIRS\n\n   output:\n   tuple val(sample_id), file(\"*_1.cor.fq\"), file(\"*_2.cor.fq\") into CORRECTED_PAIRS\n\n   script:\n   \"\"\"\n   echo \"Error correction with files: $forward_reads $reverse_reads\"\n\n   perl /rcorrector/run_rcorrector.pl -1 $forward_reads -2 $reverse_reads\n   \"\"\"\n}",
        "nb_lignes_process": 13,
        "string_script": "   \"\"\"\n   echo \"Error correction with files: $forward_reads $reverse_reads\"\n\n   perl /rcorrector/run_rcorrector.pl -1 $forward_reads -2 $reverse_reads\n   \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "TRIMMED_PAIRS"
        ],
        "nb_inputs": 1,
        "outputs": [
            "CORRECTED_PAIRS"
        ],
        "nb_outputs": 1,
        "name_workflow": "SystemsGenetics__transcript_corral",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "discard_unfixables": {
        "name_process": "discard_unfixables",
        "string_process": "\nprocess discard_unfixables {\n   input:\n   tuple val(sample_id), file(forward_reads), file(reverse_reads) from CORRECTED_PAIRS\n\n   output:\n   tuple val(sample_id), file(\"*_1.cor.fq\"), file(\"*_2.cor.fq\") into CORRECTED_PAIRS_CLEAN\n\n   script:\n   \"\"\"\n   echo \"Removing unfixable reads from corrected files: $forward_reads $reverse_reads\"\n\n   python ${workflow.launchDir}/bin/FilterUncorrectedReadsPEfastq.py -1 $forward_reads -2 $reverse_reads -s $sample_id\n   \"\"\"\n}",
        "nb_lignes_process": 13,
        "string_script": "   \"\"\"\n   echo \"Removing unfixable reads from corrected files: $forward_reads $reverse_reads\"\n\n   python ${workflow.launchDir}/bin/FilterUncorrectedReadsPEfastq.py -1 $forward_reads -2 $reverse_reads -s $sample_id\n   \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "CORRECTED_PAIRS"
        ],
        "nb_inputs": 1,
        "outputs": [
            "CORRECTED_PAIRS_CLEAN"
        ],
        "nb_outputs": 1,
        "name_workflow": "SystemsGenetics__transcript_corral",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "remove_rRNA": {
        "name_process": "remove_rRNA",
        "string_process": "\nprocess remove_rRNA {\n  input:\n  tuple val(sample_id), file(forward_reads), file(reverse_reads) from CORRECTED_PAIRS_CLEAN\n\n  output:\n  file(\"*whitelist_paired_unaligned_1.fq\") into FORWARD_READS_FOR_ASSEMBLY\n  file(\"*whitelist_paired_unaligned_2.fq\") into REVERSE_READS_FOR_ASSEMBLY\n\n  script:\n  \"\"\"\n  mkdir logs\n\n  bowtie2 --quiet \\\n    --very-sensitive-local \\\n    --phred33 \\\n    -x /SILVA_db/combined_silva_reference \\\n    -1 \"$forward_reads\" \\\n    -2 \"$reverse_reads\" \\\n    --met-file logs/$sample_id\"_bowtie2_metrics.txt\" \\\n    --al-conc $sample_id\"_blacklist_paired_aligned_%.fq\" \\\n    --un-conc $sample_id\"_whitelist_paired_unaligned_%.fq\" \\\n    --al $sample_id\"_blacklist_unpaired_aligned_%.fq\" \\\n    --un $sample_id\"_whitelist_unpaired_unaligned_%.fq\"\n  \"\"\"\n}",
        "nb_lignes_process": 24,
        "string_script": "  \"\"\"\n  mkdir logs\n\n  bowtie2 --quiet \\\n    --very-sensitive-local \\\n    --phred33 \\\n    -x /SILVA_db/combined_silva_reference \\\n    -1 \"$forward_reads\" \\\n    -2 \"$reverse_reads\" \\\n    --met-file logs/$sample_id\"_bowtie2_metrics.txt\" \\\n    --al-conc $sample_id\"_blacklist_paired_aligned_%.fq\" \\\n    --un-conc $sample_id\"_whitelist_paired_unaligned_%.fq\" \\\n    --al $sample_id\"_blacklist_unpaired_aligned_%.fq\" \\\n    --un $sample_id\"_whitelist_unpaired_unaligned_%.fq\"\n  \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [
            "Rbowtie2"
        ],
        "tools_url": [
            "https://bio.tools/rbowtie2"
        ],
        "tools_dico": [
            {
                "name": "Rbowtie2",
                "uri": "https://bio.tools/rbowtie2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence merging"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence splicing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package provides an R wrapper of the popular bowtie2 sequencing reads aligner and AdapterRemoval, a convenient tool for rapid adapter trimming, identification, and read merging.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/Rbowtie2.html"
            }
        ],
        "inputs": [
            "CORRECTED_PAIRS_CLEAN"
        ],
        "nb_inputs": 1,
        "outputs": [
            "FORWARD_READS_FOR_ASSEMBLY",
            "REVERSE_READS_FOR_ASSEMBLY"
        ],
        "nb_outputs": 2,
        "name_workflow": "SystemsGenetics__transcript_corral",
        "directive": [],
        "when": "",
        "stub": ""
    },
    "assembly_Trinity": {
        "name_process": "assembly_Trinity",
        "string_process": "\nprocess assembly_Trinity {\n\nwhen: \nparams.use_trinity == true\n\ninput:\nfile(forward_reads) from FORWARD_READS_FOR_TRINITY\nfile(reverse_reads) from REVERSE_READS_FOR_TRINITY\n\noutput:\nfile(\"Trinity.fasta\") into TRANSCRIPTOME_ASSEMBLIES\n\nscript:\n\"\"\"\nTrinity \\\n  --seqType fq \\\n  --CPU ${params.max_cpus} \\\n  --max_memory ${params.trinity_mem} \\\n  --min_kmer_cov 3 \\\n  --left \"$forward_reads\" \\\n  --right \"$reverse_reads\"\n\"\"\"\n}",
        "nb_lignes_process": 22,
        "string_script": "\"\"\"\nTrinity \\\n  --seqType fq \\\n  --CPU ${params.max_cpus} \\\n  --max_memory ${params.trinity_mem} \\\n  --min_kmer_cov 3 \\\n  --left \"$forward_reads\" \\\n  --right \"$reverse_reads\"\n\"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "Trinity"
        ],
        "tools_url": [
            "https://bio.tools/trinity"
        ],
        "tools_dico": [
            {
                "name": "Trinity",
                "uri": "https://bio.tools/trinity",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "Gene transcripts"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Gene expression"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3512",
                            "term": "mRNA features"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0203",
                            "term": "Expression"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3258",
                                    "term": "Transcriptome assembly"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Trinity is a transcriptome assembler which relies on three different tools, inchworm an assembler, chrysalis which pools contigs and butterfly which amongst others compacts a graph resulting from butterfly with reads.",
                "homepage": "https://github.com/trinityrnaseq/trinityrnaseq/wiki"
            }
        ],
        "inputs": [
            "FORWARD_READS_FOR_TRINITY",
            "REVERSE_READS_FOR_TRINITY"
        ],
        "nb_inputs": 2,
        "outputs": [
            "TRANSCRIPTOME_ASSEMBLIES"
        ],
        "nb_outputs": 1,
        "name_workflow": "SystemsGenetics__transcript_corral",
        "directive": [],
        "when": "params.use_trinity == true",
        "stub": ""
    },
    "unify_assembly_ids": {
        "name_process": "unify_assembly_ids",
        "string_process": "\nprocess unify_assembly_ids {\nwhen: \nparams.meta_assembly == true\n\ninput:\nfile(combined_assembly) from COMBINED_ASSEMBLY\n\noutput:\nfile(\"unified_transcriptome.fasta\") into UNIFIED_COMBINED_ASSEMBLY\n\nscript:\n\"\"\"\ntrformat.pl \\\n  -output unified_transcriptome.fasta \\\n  -input $combined_assembly\n\"\"\"\n}",
        "nb_lignes_process": 16,
        "string_script": "\"\"\"\ntrformat.pl \\\n  -output unified_transcriptome.fasta \\\n  -input $combined_assembly\n\"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "COMBINED_ASSEMBLY"
        ],
        "nb_inputs": 1,
        "outputs": [
            "UNIFIED_COMBINED_ASSEMBLY"
        ],
        "nb_outputs": 1,
        "name_workflow": "SystemsGenetics__transcript_corral",
        "directive": [],
        "when": "params.meta_assembly == true",
        "stub": ""
    },
    "evigene": {
        "name_process": "evigene",
        "string_process": "\nprocess evigene {\n  when: \nparams.meta_assembly == true\n\ninput:\nfile(unified_assembly) from UNIFIED_COMBINED_ASSEMBLY\n\noutput:\nfile(\"okayset/unified_transcriptome.okay.mrna\") into EVIGENE_OKAY_SET\n\nscript:\n\"\"\"\ntr2aacds4.pl \\\n  -logfile \\\n  -cdnaseq $unified_assembly \\\n  -NCPU ${params.max_cpus} \\\n  -MAXMEM ${params.max_memory}\n\"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "\"\"\"\ntr2aacds4.pl \\\n  -logfile \\\n  -cdnaseq $unified_assembly \\\n  -NCPU ${params.max_cpus} \\\n  -MAXMEM ${params.max_memory}\n\"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "UNIFIED_COMBINED_ASSEMBLY"
        ],
        "nb_inputs": 1,
        "outputs": [
            "EVIGENE_OKAY_SET"
        ],
        "nb_outputs": 1,
        "name_workflow": "SystemsGenetics__transcript_corral",
        "directive": [],
        "when": "params.meta_assembly == true",
        "stub": ""
    }
}