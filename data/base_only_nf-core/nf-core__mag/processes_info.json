{
    "BUSCO_PLOT": {
        "name_process": "BUSCO_PLOT",
        "string_process": "\nprocess BUSCO_PLOT {\n    tag \"${meta.assembler}-${meta.id}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::busco=5.1.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/busco:5.1.0--py_1\"\n    } else {\n        container \"quay.io/biocontainers/busco:5.1.0--py_1\"\n    }\n\n    input:\n    tuple val(meta), path(summaries)\n\n    output:\n    path(\"${meta.assembler}-${meta.id}.*.busco_figure.png\") , optional:true, emit: png\n    path(\"${meta.assembler}-${meta.id}.*.busco_figure.R\")   , optional:true, emit: rscript\n    path '*.version.txt'                                                   , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    if [ -n \"${summaries}\" ]\n    then\n        # replace dots in bin names within summary file names by underscores\n        # currently (BUSCO v5.1.0) generate_plot.py does not allow further dots\n        for sum in ${summaries}; do\n            [[ \\${sum} =~ short_summary.([_[:alnum:]]+).([_[:alnum:]]+).${meta.assembler}-${meta.id}.(.+).txt ]];\n            mode=\\${BASH_REMATCH[1]}\n            db_name=\\${BASH_REMATCH[2]}\n            bin=\"${meta.assembler}-${meta.id}.\\${BASH_REMATCH[3]}\"\n            bin_new=\"\\${bin//./_}\"\n            mv \\${sum} short_summary.\\${mode}.\\${db_name}.\\${bin_new}.txt\n        done\n        generate_plot.py --working_directory .\n\n        mv busco_figure.png \"${meta.assembler}-${meta.id}.\\${mode}.\\${db_name}.busco_figure.png\"\n        mv busco_figure.R \"${meta.assembler}-${meta.id}.\\${mode}.\\${db_name}.busco_figure.R\"\n    fi\n\n    busco --version | sed \"s/BUSCO //\" > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 45,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    if [ -n \"${summaries}\" ]\n    then\n        # replace dots in bin names within summary file names by underscores\n        # currently (BUSCO v5.1.0) generate_plot.py does not allow further dots\n        for sum in ${summaries}; do\n            [[ \\${sum} =~ short_summary.([_[:alnum:]]+).([_[:alnum:]]+).${meta.assembler}-${meta.id}.(.+).txt ]];\n            mode=\\${BASH_REMATCH[1]}\n            db_name=\\${BASH_REMATCH[2]}\n            bin=\"${meta.assembler}-${meta.id}.\\${BASH_REMATCH[3]}\"\n            bin_new=\"\\${bin//./_}\"\n            mv \\${sum} short_summary.\\${mode}.\\${db_name}.\\${bin_new}.txt\n        done\n        generate_plot.py --working_directory .\n\n        mv busco_figure.png \"${meta.assembler}-${meta.id}.\\${mode}.\\${db_name}.busco_figure.png\"\n        mv busco_figure.R \"${meta.assembler}-${meta.id}.\\${mode}.\\${db_name}.busco_figure.R\"\n    fi\n\n    busco --version | sed \"s/BUSCO //\" > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 21,
        "language_script": "bash",
        "tools": [
            "BUSCO"
        ],
        "tools_url": [
            "https://bio.tools/busco"
        ],
        "tools_dico": [
            {
                "name": "BUSCO",
                "uri": "https://bio.tools/busco",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly validation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly quality evaluation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Assembly QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Assembly quality evaluation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly QC"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_1234",
                                "term": "Sequence set (nucleic acid)"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "Provides measures for quantitative assessment of genome assembly, gene set, and transcriptome completeness based on evolutionarily informed expectations of gene content from near-universal single-copy orthologs.",
                "homepage": "http://busco.ezlab.org/"
            }
        ],
        "inputs": [
            "meta",
            "summaries"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${meta.assembler}-${meta.id}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:[]) }",
            "conda (params.enable_conda ? \"bioconda::busco=5.1.0\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/busco:5.1.0--py_1\" } else { container \"quay.io/biocontainers/busco:5.1.0--py_1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "KRONA": {
        "name_process": "KRONA",
        "string_process": "\nprocess KRONA {\n    tag \"${meta.classifier}-${meta.id}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['classifier', 'id']) }\n\n    conda (params.enable_conda ? \"bioconda::krona=2.7.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/krona:2.7.1--pl526_5\"\n    } else {\n        container \"quay.io/biocontainers/krona:2.7.1--pl526_5\"\n    }\n\n    input:\n    tuple val(meta), path(report)\n    path  \"taxonomy/taxonomy.tab\"\n\n    output:\n    path \"*.html\"       , emit: html\n    path '*.version.txt', emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    ktImportTaxonomy \"$report\" -tax taxonomy\n    echo \\$(ktImportTaxonomy 2>&1) | sed 's/^.*KronaTools //; s/ - ktImportTaxonomy.*//' > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    ktImportTaxonomy \"$report\" -tax taxonomy\n    echo \\$(ktImportTaxonomy 2>&1) | sed 's/^.*KronaTools //; s/ - ktImportTaxonomy.*//' > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "report"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${meta.classifier}-${meta.id}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['classifier', 'id']) }",
            "conda (params.enable_conda ? \"bioconda::krona=2.7.1\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/krona:2.7.1--pl526_5\" } else { container \"quay.io/biocontainers/krona:2.7.1--pl526_5\" }"
        ],
        "when": "",
        "stub": ""
    },
    "MAG_DEPTHS_PLOT": {
        "name_process": "MAG_DEPTHS_PLOT",
        "string_process": "\nprocess MAG_DEPTHS_PLOT {\n    tag \"${meta.assembler}-${meta.id}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['assembler', 'id']) }\n\n    conda (params.enable_conda ? \"conda-forge::python=3.9 conda-forge::pandas=1.3.0 anaconda::seaborn=0.11.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-d14219255233ee6cacc427e28a7caf8ee42e8c91:0a22c7568e4a509925048454dad9ab37fa8fe776-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-d14219255233ee6cacc427e28a7caf8ee42e8c91:0a22c7568e4a509925048454dad9ab37fa8fe776-0\"\n    }\n\n    input:\n    tuple val(meta), path(depths)\n    path(sample_groups)\n\n    output:\n    tuple val(meta), path(\"${meta.assembler}-${meta.id}-binDepths.heatmap.png\"), emit: heatmap\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    plot_mag_depths.py --bin_depths ${depths} \\\n                    --groups ${sample_groups} \\\n                    --out \"${meta.assembler}-${meta.id}-binDepths.heatmap.png\"\n    \"\"\"\n}",
        "nb_lignes_process": 28,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    plot_mag_depths.py --bin_depths ${depths} \\\n                    --groups ${sample_groups} \\\n                    --out \"${meta.assembler}-${meta.id}-binDepths.heatmap.png\"\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "depths",
            "sample_groups"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${meta.assembler}-${meta.id}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['assembler', 'id']) }",
            "conda (params.enable_conda ? \"conda-forge::python=3.9 conda-forge::pandas=1.3.0 anaconda::seaborn=0.11.0\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/mulled-v2-d14219255233ee6cacc427e28a7caf8ee42e8c91:0a22c7568e4a509925048454dad9ab37fa8fe776-0\" } else { container \"quay.io/biocontainers/mulled-v2-d14219255233ee6cacc427e28a7caf8ee42e8c91:0a22c7568e4a509925048454dad9ab37fa8fe776-0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "CAT_DB": {
        "name_process": "CAT_DB",
        "string_process": "\nprocess CAT_DB {\n    tag \"${database.baseName}\"\n\n    conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\"\n    } else {\n        container \"biocontainers/biocontainers:v1.2.0_cv1\"\n    }\n\n    input:\n    path(database)\n\n    output:\n    tuple val(\"${database.toString().replace(\".tar.gz\", \"\")}\"), path(\"database/*\"), path(\"taxonomy/*\"), emit: db\n\n    script:\n    \"\"\"\n    mkdir catDB\n    tar -xf ${database} -C catDB\n    mv `find catDB/ -type d -name \"*taxonomy*\"` taxonomy/\n    mv `find catDB/ -type d -name \"*database*\"` database/\n    \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "    \"\"\"\n    mkdir catDB\n    tar -xf ${database} -C catDB\n    mv `find catDB/ -type d -name \"*taxonomy*\"` taxonomy/\n    mv `find catDB/ -type d -name \"*database*\"` database/\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "database"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${database.baseName}\"",
            "conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\" } else { container \"biocontainers/biocontainers:v1.2.0_cv1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "BOWTIE2_ASSEMBLY_ALIGN": {
        "name_process": "BOWTIE2_ASSEMBLY_ALIGN",
        "string_process": "\nprocess BOWTIE2_ASSEMBLY_ALIGN {\n    tag \"${assembly_meta.assembler}-${assembly_meta.id}-${reads_meta.id}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:assembly_meta, publish_by_meta:['assembler', 'id']) }\n\n    conda (params.enable_conda ? \"bioconda::bowtie2=2.4.2 bioconda::samtools=1.11 conda-forge::pigz=2.3.4\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:577a697be67b5ae9b16f637fd723b8263a3898b3-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:577a697be67b5ae9b16f637fd723b8263a3898b3-0\"\n    }\n\n    input:\n    tuple val(assembly_meta), path(assembly), path(index), val(reads_meta), path(reads)\n\n    output:\n    tuple val(assembly_meta), path(assembly), path(\"${assembly_meta.assembler}-${assembly_meta.id}-${reads_meta.id}.bam\"), path(\"${assembly_meta.assembler}-${assembly_meta.id}-${reads_meta.id}.bam.bai\"), emit: mappings\n    tuple val(assembly_meta), val(reads_meta), path(\"*.bowtie2.log\")                                                                                                                                      , emit: log\n    path '*.version.txt'                                                                                                                                                                                  , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def name = \"${assembly_meta.assembler}-${assembly_meta.id}-${reads_meta.id}\"\n    def input = params.single_end ? \"-U \\\"${reads}\\\"\" :  \"-1 \\\"${reads[0]}\\\" -2 \\\"${reads[1]}\\\"\"\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.rev.1.bt2l\" -o -name \"*.rev.1.bt2\" | sed 's/.rev.1.bt2l//' | sed 's/.rev.1.bt2//'`\n    bowtie2 -p \"${task.cpus}\" -x \\$INDEX $input 2> \"${name}.bowtie2.log\" | \\\n        samtools view -@ \"${task.cpus}\" -bS | \\\n        samtools sort -@ \"${task.cpus}\" -o \"${name}.bam\"\n    samtools index \"${name}.bam\"\n\n    if [ ${name} = \"${assembly_meta.assembler}-${assembly_meta.id}-${assembly_meta.id}\" ] ; then\n        mv \"${name}.bowtie2.log\" \"${assembly_meta.assembler}-${assembly_meta.id}.bowtie2.log\"\n    fi\n\n    echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//' > ${software}_assembly.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 39,
        "string_script": "    def software = getSoftwareName(task.process)\n    def name = \"${assembly_meta.assembler}-${assembly_meta.id}-${reads_meta.id}\"\n    def input = params.single_end ? \"-U \\\"${reads}\\\"\" :  \"-1 \\\"${reads[0]}\\\" -2 \\\"${reads[1]}\\\"\"\n    \"\"\"\n    INDEX=`find -L ./ -name \"*.rev.1.bt2l\" -o -name \"*.rev.1.bt2\" | sed 's/.rev.1.bt2l//' | sed 's/.rev.1.bt2//'`\n    bowtie2 -p \"${task.cpus}\" -x \\$INDEX $input 2> \"${name}.bowtie2.log\" | \\\n        samtools view -@ \"${task.cpus}\" -bS | \\\n        samtools sort -@ \"${task.cpus}\" -o \"${name}.bam\"\n    samtools index \"${name}.bam\"\n\n    if [ ${name} = \"${assembly_meta.assembler}-${assembly_meta.id}-${assembly_meta.id}\" ] ; then\n        mv \"${name}.bowtie2.log\" \"${assembly_meta.assembler}-${assembly_meta.id}.bowtie2.log\"\n    fi\n\n    echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//' > ${software}_assembly.version.txt\n    \"\"\"",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "Rbowtie2",
            "SAMtools"
        ],
        "tools_url": [
            "https://bio.tools/rbowtie2",
            "https://bio.tools/samtools"
        ],
        "tools_dico": [
            {
                "name": "Rbowtie2",
                "uri": "https://bio.tools/rbowtie2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence merging"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence splicing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package provides an R wrapper of the popular bowtie2 sequencing reads aligner and AdapterRemoval, a convenient tool for rapid adapter trimming, identification, and read merging.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/Rbowtie2.html"
            },
            {
                "name": "SAMtools",
                "uri": "https://bio.tools/samtools",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "Rare diseases"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0102",
                            "term": "Mapping"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3325",
                            "term": "https://en.wikipedia.org/wiki/Rare_disease"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3096",
                                    "term": "Editing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Parsing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Indexing"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Data visualisation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0337",
                                    "term": "Rendering"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Data loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_1812",
                                    "term": "Loading"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File format conversion"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "File formatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0335",
                                    "term": "Reformatting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Data indexing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0227",
                                    "term": "Database indexing"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_0924",
                                "term": "Sequence trace"
                            }
                        ]
                    }
                ],
                "description": "A software package with various utilities for processing alignments in the SAM format, including variant calling and alignment viewing.",
                "homepage": "http://www.htslib.org/"
            }
        ],
        "inputs": [
            "assembly_meta",
            "reads_meta",
            "assembly",
            "index",
            "reads"
        ],
        "nb_inputs": 5,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${assembly_meta.assembler}-${assembly_meta.id}-${reads_meta.id}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:assembly_meta, publish_by_meta:['assembler', 'id']) }",
            "conda (params.enable_conda ? \"bioconda::bowtie2=2.4.2 bioconda::samtools=1.11 conda-forge::pigz=2.3.4\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:577a697be67b5ae9b16f637fd723b8263a3898b3-0\" } else { container \"quay.io/biocontainers/mulled-v2-ac74a7f02cebcfcc07d8e8d1d750af9c83b4d45a:577a697be67b5ae9b16f637fd723b8263a3898b3-0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "FILTLONG": {
        "name_process": "FILTLONG",
        "string_process": "\nprocess FILTLONG {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"bioconda::filtlong=0.2.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/filtlong:0.2.0--he513fc3_3\"\n    } else {\n        container \"quay.io/biocontainers/filtlong:0.2.0--he513fc3_3\"\n    }\n\n    input:\n    tuple val(meta), path(long_reads), path(short_reads_1), path(short_reads_2)\n\n    output:\n    tuple val(meta), path(\"${meta.id}_lr_filtlong.fastq.gz\"), emit: reads\n    path '*.version.txt'                                    , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    filtlong \\\n        -1 ${short_reads_1} \\\n        -2 ${short_reads_2} \\\n        --min_length ${params.longreads_min_length} \\\n        --keep_percent ${params.longreads_keep_percent} \\\n        --trim \\\n        --length_weight ${params.longreads_length_weight} \\\n        ${long_reads} | gzip > ${meta.id}_lr_filtlong.fastq.gz\n\n    filtlong --version | sed -e \"s/Filtlong v//g\" > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    filtlong \\\n        -1 ${short_reads_1} \\\n        -2 ${short_reads_2} \\\n        --min_length ${params.longreads_min_length} \\\n        --keep_percent ${params.longreads_keep_percent} \\\n        --trim \\\n        --length_weight ${params.longreads_length_weight} \\\n        ${long_reads} | gzip > ${meta.id}_lr_filtlong.fastq.gz\n\n    filtlong --version | sed -e \"s/Filtlong v//g\" > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "Filtlong"
        ],
        "tools_url": [
            "https://bio.tools/Filtlong"
        ],
        "tools_dico": [
            {
                "name": "Filtlong",
                "uri": "https://bio.tools/Filtlong",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0749",
                            "term": "Transcription factors and regulatory sites"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0798",
                            "term": "Mobile genetic elements"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Whole genome sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "Genome sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3673",
                            "term": "WGS"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0310",
                                    "term": "Sequence assembly"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Sequence trimming"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3482",
                                    "term": "Antimicrobial resistance prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3472",
                                    "term": "k-mer counting"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3359",
                                    "term": "Splitting"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3192",
                                    "term": "Trimming"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3359",
                                    "term": "File splitting"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Complete hybrid genome assembly of clinical multidrug-resistant Bacteroides fragilis isolates enables comprehensive identification of antimicrobial-resistance genes and plasmids.\n\nquality filtering tool for long reads.\n\nFiltlong is a tool for filtering long reads by quality. It can take a set of long reads and produce a smaller, better subset. It uses both read length (longer is better) and read identity (higher is better) when choosing which reads pass the filter.\n\n||| CORRECT NAME OF TOOL COULD ALSO BE 'Unicycler' (bio.tools/unicycler), 'Canu-corrected ONT', 'AMR', 'fragilis'",
                "homepage": "https://github.com/rrwick/Filtlong"
            }
        ],
        "inputs": [
            "meta",
            "long_reads",
            "short_reads_1",
            "short_reads_2"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"$meta.id\"",
            "conda (params.enable_conda ? \"bioconda::filtlong=0.2.0\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/filtlong:0.2.0--he513fc3_3\" } else { container \"quay.io/biocontainers/filtlong:0.2.0--he513fc3_3\" }"
        ],
        "when": "",
        "stub": ""
    },
    "PRODIGAL": {
        "name_process": "PRODIGAL",
        "string_process": "\nprocess PRODIGAL {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::prodigal=2.6.3\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/prodigal:2.6.3--h516909a_2\"\n    } else {\n        container \"quay.io/biocontainers/prodigal:2.6.3--h516909a_2\"\n    }\n\n    input:\n    tuple val(meta), path(genome)\n    val(output_format)\n\n    output:\n    tuple val(meta), path(\"${prefix}.${output_format}\"), emit: gene_annotations\n    tuple val(meta), path(\"${prefix}.fna\"), emit: nucleotide_fasta\n    tuple val(meta), path(\"${prefix}.faa\"), emit: amino_acid_fasta\n    tuple val(meta), path(\"${prefix}_all.txt\"), emit: all_gene_annotations\n    path \"versions.yml\"           , emit: versions\n\n    script:\n    prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    prodigal -i \"${genome}\" \\\\\n        $options.args \\\\\n        -f $output_format \\\\\n        -d \"${prefix}.fna\" \\\\\n        -o \"${prefix}.${output_format}\" \\\\\n        -a \"${prefix}.faa\" \\\\\n        -s \"${prefix}_all.txt\"\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(prodigal -v 2>&1 | sed -n 's/Prodigal V\\\\(.*\\\\):.*/\\\\1/p')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 41,
        "string_script": "    prefix = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    prodigal -i \"${genome}\" \\\\\n        $options.args \\\\\n        -f $output_format \\\\\n        -d \"${prefix}.fna\" \\\\\n        -o \"${prefix}.${output_format}\" \\\\\n        -a \"${prefix}.faa\" \\\\\n        -s \"${prefix}_all.txt\"\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(prodigal -v 2>&1 | sed -n 's/Prodigal V\\\\(.*\\\\):.*/\\\\1/p')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "genome",
            "output_format"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? \"bioconda::prodigal=2.6.3\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/prodigal:2.6.3--h516909a_2\" } else { container \"quay.io/biocontainers/prodigal:2.6.3--h516909a_2\" }"
        ],
        "when": "",
        "stub": ""
    },
    "BUSCO": {
        "name_process": "BUSCO",
        "string_process": "\nprocess BUSCO {\n    tag \"${bin}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> filename.indexOf(\"busco_downloads\") == -1 ? saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:[]) : null }\n\n    conda (params.enable_conda ? \"bioconda::busco=5.1.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/busco:5.1.0--py_1\"\n    } else {\n        container \"quay.io/biocontainers/busco:5.1.0--py_1\"\n    }\n\n    input:\n    tuple val(meta), path(bin)\n    path(db)\n    path(download_folder)\n\n    output:\n    tuple val(meta), path(\"short_summary.domain.*.${bin}.txt\")          , optional:true , emit: summary_domain\n    tuple val(meta), path(\"short_summary.specific_lineage.*.${bin}.txt\"), optional:true , emit: summary_specific\n    tuple env(most_spec_db), path('busco_downloads/')                   , optional:true , emit: busco_downloads\n    path(\"${bin}_busco.log\")\n    path(\"${bin}_busco.err\")\n    path(\"${bin}_buscos.*.faa.gz\")                                      , optional:true\n    path(\"${bin}_buscos.*.fna.gz\")                                      , optional:true\n    path(\"${bin}_prodigal.gff\")                                         , optional:true , emit: prodigal_genes\n    tuple val(meta), path(\"${bin}_busco.failed_bin.txt\")                , optional:true , emit: failed_bin\n    path '*.version.txt'                                                                , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def cp_augustus_config = \"Y\"\n    if( workflow.profile.toString().indexOf(\"conda\") != -1)\n        cp_augustus_config = \"N\"\n\n    def lineage_dataset_provided = \"N\"\n    if (params.busco_reference)\n        lineage_dataset_provided = \"Y\"\n\n    def p = \"--auto-lineage\"\n    if (params.busco_reference){\n        p = \"--lineage_dataset dataset/${db}\"\n    } else {\n        if (params.busco_auto_lineage_prok)\n            p = \"--auto-lineage-prok\"\n        if (params.busco_download_path)\n            p += \" --offline --download_path ${download_folder}\"\n    }\n    \"\"\"\n    # ensure augustus has write access to config directory\n    if [ ${cp_augustus_config} = \"Y\" ] ; then\n        cp -r /usr/local/config/ augustus_config/\n        export AUGUSTUS_CONFIG_PATH=augustus_config\n    fi\n\n    # place db in extra folder to ensure BUSCO recognizes it as path (instead of downloading it)\n    if [ ${lineage_dataset_provided} = \"Y\" ] ; then\n        mkdir dataset\n        mv ${db} dataset/\n    fi\n\n    # set nullgob: if pattern matches no files, expand to a null string rather than to itself\n    shopt -s nullglob\n\n    # only used for saving busco downloads\n    most_spec_db=\"NA\"\n\n    if busco ${p} \\\n        --mode genome \\\n        --in ${bin} \\\n        --cpu \"${task.cpus}\" \\\n        --out \"BUSCO\" > ${bin}_busco.log 2> ${bin}_busco.err; then\n\n        # get name of used specific lineage dataset\n        summaries=(BUSCO/short_summary.specific.*.BUSCO.txt)\n        if [ \\${#summaries[@]} -ne 1 ]; then\n            echo \"ERROR: none or multiple 'BUSCO/short_summary.specific.*.BUSCO.txt' files found. Expected one.\"\n            exit 1\n        fi\n        [[ \\$summaries =~ BUSCO/short_summary.specific.(.*).BUSCO.txt ]];\n        db_name_spec=\"\\${BASH_REMATCH[1]}\"\n        most_spec_db=\\${db_name_spec}\n        echo \"Used specific lineage dataset: \\${db_name_spec}\"\n\n        if [ ${lineage_dataset_provided} = \"Y\" ]; then\n            cp BUSCO/short_summary.specific.\\${db_name_spec}.BUSCO.txt short_summary.specific_lineage.\\${db_name_spec}.${bin}.txt\n\n            # if lineage dataset is provided, BUSCO analysis does not fail in case no genes can be found as when using the auto selection setting\n            # report bin as failed to allow consistent warnings within the pipeline for both settings\n            if egrep -q \\$'WARNING:\\tBUSCO did not find any match.' ${bin}_busco.log ; then\n                echo \"WARNING: BUSCO could not find any genes for the provided lineage dataset! See also ${bin}_busco.log.\"\n                echo -e \"${bin}\\tNo genes\" > \"${bin}_busco.failed_bin.txt\"\n            fi\n        else\n            # auto lineage selection\n            if { egrep -q \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log \\\n                && egrep -q \\$'INFO:\\tLineage \\\\S+ is selected, supported by ' ${bin}_busco.log ; } || \\\n                { egrep -q \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log \\\n                && egrep -q \\$'INFO:\\tThe results from the Prodigal gene predictor indicate that your data belongs to the mollicutes clade. Testing subclades...' ${bin}_busco.log \\\n                && egrep -q \\$'INFO:\\tUsing local lineages directory ' ${bin}_busco.log ; }; then\n                # the second statement is necessary, because certain mollicute clades use a different genetic code, are not part of the BUSCO placement tree, are tested separately\n                # and cause different log messages\n                echo \"Domain and specific lineage could be selected by BUSCO.\"\n                cp BUSCO/short_summary.specific.\\${db_name_spec}.BUSCO.txt short_summary.specific_lineage.\\${db_name_spec}.${bin}.txt\n\n                db_name_gen=\"\"\n                summaries_gen=(BUSCO/short_summary.generic.*.BUSCO.txt)\n                if [ \\${#summaries_gen[@]} -lt 1 ]; then\n                    echo \"No 'BUSCO/short_summary.generic.*.BUSCO.txt' file found. Assuming selected domain and specific lineages are the same.\"\n                    cp BUSCO/short_summary.specific.\\${db_name_spec}.BUSCO.txt short_summary.domain.\\${db_name_spec}.${bin}.txt\n                    db_name_gen=\\${db_name_spec}\n                else\n                    [[ \\$summaries_gen =~ BUSCO/short_summary.generic.(.*).BUSCO.txt ]];\n                    db_name_gen=\"\\${BASH_REMATCH[1]}\"\n                    echo \"Used generic lineage dataset: \\${db_name_gen}\"\n                    cp BUSCO/short_summary.generic.\\${db_name_gen}.BUSCO.txt short_summary.domain.\\${db_name_gen}.${bin}.txt\n                fi\n\n                for f in BUSCO/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*faa; do\n                    cat BUSCO/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*faa | gzip >${bin}_buscos.\\${db_name_gen}.faa.gz\n                    break\n                done\n                for f in BUSCO/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*fna; do\n                    cat BUSCO/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*fna | gzip >${bin}_buscos.\\${db_name_gen}.fna.gz\n                    break\n                done\n\n            elif egrep -q \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log && egrep -q \\$'INFO:\\tNot enough markers were placed on the tree \\\\([0-9]*\\\\). Root lineage \\\\S+ is kept' ${bin}_busco.log ; then\n                echo \"Domain could be selected by BUSCO, but no more specific lineage.\"\n                cp BUSCO/short_summary.specific.\\${db_name_spec}.BUSCO.txt short_summary.domain.\\${db_name_spec}.${bin}.txt\n\n            elif egrep -q \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log && egrep -q \\$'INFO:\\tRunning virus detection pipeline' ${bin}_busco.log ; then\n                # TODO double-check if selected dataset is not one of bacteria_*, archaea_*, eukaryota_*?\n                echo \"Domain could not be selected by BUSCO, but virus dataset was selected.\"\n                cp BUSCO/short_summary.specific.\\${db_name_spec}.BUSCO.txt short_summary.specific_lineage.\\${db_name_spec}.${bin}.txt\n            else\n                echo \"ERROR: Some not expected case occurred! See ${bin}_busco.log.\" >&2\n                exit 1\n            fi\n        fi\n\n        for f in BUSCO/run_\\${db_name_spec}/busco_sequences/single_copy_busco_sequences/*faa; do\n            cat BUSCO/run_\\${db_name_spec}/busco_sequences/single_copy_busco_sequences/*faa | gzip >${bin}_buscos.\\${db_name_spec}.faa.gz\n            break\n        done\n        for f in BUSCO/run_\\${db_name_spec}/busco_sequences/single_copy_busco_sequences/*fna; do\n            cat BUSCO/run_\\${db_name_spec}/busco_sequences/single_copy_busco_sequences/*fna | gzip >${bin}_buscos.\\${db_name_spec}.fna.gz\n            break\n        done\n\n    elif egrep -q \\$'ERROR:\\tNo genes were recognized by BUSCO' ${bin}_busco.err ; then\n        echo \"WARNING: BUSCO analysis failed due to no recognized genes! See also ${bin}_busco.err.\"\n        echo -e \"${bin}\\tNo genes\" > \"${bin}_busco.failed_bin.txt\"\n\n    elif egrep -q \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log && egrep -q \\$'ERROR:\\tPlacements failed' ${bin}_busco.err ; then\n        echo \"WARNING: BUSCO analysis failed due to failed placements! See also ${bin}_busco.err. Still using results for selected generic lineage dataset.\"\n        echo -e \"${bin}\\tPlacements failed\" > \"${bin}_busco.failed_bin.txt\"\n\n        message=\\$(egrep \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log)\n        [[ \\$message =~ INFO:[[:space:]]([_[:alnum:]]+)[[:space:]]selected ]];\n        db_name_gen=\"\\${BASH_REMATCH[1]}\"\n        most_spec_db=\\${db_name_gen}\n        echo \"Used generic lineage dataset: \\${db_name_gen}\"\n        cp BUSCO/auto_lineage/run_\\${db_name_gen}/short_summary.txt short_summary.domain.\\${db_name_gen}.${bin}.txt\n\n        for f in BUSCO/auto_lineage/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*faa; do\n            cat BUSCO/auto_lineage/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*faa | gzip >${bin}_buscos.\\${db_name_gen}.faa.gz\n            break\n        done\n        for f in BUSCO/auto_lineage/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*fna; do\n            cat BUSCO/auto_lineage/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*fna | gzip >${bin}_buscos.\\${db_name_gen}.fna.gz\n            break\n        done\n\n    else\n        echo \"ERROR: BUSCO analysis failed for some unknown reason! See also ${bin}_busco.err.\" >&2\n        exit 1\n    fi\n\n    # additionally output genes predicted with Prodigal (GFF3)\n    if [ -f BUSCO/logs/prodigal_out.log ]; then\n        mv BUSCO/logs/prodigal_out.log \"${bin}_prodigal.gff\"\n    fi\n\n    busco --version | sed \"s/BUSCO //\" > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 188,
        "string_script": "    def software = getSoftwareName(task.process)\n    def cp_augustus_config = \"Y\"\n    if( workflow.profile.toString().indexOf(\"conda\") != -1)\n        cp_augustus_config = \"N\"\n\n    def lineage_dataset_provided = \"N\"\n    if (params.busco_reference)\n        lineage_dataset_provided = \"Y\"\n\n    def p = \"--auto-lineage\"\n    if (params.busco_reference){\n        p = \"--lineage_dataset dataset/${db}\"\n    } else {\n        if (params.busco_auto_lineage_prok)\n            p = \"--auto-lineage-prok\"\n        if (params.busco_download_path)\n            p += \" --offline --download_path ${download_folder}\"\n    }\n    \"\"\"\n    # ensure augustus has write access to config directory\n    if [ ${cp_augustus_config} = \"Y\" ] ; then\n        cp -r /usr/local/config/ augustus_config/\n        export AUGUSTUS_CONFIG_PATH=augustus_config\n    fi\n\n    # place db in extra folder to ensure BUSCO recognizes it as path (instead of downloading it)\n    if [ ${lineage_dataset_provided} = \"Y\" ] ; then\n        mkdir dataset\n        mv ${db} dataset/\n    fi\n\n    # set nullgob: if pattern matches no files, expand to a null string rather than to itself\n    shopt -s nullglob\n\n    # only used for saving busco downloads\n    most_spec_db=\"NA\"\n\n    if busco ${p} \\\n        --mode genome \\\n        --in ${bin} \\\n        --cpu \"${task.cpus}\" \\\n        --out \"BUSCO\" > ${bin}_busco.log 2> ${bin}_busco.err; then\n\n        # get name of used specific lineage dataset\n        summaries=(BUSCO/short_summary.specific.*.BUSCO.txt)\n        if [ \\${#summaries[@]} -ne 1 ]; then\n            echo \"ERROR: none or multiple 'BUSCO/short_summary.specific.*.BUSCO.txt' files found. Expected one.\"\n            exit 1\n        fi\n        [[ \\$summaries =~ BUSCO/short_summary.specific.(.*).BUSCO.txt ]];\n        db_name_spec=\"\\${BASH_REMATCH[1]}\"\n        most_spec_db=\\${db_name_spec}\n        echo \"Used specific lineage dataset: \\${db_name_spec}\"\n\n        if [ ${lineage_dataset_provided} = \"Y\" ]; then\n            cp BUSCO/short_summary.specific.\\${db_name_spec}.BUSCO.txt short_summary.specific_lineage.\\${db_name_spec}.${bin}.txt\n\n            # if lineage dataset is provided, BUSCO analysis does not fail in case no genes can be found as when using the auto selection setting\n            # report bin as failed to allow consistent warnings within the pipeline for both settings\n            if egrep -q \\$'WARNING:\\tBUSCO did not find any match.' ${bin}_busco.log ; then\n                echo \"WARNING: BUSCO could not find any genes for the provided lineage dataset! See also ${bin}_busco.log.\"\n                echo -e \"${bin}\\tNo genes\" > \"${bin}_busco.failed_bin.txt\"\n            fi\n        else\n            # auto lineage selection\n            if { egrep -q \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log \\\n                && egrep -q \\$'INFO:\\tLineage \\\\S+ is selected, supported by ' ${bin}_busco.log ; } || \\\n                { egrep -q \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log \\\n                && egrep -q \\$'INFO:\\tThe results from the Prodigal gene predictor indicate that your data belongs to the mollicutes clade. Testing subclades...' ${bin}_busco.log \\\n                && egrep -q \\$'INFO:\\tUsing local lineages directory ' ${bin}_busco.log ; }; then\n                # the second statement is necessary, because certain mollicute clades use a different genetic code, are not part of the BUSCO placement tree, are tested separately\n                # and cause different log messages\n                echo \"Domain and specific lineage could be selected by BUSCO.\"\n                cp BUSCO/short_summary.specific.\\${db_name_spec}.BUSCO.txt short_summary.specific_lineage.\\${db_name_spec}.${bin}.txt\n\n                db_name_gen=\"\"\n                summaries_gen=(BUSCO/short_summary.generic.*.BUSCO.txt)\n                if [ \\${#summaries_gen[@]} -lt 1 ]; then\n                    echo \"No 'BUSCO/short_summary.generic.*.BUSCO.txt' file found. Assuming selected domain and specific lineages are the same.\"\n                    cp BUSCO/short_summary.specific.\\${db_name_spec}.BUSCO.txt short_summary.domain.\\${db_name_spec}.${bin}.txt\n                    db_name_gen=\\${db_name_spec}\n                else\n                    [[ \\$summaries_gen =~ BUSCO/short_summary.generic.(.*).BUSCO.txt ]];\n                    db_name_gen=\"\\${BASH_REMATCH[1]}\"\n                    echo \"Used generic lineage dataset: \\${db_name_gen}\"\n                    cp BUSCO/short_summary.generic.\\${db_name_gen}.BUSCO.txt short_summary.domain.\\${db_name_gen}.${bin}.txt\n                fi\n\n                for f in BUSCO/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*faa; do\n                    cat BUSCO/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*faa | gzip >${bin}_buscos.\\${db_name_gen}.faa.gz\n                    break\n                done\n                for f in BUSCO/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*fna; do\n                    cat BUSCO/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*fna | gzip >${bin}_buscos.\\${db_name_gen}.fna.gz\n                    break\n                done\n\n            elif egrep -q \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log && egrep -q \\$'INFO:\\tNot enough markers were placed on the tree \\\\([0-9]*\\\\). Root lineage \\\\S+ is kept' ${bin}_busco.log ; then\n                echo \"Domain could be selected by BUSCO, but no more specific lineage.\"\n                cp BUSCO/short_summary.specific.\\${db_name_spec}.BUSCO.txt short_summary.domain.\\${db_name_spec}.${bin}.txt\n\n            elif egrep -q \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log && egrep -q \\$'INFO:\\tRunning virus detection pipeline' ${bin}_busco.log ; then\n                # TODO double-check if selected dataset is not one of bacteria_*, archaea_*, eukaryota_*?\n                echo \"Domain could not be selected by BUSCO, but virus dataset was selected.\"\n                cp BUSCO/short_summary.specific.\\${db_name_spec}.BUSCO.txt short_summary.specific_lineage.\\${db_name_spec}.${bin}.txt\n            else\n                echo \"ERROR: Some not expected case occurred! See ${bin}_busco.log.\" >&2\n                exit 1\n            fi\n        fi\n\n        for f in BUSCO/run_\\${db_name_spec}/busco_sequences/single_copy_busco_sequences/*faa; do\n            cat BUSCO/run_\\${db_name_spec}/busco_sequences/single_copy_busco_sequences/*faa | gzip >${bin}_buscos.\\${db_name_spec}.faa.gz\n            break\n        done\n        for f in BUSCO/run_\\${db_name_spec}/busco_sequences/single_copy_busco_sequences/*fna; do\n            cat BUSCO/run_\\${db_name_spec}/busco_sequences/single_copy_busco_sequences/*fna | gzip >${bin}_buscos.\\${db_name_spec}.fna.gz\n            break\n        done\n\n    elif egrep -q \\$'ERROR:\\tNo genes were recognized by BUSCO' ${bin}_busco.err ; then\n        echo \"WARNING: BUSCO analysis failed due to no recognized genes! See also ${bin}_busco.err.\"\n        echo -e \"${bin}\\tNo genes\" > \"${bin}_busco.failed_bin.txt\"\n\n    elif egrep -q \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log && egrep -q \\$'ERROR:\\tPlacements failed' ${bin}_busco.err ; then\n        echo \"WARNING: BUSCO analysis failed due to failed placements! See also ${bin}_busco.err. Still using results for selected generic lineage dataset.\"\n        echo -e \"${bin}\\tPlacements failed\" > \"${bin}_busco.failed_bin.txt\"\n\n        message=\\$(egrep \\$'INFO:\\t\\\\S+ selected' ${bin}_busco.log)\n        [[ \\$message =~ INFO:[[:space:]]([_[:alnum:]]+)[[:space:]]selected ]];\n        db_name_gen=\"\\${BASH_REMATCH[1]}\"\n        most_spec_db=\\${db_name_gen}\n        echo \"Used generic lineage dataset: \\${db_name_gen}\"\n        cp BUSCO/auto_lineage/run_\\${db_name_gen}/short_summary.txt short_summary.domain.\\${db_name_gen}.${bin}.txt\n\n        for f in BUSCO/auto_lineage/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*faa; do\n            cat BUSCO/auto_lineage/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*faa | gzip >${bin}_buscos.\\${db_name_gen}.faa.gz\n            break\n        done\n        for f in BUSCO/auto_lineage/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*fna; do\n            cat BUSCO/auto_lineage/run_\\${db_name_gen}/busco_sequences/single_copy_busco_sequences/*fna | gzip >${bin}_buscos.\\${db_name_gen}.fna.gz\n            break\n        done\n\n    else\n        echo \"ERROR: BUSCO analysis failed for some unknown reason! See also ${bin}_busco.err.\" >&2\n        exit 1\n    fi\n\n    # additionally output genes predicted with Prodigal (GFF3)\n    if [ -f BUSCO/logs/prodigal_out.log ]; then\n        mv BUSCO/logs/prodigal_out.log \"${bin}_prodigal.gff\"\n    fi\n\n    busco --version | sed \"s/BUSCO //\" > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 155,
        "language_script": "bash",
        "tools": [
            "BreakSeq",
            "BUSCO"
        ],
        "tools_url": [
            "https://bio.tools/breakseq",
            "https://bio.tools/busco"
        ],
        "tools_dico": [
            {
                "name": "BreakSeq",
                "uri": "https://bio.tools/breakseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "Structural variation"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "Genomic structural variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3175",
                            "term": "DNA structural variation"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read mapping"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Read alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment construction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment generation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Oligonucleotide alignment"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short sequence read mapping"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3198",
                                    "term": "Short read alignment"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Database of known human breakpoint junctions and software to search short reads against them.",
                "homepage": "http://sv.gersteinlab.org/breakseq/"
            },
            {
                "name": "BUSCO",
                "uri": "https://bio.tools/busco",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3308",
                            "term": "Transcriptomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly validation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly quality evaluation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Assembly QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Assembly quality evaluation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3180",
                                    "term": "Sequence assembly QC"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_1234",
                                "term": "Sequence set (nucleic acid)"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "Provides measures for quantitative assessment of genome assembly, gene set, and transcriptome completeness based on evolutionarily informed expectations of gene content from near-universal single-copy orthologs.",
                "homepage": "http://busco.ezlab.org/"
            }
        ],
        "inputs": [
            "meta",
            "bin",
            "db",
            "download_folder"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${bin}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> filename.indexOf(\"busco_downloads\") == -1 ? saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:[]) : null }",
            "conda (params.enable_conda ? \"bioconda::busco=5.1.0\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/busco:5.1.0--py_1\" } else { container \"quay.io/biocontainers/busco:5.1.0--py_1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "BIN_SUMMARY": {
        "name_process": "BIN_SUMMARY",
        "string_process": "\nprocess BIN_SUMMARY {\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"conda-forge::pandas=1.1.5\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/pandas:1.1.5\"\n    } else {\n        container \"quay.io/biocontainers/pandas:1.1.5\"\n    }\n\n    input:\n    path(bin_depths)\n    path(busco_sum)\n    path(quast_sum)\n    path(gtdbtk_sum)\n\n    output:\n    path(\"bin_summary.tsv\"), emit: summary\n\n    script:\n    def busco_summary  = busco_sum.sort().size() > 0 ?  \"--busco_summary ${busco_sum}\" : \"\"\n    def quast_summary  = quast_sum.sort().size() > 0 ?  \"--quast_summary ${quast_sum}\" : \"\"\n    def gtdbtk_summary = gtdbtk_sum.sort().size() > 0 ? \"--gtdbtk_summary ${gtdbtk_sum}\" : \"\"\n    \"\"\"\n    combine_tables.py --depths_summary ${bin_depths} \\\n                    $busco_summary \\\n                    $quast_summary \\\n                    $gtdbtk_summary \\\n                    --out bin_summary.tsv\n    \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "    def busco_summary  = busco_sum.sort().size() > 0 ?  \"--busco_summary ${busco_sum}\" : \"\"\n    def quast_summary  = quast_sum.sort().size() > 0 ?  \"--quast_summary ${quast_sum}\" : \"\"\n    def gtdbtk_summary = gtdbtk_sum.sort().size() > 0 ? \"--gtdbtk_summary ${gtdbtk_sum}\" : \"\"\n    \"\"\"\n    combine_tables.py --depths_summary ${bin_depths} \\\n                    $busco_summary \\\n                    $quast_summary \\\n                    $gtdbtk_summary \\\n                    --out bin_summary.tsv\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "bin_depths",
            "busco_sum",
            "quast_sum",
            "gtdbtk_sum"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }",
            "conda (params.enable_conda ? \"conda-forge::pandas=1.1.5\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/pandas:1.1.5\" } else { container \"quay.io/biocontainers/pandas:1.1.5\" }"
        ],
        "when": "",
        "stub": ""
    },
    "GTDBTK_SUMMARY": {
        "name_process": "GTDBTK_SUMMARY",
        "string_process": "\nprocess GTDBTK_SUMMARY {\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"conda-forge::pandas=1.1.5\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/pandas:1.1.5\"\n    } else {\n        container \"quay.io/biocontainers/pandas:1.1.5\"\n    }\n\n    input:\n    path(qc_discarded_bins)\n    path(gtdbtk_summaries)\n    path(filtered_bins)\n    path(failed_bins)\n\n    output:\n    path \"gtdbtk_summary.tsv\", emit: summary\n\n    script:\n    def discarded = qc_discarded_bins.sort().size() > 0 ? \"--qc_discarded_bins ${qc_discarded_bins}\" : \"\"\n    def summaries = gtdbtk_summaries.sort().size() > 0 ?  \"--summaries ${gtdbtk_summaries}\" : \"\"\n    def filtered  = filtered_bins.sort().size() > 0 ?     \"--filtered_bins ${filtered_bins}\" : \"\"\n    def failed    = failed_bins.sort().size() > 0 ?       \"--failed_bins ${failed_bins}\" : \"\"\n    \"\"\"\n    summary_gtdbtk.py $options.args $discarded $summaries $filtered $failed --out gtdbtk_summary.tsv\n    \"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "    def discarded = qc_discarded_bins.sort().size() > 0 ? \"--qc_discarded_bins ${qc_discarded_bins}\" : \"\"\n    def summaries = gtdbtk_summaries.sort().size() > 0 ?  \"--summaries ${gtdbtk_summaries}\" : \"\"\n    def filtered  = filtered_bins.sort().size() > 0 ?     \"--filtered_bins ${filtered_bins}\" : \"\"\n    def failed    = failed_bins.sort().size() > 0 ?       \"--failed_bins ${failed_bins}\" : \"\"\n    \"\"\"\n    summary_gtdbtk.py $options.args $discarded $summaries $filtered $failed --out gtdbtk_summary.tsv\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "qc_discarded_bins",
            "gtdbtk_summaries",
            "filtered_bins",
            "failed_bins"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }",
            "conda (params.enable_conda ? \"conda-forge::pandas=1.1.5\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/pandas:1.1.5\" } else { container \"quay.io/biocontainers/pandas:1.1.5\" }"
        ],
        "when": "",
        "stub": ""
    },
    "BOWTIE2_ASSEMBLY_BUILD": {
        "name_process": "BOWTIE2_ASSEMBLY_BUILD",
        "string_process": "\nprocess BOWTIE2_ASSEMBLY_BUILD {\n    tag \"${meta.assembler}-${meta.id}\"\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.2' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/bowtie2:2.4.2--py38h1c8e9b9_1'\n    } else {\n        container 'quay.io/biocontainers/bowtie2:2.4.2--py38h1c8e9b9_1'\n    }\n\n    input:\n    tuple val(meta), path(assembly)\n\n    output:\n    tuple val(meta), path(assembly), path('bt2_index_base*'), emit: assembly_index\n    path '*.version.txt'                                    , emit: version\n\n    script:\n    def software  = getSoftwareName(task.process)\n    \"\"\"\n    mkdir bowtie\n    bowtie2-build --threads $task.cpus $assembly \"bt2_index_base\"\n    bowtie2 --version > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 24,
        "string_script": "    def software  = getSoftwareName(task.process)\n    \"\"\"\n    mkdir bowtie\n    bowtie2-build --threads $task.cpus $assembly \"bt2_index_base\"\n    bowtie2 --version > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "Rbowtie2"
        ],
        "tools_url": [
            "https://bio.tools/rbowtie2"
        ],
        "tools_dico": [
            {
                "name": "Rbowtie2",
                "uri": "https://bio.tools/rbowtie2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence merging"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence splicing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package provides an R wrapper of the popular bowtie2 sequencing reads aligner and AdapterRemoval, a convenient tool for rapid adapter trimming, identification, and read merging.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/Rbowtie2.html"
            }
        ],
        "inputs": [
            "meta",
            "assembly"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${meta.assembler}-${meta.id}\"",
            "conda (params.enable_conda ? 'bioconda::bowtie2=2.4.2' : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container 'https://depot.galaxyproject.org/singularity/bowtie2:2.4.2--py38h1c8e9b9_1' } else { container 'quay.io/biocontainers/bowtie2:2.4.2--py38h1c8e9b9_1' }"
        ],
        "when": "",
        "stub": ""
    },
    "BUSCO_SUMMARY": {
        "name_process": "BUSCO_SUMMARY",
        "string_process": "\nprocess BUSCO_SUMMARY {\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"conda-forge::pandas=1.1.5\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/pandas:1.1.5\"\n    } else {\n        container \"quay.io/biocontainers/pandas:1.1.5\"\n    }\n\n    input:\n    path(summaries_domain)\n    path(summaries_specific)\n    path(failed_bins)\n\n    output:\n    path \"busco_summary.tsv\", emit: summary\n\n    script:\n    def auto = params.busco_reference ? \"\" : \"-a\"\n    def ss = summaries_specific.sort().size() > 0 ? \"-ss ${summaries_specific}\" : \"\"\n    def sd = summaries_domain.sort().size() > 0 ? \"-sd ${summaries_domain}\" : \"\"\n    def f = \"\"\n    if (!params.busco_reference && failed_bins.sort().size() > 0)\n        f = \"-f ${failed_bins}\"\n    \"\"\"\n    summary_busco.py $auto $ss $sd $f -o busco_summary.tsv\n    \"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "    def auto = params.busco_reference ? \"\" : \"-a\"\n    def ss = summaries_specific.sort().size() > 0 ? \"-ss ${summaries_specific}\" : \"\"\n    def sd = summaries_domain.sort().size() > 0 ? \"-sd ${summaries_domain}\" : \"\"\n    def f = \"\"\n    if (!params.busco_reference && failed_bins.sort().size() > 0)\n        f = \"-f ${failed_bins}\"\n    \"\"\"\n    summary_busco.py $auto $ss $sd $f -o busco_summary.tsv\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "summaries_domain",
            "summaries_specific",
            "failed_bins"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }",
            "conda (params.enable_conda ? \"conda-forge::pandas=1.1.5\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/pandas:1.1.5\" } else { container \"quay.io/biocontainers/pandas:1.1.5\" }"
        ],
        "when": "",
        "stub": ""
    },
    "SPADES": {
        "name_process": "SPADES",
        "string_process": "\nprocess SPADES {\n    tag \"$meta.id\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::spades=3.15.3\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/spades:3.15.3--h95f258a_0\"\n    } else {\n        container \"quay.io/biocontainers/spades:3.15.3--h95f258a_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"${meta.id}_scaffolds.fasta\"), emit: assembly\n    path \"${meta.id}.log\"                              , emit: log\n    path \"${meta.id}_contigs.fasta.gz\"                 , emit: contigs_gz\n    path \"${meta.id}_scaffolds.fasta.gz\"               , emit: assembly_gz\n    path \"${meta.id}_graph.gfa.gz\"                     , emit: graph\n    path '*.version.txt'                               , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    maxmem = task.memory.toGiga()\n    if ( params.spades_fix_cpus == -1 || task.cpus == params.spades_fix_cpus )\n        \"\"\"\n        metaspades.py \\\n            ${params.spades_options} \\\n            --threads \"${task.cpus}\" \\\n            --memory $maxmem \\\n            --pe1-1 ${reads[0]} \\\n            --pe1-2 ${reads[1]} \\\n            -o spades\n        mv spades/assembly_graph_with_scaffolds.gfa ${meta.id}_graph.gfa\n        mv spades/scaffolds.fasta ${meta.id}_scaffolds.fasta\n        mv spades/contigs.fasta ${meta.id}_contigs.fasta\n        mv spades/spades.log ${meta.id}.log\n        gzip \"${meta.id}_contigs.fasta\"\n        gzip \"${meta.id}_graph.gfa\"\n        gzip -c \"${meta.id}_scaffolds.fasta\" > \"${meta.id}_scaffolds.fasta.gz\"\n\n        metaspades.py --version | sed \"s/SPAdes v//; s/ \\\\[.*//\" > ${software}.version.txt\n        \"\"\"\n    else\n        error \"ERROR: '--spades_fix_cpus' was specified, but not succesfully applied. Likely this is caused by changed process properties in a custom config file.\"\n}",
        "nb_lignes_process": 49,
        "string_script": "    def software = getSoftwareName(task.process)\n    maxmem = task.memory.toGiga()\n    if ( params.spades_fix_cpus == -1 || task.cpus == params.spades_fix_cpus )\n        \"\"\"\n        metaspades.py \\\n            ${params.spades_options} \\\n            --threads \"${task.cpus}\" \\\n            --memory $maxmem \\\n            --pe1-1 ${reads[0]} \\\n            --pe1-2 ${reads[1]} \\\n            -o spades\n        mv spades/assembly_graph_with_scaffolds.gfa ${meta.id}_graph.gfa\n        mv spades/scaffolds.fasta ${meta.id}_scaffolds.fasta\n        mv spades/contigs.fasta ${meta.id}_contigs.fasta\n        mv spades/spades.log ${meta.id}.log\n        gzip \"${meta.id}_contigs.fasta\"\n        gzip \"${meta.id}_graph.gfa\"\n        gzip -c \"${meta.id}_scaffolds.fasta\" > \"${meta.id}_scaffolds.fasta.gz\"\n\n        metaspades.py --version | sed \"s/SPAdes v//; s/ \\\\[.*//\" > ${software}.version.txt\n        \"\"\"\n    else\n        error \"ERROR: '--spades_fix_cpus' was specified, but not succesfully applied. Likely this is caused by changed process properties in a custom config file.\"",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"$meta.id\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? \"bioconda::spades=3.15.3\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/spades:3.15.3--h95f258a_0\" } else { container \"quay.io/biocontainers/spades:3.15.3--h95f258a_0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "FASTP": {
        "name_process": "FASTP",
        "string_process": "\nprocess FASTP {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? 'bioconda::fastp=0.20.1' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/fastp:0.20.1--h8b12597_0'\n    } else {\n        container 'quay.io/biocontainers/fastp:0.20.1--h8b12597_0'\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path('*.trim.fastq.gz'), emit: reads\n    tuple val(meta), path('*.json')         , emit: json\n    tuple val(meta), path('*.html')         , emit: html\n    tuple val(meta), path('*.log')          , emit: log\n    path '*.version.txt'                    , emit: version\n    tuple val(meta), path('*.fail.fastq.gz'), optional:true, emit: reads_fail\n\n    script:\n                                                                           \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        def fail_fastq = params.save_trimmed_fail ? \"--failed_out ${prefix}.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            --out1 ${prefix}.trim.fastq.gz \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        def fail_fastq = params.save_trimmed_fail ? \"--unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\n            --out1 ${prefix}_1.trim.fastq.gz \\\\\n            --out2 ${prefix}_2.trim.fastq.gz \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            --thread $task.cpus \\\\\n            --detect_adapter_for_pe \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    }\n}",
        "nb_lignes_process": 65,
        "string_script": "    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        def fail_fastq = params.save_trimmed_fail ? \"--failed_out ${prefix}.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}.fastq.gz \\\\\n            --out1 ${prefix}.trim.fastq.gz \\\\\n            --thread $task.cpus \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        def fail_fastq = params.save_trimmed_fail ? \"--unpaired1 ${prefix}_1.fail.fastq.gz --unpaired2 ${prefix}_2.fail.fastq.gz\" : ''\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastp \\\\\n            --in1 ${prefix}_1.fastq.gz \\\\\n            --in2 ${prefix}_2.fastq.gz \\\\\n            --out1 ${prefix}_1.trim.fastq.gz \\\\\n            --out2 ${prefix}_2.trim.fastq.gz \\\\\n            --json ${prefix}.fastp.json \\\\\n            --html ${prefix}.fastp.html \\\\\n            $fail_fastq \\\\\n            --thread $task.cpus \\\\\n            --detect_adapter_for_pe \\\\\n            $options.args \\\\\n            2> ${prefix}.fastp.log\n\n        echo \\$(fastp --version 2>&1) | sed -e \"s/fastp //g\" > ${software}.version.txt\n        \"\"\"\n    }",
        "nb_lignes_script": 37,
        "language_script": "bash",
        "tools": [
            "fastPHASE"
        ],
        "tools_url": [
            "https://bio.tools/fastphase"
        ],
        "tools_dico": [
            {
                "name": "fastPHASE",
                "uri": "https://bio.tools/fastphase",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3056",
                            "term": "Population genetics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3454",
                                    "term": "Phasing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Imputation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3557",
                                    "term": "Data imputation"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "fastPHASE is a program to estimate missing genotypes and unobserved haplotypes. It is an implementation of the model described in Scheet & Stephens (2006). This is a cluster-based model for haplotype variation, and gains its utility from implicitly modeling the genealogy of chromosomes in a random sample from a population as a tree but summarizing all haplotype variation in the \"tips\" of the trees.",
                "homepage": "http://scheet.org/software.html"
            }
        ],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? 'bioconda::fastp=0.20.1' : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container 'https://depot.galaxyproject.org/singularity/fastp:0.20.1--h8b12597_0' } else { container 'quay.io/biocontainers/fastp:0.20.1--h8b12597_0' }"
        ],
        "when": "",
        "stub": ""
    },
    "POOL_SINGLE_READS": {
        "name_process": "POOL_SINGLE_READS",
        "string_process": "\nprocess POOL_SINGLE_READS {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\"\n    } else {\n        container \"biocontainers/biocontainers:v1.2.0_cv1\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"pooled_${meta.id}.fastq.gz\"), emit: reads\n\n    script:\n    \"\"\"\n    cat ${reads} > \"pooled_${meta.id}.fastq.gz\"\n    \"\"\"\n}",
        "nb_lignes_process": 20,
        "string_script": "    \"\"\"\n    cat ${reads} > \"pooled_${meta.id}.fastq.gz\"\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"$meta.id\"",
            "conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\" } else { container \"biocontainers/biocontainers:v1.2.0_cv1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "MULTIQC": {
        "name_process": "MULTIQC",
        "string_process": "\nprocess MULTIQC {\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename: filename, options: params.options, publish_dir: getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\"\n    }\n\n    input:\n    path multiqc_files\n    path mqc_custom_config\n    path 'fastqc_raw/*'\n    path 'fastp/*'\n    path 'fastqc_trimmed/*'\n    path host_removal\n    path 'quast*/*'\n    path 'bowtie2log/*'\n    path short_summary\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"*.version.txt\"       , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    read_type = params.single_end ? \"--single_end\" : ''\n    if ( params.host_fasta || params.host_genome ) {\n        \"\"\"\n        # get multiqc parsed data for bowtie2\n        multiqc -f $custom_config_file *.bowtie2.log\n        multiqc_to_custom_tsv.py ${read_type}\n        # run multiqc using custom content file instead of original bowtie2 log files\n        multiqc -f $custom_config_file --ignore \"*.bowtie2.log\" .\n        multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        multiqc -f $options.args .\n        multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n        \"\"\"\n    }\n}",
        "nb_lignes_process": 49,
        "string_script": "    def software = getSoftwareName(task.process)\n    custom_config_file = params.multiqc_config ? \"--config $mqc_custom_config\" : ''\n    read_type = params.single_end ? \"--single_end\" : ''\n    if ( params.host_fasta || params.host_genome ) {\n        \"\"\"\n        # get multiqc parsed data for bowtie2\n        multiqc -f $custom_config_file *.bowtie2.log\n        multiqc_to_custom_tsv.py ${read_type}\n        # run multiqc using custom content file instead of original bowtie2 log files\n        multiqc -f $custom_config_file --ignore \"*.bowtie2.log\" .\n        multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        multiqc -f $options.args .\n        multiqc --version | sed -e \"s/multiqc, version //g\" > ${software}.version.txt\n        \"\"\"\n    }",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [
            "MultiQC"
        ],
        "tools_url": [
            "https://bio.tools/multiqc"
        ],
        "tools_dico": [
            {
                "name": "MultiQC",
                "uri": "https://bio.tools/multiqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0091",
                            "term": "Bioinformatics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2428",
                                    "term": "Validation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2048",
                                "term": "Report"
                            }
                        ]
                    }
                ],
                "description": "MultiQC aggregates results from multiple bioinformatics analyses across many samples into a single report. It searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.",
                "homepage": "http://multiqc.info/"
            }
        ],
        "inputs": [
            "multiqc_files",
            "mqc_custom_config",
            "host_removal",
            "short_summary"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "label 'process_medium'",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename: filename, options: params.options, publish_dir: getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }",
            "conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0\" } else { container \"quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "MAG_DEPTHS_SUMMARY": {
        "name_process": "MAG_DEPTHS_SUMMARY",
        "string_process": "\nprocess MAG_DEPTHS_SUMMARY {\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"conda-forge::pandas=1.1.5\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/pandas:1.1.5\"\n    } else {\n        container \"quay.io/biocontainers/pandas:1.1.5\"\n    }\n\n    input:\n    path(mag_depths)\n\n    output:\n    path(\"bin_depths_summary.tsv\"), emit: summary\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    get_mag_depths_summary.py --depths ${mag_depths} \\\n                            --out \"bin_depths_summary.tsv\"\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    get_mag_depths_summary.py --depths ${mag_depths} \\\n                            --out \"bin_depths_summary.tsv\"\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "mag_depths"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }",
            "conda (params.enable_conda ? \"conda-forge::pandas=1.1.5\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/pandas:1.1.5\" } else { container \"quay.io/biocontainers/pandas:1.1.5\" }"
        ],
        "when": "",
        "stub": ""
    },
    "BOWTIE2_REMOVAL_ALIGN": {
        "name_process": "BOWTIE2_REMOVAL_ALIGN",
        "string_process": "\nprocess BOWTIE2_REMOVAL_ALIGN {\n    tag \"${meta.id}-${options.suffix}\"\n    publishDir \"${params.outdir}/\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::bowtie2=2.4.2\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/bowtie2:2.4.2--py38h1c8e9b9_1\"\n    } else {\n        container \"quay.io/biocontainers/bowtie2:2.4.2--py38h1c8e9b9_1\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path  index\n\n    output:\n    tuple val(meta), path(\"*.unmapped*.fastq.gz\") , emit: reads\n    path  \"*.mapped*.read_ids.txt\", optional:true , emit: read_ids\n    tuple val(meta), path(\"*.bowtie2.log\")        , emit: log\n    path  '*.version.txt'                         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix    = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    def sensitivity = params.host_removal_verysensitive ? \"--very-sensitive\" : \"--sensitive\"\n    def save_ids = params.host_removal_save_ids ? \"Y\" : \"N\"\n    if (!meta.single_end){\n        \"\"\"\n        bowtie2 -p ${task.cpus} \\\n                -x ${index[0].getSimpleName()} \\\n                -1 \"${reads[0]}\" -2 \"${reads[1]}\" \\\n                $sensitivity \\\n                --un-conc-gz ${prefix}.unmapped_%.fastq.gz \\\n                --al-conc-gz ${prefix}.mapped_%.fastq.gz \\\n                1> /dev/null \\\n                2> ${prefix}.bowtie2.log\n        if [ ${save_ids} = \"Y\" ] ; then\n            gunzip -c ${prefix}.mapped_1.fastq.gz | awk '{if(NR%4==1) print substr(\\$0, 2)}' | LC_ALL=C sort > ${prefix}.mapped_1.read_ids.txt\n            gunzip -c ${prefix}.mapped_2.fastq.gz | awk '{if(NR%4==1) print substr(\\$0, 2)}' | LC_ALL=C sort > ${prefix}.mapped_2.read_ids.txt\n        fi\n        rm -f ${prefix}.mapped_*.fastq.gz\n\n        echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//' > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        bowtie2 -p ${task.cpus} \\\n                -x ${index[0].getSimpleName()} \\\n                -U ${reads} \\\n                $sensitivity \\\n                --un-gz ${prefix}.unmapped.fastq.gz \\\n                --al-gz ${prefix}.mapped.fastq.gz \\\n                1> /dev/null \\\n                2> ${prefix}.bowtie2.log\n        if [ ${save_ids} = \"Y\" ] ; then\n            gunzip -c ${prefix}.mapped.fastq.gz | awk '{if(NR%4==1) print substr(\\$0, 2)}' | LC_ALL=C sort > ${prefix}.mapped.read_ids.txt\n        fi\n        rm -f ${prefix}.mapped.fastq.gz\n\n        echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//' > ${software}.version.txt\n        \"\"\"\n    }\n}",
        "nb_lignes_process": 64,
        "string_script": "    def software = getSoftwareName(task.process)\n    def prefix    = options.suffix ? \"${meta.id}.${options.suffix}\" : \"${meta.id}\"\n    def sensitivity = params.host_removal_verysensitive ? \"--very-sensitive\" : \"--sensitive\"\n    def save_ids = params.host_removal_save_ids ? \"Y\" : \"N\"\n    if (!meta.single_end){\n        \"\"\"\n        bowtie2 -p ${task.cpus} \\\n                -x ${index[0].getSimpleName()} \\\n                -1 \"${reads[0]}\" -2 \"${reads[1]}\" \\\n                $sensitivity \\\n                --un-conc-gz ${prefix}.unmapped_%.fastq.gz \\\n                --al-conc-gz ${prefix}.mapped_%.fastq.gz \\\n                1> /dev/null \\\n                2> ${prefix}.bowtie2.log\n        if [ ${save_ids} = \"Y\" ] ; then\n            gunzip -c ${prefix}.mapped_1.fastq.gz | awk '{if(NR%4==1) print substr(\\$0, 2)}' | LC_ALL=C sort > ${prefix}.mapped_1.read_ids.txt\n            gunzip -c ${prefix}.mapped_2.fastq.gz | awk '{if(NR%4==1) print substr(\\$0, 2)}' | LC_ALL=C sort > ${prefix}.mapped_2.read_ids.txt\n        fi\n        rm -f ${prefix}.mapped_*.fastq.gz\n\n        echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//' > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        bowtie2 -p ${task.cpus} \\\n                -x ${index[0].getSimpleName()} \\\n                -U ${reads} \\\n                $sensitivity \\\n                --un-gz ${prefix}.unmapped.fastq.gz \\\n                --al-gz ${prefix}.mapped.fastq.gz \\\n                1> /dev/null \\\n                2> ${prefix}.bowtie2.log\n        if [ ${save_ids} = \"Y\" ] ; then\n            gunzip -c ${prefix}.mapped.fastq.gz | awk '{if(NR%4==1) print substr(\\$0, 2)}' | LC_ALL=C sort > ${prefix}.mapped.read_ids.txt\n        fi\n        rm -f ${prefix}.mapped.fastq.gz\n\n        echo \\$(bowtie2 --version 2>&1) | sed 's/^.*bowtie2-align-s version //; s/ .*\\$//' > ${software}.version.txt\n        \"\"\"\n    }",
        "nb_lignes_script": 39,
        "language_script": "bash",
        "tools": [
            "Rbowtie2",
            "NullSeq"
        ],
        "tools_url": [
            "https://bio.tools/rbowtie2",
            "https://bio.tools/nullseq"
        ],
        "tools_dico": [
            {
                "name": "Rbowtie2",
                "uri": "https://bio.tools/rbowtie2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence merging"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence splicing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package provides an R wrapper of the popular bowtie2 sequencing reads aligner and AdapterRemoval, a convenient tool for rapid adapter trimming, identification, and read merging.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/Rbowtie2.html"
            },
            {
                "name": "NullSeq",
                "uri": "https://bio.tools/nullseq",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0364",
                                    "term": "Random sequence generation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Creates Random Coding Sequences with specified GC content and Amino Acid usage.",
                "homepage": "https://github.com/amarallab/NullSeq"
            }
        ],
        "inputs": [
            "meta",
            "reads",
            "index"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${meta.id}-${options.suffix}\"",
            "publishDir \"${params.outdir}/\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? \"bioconda::bowtie2=2.4.2\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/bowtie2:2.4.2--py38h1c8e9b9_1\" } else { container \"quay.io/biocontainers/bowtie2:2.4.2--py38h1c8e9b9_1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "CAT": {
        "name_process": "CAT",
        "string_process": "\nprocess CAT {\n    tag \"${meta.assembler}-${meta.id}-${db_name}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['assembler']) }\n\n    conda (params.enable_conda ? \"bioconda::cat=4.6 bioconda::diamond=2.0.6\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-75e2a26f10cbf3629edf2d1600db3fed5ebe6e04:eae321284604f7dabbdf121e3070bda907b91266-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-75e2a26f10cbf3629edf2d1600db3fed5ebe6e04:eae321284604f7dabbdf121e3070bda907b91266-0\"\n    }\n\n    input:\n    tuple val(meta), path(\"bins/*\")\n    tuple val(db_name), path(\"database/*\"), path(\"taxonomy/*\")\n\n    output:\n    path(\"*.names.txt.gz\")                 , emit: tax_classification\n    path(\"raw/*.ORF2LCA.txt.gz\")           , emit: orf2lca\n    path(\"raw/*.predicted_proteins.faa.gz\"), emit: faa\n    path(\"raw/*.predicted_proteins.gff.gz\"), emit: gff\n    path(\"raw/*.log\")                      , emit: log\n    path(\"raw/*.bin2classification.txt.gz\"), emit: tax_classification_taxids\n    path '*.version.txt'                   , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    CAT bins -b \"bins/\" -d database/ -t taxonomy/ -n \"${task.cpus}\" -s .fa --top 6 -o \"${meta.assembler}-${meta.id}\" --I_know_what_Im_doing\n    CAT add_names -i \"${meta.assembler}-${meta.id}.ORF2LCA.txt\" -o \"${meta.assembler}-${meta.id}.ORF2LCA.names.txt\" -t taxonomy/\n    CAT add_names -i \"${meta.assembler}-${meta.id}.bin2classification.txt\" -o \"${meta.assembler}-${meta.id}.bin2classification.names.txt\" -t taxonomy/\n\n    mkdir raw\n    mv *.ORF2LCA.txt *.predicted_proteins.faa *.predicted_proteins.gff *.log *.bin2classification.txt raw/\n    gzip \"raw/${meta.assembler}-${meta.id}.ORF2LCA.txt\" \\\n        \"raw/${meta.assembler}-${meta.id}.concatenated.predicted_proteins.faa\" \\\n        \"raw/${meta.assembler}-${meta.id}.concatenated.predicted_proteins.gff\" \\\n        \"raw/${meta.assembler}-${meta.id}.bin2classification.txt\" \\\n        \"${meta.assembler}-${meta.id}.ORF2LCA.names.txt\" \\\n        \"${meta.assembler}-${meta.id}.bin2classification.names.txt\"\n\n    CAT --version | sed \"s/CAT v//; s/(.*//\" > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 45,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    CAT bins -b \"bins/\" -d database/ -t taxonomy/ -n \"${task.cpus}\" -s .fa --top 6 -o \"${meta.assembler}-${meta.id}\" --I_know_what_Im_doing\n    CAT add_names -i \"${meta.assembler}-${meta.id}.ORF2LCA.txt\" -o \"${meta.assembler}-${meta.id}.ORF2LCA.names.txt\" -t taxonomy/\n    CAT add_names -i \"${meta.assembler}-${meta.id}.bin2classification.txt\" -o \"${meta.assembler}-${meta.id}.bin2classification.names.txt\" -t taxonomy/\n\n    mkdir raw\n    mv *.ORF2LCA.txt *.predicted_proteins.faa *.predicted_proteins.gff *.log *.bin2classification.txt raw/\n    gzip \"raw/${meta.assembler}-${meta.id}.ORF2LCA.txt\" \\\n        \"raw/${meta.assembler}-${meta.id}.concatenated.predicted_proteins.faa\" \\\n        \"raw/${meta.assembler}-${meta.id}.concatenated.predicted_proteins.gff\" \\\n        \"raw/${meta.assembler}-${meta.id}.bin2classification.txt\" \\\n        \"${meta.assembler}-${meta.id}.ORF2LCA.names.txt\" \\\n        \"${meta.assembler}-${meta.id}.bin2classification.names.txt\"\n\n    CAT --version | sed \"s/CAT v//; s/(.*//\" > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [
            "CATO"
        ],
        "tools_url": [
            "https://bio.tools/cato"
        ],
        "tools_dico": [
            {
                "name": "CATO",
                "uri": "https://bio.tools/cato",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0199",
                            "term": "Genetic variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0199",
                            "term": "DNA variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2403",
                                    "term": "Sequence analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2403",
                                    "term": "Sequence analysis (general)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "CATO (Clone Alignment Tool) allows a user to align, evaluate, edit, and select clone sequences based on comparisons to reference sequences.",
                "homepage": "https://sourceforge.net/projects/cato-clone-alignment-tool/"
            }
        ],
        "inputs": [
            "meta",
            "db_name"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${meta.assembler}-${meta.id}-${db_name}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['assembler']) }",
            "conda (params.enable_conda ? \"bioconda::cat=4.6 bioconda::diamond=2.0.6\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/mulled-v2-75e2a26f10cbf3629edf2d1600db3fed5ebe6e04:eae321284604f7dabbdf121e3070bda907b91266-0\" } else { container \"quay.io/biocontainers/mulled-v2-75e2a26f10cbf3629edf2d1600db3fed5ebe6e04:eae321284604f7dabbdf121e3070bda907b91266-0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "GTDBTK_DB_PREPARATION": {
        "name_process": "GTDBTK_DB_PREPARATION",
        "string_process": "\nprocess GTDBTK_DB_PREPARATION {\n    tag \"${database}\"\n\n    conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\"\n    } else {\n        container \"biocontainers/biocontainers:v1.2.0_cv1\"\n    }\n\n    input:\n    path(database)\n\n    output:\n    tuple val(\"${database.toString().replace(\".tar.gz\", \"\")}\"), path(\"database/*\")\n\n    script:\n    \"\"\"\n    mkdir database\n    tar -xzf ${database} -C database --strip 1\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "    \"\"\"\n    mkdir database\n    tar -xzf ${database} -C database --strip 1\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "database"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${database}\"",
            "conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\" } else { container \"biocontainers/biocontainers:v1.2.0_cv1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "QUAST": {
        "name_process": "QUAST",
        "string_process": "\nprocess QUAST {\n    tag \"${meta.assembler}-${meta.id}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['assembler', 'id']) }\n\n    conda (params.enable_conda ? \"bioconda::quast=5.0.2\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/quast:5.0.2--py37pl526hb5aa323_2\"\n    } else {\n        container \"quay.io/biocontainers/quast:5.0.2--py37pl526hb5aa323_2\"\n    }\n\n    input:\n    tuple val(meta), path(assembly)\n\n    output:\n    path \"QUAST/*\" , emit: qc\n    path '*.version.txt'   , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    metaquast.py --threads \"${task.cpus}\" --rna-finding --max-ref-number 0 -l \"${meta.assembler}-${meta.id}\" \"${assembly}\" -o \"QUAST\"\n    metaquast.py --version | sed \"s/QUAST v//; s/ (MetaQUAST mode)//\" > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    metaquast.py --threads \"${task.cpus}\" --rna-finding --max-ref-number 0 -l \"${meta.assembler}-${meta.id}\" \"${assembly}\" -o \"QUAST\"\n    metaquast.py --version | sed \"s/QUAST v//; s/ (MetaQUAST mode)//\" > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "assembly"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${meta.assembler}-${meta.id}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['assembler', 'id']) }",
            "conda (params.enable_conda ? \"bioconda::quast=5.0.2\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/quast:5.0.2--py37pl526hb5aa323_2\" } else { container \"quay.io/biocontainers/quast:5.0.2--py37pl526hb5aa323_2\" }"
        ],
        "when": "",
        "stub": ""
    },
    "MAG_DEPTHS": {
        "name_process": "MAG_DEPTHS",
        "string_process": "\nprocess MAG_DEPTHS {\n    tag \"${meta.assembler}-${meta.id}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['assembler', 'id']) }\n\n                                                                                                                            \n    conda (params.enable_conda ? \"bioconda::metabat2=2.15 conda-forge::python=3.6.7 conda-forge::biopython=1.74 conda-forge::pandas=1.1.5\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-e25d1fa2bb6cbacd47a4f8b2308bd01ba38c5dd7:75310f02364a762e6ba5206fcd11d7529534ed6e-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-e25d1fa2bb6cbacd47a4f8b2308bd01ba38c5dd7:75310f02364a762e6ba5206fcd11d7529534ed6e-0\"\n    }\n\n    input:\n    tuple val(meta), path(bins)\n    path(contig_depths)\n\n    output:\n    tuple val(meta), path(\"${meta.assembler}-${meta.id}-binDepths.tsv\"), emit: depths\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    get_mag_depths.py --bins ${bins} \\\n                    --depths ${contig_depths} \\\n                    --assembly_name \"${meta.assembler}-${meta.id}\" \\\n                    --out \"${meta.assembler}-${meta.id}-binDepths.tsv\"\n    \"\"\"\n}",
        "nb_lignes_process": 30,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    get_mag_depths.py --bins ${bins} \\\n                    --depths ${contig_depths} \\\n                    --assembly_name \"${meta.assembler}-${meta.id}\" \\\n                    --out \"${meta.assembler}-${meta.id}-binDepths.tsv\"\n    \"\"\"",
        "nb_lignes_script": 6,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "bins",
            "contig_depths"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${meta.assembler}-${meta.id}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['assembler', 'id']) }",
            "conda (params.enable_conda ? \"bioconda::metabat2=2.15 conda-forge::python=3.6.7 conda-forge::biopython=1.74 conda-forge::pandas=1.1.5\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/mulled-v2-e25d1fa2bb6cbacd47a4f8b2308bd01ba38c5dd7:75310f02364a762e6ba5206fcd11d7529534ed6e-0\" } else { container \"quay.io/biocontainers/mulled-v2-e25d1fa2bb6cbacd47a4f8b2308bd01ba38c5dd7:75310f02364a762e6ba5206fcd11d7529534ed6e-0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "NANOLYSE": {
        "name_process": "NANOLYSE",
        "string_process": "\nprocess NANOLYSE {\n    tag \"$meta.id\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::nanolyse=1.1.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/nanolyse:1.1.0--py36_1\"\n    } else {\n        container \"quay.io/biocontainers/nanolyse:1.1.0--py36_1\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    path nanolyse_db\n\n    output:\n    tuple val(meta), path(\"${meta.id}_nanolyse.fastq.gz\"), emit: reads\n    path  \"${meta.id}_nanolyse.log\"                      , emit: log\n    path '*.version.txt'                                 , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    cat ${reads} | NanoLyse --reference $nanolyse_db | gzip > ${meta.id}_nanolyse.fastq.gz\n    echo \"NanoLyse reference: $params.lambda_reference\" >${meta.id}_nanolyse.log\n    cat ${reads} | echo \"total reads before NanoLyse: \\$((`wc -l`/4))\" >>${meta.id}_nanolyse.log\n    gunzip -c ${meta.id}_nanolyse.fastq.gz | echo \"total reads after NanoLyse: \\$((`wc -l`/4))\" >> ${meta.id}_nanolyse.log\n\n    NanoLyse --version | sed -e \"s/NanoLyse //g\" > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    cat ${reads} | NanoLyse --reference $nanolyse_db | gzip > ${meta.id}_nanolyse.fastq.gz\n    echo \"NanoLyse reference: $params.lambda_reference\" >${meta.id}_nanolyse.log\n    cat ${reads} | echo \"total reads before NanoLyse: \\$((`wc -l`/4))\" >>${meta.id}_nanolyse.log\n    gunzip -c ${meta.id}_nanolyse.fastq.gz | echo \"total reads after NanoLyse: \\$((`wc -l`/4))\" >> ${meta.id}_nanolyse.log\n\n    NanoLyse --version | sed -e \"s/NanoLyse //g\" > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads",
            "nanolyse_db"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"$meta.id\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? \"bioconda::nanolyse=1.1.0\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/nanolyse:1.1.0--py36_1\" } else { container \"quay.io/biocontainers/nanolyse:1.1.0--py36_1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "KRAKEN2": {
        "name_process": "KRAKEN2",
        "string_process": "\nprocess KRAKEN2 {\n    tag \"${meta.id}-${db_name}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::kraken2=2.0.8_beta\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/kraken2:2.0.8_beta--pl526hc9558a2_2\"\n    } else {\n        container \"quay.io/biocontainers/kraken2:2.0.8_beta--pl526hc9558a2_2\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(db_name), path(\"database/*\")\n\n    output:\n    tuple val(\"kraken2\"), val(meta), path(\"results.krona\"), emit: results_for_krona\n    path  \"kraken2_report.txt\"                            , emit: report\n    path  '*.version.txt'                                 , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def input = meta.single_end ? \"\\\"${reads}\\\"\" :  \"--paired \\\"${reads[0]}\\\" \\\"${reads[1]}\\\"\"\n    \"\"\"\n    kraken2 \\\n        --report-zero-counts \\\n        --threads ${task.cpus} \\\n        --db database \\\n        --report kraken2_report.txt \\\n        $input \\\n        > kraken2.kraken\n    cat kraken2.kraken | cut -f 2,3 > results.krona\n\n    echo \\$(kraken2 --version 2>&1) | sed 's/Kraken version //; s/ Copyright.*//' > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "    def software = getSoftwareName(task.process)\n    def input = meta.single_end ? \"\\\"${reads}\\\"\" :  \"--paired \\\"${reads[0]}\\\" \\\"${reads[1]}\\\"\"\n    \"\"\"\n    kraken2 \\\n        --report-zero-counts \\\n        --threads ${task.cpus} \\\n        --db database \\\n        --report kraken2_report.txt \\\n        $input \\\n        > kraken2.kraken\n    cat kraken2.kraken | cut -f 2,3 > results.krona\n\n    echo \\$(kraken2 --version 2>&1) | sed 's/Kraken version //; s/ Copyright.*//' > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [
            "kraken2"
        ],
        "tools_url": [
            "https://bio.tools/kraken2"
        ],
        "tools_dico": [
            {
                "name": "kraken2",
                "uri": "https://bio.tools/kraken2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0637",
                            "term": "Taxonomy"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomic classification"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3460",
                                    "term": "Taxonomy assignment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_3494",
                                "term": "DNA sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_3028",
                                "term": "Taxonomy"
                            }
                        ]
                    }
                ],
                "description": "Kraken 2 is the newest version of Kraken, a taxonomic classification system using exact k-mer matches to achieve high accuracy and fast classification speeds. This classifier matches each k-mer within a query sequence to the lowest common ancestor (LCA) of all genomes containing the given k-mer. The k-mer assignments inform the classification algorithm.",
                "homepage": "https://ccb.jhu.edu/software/kraken2/"
            }
        ],
        "inputs": [
            "meta",
            "reads",
            "db_name"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${meta.id}-${db_name}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? \"bioconda::kraken2=2.0.8_beta\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/kraken2:2.0.8_beta--pl526hc9558a2_2\" } else { container \"quay.io/biocontainers/kraken2:2.0.8_beta--pl526hc9558a2_2\" }"
        ],
        "when": "",
        "stub": ""
    },
    "SPADESHYBRID": {
        "name_process": "SPADESHYBRID",
        "string_process": "\nprocess SPADESHYBRID {\n    tag \"$meta.id\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::spades=3.15.3\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/spades:3.15.3--h95f258a_0\"\n    } else {\n        container \"quay.io/biocontainers/spades:3.15.3--h95f258a_0\"\n    }\n\n    input:\n    tuple val(meta), path(long_reads), path(short_reads)\n\n    output:\n    tuple val(meta), path(\"${meta.id}_scaffolds.fasta\"), emit: assembly\n    path \"${meta.id}.log\"                              , emit: log\n    path \"${meta.id}_contigs.fasta.gz\"                 , emit: contigs_gz\n    path \"${meta.id}_scaffolds.fasta.gz\"               , emit: assembly_gz\n    path \"${meta.id}_graph.gfa.gz\"                     , emit: graph\n    path '*.version.txt'                               , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    maxmem = task.memory.toGiga()\n    if ( params.spadeshybrid_fix_cpus == -1 || task.cpus == params.spadeshybrid_fix_cpus )\n        \"\"\"\n        metaspades.py \\\n            ${params.spades_options} \\\n            --threads \"${task.cpus}\" \\\n            --memory $maxmem \\\n            --pe1-1 ${short_reads[0]} \\\n            --pe1-2 ${short_reads[1]} \\\n            --nanopore ${long_reads} \\\n            -o spades\n        mv spades/assembly_graph_with_scaffolds.gfa ${meta.id}_graph.gfa\n        mv spades/scaffolds.fasta ${meta.id}_scaffolds.fasta\n        mv spades/contigs.fasta ${meta.id}_contigs.fasta\n        mv spades/spades.log ${meta.id}.log\n        gzip \"${meta.id}_contigs.fasta\"\n        gzip \"${meta.id}_graph.gfa\"\n        gzip -c \"${meta.id}_scaffolds.fasta\" > \"${meta.id}_scaffolds.fasta.gz\"\n\n        metaspades.py --version | sed \"s/SPAdes v//; s/ \\\\[.*//\" > ${software}.version.txt\n        \"\"\"\n    else\n        error \"ERROR: '--spadeshybrid_fix_cpus' was specified, but not succesfully applied. Likely this is caused by changed process properties in a custom config file.\"\n}",
        "nb_lignes_process": 50,
        "string_script": "    def software = getSoftwareName(task.process)\n    maxmem = task.memory.toGiga()\n    if ( params.spadeshybrid_fix_cpus == -1 || task.cpus == params.spadeshybrid_fix_cpus )\n        \"\"\"\n        metaspades.py \\\n            ${params.spades_options} \\\n            --threads \"${task.cpus}\" \\\n            --memory $maxmem \\\n            --pe1-1 ${short_reads[0]} \\\n            --pe1-2 ${short_reads[1]} \\\n            --nanopore ${long_reads} \\\n            -o spades\n        mv spades/assembly_graph_with_scaffolds.gfa ${meta.id}_graph.gfa\n        mv spades/scaffolds.fasta ${meta.id}_scaffolds.fasta\n        mv spades/contigs.fasta ${meta.id}_contigs.fasta\n        mv spades/spades.log ${meta.id}.log\n        gzip \"${meta.id}_contigs.fasta\"\n        gzip \"${meta.id}_graph.gfa\"\n        gzip -c \"${meta.id}_scaffolds.fasta\" > \"${meta.id}_scaffolds.fasta.gz\"\n\n        metaspades.py --version | sed \"s/SPAdes v//; s/ \\\\[.*//\" > ${software}.version.txt\n        \"\"\"\n    else\n        error \"ERROR: '--spadeshybrid_fix_cpus' was specified, but not succesfully applied. Likely this is caused by changed process properties in a custom config file.\"",
        "nb_lignes_script": 23,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "long_reads",
            "short_reads"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"$meta.id\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? \"bioconda::spades=3.15.3\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/spades:3.15.3--h95f258a_0\" } else { container \"quay.io/biocontainers/spades:3.15.3--h95f258a_0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "PROKKA": {
        "name_process": "PROKKA",
        "string_process": "\nprocess PROKKA {\n    tag \"$meta.id\"\n    label 'process_low'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::prokka=1.14.6\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0\"\n    } else {\n        container \"quay.io/biocontainers/prokka:1.14.6--pl526_0\"\n    }\n\n    input:\n    tuple val(meta), path(fasta)\n    path proteins\n    path prodigal_tf\n\n    output:\n    tuple val(meta), path(\"${prefix}/*.gff\"), emit: gff\n    tuple val(meta), path(\"${prefix}/*.gbk\"), emit: gbk\n    tuple val(meta), path(\"${prefix}/*.fna\"), emit: fna\n    tuple val(meta), path(\"${prefix}/*.faa\"), emit: faa\n    tuple val(meta), path(\"${prefix}/*.ffn\"), emit: ffn\n    tuple val(meta), path(\"${prefix}/*.sqn\"), emit: sqn\n    tuple val(meta), path(\"${prefix}/*.fsa\"), emit: fsa\n    tuple val(meta), path(\"${prefix}/*.tbl\"), emit: tbl\n    tuple val(meta), path(\"${prefix}/*.err\"), emit: err\n    tuple val(meta), path(\"${prefix}/*.log\"), emit: log\n    tuple val(meta), path(\"${prefix}/*.txt\"), emit: txt\n    tuple val(meta), path(\"${prefix}/*.tsv\"), emit: tsv\n    path \"versions.yml\" , emit: versions\n\n    script:\n    prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    \"\"\"\n    prokka \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 52,
        "string_script": "    prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    def proteins_opt = proteins ? \"--proteins ${proteins[0]}\" : \"\"\n    def prodigal_opt = prodigal_tf ? \"--prodigaltf ${prodigal_tf[0]}\" : \"\"\n    \"\"\"\n    prokka \\\\\n        $options.args \\\\\n        --cpus $task.cpus \\\\\n        --prefix $prefix \\\\\n        $proteins_opt \\\\\n        $prodigal_tf \\\\\n        $fasta\n\n    cat <<-END_VERSIONS > versions.yml\n    ${getProcessName(task.process)}:\n        ${getSoftwareName(task.process)}: \\$(echo \\$(prokka --version 2>&1) | sed 's/^.*prokka //')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [
            "Prokka"
        ],
        "tools_url": [
            "https://bio.tools/prokka"
        ],
        "tools_dico": [
            {
                "name": "Prokka",
                "uri": "https://bio.tools/prokka",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Model organisms"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0622",
                            "term": "Genomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0781",
                            "term": "Virology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0621",
                            "term": "Organisms"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "Coding region prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0362",
                                    "term": "Genome annotation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "ORF prediction"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0436",
                                    "term": "ORF finding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene finding"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2454",
                                    "term": "Gene calling"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Software tool to annotate bacterial, archaeal and viral genomes quickly and produce standards-compliant output files.",
                "homepage": "https://github.com/tseemann/prokka"
            }
        ],
        "inputs": [
            "meta",
            "fasta",
            "proteins",
            "prodigal_tf"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_low'",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? \"bioconda::prokka=1.14.6\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/prokka:1.14.6--pl526_0\" } else { container \"quay.io/biocontainers/prokka:1.14.6--pl526_0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "BUSCO_SAVE_DOWNLOAD": {
        "name_process": "BUSCO_SAVE_DOWNLOAD",
        "string_process": "\nprocess BUSCO_SAVE_DOWNLOAD {\n                                                                                             \n    maxForks 1\n\n                                                                                    \n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        overwrite: false,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\"\n    } else {\n        container \"biocontainers/biocontainers:v1.2.0_cv1\"\n    }\n\n    input:\n    path(busco_downloads)\n\n    output:\n    path('busco_downloads/**', includeInputs: true)\n\n    script:\n    \"\"\"\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "    \"\"\"\n    \"\"\"",
        "nb_lignes_script": 1,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "busco_downloads"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "maxForks 1",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , overwrite: false , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }",
            "conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\" } else { container \"biocontainers/biocontainers:v1.2.0_cv1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "CENTRIFUGE": {
        "name_process": "CENTRIFUGE",
        "string_process": "\nprocess CENTRIFUGE {\n    tag \"${meta.id}-${db_name}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::centrifuge=1.0.4_beta\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/centrifuge:1.0.4_beta--he513fc3_5\"\n    } else {\n        container \"quay.io/biocontainers/centrifuge:1.0.4_beta--he513fc3_5\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n    tuple val(db_name), path(db)\n\n    output:\n    tuple val(\"centrifuge\"), val(meta), path(\"results.krona\"), emit: results_for_krona\n    path \"report.txt\"                                        , emit: report\n    path \"kreport.txt\"                                       , emit: kreport\n    path '*.version.txt'                                     , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def input = meta.single_end ? \"-U \\\"${reads}\\\"\" :  \"-1 \\\"${reads[0]}\\\" -2 \\\"${reads[1]}\\\"\"\n    \"\"\"\n    centrifuge -x \"${db_name}\" \\\n        -p ${task.cpus} \\\n        --report-file report.txt \\\n        -S results.txt \\\n        $input\n    centrifuge-kreport -x \"${db_name}\" results.txt > kreport.txt\n    cat results.txt | cut -f 1,3 > results.krona\n\n    centrifuge --version | sed -n 1p | sed 's/^.*centrifuge-class version //' > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "    def software = getSoftwareName(task.process)\n    def input = meta.single_end ? \"-U \\\"${reads}\\\"\" :  \"-1 \\\"${reads[0]}\\\" -2 \\\"${reads[1]}\\\"\"\n    \"\"\"\n    centrifuge -x \"${db_name}\" \\\n        -p ${task.cpus} \\\n        --report-file report.txt \\\n        -S results.txt \\\n        $input\n    centrifuge-kreport -x \"${db_name}\" results.txt > kreport.txt\n    cat results.txt | cut -f 1,3 > results.krona\n\n    centrifuge --version | sed -n 1p | sed 's/^.*centrifuge-class version //' > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 12,
        "language_script": "bash",
        "tools": [
            "Centrifuge"
        ],
        "tools_url": [
            "https://bio.tools/centrifuge"
        ],
        "tools_dico": [
            {
                "name": "Centrifuge",
                "uri": "https://bio.tools/centrifuge",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3301",
                            "term": "Microbiology"
                        }
                    ],
                    []
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Nucleic acid sequence analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2478",
                                    "term": "Sequence analysis (nucleic acid)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "A very rapid and memory-efficient system for the classification of DNA sequences from microbial samples. The system uses a novel indexing scheme based on the Burrows-Wheeler transform and the Ferragina-Manzini index, optimized specifically for the metagenomic classification problem. Together these advances enable timely and accurate analysis of large metagenomics data sets on conventional desktop computers.",
                "homepage": "https://ccb.jhu.edu/software/centrifuge/"
            }
        ],
        "inputs": [
            "meta",
            "reads",
            "db_name",
            "db"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${meta.id}-${db_name}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? \"bioconda::centrifuge=1.0.4_beta\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/centrifuge:1.0.4_beta--he513fc3_5\" } else { container \"quay.io/biocontainers/centrifuge:1.0.4_beta--he513fc3_5\" }"
        ],
        "when": "",
        "stub": ""
    },
    "METABAT2": {
        "name_process": "METABAT2",
        "string_process": "\nprocess METABAT2 {\n    tag \"${meta.assembler}-${meta.id}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['assembler']) }\n\n    conda (params.enable_conda ? \"bioconda::metabat2=2.15 conda-forge::python=3.6.7 conda-forge::biopython=1.74 conda-forge::pandas=1.1.5\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-e25d1fa2bb6cbacd47a4f8b2308bd01ba38c5dd7:75310f02364a762e6ba5206fcd11d7529534ed6e-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-e25d1fa2bb6cbacd47a4f8b2308bd01ba38c5dd7:75310f02364a762e6ba5206fcd11d7529534ed6e-0\"\n    }\n\n    input:\n    tuple val(meta), path(assembly), path(bam), path(bai)\n\n    output:\n    tuple val(meta), path(\"MetaBAT2/*.fa\")            , emit: bins\n    path \"${meta.assembler}-${meta.id}-depth.txt.gz\"  , emit: depths\n    path \"MetaBAT2/discarded/*\"                       , emit: discarded\n    path '*.version.txt'                              , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    OMP_NUM_THREADS=${task.cpus} jgi_summarize_bam_contig_depths --outputDepth depth.txt ${bam}\n    metabat2 -t \"${task.cpus}\" -i \"${assembly}\" -a depth.txt -o \"MetaBAT2/${meta.assembler}-${meta.id}\" -m ${params.min_contig_size} --unbinned --seed ${params.metabat_rng_seed}\n\n    gzip depth.txt\n    mv depth.txt.gz \"${meta.assembler}-${meta.id}-depth.txt.gz\"\n\n    # save unbinned contigs above thresholds into individual files, dump others in one file\n    split_fasta.py \"MetaBAT2/${meta.assembler}-${meta.id}.unbinned.fa\" ${params.min_length_unbinned_contigs} ${params.max_unbinned_contigs} ${params.min_contig_size}\n\n    # delete splitted file so that it doesnt end up in following processes\n    rm \"MetaBAT2/${meta.assembler}-${meta.id}.unbinned.fa\"\n\n    mkdir MetaBAT2/discarded\n    gzip \"MetaBAT2/${meta.assembler}-${meta.id}.lowDepth.fa\" \\\n        \"MetaBAT2/${meta.assembler}-${meta.id}.tooShort.fa\" \\\n        \"MetaBAT2/${meta.assembler}-${meta.id}.unbinned.pooled.fa\" \\\n        \"MetaBAT2/${meta.assembler}-${meta.id}.unbinned.remaining.fa\"\n    mv \"MetaBAT2/${meta.assembler}-${meta.id}\".*.fa.gz MetaBAT2/discarded/\n\n    echo \\$(metabat2 --help 2>&1) | sed \"s/^.*version 2\\\\://; s/ (Bioconda.*//\" > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 47,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    OMP_NUM_THREADS=${task.cpus} jgi_summarize_bam_contig_depths --outputDepth depth.txt ${bam}\n    metabat2 -t \"${task.cpus}\" -i \"${assembly}\" -a depth.txt -o \"MetaBAT2/${meta.assembler}-${meta.id}\" -m ${params.min_contig_size} --unbinned --seed ${params.metabat_rng_seed}\n\n    gzip depth.txt\n    mv depth.txt.gz \"${meta.assembler}-${meta.id}-depth.txt.gz\"\n\n    # save unbinned contigs above thresholds into individual files, dump others in one file\n    split_fasta.py \"MetaBAT2/${meta.assembler}-${meta.id}.unbinned.fa\" ${params.min_length_unbinned_contigs} ${params.max_unbinned_contigs} ${params.min_contig_size}\n\n    # delete splitted file so that it doesnt end up in following processes\n    rm \"MetaBAT2/${meta.assembler}-${meta.id}.unbinned.fa\"\n\n    mkdir MetaBAT2/discarded\n    gzip \"MetaBAT2/${meta.assembler}-${meta.id}.lowDepth.fa\" \\\n        \"MetaBAT2/${meta.assembler}-${meta.id}.tooShort.fa\" \\\n        \"MetaBAT2/${meta.assembler}-${meta.id}.unbinned.pooled.fa\" \\\n        \"MetaBAT2/${meta.assembler}-${meta.id}.unbinned.remaining.fa\"\n    mv \"MetaBAT2/${meta.assembler}-${meta.id}\".*.fa.gz MetaBAT2/discarded/\n\n    echo \\$(metabat2 --help 2>&1) | sed \"s/^.*version 2\\\\://; s/ (Bioconda.*//\" > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "assembly",
            "bam",
            "bai"
        ],
        "nb_inputs": 4,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${meta.assembler}-${meta.id}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['assembler']) }",
            "conda (params.enable_conda ? \"bioconda::metabat2=2.15 conda-forge::python=3.6.7 conda-forge::biopython=1.74 conda-forge::pandas=1.1.5\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/mulled-v2-e25d1fa2bb6cbacd47a4f8b2308bd01ba38c5dd7:75310f02364a762e6ba5206fcd11d7529534ed6e-0\" } else { container \"quay.io/biocontainers/mulled-v2-e25d1fa2bb6cbacd47a4f8b2308bd01ba38c5dd7:75310f02364a762e6ba5206fcd11d7529534ed6e-0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "BUSCO_DB_PREPARATION": {
        "name_process": "BUSCO_DB_PREPARATION",
        "string_process": "\nprocess BUSCO_DB_PREPARATION {\n    tag \"${database.baseName}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> params.save_busco_reference ? saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) : null }\n\n    conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\"\n    } else {\n        container \"biocontainers/biocontainers:v1.2.0_cv1\"\n    }\n\n    input:\n    path database\n\n    output:\n    path \"buscodb/*\", emit: db\n    path database\n\n    script:\n    \"\"\"\n    mkdir buscodb\n    tar -xf ${database} -C buscodb\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "    \"\"\"\n    mkdir buscodb\n    tar -xf ${database} -C buscodb\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "database"
        ],
        "nb_inputs": 1,
        "outputs": [
            "database"
        ],
        "nb_outputs": 1,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${database.baseName}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> params.save_busco_reference ? saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) : null }",
            "conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\" } else { container \"biocontainers/biocontainers:v1.2.0_cv1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "QUAST_BINS": {
        "name_process": "QUAST_BINS",
        "string_process": "\nprocess QUAST_BINS {\n    tag \"${meta.assembler}-${meta.id}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['assembler', 'id']) }\n\n    conda (params.enable_conda ? \"bioconda::quast=5.0.2\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/quast:5.0.2--py37pl526hb5aa323_2\"\n    } else {\n        container \"quay.io/biocontainers/quast:5.0.2--py37pl526hb5aa323_2\"\n    }\n\n    input:\n    tuple val(meta), path(bins)\n\n    output:\n    path \"QUAST/*\", type: 'dir'\n    path \"QUAST/*-quast_summary.tsv\", emit: quast_bin_summaries\n    path '*.version.txt'            , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    BINS=\\$(echo \\\"$bins\\\" | sed 's/[][]//g')\n    IFS=', ' read -r -a bins <<< \\\"\\$BINS\\\"\n    for bin in \\\"\\${bins[@]}\\\"; do\n        metaquast.py --threads \"${task.cpus}\" --max-ref-number 0 --rna-finding --gene-finding -l \"\\${bin}\" \"\\${bin}\" -o \"QUAST/\\${bin}\"\n        if ! [ -f \"QUAST/${meta.assembler}-${meta.id}-quast_summary.tsv\" ]; then\n            cp \"QUAST/\\${bin}/transposed_report.tsv\" \"QUAST/${meta.assembler}-${meta.id}-quast_summary.tsv\"\n        else\n            tail -n +2 \"QUAST/\\${bin}/transposed_report.tsv\" >> \"QUAST/${meta.assembler}-${meta.id}-quast_summary.tsv\"\n        fi\n    done\n\n    metaquast.py --version | sed \"s/QUAST v//; s/ (MetaQUAST mode)//\" > ${software}_bins.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    BINS=\\$(echo \\\"$bins\\\" | sed 's/[][]//g')\n    IFS=', ' read -r -a bins <<< \\\"\\$BINS\\\"\n    for bin in \\\"\\${bins[@]}\\\"; do\n        metaquast.py --threads \"${task.cpus}\" --max-ref-number 0 --rna-finding --gene-finding -l \"\\${bin}\" \"\\${bin}\" -o \"QUAST/\\${bin}\"\n        if ! [ -f \"QUAST/${meta.assembler}-${meta.id}-quast_summary.tsv\" ]; then\n            cp \"QUAST/\\${bin}/transposed_report.tsv\" \"QUAST/${meta.assembler}-${meta.id}-quast_summary.tsv\"\n        else\n            tail -n +2 \"QUAST/\\${bin}/transposed_report.tsv\" >> \"QUAST/${meta.assembler}-${meta.id}-quast_summary.tsv\"\n        fi\n    done\n\n    metaquast.py --version | sed \"s/QUAST v//; s/ (MetaQUAST mode)//\" > ${software}_bins.version.txt\n    \"\"\"",
        "nb_lignes_script": 14,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "bins"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${meta.assembler}-${meta.id}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['assembler', 'id']) }",
            "conda (params.enable_conda ? \"bioconda::quast=5.0.2\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/quast:5.0.2--py37pl526hb5aa323_2\" } else { container \"quay.io/biocontainers/quast:5.0.2--py37pl526hb5aa323_2\" }"
        ],
        "when": "",
        "stub": ""
    },
    "QUAST_BINS_SUMMARY": {
        "name_process": "QUAST_BINS_SUMMARY",
        "string_process": "\nprocess QUAST_BINS_SUMMARY {\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\"\n    } else {\n        container \"biocontainers/biocontainers:v1.2.0_cv1\"\n    }\n\n    input:\n    path(summaries)\n\n    output:\n    path(\"quast_summary.tsv\"), emit: summary\n\n    script:\n    \"\"\"\n    QUAST_BIN=\\$(echo \\\"$summaries\\\" | sed 's/[][]//g')\n    IFS=', ' read -r -a quast_bin <<< \\\"\\$QUAST_BIN\\\"\n    for quast_file in \\\"\\${quast_bin[@]}\\\"; do\n        if ! [ -f \"quast_summary.tsv\" ]; then\n            cp \"\\${quast_file}\" \"quast_summary.tsv\"\n        else\n            tail -n +2 \"\\${quast_file}\" >> \"quast_summary.tsv\"\n        fi\n    done\n    \"\"\"\n}",
        "nb_lignes_process": 31,
        "string_script": "    \"\"\"\n    QUAST_BIN=\\$(echo \\\"$summaries\\\" | sed 's/[][]//g')\n    IFS=', ' read -r -a quast_bin <<< \\\"\\$QUAST_BIN\\\"\n    for quast_file in \\\"\\${quast_bin[@]}\\\"; do\n        if ! [ -f \"quast_summary.tsv\" ]; then\n            cp \"\\${quast_file}\" \"quast_summary.tsv\"\n        else\n            tail -n +2 \"\\${quast_file}\" >> \"quast_summary.tsv\"\n        fi\n    done\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "summaries"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) }",
            "conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\" } else { container \"biocontainers/biocontainers:v1.2.0_cv1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "PORECHOP": {
        "name_process": "PORECHOP",
        "string_process": "\nprocess PORECHOP {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"bioconda::porechop=0.2.3_seqan2.1.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/porechop:0.2.3_seqan2.1.1--py36h2d50403_3\"\n    } else {\n        container \"quay.io/biocontainers/porechop:0.2.3_seqan2.1.1--py36h2d50403_3\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"${meta.id}_porechop.fastq\")  , emit: reads\n    path '*.version.txt'                                , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    porechop -i ${reads} -t ${task.cpus} -o ${meta.id}_porechop.fastq\n    porechop --version > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 23,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    porechop -i ${reads} -t ${task.cpus} -o ${meta.id}_porechop.fastq\n    porechop --version > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"$meta.id\"",
            "conda (params.enable_conda ? \"bioconda::porechop=0.2.3_seqan2.1.1\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/porechop:0.2.3_seqan2.1.1--py36h2d50403_3\" } else { container \"quay.io/biocontainers/porechop:0.2.3_seqan2.1.1--py36h2d50403_3\" }"
        ],
        "when": "",
        "stub": ""
    },
    "GET_SOFTWARE_VERSIONS": {
        "name_process": "GET_SOFTWARE_VERSIONS",
        "string_process": "\nprocess GET_SOFTWARE_VERSIONS {\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }\n\n    conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/python:3.8.3\"\n    } else {\n        container \"quay.io/biocontainers/python:3.8.3\"\n    }\n\n    cache false\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.tsv\"     , emit: tsv\n    path 'software_versions_mqc.yaml', emit: yaml\n\n    script:                                                                 \n    \"\"\"\n    echo $workflow.manifest.version > pipeline.version.txt\n    echo $workflow.nextflow.version > nextflow.version.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"\n}",
        "nb_lignes_process": 27,
        "string_script": "    \"\"\"\n    echo $workflow.manifest.version > pipeline.version.txt\n    echo $workflow.nextflow.version > nextflow.version.txt\n    scrape_software_versions.py &> software_versions_mqc.yaml\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "versions"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:'pipeline_info', meta:[:], publish_by_meta:[]) }",
            "conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/python:3.8.3\" } else { container \"quay.io/biocontainers/python:3.8.3\" }",
            "cache false"
        ],
        "when": "",
        "stub": ""
    },
    "MEGAHIT": {
        "name_process": "MEGAHIT",
        "string_process": "\nprocess MEGAHIT {\n    tag \"$meta.id\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::megahit=1.2.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/megahit:1.2.9--h2e03b76_1\"\n    } else {\n        container \"quay.io/biocontainers/megahit:1.2.9--h2e03b76_1\"\n    }\n\n    input:\n    tuple val(meta), path(reads1), path(reads2)\n\n    output:\n    tuple val(meta), path(\"MEGAHIT/${meta.id}.contigs.fa\"), emit: assembly\n    path \"MEGAHIT/*.log\"                                  , emit: log\n    path \"MEGAHIT/${meta.id}.contigs.fa.gz\"               , emit: assembly_gz\n    path '*.version.txt'                                  , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def input = params.single_end ? \"-r \\\"\" + reads1.join(\",\") + \"\\\"\" : \"-1 \\\"\" + reads1.join(\",\") + \"\\\" -2 \\\"\" + reads2.join(\",\") + \"\\\"\"\n    mem = task.memory.toBytes()\n    if ( !params.megahit_fix_cpu_1 || task.cpus == 1 )\n        \"\"\"\n        megahit ${params.megahit_options} -t \"${task.cpus}\" -m $mem $input -o MEGAHIT --out-prefix \"${meta.id}\"\n        gzip -c \"MEGAHIT/${meta.id}.contigs.fa\" > \"MEGAHIT/${meta.id}.contigs.fa.gz\"\n\n        megahit --version | sed \"s/MEGAHIT v//\" > ${software}.version.txt\n        \"\"\"\n    else\n        error \"ERROR: '--megahit_fix_cpu_1' was specified, but not succesfully applied. Likely this is caused by changed process properties in a custom config file.\"\n}",
        "nb_lignes_process": 36,
        "string_script": "    def software = getSoftwareName(task.process)\n    def input = params.single_end ? \"-r \\\"\" + reads1.join(\",\") + \"\\\"\" : \"-1 \\\"\" + reads1.join(\",\") + \"\\\" -2 \\\"\" + reads2.join(\",\") + \"\\\"\"\n    mem = task.memory.toBytes()\n    if ( !params.megahit_fix_cpu_1 || task.cpus == 1 )\n        \"\"\"\n        megahit ${params.megahit_options} -t \"${task.cpus}\" -m $mem $input -o MEGAHIT --out-prefix \"${meta.id}\"\n        gzip -c \"MEGAHIT/${meta.id}.contigs.fa\" > \"MEGAHIT/${meta.id}.contigs.fa.gz\"\n\n        megahit --version | sed \"s/MEGAHIT v//\" > ${software}.version.txt\n        \"\"\"\n    else\n        error \"ERROR: '--megahit_fix_cpu_1' was specified, but not succesfully applied. Likely this is caused by changed process properties in a custom config file.\"",
        "nb_lignes_script": 11,
        "language_script": "bash",
        "tools": [
            "MEMo",
            "MEGAHIT"
        ],
        "tools_url": [
            "https://bio.tools/memo_cancer",
            "https://bio.tools/megahit"
        ],
        "tools_dico": [
            {
                "name": "MEMo",
                "uri": "https://bio.tools/memo_cancer",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0602",
                            "term": "Molecular interactions, pathways and networks"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2259",
                            "term": "Systems biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2640",
                            "term": "Oncology"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_2640",
                            "term": "Cancer biology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_2640",
                            "term": "https://en.wikipedia.org/wiki/Oncology"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3439",
                                    "term": "Pathway or network prediction"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Method for identifying mutually exclusive driver networks in cancer. The method identifies networks defined by three properties: first, member genes are recurrently altered via somatic mutation or copy number changes; second, member genes are likely to participate in the same biological pathway or process, as determined from prior pathway and network knowledge; and third, genomic events within the network exhibit a statistically significant level of mutual exclusivity.",
                "homepage": "http://cbio.mskcc.org/tools/memo/"
            },
            {
                "name": "MEGAHIT",
                "uri": "https://bio.tools/megahit",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0610",
                            "term": "Ecology"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3174",
                            "term": "Metagenomics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0196",
                            "term": "Sequence assembly"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genome assembly"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Sequence assembly (genome assembly)"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0525",
                                    "term": "Genomic assembly"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "Single node assembler for large and complex metagenomics NGS reads, such as soil. It makes use of succinct de Bruijn graph to achieve low memory usage, whereas its goal is not to make memory usage as low as possible.",
                "homepage": "https://github.com/voutcn/megahit"
            }
        ],
        "inputs": [
            "meta",
            "reads1",
            "reads2"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"$meta.id\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? \"bioconda::megahit=1.2.9\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/megahit:1.2.9--h2e03b76_1\" } else { container \"quay.io/biocontainers/megahit:1.2.9--h2e03b76_1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "KRONA_DB": {
        "name_process": "KRONA_DB",
        "string_process": "\nprocess KRONA_DB {\n\n    conda (params.enable_conda ? \"bioconda::krona=2.7.1\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/krona:2.7.1--pl526_5\"\n    } else {\n        container \"quay.io/biocontainers/krona:2.7.1--pl526_5\"\n    }\n\n    output:\n    path(\"taxonomy/taxonomy.tab\"), emit: db\n    path '*.version.txt'         , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    \"\"\"\n    ktUpdateTaxonomy.sh taxonomy\n    echo \\$(ktImportTaxonomy 2>&1) | sed 's/^.*KronaTools //; s/ - ktImportTaxonomy.*//' > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 19,
        "string_script": "    def software = getSoftwareName(task.process)\n    \"\"\"\n    ktUpdateTaxonomy.sh taxonomy\n    echo \\$(ktImportTaxonomy 2>&1) | sed 's/^.*KronaTools //; s/ - ktImportTaxonomy.*//' > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 4,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "conda (params.enable_conda ? \"bioconda::krona=2.7.1\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/krona:2.7.1--pl526_5\" } else { container \"quay.io/biocontainers/krona:2.7.1--pl526_5\" }"
        ],
        "when": "",
        "stub": ""
    },
    "KRAKEN2_DB_PREPARATION": {
        "name_process": "KRAKEN2_DB_PREPARATION",
        "string_process": "\nprocess KRAKEN2_DB_PREPARATION {\n    conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\"\n    } else {\n        container \"biocontainers/biocontainers:v1.2.0_cv1\"\n    }\n\n    input:\n    path db\n\n    output:\n    tuple val(\"${db.simpleName}\"), path(\"database/*.k2d\"), emit: db\n\n    script:\n    \"\"\"\n    mkdir db_tmp\n    tar -xf \"${db}\" -C db_tmp\n    mkdir database\n    mv `find db_tmp/ -name \"*.k2d\"` database/\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "    \"\"\"\n    mkdir db_tmp\n    tar -xf \"${db}\" -C db_tmp\n    mkdir database\n    mv `find db_tmp/ -name \"*.k2d\"` database/\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "db"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\" } else { container \"biocontainers/biocontainers:v1.2.0_cv1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "FASTQC": {
        "name_process": "FASTQC",
        "string_process": "\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\"\n    } else {\n        container \"quay.io/biocontainers/fastqc:0.11.9--0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"*.version.txt\"          , emit: version\n\n    script:\n                                                                          \n    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }\n}",
        "nb_lignes_process": 40,
        "string_script": "    def software = getSoftwareName(task.process)\n    def prefix   = options.suffix ? \"${meta.id}${options.suffix}\" : \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $options.args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n        fastqc --version | sed -e \"s/FastQC v//g\" > ${software}.version.txt\n        \"\"\"\n    }",
        "nb_lignes_script": 15,
        "language_script": "bash",
        "tools": [
            "FastQC"
        ],
        "tools_url": [
            "https://bio.tools/fastqc"
        ],
        "tools_dico": [
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            }
        ],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0\" } else { container \"quay.io/biocontainers/fastqc:0.11.9--0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "CENTRIFUGE_DB_PREPARATION": {
        "name_process": "CENTRIFUGE_DB_PREPARATION",
        "string_process": "\nprocess CENTRIFUGE_DB_PREPARATION {\n    conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\"\n    } else {\n        container \"biocontainers/biocontainers:v1.2.0_cv1\"\n    }\n\n    input:\n    path db\n\n    output:\n    tuple val(\"${db.toString().replace(\".tar.gz\", \"\")}\"), path(\"*.cf\"), emit: db\n\n    script:\n    \"\"\"\n    tar -xf \"${db}\"\n    \"\"\"\n}",
        "nb_lignes_process": 18,
        "string_script": "    \"\"\"\n    tar -xf \"${db}\"\n    \"\"\"",
        "nb_lignes_script": 2,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "db"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\" } else { container \"biocontainers/biocontainers:v1.2.0_cv1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "GTDBTK_CLASSIFY": {
        "name_process": "GTDBTK_CLASSIFY",
        "string_process": "\nprocess GTDBTK_CLASSIFY {\n    tag \"${meta.assembler}-${meta.id}\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['assembler', 'id']) }\n\n    conda (params.enable_conda ? \"bioconda::gtdbtk=1.5.0\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/gtdbtk:1.5.0--pyhdfd78af_0\"\n    } else {\n        container \"quay.io/biocontainers/gtdbtk:1.5.0--pyhdfd78af_0\"\n    }\n\n    input:\n    tuple val(meta), path(\"bins/*\")\n    tuple val(db_name), path(\"database/*\")\n\n    output:\n    path \"gtdbtk.${meta.assembler}-${meta.id}.*.summary.tsv\"        , emit: summary\n    path \"gtdbtk.${meta.assembler}-${meta.id}.*.classify.tree.gz\"   , emit: tree\n    path \"gtdbtk.${meta.assembler}-${meta.id}.*.markers_summary.tsv\", emit: markers\n    path \"gtdbtk.${meta.assembler}-${meta.id}.*.msa.fasta.gz\"       , emit: msa\n    path \"gtdbtk.${meta.assembler}-${meta.id}.*.user_msa.fasta\"     , emit: user_msa\n    path \"gtdbtk.${meta.assembler}-${meta.id}.*.filtered.tsv\"       , emit: filtered\n    path \"gtdbtk.${meta.assembler}-${meta.id}.log\"                  , emit: log\n    path \"gtdbtk.${meta.assembler}-${meta.id}.warnings.log\"         , emit: warnings\n    path \"gtdbtk.${meta.assembler}-${meta.id}.failed_genomes.tsv\"   , emit: failed\n    path '*.version.txt'                                            , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def pplacer_scratch = params.gtdbtk_pplacer_scratch ? \"--scratch_dir pplacer_tmp\" : \"\"\n    \"\"\"\n    export GTDBTK_DATA_PATH=\"\\${PWD}/database\"\n    if [ ${pplacer_scratch} != \"\" ] ; then\n        mkdir pplacer_tmp\n    fi\n\n    gtdbtk classify_wf $options.args \\\n                    --genome_dir bins \\\n                    --prefix \"gtdbtk.${meta.assembler}-${meta.id}\" \\\n                    --out_dir \"\\${PWD}\" \\\n                    --cpus ${task.cpus} \\\n                    --pplacer_cpus ${params.gtdbtk_pplacer_cpus} \\\n                    ${pplacer_scratch} \\\n                    --min_perc_aa ${params.gtdbtk_min_perc_aa} \\\n                    --min_af ${params.gtdbtk_min_af}\n\n    gzip \"gtdbtk.${meta.assembler}-${meta.id}\".*.classify.tree \"gtdbtk.${meta.assembler}-${meta.id}\".*.msa.fasta\n    mv gtdbtk.log \"gtdbtk.${meta.assembler}-${meta.id}.log\"\n    mv gtdbtk.warnings.log \"gtdbtk.${meta.assembler}-${meta.id}.warnings.log\"\n    gtdbtk --version | sed \"s/gtdbtk: version //; s/ Copyright.*//\" > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 54,
        "string_script": "    def software = getSoftwareName(task.process)\n    def pplacer_scratch = params.gtdbtk_pplacer_scratch ? \"--scratch_dir pplacer_tmp\" : \"\"\n    \"\"\"\n    export GTDBTK_DATA_PATH=\"\\${PWD}/database\"\n    if [ ${pplacer_scratch} != \"\" ] ; then\n        mkdir pplacer_tmp\n    fi\n\n    gtdbtk classify_wf $options.args \\\n                    --genome_dir bins \\\n                    --prefix \"gtdbtk.${meta.assembler}-${meta.id}\" \\\n                    --out_dir \"\\${PWD}\" \\\n                    --cpus ${task.cpus} \\\n                    --pplacer_cpus ${params.gtdbtk_pplacer_cpus} \\\n                    ${pplacer_scratch} \\\n                    --min_perc_aa ${params.gtdbtk_min_perc_aa} \\\n                    --min_af ${params.gtdbtk_min_af}\n\n    gzip \"gtdbtk.${meta.assembler}-${meta.id}\".*.classify.tree \"gtdbtk.${meta.assembler}-${meta.id}\".*.msa.fasta\n    mv gtdbtk.log \"gtdbtk.${meta.assembler}-${meta.id}.log\"\n    mv gtdbtk.warnings.log \"gtdbtk.${meta.assembler}-${meta.id}.warnings.log\"\n    gtdbtk --version | sed \"s/gtdbtk: version //; s/ Copyright.*//\" > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "db_name"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"${meta.assembler}-${meta.id}\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['assembler', 'id']) }",
            "conda (params.enable_conda ? \"bioconda::gtdbtk=1.5.0\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/gtdbtk:1.5.0--pyhdfd78af_0\" } else { container \"quay.io/biocontainers/gtdbtk:1.5.0--pyhdfd78af_0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "BOWTIE2_REMOVAL_BUILD": {
        "name_process": "BOWTIE2_REMOVAL_BUILD",
        "string_process": "\nprocess BOWTIE2_REMOVAL_BUILD {\n    tag \"$fasta\"\n\n    conda (params.enable_conda ? 'bioconda::bowtie2=2.4.2' : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container 'https://depot.galaxyproject.org/singularity/bowtie2:2.4.2--py38h1c8e9b9_1'\n    } else {\n        container 'quay.io/biocontainers/bowtie2:2.4.2--py38h1c8e9b9_1'\n    }\n\n    input:\n    path fasta\n\n    output:\n    path 'bt2_index_base*', emit: index\n    path '*.version.txt'  , emit: version\n\n    script:\n    def software  = getSoftwareName(task.process)\n    \"\"\"\n    mkdir bowtie\n    bowtie2-build --threads $task.cpus $fasta \"bt2_index_base\"\n    bowtie2 --version > ${software}_removal.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 24,
        "string_script": "    def software  = getSoftwareName(task.process)\n    \"\"\"\n    mkdir bowtie\n    bowtie2-build --threads $task.cpus $fasta \"bt2_index_base\"\n    bowtie2 --version > ${software}_removal.version.txt\n    \"\"\"",
        "nb_lignes_script": 5,
        "language_script": "bash",
        "tools": [
            "Rbowtie2"
        ],
        "tools_url": [
            "https://bio.tools/rbowtie2"
        ],
        "tools_dico": [
            {
                "name": "Rbowtie2",
                "uri": "https://bio.tools/rbowtie2",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence merging"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0232",
                                    "term": "Sequence splicing"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "This package provides an R wrapper of the popular bowtie2 sequencing reads aligner and AdapterRemoval, a convenient tool for rapid adapter trimming, identification, and read merging.",
                "homepage": "http://bioconductor.org/packages/release/bioc/html/Rbowtie2.html"
            }
        ],
        "inputs": [
            "fasta"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"$fasta\"",
            "conda (params.enable_conda ? 'bioconda::bowtie2=2.4.2' : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container 'https://depot.galaxyproject.org/singularity/bowtie2:2.4.2--py38h1c8e9b9_1' } else { container 'quay.io/biocontainers/bowtie2:2.4.2--py38h1c8e9b9_1' }"
        ],
        "when": "",
        "stub": ""
    },
    "NANOPLOT": {
        "name_process": "NANOPLOT",
        "string_process": "\nprocess NANOPLOT {\n    tag \"$meta.id\"\n\n    publishDir \"${params.outdir}\",\n        mode: params.publish_dir_mode,\n        saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }\n\n    conda (params.enable_conda ? \"bioconda::nanoplot=1.26.3\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/nanoplot:1.26.3--py_0\"\n    } else {\n        container \"quay.io/biocontainers/nanoplot:1.26.3--py_0\"\n    }\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    path '*.png'        , emit: png\n    path '*.html'       , emit: html\n    path '*.txt'        , emit: txt\n    path '*.version.txt', emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def prefix = options.suffix ? \"-p ${options.suffix}_\" : ''\n    def title  = options.suffix ? \"${meta.id}_${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    NanoPlot -t ${task.cpus} \\\n            ${prefix} \\\n            --title ${title} \\\n            -c darkblue \\\n            --fastq ${reads}\n    NanoPlot --version | sed -e \"s/NanoPlot //g\" > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 35,
        "string_script": "    def software = getSoftwareName(task.process)\n    def prefix = options.suffix ? \"-p ${options.suffix}_\" : ''\n    def title  = options.suffix ? \"${meta.id}_${options.suffix}\" : \"${meta.id}\"\n    \"\"\"\n    NanoPlot -t ${task.cpus} \\\n            ${prefix} \\\n            --title ${title} \\\n            -c darkblue \\\n            --fastq ${reads}\n    NanoPlot --version | sed -e \"s/NanoPlot //g\" > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 10,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"$meta.id\"",
            "publishDir \"${params.outdir}\" , mode: params.publish_dir_mode , saveAs: { filename -> saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:meta, publish_by_meta:['id']) }",
            "conda (params.enable_conda ? \"bioconda::nanoplot=1.26.3\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/nanoplot:1.26.3--py_0\" } else { container \"quay.io/biocontainers/nanoplot:1.26.3--py_0\" }"
        ],
        "when": "",
        "stub": ""
    },
    "POOL_PAIRED_READS": {
        "name_process": "POOL_PAIRED_READS",
        "string_process": "\nprocess POOL_PAIRED_READS {\n    tag \"$meta.id\"\n\n    conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\"\n    } else {\n        container \"biocontainers/biocontainers:v1.2.0_cv1\"\n    }\n\n    input:\n    tuple val(meta), path(reads1), path(reads2)\n\n    output:\n    tuple val(meta), path(\"pooled_${meta.id}_*.fastq.gz\"), emit: reads\n\n    script:\n    \"\"\"\n    cat ${reads1} > \"pooled_${meta.id}_1.fastq.gz\"\n    cat ${reads2} > \"pooled_${meta.id}_2.fastq.gz\"\n    \"\"\"\n}",
        "nb_lignes_process": 21,
        "string_script": "    \"\"\"\n    cat ${reads1} > \"pooled_${meta.id}_1.fastq.gz\"\n    cat ${reads2} > \"pooled_${meta.id}_2.fastq.gz\"\n    \"\"\"",
        "nb_lignes_script": 3,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "meta",
            "reads1",
            "reads2"
        ],
        "nb_inputs": 3,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "tag \"$meta.id\"",
            "conda (params.enable_conda ? \"conda-forge::sed=4.7\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://containers.biocontainers.pro/s3/SingImgsRepo/biocontainers/v1.2.0_cv1/biocontainers_v1.2.0_cv1.img\" } else { container \"biocontainers/biocontainers:v1.2.0_cv1\" }"
        ],
        "when": "",
        "stub": ""
    },
    "CAT_DB_GENERATE": {
        "name_process": "CAT_DB_GENERATE",
        "string_process": "\nprocess CAT_DB_GENERATE {\n\n    publishDir \"${params.outdir}\",\n        mode: 'move',\n        saveAs: { filename -> params.save_cat_db ? saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) : null }\n\n    conda (params.enable_conda ? \"bioconda::cat=4.6 bioconda::diamond=2.0.6\" : null)\n    if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) {\n        container \"https://depot.galaxyproject.org/singularity/mulled-v2-75e2a26f10cbf3629edf2d1600db3fed5ebe6e04:eae321284604f7dabbdf121e3070bda907b91266-0\"\n    } else {\n        container \"quay.io/biocontainers/mulled-v2-75e2a26f10cbf3629edf2d1600db3fed5ebe6e04:eae321284604f7dabbdf121e3070bda907b91266-0\"\n    }\n\n    output:\n    tuple env(DB_NAME), path(\"database/*\"), path(\"taxonomy/*\"), emit: db\n    path(\"CAT_prepare_*.tar.gz\"), optional:true               , emit: db_tar_gz\n    path '*.version.txt'                                      , emit: version\n\n    script:\n    def software = getSoftwareName(task.process)\n    def save_db = params.save_cat_db ? \"Y\" : \"N\"\n    \"\"\"\n    CAT prepare --fresh\n\n    # get name/date of generated datase\n    out=(*_taxonomy/)\n    [[ \\$out =~ (.*)_taxonomy/ ]];\n    DB_NAME=\"CAT_prepare_\\${BASH_REMATCH[1]}\"\n\n    mv *_taxonomy taxonomy\n    mv *_database database\n    rm database/*.nr.gz\n    if [ ${save_db} = \"Y\" ] ; then\n        tar -cf - taxonomy database | gzip > \"\\${DB_NAME}\".tar.gz\n    fi\n\n    CAT --version | sed \"s/CAT v//; s/(.*//\" > ${software}.version.txt\n    \"\"\"\n}",
        "nb_lignes_process": 38,
        "string_script": "    def software = getSoftwareName(task.process)\n    def save_db = params.save_cat_db ? \"Y\" : \"N\"\n    \"\"\"\n    CAT prepare --fresh\n\n    # get name/date of generated datase\n    out=(*_taxonomy/)\n    [[ \\$out =~ (.*)_taxonomy/ ]];\n    DB_NAME=\"CAT_prepare_\\${BASH_REMATCH[1]}\"\n\n    mv *_taxonomy taxonomy\n    mv *_database database\n    rm database/*.nr.gz\n    if [ ${save_db} = \"Y\" ] ; then\n        tar -cf - taxonomy database | gzip > \"\\${DB_NAME}\".tar.gz\n    fi\n\n    CAT --version | sed \"s/CAT v//; s/(.*//\" > ${software}.version.txt\n    \"\"\"",
        "nb_lignes_script": 18,
        "language_script": "bash",
        "tools": [
            "CATO"
        ],
        "tools_url": [
            "https://bio.tools/cato"
        ],
        "tools_dico": [
            {
                "name": "CATO",
                "uri": "https://bio.tools/cato",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_0199",
                            "term": "Genetic variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_0199",
                            "term": "DNA variation"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2403",
                                    "term": "Sequence analysis"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2403",
                                    "term": "Sequence analysis (general)"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "CATO (Clone Alignment Tool) allows a user to align, evaluate, edit, and select clone sequences based on comparisons to reference sequences.",
                "homepage": "https://sourceforge.net/projects/cato-clone-alignment-tool/"
            }
        ],
        "inputs": [],
        "nb_inputs": 0,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__mag",
        "directive": [
            "publishDir \"${params.outdir}\" , mode: 'move' , saveAs: { filename -> params.save_cat_db ? saveFiles(filename:filename, options:params.options, publish_dir:getSoftwareName(task.process), meta:[:], publish_by_meta:[]) : null }",
            "conda (params.enable_conda ? \"bioconda::cat=4.6 bioconda::diamond=2.0.6\" : null) if (workflow.containerEngine == 'singularity' && !params.singularity_pull_docker_container) { container \"https://depot.galaxyproject.org/singularity/mulled-v2-75e2a26f10cbf3629edf2d1600db3fed5ebe6e04:eae321284604f7dabbdf121e3070bda907b91266-0\" } else { container \"quay.io/biocontainers/mulled-v2-75e2a26f10cbf3629edf2d1600db3fed5ebe6e04:eae321284604f7dabbdf121e3070bda907b91266-0\" }"
        ],
        "when": "",
        "stub": ""
    }
}