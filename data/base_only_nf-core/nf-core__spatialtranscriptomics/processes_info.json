{
    "MULTIQC": {
        "name_process": "MULTIQC",
        "string_process": "process MULTIQC {\n    label 'process_medium'\n\n    conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path multiqc_files\n\n    output:\n    path \"*multiqc_report.html\", emit: report\n    path \"*_data\"              , emit: data\n    path \"*_plots\"             , optional:true, emit: plots\n    path \"versions.yml\"        , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "    def args = task.ext.args ?: ''\n    \"\"\"\n    multiqc -f $args .\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        multiqc: \\$( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 8,
        "language_script": "bash",
        "tools": [
            "MultiQC"
        ],
        "tools_url": [
            "https://bio.tools/multiqc"
        ],
        "tools_dico": [
            {
                "name": "MultiQC",
                "uri": "https://bio.tools/multiqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0091",
                            "term": "Bioinformatics"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2428",
                                    "term": "Validation"
                                }
                            ],
                            []
                        ],
                        "input": [],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2048",
                                "term": "Report"
                            }
                        ]
                    }
                ],
                "description": "MultiQC aggregates results from multiple bioinformatics analyses across many samples into a single report. It searches a given directory for analysis logs and compiles a HTML report. It's a general use tool, perfect for summarising the output from numerous bioinformatics tools.",
                "homepage": "http://multiqc.info/"
            }
        ],
        "inputs": [
            "multiqc_files"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__spatialtranscriptomics",
        "directive": [
            "label 'process_medium'",
            "conda (params.enable_conda ? 'bioconda::multiqc=1.11' : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' : 'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "MITO_LOAD": {
        "name_process": "MITO_LOAD",
        "string_process": " process MITO_LOAD {\n    \n    label \"python_process_low\"\n    \n    input:\n    val sample_id\n    val outdir\n     \n    output:\n    tuple val(sample_id), env(fname)\n              \n    script:\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n        \n    \"\"\"  \n    #!/bin/bash\n    \n    mitoUrl=\"ftp://ftp.broadinstitute.org/distribution/metabolic/papers/Pagliarini/MitoCarta2.0/${sample_info.species}.MitoCarta2.0.txt\"\n\n    fname=${outdir}/`basename \"\\${mitoUrl}\"`\n    echo saving to: \\$fname\n    \n    [ ! -d ${outdir} ] && mkdir ${outdir}\n    \n    if [ ! -f \\$fname ]\n    then\n        wget --quiet \\${mitoUrl} --output-document=\\$fname\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n        \n    \"\"\"  \n    #!/bin/bash\n    \n    mitoUrl=\"ftp://ftp.broadinstitute.org/distribution/metabolic/papers/Pagliarini/MitoCarta2.0/${sample_info.species}.MitoCarta2.0.txt\"\n\n    fname=${outdir}/`basename \"\\${mitoUrl}\"`\n    echo saving to: \\$fname\n    \n    [ ! -d ${outdir} ] && mkdir ${outdir}\n    \n    if [ ! -f \\$fname ]\n    then\n        wget --quiet \\${mitoUrl} --output-document=\\$fname\n    fi\n    \"\"\"",
        "nb_lignes_script": 17,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_id",
            "outdir"
        ],
        "nb_inputs": 2,
        "outputs": [
            "sample_id"
        ],
        "nb_outputs": 1,
        "name_workflow": "nf-core__spatialtranscriptomics",
        "directive": [
            "label \"python_process_low\""
        ],
        "when": "",
        "stub": ""
    },
    "READ_ST_AND_SC_SCANPY": {
        "name_process": "READ_ST_AND_SC_SCANPY",
        "string_process": " process READ_ST_AND_SC_SCANPY {\n \n    label \"python_process_low\"\n    \n    input:\n    tuple val(sample_id), file(state)\n    val outdir\n    \n    output:\n    tuple val(sample_id), env(outpath)\n         \n    script:\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n      \n    \"\"\"\n    #!/bin/bash\n    \n    dname=${outdir}/${sample_id}\n      \n    [ ! -d \\${dname} ] && mkdir \\${dname}\n\n    python $projectDir/bin/script_read_st_data.py --outsPath=${sample_info.st_data_dir} --saveFile=\\${dname}/st_adata_raw.h5ad --countsFile=raw_feature_bc_matrix.h5 --npCountsOutputName=st_adata_counts_in_tissue.npz --minCounts=$params.STload_minCounts --minCells=$params.STload_minCells\n\n    python $projectDir/bin/script_read_sc_data.py --outsPath=${sample_info.sc_data_dir} --saveFile=\\${dname}/sc_adata_raw.h5ad --npCountsOutputName=sc_adata_counts.npz --minCounts=$params.SCload_minCounts --minCells=$params.SCload_minCells --minGenes=$params.SCload_minGenes\n\n    if [[ -s \\${dname}/st_adata_raw.h5ad ]] && \\\n      [[ -s \\${dname}/sc_adata_raw.h5ad ]] && \\\n      [[ -s \\${dname}/st_adata_counts_in_tissue.npz ]] && \\\n      [[ -s \\${dname}/sc_adata_counts.npz ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 36,
        "string_script": "    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n      \n    \"\"\"\n    #!/bin/bash\n    \n    dname=${outdir}/${sample_id}\n      \n    [ ! -d \\${dname} ] && mkdir \\${dname}\n\n    python $projectDir/bin/script_read_st_data.py --outsPath=${sample_info.st_data_dir} --saveFile=\\${dname}/st_adata_raw.h5ad --countsFile=raw_feature_bc_matrix.h5 --npCountsOutputName=st_adata_counts_in_tissue.npz --minCounts=$params.STload_minCounts --minCells=$params.STload_minCells\n\n    python $projectDir/bin/script_read_sc_data.py --outsPath=${sample_info.sc_data_dir} --saveFile=\\${dname}/sc_adata_raw.h5ad --npCountsOutputName=sc_adata_counts.npz --minCounts=$params.SCload_minCounts --minCells=$params.SCload_minCells --minGenes=$params.SCload_minGenes\n\n    if [[ -s \\${dname}/st_adata_raw.h5ad ]] && \\\n      [[ -s \\${dname}/sc_adata_raw.h5ad ]] && \\\n      [[ -s \\${dname}/st_adata_counts_in_tissue.npz ]] && \\\n      [[ -s \\${dname}/sc_adata_counts.npz ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"",
        "nb_lignes_script": 24,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_id",
            "state",
            "outdir"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sample_id"
        ],
        "nb_outputs": 1,
        "name_workflow": "nf-core__spatialtranscriptomics",
        "directive": [
            "label \"python_process_low\""
        ],
        "when": "",
        "stub": ""
    },
    "ST_CALCULATE_SUM_FACTORS": {
        "name_process": "ST_CALCULATE_SUM_FACTORS",
        "string_process": " process ST_CALCULATE_SUM_FACTORS {\n    \n    label \"r_process\"\n    cpus 2\n    memory '8.GB'\n     \n    input:\n    tuple val(sample_id), file(state)\n    val outdir\n    \n    output:\n    tuple val(sample_id), env(outpath)\n    \n    \"\"\"\n    #!/bin/bash\n    \n    dname=${outdir}/${sample_id}\n    \n    Rscript $projectDir/bin/calculateSumFactors.R --filePath=\\${dname}/ --npCountsOutputName=st_adata_counts_in_tissue.npz --npFactorsOutputName=st_adata_counts_in_tissue_factors.npz\n    Rscript $projectDir/bin/calculateSumFactors.R --filePath=\\${dname}/ --npCountsOutputName=sc_adata_counts.npz --npFactorsOutputName=sc_adata_counts_factors.npz\n    \n    if [[ -s \\${dname}/st_adata_counts_in_tissue_factors.npz ]] && \\\n      [[ -s \\${dname}/sc_adata_counts_factors.npz ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi \n    \"\"\"\n}",
        "nb_lignes_process": 29,
        "string_script": "\"\"\"\n    #!/bin/bash\n    \n    dname=${outdir}/${sample_id}\n    \n    Rscript $projectDir/bin/calculateSumFactors.R --filePath=\\${dname}/ --npCountsOutputName=st_adata_counts_in_tissue.npz --npFactorsOutputName=st_adata_counts_in_tissue_factors.npz\n    Rscript $projectDir/bin/calculateSumFactors.R --filePath=\\${dname}/ --npCountsOutputName=sc_adata_counts.npz --npFactorsOutputName=sc_adata_counts_factors.npz\n    \n    if [[ -s \\${dname}/st_adata_counts_in_tissue_factors.npz ]] && \\\n      [[ -s \\${dname}/sc_adata_counts_factors.npz ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi \n    \"\"\"",
        "nb_lignes_script": 16,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_id",
            "state",
            "outdir"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sample_id"
        ],
        "nb_outputs": 1,
        "name_workflow": "nf-core__spatialtranscriptomics",
        "directive": [
            "label \"r_process\"",
            "cpus 2",
            "memory '8.GB'"
        ],
        "when": "",
        "stub": ""
    },
    "ST_PREPROCESS": {
        "name_process": "ST_PREPROCESS",
        "string_process": " process ST_PREPROCESS {\n    \n    label \"python_process\"\n    cpus 2\n    memory '4.GB'\n     \n    input:\n    tuple val(sample_id), file(state)\n    val outdir\n    \n    output:\n    tuple val(sample_id), env(outpath)\n    \n    script:\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n    \n    \"\"\"\n    #!/bin/bash\n    \n    dname=${outdir}/${sample_id}\n    \n    mitoFile=${outdir}/${sample_info.species}.MitoCarta2.0.txt\n    \n    python $projectDir/bin/stPreprocess.py --filePath=\\${dname}/ --npFactorsOutputName=st_adata_counts_in_tissue_factors.npz --rawAdata=st_adata_raw.h5ad --mitoFile=\\$mitoFile --pltFigSize=$params.STpreprocess_pltFigSize --minCounts=$params.STpreprocess_minCounts --minGenes=$params.STpreprocess_minGenes --minCells=$params.STpreprocess_minCells --histplotQCmaxTotalCounts=$params.STpreprocess_histplotQCmaxTotalCounts --histplotQCminGeneCounts=$params.STpreprocess_histplotQCminGeneCounts --histplotQCbins=$params.STpreprocess_histplotQCbins\n\n    if [[ -s \\${dname}/st_adata_norm.h5ad ]] && \\\n      [[ -s \\${dname}/st_adata_X.npz ]] && \\\n      [[ -s \\${dname}/st_adata.var.csv ]] && \\\n      [[ -s \\${dname}/st_adata.obs.csv ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 36,
        "string_script": "    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n    \n    \"\"\"\n    #!/bin/bash\n    \n    dname=${outdir}/${sample_id}\n    \n    mitoFile=${outdir}/${sample_info.species}.MitoCarta2.0.txt\n    \n    python $projectDir/bin/stPreprocess.py --filePath=\\${dname}/ --npFactorsOutputName=st_adata_counts_in_tissue_factors.npz --rawAdata=st_adata_raw.h5ad --mitoFile=\\$mitoFile --pltFigSize=$params.STpreprocess_pltFigSize --minCounts=$params.STpreprocess_minCounts --minGenes=$params.STpreprocess_minGenes --minCells=$params.STpreprocess_minCells --histplotQCmaxTotalCounts=$params.STpreprocess_histplotQCmaxTotalCounts --histplotQCminGeneCounts=$params.STpreprocess_histplotQCminGeneCounts --histplotQCbins=$params.STpreprocess_histplotQCbins\n\n    if [[ -s \\${dname}/st_adata_norm.h5ad ]] && \\\n      [[ -s \\${dname}/st_adata_X.npz ]] && \\\n      [[ -s \\${dname}/st_adata.var.csv ]] && \\\n      [[ -s \\${dname}/st_adata.obs.csv ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_id",
            "state",
            "outdir"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sample_id"
        ],
        "nb_outputs": 1,
        "name_workflow": "nf-core__spatialtranscriptomics",
        "directive": [
            "label \"python_process\"",
            "cpus 2",
            "memory '4.GB'"
        ],
        "when": "",
        "stub": ""
    },
    "SC_PREPROCESS": {
        "name_process": "SC_PREPROCESS",
        "string_process": " process SC_PREPROCESS {\n    \n    label \"python_process\"\n    cpus 2\n    memory '4.GB'\n         \n    input:\n    tuple val(sample_id), file(state)\n    val outdir\n    \n    output:\n    tuple val(sample_id), env(outpath)\n    \n    script:\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n      \n    \"\"\"\n    #!/bin/bash\n    \n    dname=${outdir}/${sample_id}\n    \n    mitoFile=${outdir}/${sample_info.species}.MitoCarta2.0.txt\n    \n    python $projectDir/bin/scPreprocess.py --filePath=\\${dname}/ --npFactorsOutputName=sc_adata_counts_factors.npz --rawAdata=sc_adata_raw.h5ad --mitoFile=\\$mitoFile --pltFigSize=$params.SCpreprocess_pltFigSize --minCounts=$params.SCpreprocess_minCounts --minGenes=$params.SCpreprocess_minGenes --minCells=$params.SCpreprocess_minCells --histplotQCmaxTotalCounts=$params.SCpreprocess_histplotQCmaxTotalCounts --histplotQCminGeneCounts=$params.SCpreprocess_histplotQCminGeneCounts --histplotQCbins=$params.SCpreprocess_histplotQCbins\n\n    if [[ -s \\${dname}/sc_adata_norm.h5ad ]] && \\\n      [[ -s \\${dname}/sc_adata_X.npz ]] && \\\n      [[ -s \\${dname}/sc_adata.var.csv ]] && \\\n      [[ -s \\${dname}/sc_adata.obs.csv ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 36,
        "string_script": "    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n      \n    \"\"\"\n    #!/bin/bash\n    \n    dname=${outdir}/${sample_id}\n    \n    mitoFile=${outdir}/${sample_info.species}.MitoCarta2.0.txt\n    \n    python $projectDir/bin/scPreprocess.py --filePath=\\${dname}/ --npFactorsOutputName=sc_adata_counts_factors.npz --rawAdata=sc_adata_raw.h5ad --mitoFile=\\$mitoFile --pltFigSize=$params.SCpreprocess_pltFigSize --minCounts=$params.SCpreprocess_minCounts --minGenes=$params.SCpreprocess_minGenes --minCells=$params.SCpreprocess_minCells --histplotQCmaxTotalCounts=$params.SCpreprocess_histplotQCmaxTotalCounts --histplotQCminGeneCounts=$params.SCpreprocess_histplotQCminGeneCounts --histplotQCbins=$params.SCpreprocess_histplotQCbins\n\n    if [[ -s \\${dname}/sc_adata_norm.h5ad ]] && \\\n      [[ -s \\${dname}/sc_adata_X.npz ]] && \\\n      [[ -s \\${dname}/sc_adata.var.csv ]] && \\\n      [[ -s \\${dname}/sc_adata.obs.csv ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_id",
            "state",
            "outdir"
        ],
        "nb_inputs": 3,
        "outputs": [
            "sample_id"
        ],
        "nb_outputs": 1,
        "name_workflow": "nf-core__spatialtranscriptomics",
        "directive": [
            "label \"python_process\"",
            "cpus 2",
            "memory '4.GB'"
        ],
        "when": "",
        "stub": ""
    },
    "DECONVOLUTION_WITH_STDECONVOLVE": {
        "name_process": "DECONVOLUTION_WITH_STDECONVOLVE",
        "string_process": " process DECONVOLUTION_WITH_STDECONVOLVE {\n    \n    label \"r_process\"\n     \n    input:\n    val sample_state\n    val outdir\n    \n    output:\n    tuple env(sample_id), env(outpath)\n    \n    script:\n    def sample_id_gr = sample_state[0]\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id_gr)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n        \n    \"\"\"\n    #!/bin/bash\n    \n    sample_id=${sample_id_gr}\n    \n    dname=${outdir}/\\${sample_id}\n       \n    Rscript $projectDir/bin/characterization_STdeconvolve.R --filePath=\\${dname}/ --outsPath=${sample_info.st_data_dir} --mtxGeneColumn=$params.STdeconvolve_mtxGeneColumn --countsFactor=$params.STdeconvolve_countsFactor --corpusRemoveAbove=$params.STdeconvolve_corpusRemoveAbove --corpusRemoveBelow=$params.STdeconvolve_corpusRemoveBelow --LDAminTopics=$params.STdeconvolve_LDAminTopics --LDAmaxTopics=$params.STdeconvolve_LDAmaxTopics --STdeconvolveScatterpiesSize=$params.STdeconvolve_ScatterpiesSize --STdeconvolveFeaturesSizeFactor=$params.STdeconvolve_FeaturesSizeFactor\n    \n    if [[ -s \\${dname}/STdeconvolve_prop_norm.csv ]] && \\\n      [[ -s \\${dname}/STdeconvolve_beta_norm.csv ]] && \\\n      [[ -s \\${dname}/STdeconvolve_sc_cluster_ids.csv ]] && \\\n      [[ -s \\${dname}/STdeconvolve_sc_pca.csv ]] && \\\n      [[ -s \\${dname}/STdeconvolve_sc_pca_feature_loadings.csv ]] && \\\n      [[ -s \\${dname}/STdeconvolve_sc_cluster_markers.csv ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 37,
        "string_script": "    def sample_id_gr = sample_state[0]\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id_gr)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n        \n    \"\"\"\n    #!/bin/bash\n    \n    sample_id=${sample_id_gr}\n    \n    dname=${outdir}/\\${sample_id}\n       \n    Rscript $projectDir/bin/characterization_STdeconvolve.R --filePath=\\${dname}/ --outsPath=${sample_info.st_data_dir} --mtxGeneColumn=$params.STdeconvolve_mtxGeneColumn --countsFactor=$params.STdeconvolve_countsFactor --corpusRemoveAbove=$params.STdeconvolve_corpusRemoveAbove --corpusRemoveBelow=$params.STdeconvolve_corpusRemoveBelow --LDAminTopics=$params.STdeconvolve_LDAminTopics --LDAmaxTopics=$params.STdeconvolve_LDAmaxTopics --STdeconvolveScatterpiesSize=$params.STdeconvolve_ScatterpiesSize --STdeconvolveFeaturesSizeFactor=$params.STdeconvolve_FeaturesSizeFactor\n    \n    if [[ -s \\${dname}/STdeconvolve_prop_norm.csv ]] && \\\n      [[ -s \\${dname}/STdeconvolve_beta_norm.csv ]] && \\\n      [[ -s \\${dname}/STdeconvolve_sc_cluster_ids.csv ]] && \\\n      [[ -s \\${dname}/STdeconvolve_sc_pca.csv ]] && \\\n      [[ -s \\${dname}/STdeconvolve_sc_pca_feature_loadings.csv ]] && \\\n      [[ -s \\${dname}/STdeconvolve_sc_cluster_markers.csv ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"",
        "nb_lignes_script": 25,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_state",
            "outdir"
        ],
        "nb_inputs": 2,
        "outputs": [
            "outpath"
        ],
        "nb_outputs": 1,
        "name_workflow": "nf-core__spatialtranscriptomics",
        "directive": [
            "label \"r_process\""
        ],
        "when": "",
        "stub": ""
    },
    "DECONVOLUTION_WITH_SPOTLIGHT": {
        "name_process": "DECONVOLUTION_WITH_SPOTLIGHT",
        "string_process": " process DECONVOLUTION_WITH_SPOTLIGHT {\n     \n    label \"r_process\"\n    \n    input:\n    val sample_state\n    val outdir\n    \n    output:\n    tuple env(sample_id), env(outpath)\n    \n    script:\n    def sample_id_gr = sample_state[0]\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id_gr)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n        \n    \"\"\"\n    #!/bin/bash\n    \n    sample_id=${sample_id_gr}\n    \n    dname=${outdir}/\\${sample_id}\n        \n    Rscript $projectDir/bin/characterization_SPOTlight.R --filePath=\\${dname}/ --outsPath=${sample_info.st_data_dir} --mtxGeneColumn=$params.SPOTlight_mtxGeneColumn --countsFactor=$params.SPOTlight_countsFactor --clusterResolution=$params.SPOTlight_clusterResolution --numberHVG=$params.SPOTlight_numberHVG --numberCellsPerCelltype=$params.SPOTlight_numberCellsPerCelltype --SPOTlightScatterpiesSize=$params.SPOTlight_ScatterpiesSize \n\n    if [[ -s \\${dname}/SPOTlight_prop_norm.csv ]] && \\\n      [[ -s \\${dname}/SPOTlight_beta_norm.csv ]] && \\\n      [[ -s \\${dname}/SPOTlight_sc_cluster_ids.csv ]] && \\\n      [[ -s \\${dname}/SPOTlight_sc_pca.csv ]] && \\\n      [[ -s \\${dname}/SPOTlight_sc_pca_feature_loadings.csv ]] && \\\n      [[ -s \\${dname}/SPOTlight_sc_cluster_markers.csv ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 37,
        "string_script": "    def sample_id_gr = sample_state[0]\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id_gr)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n        \n    \"\"\"\n    #!/bin/bash\n    \n    sample_id=${sample_id_gr}\n    \n    dname=${outdir}/\\${sample_id}\n        \n    Rscript $projectDir/bin/characterization_SPOTlight.R --filePath=\\${dname}/ --outsPath=${sample_info.st_data_dir} --mtxGeneColumn=$params.SPOTlight_mtxGeneColumn --countsFactor=$params.SPOTlight_countsFactor --clusterResolution=$params.SPOTlight_clusterResolution --numberHVG=$params.SPOTlight_numberHVG --numberCellsPerCelltype=$params.SPOTlight_numberCellsPerCelltype --SPOTlightScatterpiesSize=$params.SPOTlight_ScatterpiesSize \n\n    if [[ -s \\${dname}/SPOTlight_prop_norm.csv ]] && \\\n      [[ -s \\${dname}/SPOTlight_beta_norm.csv ]] && \\\n      [[ -s \\${dname}/SPOTlight_sc_cluster_ids.csv ]] && \\\n      [[ -s \\${dname}/SPOTlight_sc_pca.csv ]] && \\\n      [[ -s \\${dname}/SPOTlight_sc_pca_feature_loadings.csv ]] && \\\n      [[ -s \\${dname}/SPOTlight_sc_cluster_markers.csv ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"",
        "nb_lignes_script": 25,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_state",
            "outdir"
        ],
        "nb_inputs": 2,
        "outputs": [
            "outpath"
        ],
        "nb_outputs": 1,
        "name_workflow": "nf-core__spatialtranscriptomics",
        "directive": [
            "label \"r_process\""
        ],
        "when": "",
        "stub": ""
    },
    "CLUSTERING_WITH_BAYESSPACE": {
        "name_process": "CLUSTERING_WITH_BAYESSPACE",
        "string_process": " process CLUSTERING_WITH_BAYESSPACE {\n    \n    label \"r_process\"\n     \n    input:\n    val sample_state\n    val outdir\n    \n    output:\n    tuple env(sample_id), env(outpath)\n    \n    script:\n    def sample_id_gr = sample_state[0]\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id_gr)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n        \n    \"\"\"\n    #!/bin/bash\n    \n    sample_id=${sample_id_gr}\n    \n    dname=${outdir}/\\${sample_id}\n    \n    Rscript $projectDir/bin/characterization_BayesSpace.R --filePath=\\${dname}/ --numberHVG=$params.BayesSpace_numberHVG --numberPCs=$params.BayesSpace_numberPCs --minClusters=$params.BayesSpace_minClusters --maxClusters=$params.BayesSpace_maxClusters --optimalQ=$params.BayesSpace_optimalQ --STplatform=$params.BayesSpace_STplatform\n\n    if [[ -s \\${dname}/bayes_spot_cluster.csv ]] && \\\n      [[ -s \\${dname}/bayes_subspot_cluster_and_coord.csv ]] && \\\n      [[ -s \\${dname}/bayes_enhanced_markers.csv ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 34,
        "string_script": "    def sample_id_gr = sample_state[0]\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id_gr)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n        \n    \"\"\"\n    #!/bin/bash\n    \n    sample_id=${sample_id_gr}\n    \n    dname=${outdir}/\\${sample_id}\n    \n    Rscript $projectDir/bin/characterization_BayesSpace.R --filePath=\\${dname}/ --numberHVG=$params.BayesSpace_numberHVG --numberPCs=$params.BayesSpace_numberPCs --minClusters=$params.BayesSpace_minClusters --maxClusters=$params.BayesSpace_maxClusters --optimalQ=$params.BayesSpace_optimalQ --STplatform=$params.BayesSpace_STplatform\n\n    if [[ -s \\${dname}/bayes_spot_cluster.csv ]] && \\\n      [[ -s \\${dname}/bayes_subspot_cluster_and_coord.csv ]] && \\\n      [[ -s \\${dname}/bayes_enhanced_markers.csv ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"",
        "nb_lignes_script": 22,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_state",
            "outdir"
        ],
        "nb_inputs": 2,
        "outputs": [
            "outpath"
        ],
        "nb_outputs": 1,
        "name_workflow": "nf-core__spatialtranscriptomics",
        "directive": [
            "label \"r_process\""
        ],
        "when": "",
        "stub": ""
    },
    "ST_SPATIALDE": {
        "name_process": "ST_SPATIALDE",
        "string_process": " process ST_SPATIALDE {\n     \n    label \"python_process\"\n    \n    input:\n    val sample_state\n    val outdir\n    \n    output:\n    tuple env(sample_id), env(outpath)\n    \n    script:\n    def sample_id_gr = sample_state[0]\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id_gr)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n        \n    \"\"\"\n    #!/bin/bash\n    \n    sample_id=${sample_id_gr}\n    \n    dname=${outdir}/\\${sample_id}\n       \n    python $projectDir/bin/stSpatialDE.py --filePath=\\${dname}/ --numberOfColumns=$params.SpatialDE_numberOfColumns\n\n    if [[ -s \\${dname}/stSpatialDE.csv ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 32,
        "string_script": "    def sample_id_gr = sample_state[0]\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id_gr)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n        \n    \"\"\"\n    #!/bin/bash\n    \n    sample_id=${sample_id_gr}\n    \n    dname=${outdir}/\\${sample_id}\n       \n    python $projectDir/bin/stSpatialDE.py --filePath=\\${dname}/ --numberOfColumns=$params.SpatialDE_numberOfColumns\n\n    if [[ -s \\${dname}/stSpatialDE.csv ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"",
        "nb_lignes_script": 20,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_state",
            "outdir"
        ],
        "nb_inputs": 2,
        "outputs": [
            "outpath"
        ],
        "nb_outputs": 1,
        "name_workflow": "nf-core__spatialtranscriptomics",
        "directive": [
            "label \"python_process\""
        ],
        "when": "",
        "stub": ""
    },
    "ST_CLUSTERING": {
        "name_process": "ST_CLUSTERING",
        "string_process": " process ST_CLUSTERING {\n     \n    label \"python_process\"\n    \n    input:\n    val sample_state\n    val outdir\n    \n    output:\n    tuple env(sample_id), env(outpath)\n    \n    script:\n    def sample_id_gr = sample_state[0]\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id_gr)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n        \n    \"\"\"\n    #!/bin/bash\n    \n    sample_id=${sample_id_gr}\n    \n    dname=${outdir}/\\${sample_id}\n         \n    python $projectDir/bin/stClusteringWorkflow.py --filePath=\\${dname}/ --resolution=$params.Clustering_resolution\n\n    if [[ -s \\${dname}/st_adata_processed.h5ad ]] && \\\n      [[ -s \\${dname}/sc_adata_processed.h5ad ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"\n}",
        "nb_lignes_process": 33,
        "string_script": "    def sample_id_gr = sample_state[0]\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id_gr)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n        \n    \"\"\"\n    #!/bin/bash\n    \n    sample_id=${sample_id_gr}\n    \n    dname=${outdir}/\\${sample_id}\n         \n    python $projectDir/bin/stClusteringWorkflow.py --filePath=\\${dname}/ --resolution=$params.Clustering_resolution\n\n    if [[ -s \\${dname}/st_adata_processed.h5ad ]] && \\\n      [[ -s \\${dname}/sc_adata_processed.h5ad ]]\n    then\n      echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    else\n      echo ERROR: Output files missing. >&2\n      exit 2\n    fi\n    \"\"\"",
        "nb_lignes_script": 21,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_state",
            "outdir"
        ],
        "nb_inputs": 2,
        "outputs": [
            "outpath"
        ],
        "nb_outputs": 1,
        "name_workflow": "nf-core__spatialtranscriptomics",
        "directive": [
            "label \"python_process\""
        ],
        "when": "",
        "stub": ""
    },
    "ALL_REPORT": {
        "name_process": "ALL_REPORT",
        "string_process": " process ALL_REPORT {\n     \n    label \"python_process_low\"\n    \n    input:\n    val sample_state\n    val outdir\n    \n    output:\n    tuple env(sample_id), env(outpath)\n    \n    script:\n    def sample_id_gr = sample_state[0]\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id_gr)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n        \n    \"\"\"\n    #!/bin/bash\n    \n    sample_id=${sample_id_gr}\n    \n    dname=${outdir}/\\${sample_id}\n    \n    echo \\${dname}/  \n    echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    \"\"\"\n}",
        "nb_lignes_process": 25,
        "string_script": "    def sample_id_gr = sample_state[0]\n    def fileName = String.format(\"%s/sample_%s.json\", outdir, sample_id_gr)\n    sample_info = new JsonSlurper().parse(new File(fileName))\n        \n    \"\"\"\n    #!/bin/bash\n    \n    sample_id=${sample_id_gr}\n    \n    dname=${outdir}/\\${sample_id}\n    \n    echo \\${dname}/  \n    echo \"completed\" > \"output.out\" && outpath=`pwd`/output.out\n    \"\"\"",
        "nb_lignes_script": 13,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "sample_state",
            "outdir"
        ],
        "nb_inputs": 2,
        "outputs": [
            "outpath"
        ],
        "nb_outputs": 1,
        "name_workflow": "nf-core__spatialtranscriptomics",
        "directive": [
            "label \"python_process_low\""
        ],
        "when": "",
        "stub": ""
    },
    "SAMPLESHEET_CHECK": {
        "name_process": "SAMPLESHEET_CHECK",
        "string_process": "\nprocess SAMPLESHEET_CHECK {\n    tag \"$samplesheet\"\n\n    conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/python:3.8.3' :\n        'quay.io/biocontainers/python:3.8.3' }\"\n\n    input:\n    path samplesheet\n\n    output:\n    path '*.csv'       , emit: csv\n    path \"versions.yml\", emit: versions\n\n    script:                                                                                    \n    \"\"\"\n    check_samplesheet.py \\\\\n        $samplesheet \\\\\n        samplesheet.valid.csv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        python: \\$(python --version | sed 's/Python //g')\n    END_VERSIONS\n    \"\"\"\n}",
        "nb_lignes_process": 26,
        "string_script": "    \"\"\"\n    check_samplesheet.py \\\\\n        $samplesheet \\\\\n        samplesheet.valid.csv\n\n    cat <<-END_VERSIONS > versions.yml\n    \"${task.process}\":\n        python: \\$(python --version | sed 's/Python //g')\n    END_VERSIONS\n    \"\"\"",
        "nb_lignes_script": 9,
        "language_script": "bash",
        "tools": [],
        "tools_url": [],
        "tools_dico": [],
        "inputs": [
            "samplesheet"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__spatialtranscriptomics",
        "directive": [
            "tag \"$samplesheet\"",
            "conda (params.enable_conda ? \"conda-forge::python=3.8.3\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/python:3.8.3' : 'quay.io/biocontainers/python:3.8.3' }\""
        ],
        "when": "",
        "stub": ""
    },
    "CUSTOM_DUMPSOFTWAREVERSIONS": {
        "name_process": "CUSTOM_DUMPSOFTWAREVERSIONS",
        "string_process": "process CUSTOM_DUMPSOFTWAREVERSIONS {\n    label 'process_low'\n\n                                                                                                  \n    conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' :\n        'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\"\n\n    input:\n    path versions\n\n    output:\n    path \"software_versions.yml\"    , emit: yml\n    path \"software_versions_mqc.yml\", emit: mqc_yml\n    path \"versions.yml\"             , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n    template 'dumpsoftwareversions.py'\n}",
        "nb_lignes_process": 19,
        "string_script": "    def args = task.ext.args ?: ''\n    template 'dumpsoftwareversions.py'",
        "nb_lignes_script": 1,
        "language_script": "bash",
        "tools": [
            "docxtemplate"
        ],
        "tools_url": [
            "https://bio.tools/docxtemplate"
        ],
        "tools_dico": [
            {
                "name": "docxtemplate",
                "uri": "https://bio.tools/docxtemplate",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3314",
                            "term": "Chemistry"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0176",
                            "term": "Molecular dynamics"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3489",
                            "term": "Database management"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3489",
                            "term": "Database administration"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_0249",
                                    "term": "Protein geometry calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0322",
                                    "term": "Molecular model refinement"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Deposition"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database deposition"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Database submission"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3431",
                                    "term": "Data deposition"
                                }
                            ]
                        ],
                        "input": [],
                        "output": []
                    }
                ],
                "description": "> VERY_LOW CONFIDENCE! | > CORRECT NAME OF TOOL COULD ALSO BE 'Phenix', 'restraints', 'Amber', 'refinement' | Improved chemistry restraints for crystallographic refinement by integrating the Amber force field into Phenix | Word templates and tools for Windows | The IUCr Word templates utilize the content management features and document styles of Word to format your manuscript and to store essential details for submission of your manuscript",
                "homepage": "http://journals.iucr.org/services/docxtemplate/"
            }
        ],
        "inputs": [
            "versions"
        ],
        "nb_inputs": 1,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__spatialtranscriptomics",
        "directive": [
            "label 'process_low'",
            "conda (params.enable_conda ? \"bioconda::multiqc=1.11\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/multiqc:1.11--pyhdfd78af_0' : 'quay.io/biocontainers/multiqc:1.11--pyhdfd78af_0' }\""
        ],
        "when": "",
        "stub": ""
    },
    "FASTQC": {
        "name_process": "FASTQC",
        "string_process": "process FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :\n        'quay.io/biocontainers/fastqc:0.11.9--0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    script:\n    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }\n}",
        "nb_lignes_process": 42,
        "string_script": "    def args = task.ext.args ?: ''\n                                                                          \n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    if (meta.single_end) {\n        \"\"\"\n        [ ! -f  ${prefix}.fastq.gz ] && ln -s $reads ${prefix}.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    } else {\n        \"\"\"\n        [ ! -f  ${prefix}_1.fastq.gz ] && ln -s ${reads[0]} ${prefix}_1.fastq.gz\n        [ ! -f  ${prefix}_2.fastq.gz ] && ln -s ${reads[1]} ${prefix}_2.fastq.gz\n        fastqc $args --threads $task.cpus ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz\n\n        cat <<-END_VERSIONS > versions.yml\n        \"${task.process}\":\n            fastqc: \\$( fastqc --version | sed -e \"s/FastQC v//g\" )\n        END_VERSIONS\n        \"\"\"\n    }",
        "nb_lignes_script": 24,
        "language_script": "bash",
        "tools": [
            "FastQC"
        ],
        "tools_url": [
            "https://bio.tools/fastqc"
        ],
        "tools_dico": [
            {
                "name": "FastQC",
                "uri": "https://bio.tools/fastqc",
                "topic": [
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "Sequencing"
                        },
                        {
                            "uri": "http://edamontology.org/topic_3572",
                            "term": "Data quality management"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequence analysis"
                        }
                    ],
                    [
                        {
                            "uri": "http://edamontology.org/topic_3168",
                            "term": "DNA-Seq"
                        },
                        {
                            "uri": "http://edamontology.org/topic_0080",
                            "term": "Sequences"
                        }
                    ]
                ],
                "function": [
                    {
                        "operation": [
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical calculation"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality control"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_0236",
                                    "term": "Sequence composition calculation"
                                }
                            ],
                            [
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Significance testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical testing"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical test"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_2238",
                                    "term": "Statistical analysis"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing QC"
                                },
                                {
                                    "uri": "http://edamontology.org/operation_3218",
                                    "term": "Sequencing quality assessment"
                                }
                            ]
                        ],
                        "input": [
                            {
                                "uri": "http://edamontology.org/data_0848",
                                "term": "Raw sequence"
                            }
                        ],
                        "output": [
                            {
                                "uri": "http://edamontology.org/data_2955",
                                "term": "Sequence report"
                            }
                        ]
                    }
                ],
                "description": "This tool aims to provide a QC report which can spot problems or biases which originate either in the sequencer or in the starting library material. It can be run in one of two modes. It can either run as a stand alone interactive application for the immediate analysis of small numbers of FastQ files, or it can be run in a non-interactive mode where it would be suitable for integrating into a larger analysis pipeline for the systematic processing of large numbers of files.",
                "homepage": "http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"
            }
        ],
        "inputs": [
            "meta",
            "reads"
        ],
        "nb_inputs": 2,
        "outputs": [],
        "nb_outputs": 0,
        "name_workflow": "nf-core__spatialtranscriptomics",
        "directive": [
            "tag \"$meta.id\"",
            "label 'process_medium'",
            "conda (params.enable_conda ? \"bioconda::fastqc=0.11.9\" : null)",
            "container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ? 'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' : 'quay.io/biocontainers/fastqc:0.11.9--0' }\""
        ],
        "when": "",
        "stub": ""
    }
}